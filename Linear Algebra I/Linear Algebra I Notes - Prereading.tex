\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\newcommand{\mbf}[1]{\mathbf{#1}}

\title{Linear Algebra I \\
    \large Prereading}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{The vector space \(\R ^ n\)}
\begin{definition}[$n$-dimensional real space]
    The space $\R ^ n$ ($n$-dimensional real space) is the collection of all (column) vectors of the form
    \[
    \mathbf{x} =
    \begin{pmatrix}
        x_1 \\
        \vdots \\
        x_n
    \end{pmatrix}.
    \]
\end{definition}
Vectors are typically denoted by bold letters such as $\mathbf{x}$ and sometimes using underlined letters $\underline{x}$ to differentiate them from ordinary real numbers.

We write $\mathbf{0}$ for the zero-vector
\[
\mathbf{0} = \begin{pmatrix}
    0 \\
    \vdots \\
    0
\end{pmatrix}
\]
this is the point of origin of $\R ^ n$.

There are two important operations which can be performed on vectors.

\textbf{Addition}: for any vectors $\mathbf{v}$ and $\mathbf{w}$, we define
\[
\mathbf{v + w} =
\begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
+
\begin{pmatrix}
    w_1 \\
    \vdots \\
    w_n
\end{pmatrix}
=
\begin{pmatrix}
    v_1 + w_1 \\
    \vdots \\
    v_n + w_n
\end{pmatrix}
\]
this is component-wise addition.


\textbf{Scalar multiplication}: for any vector $\mathbf{v} \in \R ^ n$ and real number $\lambda \in \R$ we define
\[
\lambda\mathbf{v} =
\lambda\begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
=
\begin{pmatrix}
    \lambda v_1 \\
    \vdots \\
    \lambda v_n
\end{pmatrix}
\]
this is component-wise multiplication by $\lambda$.

\begin{proposition}
    These operations satisfy the axioms of a real vector space
\end{proposition}

{\large \textbf{Axioms for Addition}}
\begin{enumerate}[label = (\roman*)]
    \item existence of additive identity: there is a vector $\mathbf{0}$ in $\R ^ n$ so that $\mathbf{v} + \mathbf{0} = \mathbf{v} = \mathbf{0} + \mathbf{v}$ for all $\mathbf{v}$ in $\R ^ n$.
    \item commutativity: $\mbf{v + w = w + v}$ for all $\mathbf{v}$ and $\mathbf{w}$ in $\R ^ n$
    \item existence of additive inverses: for each $\mbf{v}$ in $\R ^ n$ there is an element $-\mbf{v}$ in $\R ^ n$ such that $\mbf{v + (-v) = 0 = (-v) + v}$
    \item associativity: $\mbf{u + (v + w) = (u + v) + w}$ for all $\mbf{u, v, w}$ in $\R ^ n$
\end{enumerate}
these axioms can be summarised by saying that $\R ^ n$ is an abelian group under addition.


{\large\textbf{Axioms for Scalar Multiplication}}
\begin{enumerate}[label = (\roman*)]
    \item multiplication by 0: $0\mbf{v = 0}$ for all $\mbf{v}$ in $\R ^ n$
    \item multiplication by 1: $1\mbf{v = v}$ for all $\mbf{v}$ in $\R ^ n$
    \item associativity: $(\lambda\mu)\mbf{v} = \lambda (\mu\mbf{v})\ \forall \lambda, \mu \in \R$ and $\forall \mbf{v} \in \R ^ n$
    \item distributivity: $(\lambda + \mu)\mbf{v} = \lambda\mbf{v} + \mu\mbf{v}$ and $\lambda (\mbf{v + w}) = \lambda\mbf{v} + \lambda\mbf{w}$ $\forall \lambda, \mu \in \R$ and $\forall \mbf{v, w} \in \R ^ n$
\end{enumerate}

\begin{proof}
    The above axioms ate trivially satisfied by $\R$ (the case $n = 1$), many of the proofs follow from this observation. Once we set
    \[
    \mbf{0} = \begin{pmatrix}
        0 \\
        \vdots \\
        0
    \end{pmatrix},
    \qquad
    -\mbf{v} = -\begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    =
    \begin{pmatrix}
        -v_1 \\
        \vdots \\
        -v_n
    \end{pmatrix}
    =
    (-1)\begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \]
    then the proofs are very straight forward
\end{proof}

\begin{definition}[Standard basis vectors]
    We set
    \[
    \mbf{e_1} = \begin{pmatrix}
        1 \\
        0 \\
        \vdots \\
        0
    \end{pmatrix},
    \ 
    \mbf{e_2} = \begin{pmatrix}
        0 \\
        1 \\
        \vdots \\
        0
    \end{pmatrix},
    \dotsc,\ 
    \mbf{e_n} = \begin{pmatrix}
        0 \\
        0 \\
        \vdots \\
        1
    \end{pmatrix}
    \]
    and call these vectors the standard basis vectors of $\R ^ n$.
\end{definition}
Sometimes the entries of a vector are referred to as the Cartesian coordinates of a point in space.

Addition can be thought of as an operation that inputs a pair of vectors, each from $\R ^ n$, and outputs a single vector in $\R ^ n$. In other words, addition is a function $\R ^ n \times \R ^ n \mapsto \R ^ n$. Similarly, scalar multiplication is a function $\R \times \R ^ n \mapsto \R ^ n$. The domains of each of these functions are called Cartesian products.

\subsection{The scalar product in $\R ^ n$}

\begin{definition}[Scalar product]
    The scalar (or dot) product is a function
    \[
    \R ^ n \times \R ^ n \mapsto \R
    \]
    defined as follows. For $\mbf{v, w}$ in $\R ^ n$ set
    \[
    \mbf{v \cdot w} =
    \begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        w_1 \\
        \vdots \\
        w_n
    \end{pmatrix}
    =
    v_1 w_1 + \cdots + v_n w_n.
    \]
    This is a real number, a scalar.
\end{definition}

The transpose $\mbf{v} ^ t$ of $\mbf{v}$ is defined to be the row vector with the same entries as the column vector $\mbf{v}$. So if
\[
\mbf{v} = \begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
\qquad\text{then}
\qquad\mbf{v} ^ t = (v_1,\ \dots,\ v_n).
\]

The dot product of $\mbf{v}$ and $\mbf{w}$ is then obtained by matrix multiplying $\mbf{v} ^ t$ and $\mbf{w}$.

\begin{proposition}[Properties of the scalar product]
\phantom{}
\begin{enumerate}[label = (\roman*)]
    \item Symmetry
    \[
    \mbf{u \cdot v = v \cdot u}\qquad\text{for all }\mbf{u, v}\text{ in } \R ^ n
    \]
    \item Linearity in first factor
    \begin{enumerate}[label = (\alph*)]
        \item $(\mbf{u} + \mbf{v}) \cdot \mbf{w} = \mbf{u} \cdot \mbf{w} + \mbf{v} \cdot \mbf{w}$\qquad for all $\mbf{u,\ v,\ w}$ in $\R ^ n$
        \item $(\lambda\mbf{u}) \cdot \mbf{v} = \lambda(\mbf{u} \cdot \mbf{v})$\qquad for all $\lambda \in \R$ and all $\mbf{u,\ v}$ in $\R ^ n$
    \end{enumerate}
    \item Linearity in the second factor
    \begin{enumerate}[label = (\alph*)]
        \item $\mbf{u} \cdot (\mbf{v} + \mbf{w}) = \mbf{u} \cdot \mbf{w} + \mbf{u} \cdot \mbf{v}$\qquad for all $\mbf{u,\ v,\ w}$ in $\R ^ n$
        \item $\mbf{u} \cdot (\lambda\mbf{v}) = \lambda(\mbf{u} \cdot \mbf{v})$\qquad for all $\lambda \in \R$ and all $\mbf{u,\ v}$ in $\R ^ n$
    \end{enumerate}
    \item Positivity
    
    $\mbf{v \cdot v} \geq 0$ for all $v$, and $\mbf{v \cdot v} = 0$ if and only if $\mbf{v = 0}$.
\end{enumerate}

\begin{proof}
    \phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item
        First we can determine some expressions for $\mbf{u\cdot v}$ and $\mbf{v\cdot u}$
        
        $\mbf{u\cdot v} = u_1 v_1 + u_2 v_2 + \dotsc + u_n v_n$ and

        $\mbf{v\cdot u} = v_1 u_1 + v_2 u_2 + \dotsc + v_n u_n$.

        Then by the commutativity of multiplication we have
        \begin{align*}
            \mbf{u\cdot v} &= u_1 v_1 + u_2 v_2 + \dotsc + u_n v_n \\
            &= v_1 u_1 + v_2 u_2 + \dotsc + v_n u_n \\
            &= \mbf{v \cdot u}
        \end{align*}
        which proves our goal as required
        
    \end{enumerate}
\end{proof}
\end{proposition}

\begin{definition}[Magnitude]
    Suppose
    \[
    \mbf{v} = \begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \]
    then $\mbf{v \cdot v} = v_1 ^ 2 + \dotsc + v_n ^ 2$ which is a non-negative real number. We define the magnitude (or length) of $\mbf{v}$, denoted $|\mbf{v}|$, by
    \[
    |\mbf{v}| = \sqrt{\mbf{v \cdot v}}.
    \]
    So long as the $n$ coordinate directions are mutually orthogonal, the magnitude
    \[
    |\mbf{v}| = \sqrt{\mbf{v \cdot v}} = \sqrt{x_1 ^ 2 + \cdots + v_n ^ 2}
    \]
    is the euclidean distance from the origin to the point with coordinates given by $\mbf{v}$. We say that a vector is non-trivial if it has positive length. It is easy to see that the only trivial vector in $\R ^ n$ is the zero vector $\mbf{0}$.
\end{definition}


We can describe the position of a point by what direction it lies and what distance away from us it is, using polar coordinates. \\


Using the length of a vector in $\R ^ 2$ we define polar co-ordinates on $\R ^ 2$ as follows. Let $\mbf{v} = \left(\frac{x}{y}\right)$ be a non-trivial vector with length $|\mbf{v}| = r > 0$. Let $\theta \in [0, 2\pi)$ be the angle at origin $\mbf{0}$ measured in an anticlockwise direction from the $x$-axis to $\mbf{v}$. By basic trigonometry
\[
\cos(\theta) = \frac{x}{r},\qquad\sin(\theta) = \frac{y}{r}.
\]
We define the polar coordinates of $\mbf{v \neq 0}$ to be the unique $(r, \theta)$ in $\R_+ \times [0, 2\pi)$ so that
\[
\mbf{v} = \begin{pmatrix}
    r\cos(\theta) \\
    r\sin(\theta)
\end{pmatrix}
=
r\begin{pmatrix}
    \cos(\theta)
    \sin(\theta)
\end{pmatrix}
\]

\begin{lemma}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ 2$ of length $r > 0 $ and $s > 0$ respectively. Let $\theta$ be the angle between them. Then
    \[
    \mbf{v \cdot w} = rs\cos(\theta).
    \]
    In particular, we have
    \[
    |\mbf{v \cdot w}| \leq |\mbf{v}||\mbf{w}|
    \]
    with equality if and only if $\mbf{v}$ and $\mbf{w}$ are collinear, i.e. are scalar multiples of each other.

    \begin{proof}
        Suppose that $\mbf{v}$ has polar coordinates $(r, \gamma)$ and $\mbf{w}$ has polar coordinates $(s, \rho)$. The angle between $\mbf{v}$ and $\mbf{w}$ is $\theta = |\gamma - \rho|$. Then
        \[
        \mbf{v} = \begin{pmatrix}
            r\cos\gamma \\
            r\sin\gamma
        \end{pmatrix},
        \qquad
        \mbf{w} = \begin{pmatrix}
            s\cos\rho \\
            s\sin\rho
        \end{pmatrix}.
        \]
        Applying the dot product using these definitions of $\mbf{v}$ and $\mbf{w}$ we obtain the following, applying trigonometric addition formula and the fact that cosine is even we get,
        \[
        \mbf{v \cdot w} =  rs\cos\gamma\cos\rho + rs\sin\gamma\sin\rho = rs(\cos(\gamma - \rho)) = rs\cos(|\gamma - \rho|) = rs\cos\theta.
        \]
    \end{proof}
\end{lemma}
Note that using this lemma we can obtain an expression for the angle between two vectors
\[
\cos(\theta) = \frac{\mbf{v \cdot w}}{|\mbf{v}||\mbf{w}|}
\]

\begin{theorem}[Cauchy-Schwarz inequality]
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ n$. Then
    \[
    |\mbf{v \cdot w}| \leq |\mbf{v}||\mbf{w}|
    \]
    with equality if and only if $\mbf{v}$ and $\mbf{w}$ are collinear

    \begin{proof}
        
    \end{proof}
\end{theorem}

\begin{definition}[Angle between two vectors]\label{def_linalg_vec_angbetvecs}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ n$ of length $r > 0$ and $s > 0$ respectively. Then we define the angle $\theta \in [0, \pi]$ between them by
    \[
    \cos(\theta) = \frac{\mbf{v \cdot w}}{rs}.
    \]
\end{definition}

We say that the vectors $\mbf{v}$ and $\mbf{w}$ are orthogonal (or perpendicular) if the angle between them is $\frac{\pi}{2}$. \\

\begin{corollary}
    The non-trivial vectors $\mbf{v}$ and $\mbf{w}$ in $\R ^ n$ are orthogonal if and only if we have $\mbf{v \cdot w} = 0$.

    \begin{proof}
        By \autoref{def_linalg_vec_angbetvecs} we have that\footnote{Since $\cos\theta$ can only be $0$ when $\theta = \frac{\pi}{2}$ then they have to be orthogonal and the converse is true too.}
        \[
        \mbf{v \cdot w} = 0 \iff \cos\theta = 0 \iff \theta = \frac{\pi}{2}.
        \]
    \end{proof}
\end{corollary}

\subsection{The vector product in $\R ^ 3$}
\begin{definition}[Vector product in $\R ^ 3$]
    The vector (or cross) product is a function
    \[
    \R ^ 3 \times \R ^ 3 \rightarrow \R ^ 3
    \]
    defined as follows. If $\mbf{v, w} \in \R ^ 3$, their cross product is given by
    \[
    \mbf{v \times w} =
    \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix}
    \times
    \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix}
    =
    \begin{pmatrix} v_2 w_3 - v_3 w_2 \\ v_3 w_1 - v_1 w_3 \\ v_1 w_2 - w_1 v_2 \end{pmatrix}
    \]
\end{definition}

\begin{example}
    \[
    \begin{pmatrix}
        1 \\ 2 \\ 3
    \end{pmatrix}
    \times
    \begin{pmatrix}
        3 \\ 2 \\ 1
    \end{pmatrix}
    = \begin{pmatrix}
        2 - 6 \\
        9 - 1 \\
        2 - 6
    \end{pmatrix}
    =
    \begin{pmatrix}
        -4 \\ 8 \\ -4
    \end{pmatrix}
    \]
\end{example}

\begin{proposition}[Properties of the cross product]
    \begin{enumerate}[label = (\roman*)]
        \item Anti-Symmetry:
        \[
        \mbf{u \times v = -v \times u}\quad\text{for all }\mbf{u, v} \in \R ^ 3
        \]
        \item Linearity in first factor
        \begin{enumerate}[label = (\arabic*)]
            \item $(\mbf{u + v}) \times \mbf{w} = \mbf{u \times w} + \mbf{v \times w}\quad\text{for all }\mbf{u, v, w} \in \R ^ 3$
            \item $(\lambda\mbf{u}) \times \mbf{v} = \lambda(\mbf{u \times v})\quad\text{for all } \lambda \in \R \text{ and all }\mbf{u, v} \in \R ^ 3$
        \end{enumerate}
        \item Linearity in the second factor
        \begin{enumerate}[label = (\arabic*)]
            \item $\mbf{u} \times (\mbf{v + w}) = \mbf{u \times w} + \mbf{u \times v}\quad\text{for all }\mbf{u, v, w} \in \R ^ 3$
            \item $\mbf{u} \times (\lambda\mbf{v}) = \lambda(\mbf{u \times v})\quad\text{for all } \lambda \in \R \text{ and all }\mbf{u, v} \in \R ^ 3$
        \end{enumerate}
        \item Orthogonality to the input
        \[
        \mbf{v} \cdot (\mbf{v \times w}) = \mbf{w} \cdot (\mbf{v \times w}) = 0\quad\text{for all vectors } \mbf{v} \text{ and } \mbf{w} \text{ in } \R ^ 3
        \]
    \end{enumerate}
\end{proposition}

\begin{lemma}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors of length $r > 0$ and $s > 0$ respectively. Let $\theta \in [0, \pi]$ be the angle between them. Then
    \[
    |\mbf{v \times w}| = rs\sin(\theta).
    \]
    In particular, we have
    \[
    \mbf{v \times w = 0}\quad\iff\quad\mbf{v}\text{ and }\mbf{w}\text{ are collinear}.
    \]

    \begin{proof}
        We have
        \begin{align*}
            \left|\begin{pmatrix}
                v_2 w_3 - v_3 w_2 \\
                v_3 w_1 - v_1 w_3 \\
                v_1 w_2 - w_1 v_2
            \end{pmatrix}\right| ^ 2
            &= (v_2w_3 - v_3w_2) ^ 2 + (v_3w_1 - v_1w_3) ^ 2 + (v_1w_2 - w_1v_2) ^ 2 \\
            &= (v_1 ^ 2 + v_2 ^ 2 + v_3 ^ 2)(w_1 ^ 2 + w_2 ^ 2 + w_3 ^ 2) - (v_1w_1 + v_2w_2 + v_3w_3) ^ 2 \\
            &= |\mbf{v}| ^ 2 |\mbf{w}| ^ 2 - (\mbf{v \cdot w}) ^ 2 \\
            &= |\mbf{v}| ^ 2 |\mbf{w}| ^ 2 - (|\mbf{v}||\mbf{w}|\cos(\theta)) ^ 2 \\
            &= r^2s^2(1 - \cos(\theta)) ^ 2 \\
            &= (rs\sin(\theta)) ^ 2.
        \end{align*}
        Since $\sin\theta \geq 0$ for $\theta \in [0, \pi]$, we are done.
    \end{proof}
\end{lemma}

\subsection{Planes and Lines in $\R ^ 3$}
\subsubsection{Planes}

\textbf{Parametric Form}

A plane $\Pi$ in $\R ^ 3$ can be described in parametric form as the collection of all points corresponding to vectors of the form
\[
\mbf{a} + \lambda\mbf{d_1} + \mu\mbf{d_2}
\]
where $\lambda$ and $\mu$ each run through $\R$. $\mbf{a}$ is any point on the plane and $\mbf{d_1}$ and $\mbf{d_2}$ are any two direction vectors for $\Pi$, which lie on the plane and are not collinear.

For example, if $\mbf{a, b}$ and $\mbf{c}$ are all points on $\Pi$ which are not collinear then we could take $\mbf{d_1 = b - a}$ and $\mbf{d_2 = c - a}$.

We write
\[
\Pi = \{\mbf{a} + \lambda\mbf{d_1} + \mu\mbf{d_2} : \lambda, \mu \in \R\}.
\]

\textbf{Using the normal vector}

We can describe a plane via its normal vector.
\[
\mbf{d}_1 \cdot (\mbf{d}_1 \times \mbf{d}_2) = 0 = \mbf{d}_2 \cdot (\mbf{d}_1 \times \mbf{d}_2)
\]
that is, $\mbf{d}_1 \times \mbf{d}_2$ is orthogonal to both $\mbf{d}_1$ and $\mbf{d}_2$.

Let us write $\mbf{n}$ for $\mbf{d}_1 \times \mbf{d}_2$. Additionally we can write the plane in terms of the dot product
\[
\mbf{n \cdot(x - a)} = 0.
\]
We can rearrange this to have the normal form of the plane $\Pi$: the plane $\Pi$ is the set of vectors satisfying
\[
\mbf{n \cdot x} = \ell
\]
where $\ell = \mbf{n \cdot a} \in \R$ since $\mbf{n \cdot a}$ doesn't depend on $\mbf{x}$ it is just a real number.


\textbf{Cartesian form}

A variation of the normal form, really just the same description but now written in Cartesian coordinates immediately follows. Set $\mbf{n} = \begin{pmatrix} a \\ b \\ c \end{pmatrix}$ and $\mbf{x} = \begin{pmatrix} x \\ y \\ z \end{pmatrix}$. Then the normal form above becomes the Cartesian (or the Cartesian form) of the plane:
\[
ax + by + cz = \ell
\]
Thus we have
\[
\Pi = \{\mbf{x} \in \R ^ 3 : \mbf{n \cdot x} = \ell\} = \left\{\begin{pmatrix} a \\ b \\ c \end{pmatrix}\,:\,ax + by + cz = \ell\,\right\}
\]

\subsubsection{Lines}
\textbf{Parametric form}

A line $L$ in $\R ^ 3$ can be described in parametric form as the collection of all points corresponding to vectors of the form
\[
\mbf{a} + t\mbf{d}
\]
where $t$ runs through $\R$.
\[
L = \{\mbf{a} + t\mbf{d} : t \in \R\}.
\]

If $\mbf{b \neq a}$ is another point on $L$, then we could take $\mbf{d = b - a}$.

\textbf{Cartesian form}

Suppose the line is given in parametric form by
\[
\begin{pmatrix}
    x \\ y \\ z
\end{pmatrix}
=
\begin{pmatrix}
    a_1 \\ a_2 \\ a_3
\end{pmatrix}
+
t\begin{pmatrix}
    d_1 \\ d_2 \\ d_3
\end{pmatrix}.
\]

If all $d_i \neq 0$ then we may solve for $t$ in each component and obtain
\[
\frac{x - a_1}{d_1} = \frac{y - a_2}{d_2} = \frac{z - a_3}{d_3}.
\]
If one of the $d_i = 0$, such as $d_3 = 0$ then we would get two equations
\[
\frac{x - a_1}{d_1} = \frac{y - a_2}{d_2}\qquad\text{and}\qquad z = a_3
\]

\textbf{Solution set for a pair of linear equations}
A pair of planes in $\R ^ 3$ is either parallel, in which case they have the same normal vector, or else they intersect in a line.
\begin{align*}
    ax + by + cz &= \ell \\
    dx + ey + fz &= m
\end{align*}
the either the two normal vectors
\[
\mbf{n}_1 = \begin{pmatrix} a \\ b \\ c \end{pmatrix}\quad\text{and}\quad\mbf{n}_2 = \begin{pmatrix} d \\ e \\ f \end{pmatrix}
\]
are multiples of each other or else the solution is a line.

\begin{definition}[Scalar triple product]
    The scalar triple product is a function
    \[
    \R ^ 3 \times \R ^ 3 \times \R ^ 3 \rightarrow \R.
    \]
    Given three vectors $\mbf{a, b, c}$ in $\R ^ 3$ we define their scalar triple product $[\mbf{a, b, c}]$ by
    \[
    [\mbf{a, b, c}] = \mbf{a} \cdot(\mbf{b} \times \mbf{c}).
    \]
\end{definition}

\begin{lemma}[Invariance under cyclic permutation]
    For any three vectors $\mbf{a, b, c}$ in $\R ^ 3$ we have
    \[
    [\mbf{a, b, c}] = [\mbf{b, c, a}] = [\mbf{c, a, b}] = -[\mbf{a, b, c}] = -[\mbf{b, c, a}] = -[\mbf{c, a, b}].
    \]
\end{lemma}

\section{Matrix algebra}

\subsection{What is a matrix?}

\begin{definition}[Matrix]
    A matrix is a rectangular array of numbers (or symbols, or functions) arranged into rows and columns.
\end{definition}

\textbf{Notation and Terminology:}
\begin{itemize}
    \item A matrix which has $m$ rows and $n$ columns is referred to as an $m \times n$ matrix. The individual entries in the matrix are sometimes referred to as its elements. The entry in the $i$th row and $j$th column of a matrix is referred to as the $(i,\,j)$th element of a matrix $M$, usually denoted $M_{ij}$.
    \item Matrices which consist of a single column are (column) vectors. Matrices which consist of a single row are row vectors.
\end{itemize}

\begin{definition}[Transpose]
    Given an $m \times n$ matrix $W$ we define $W ^ t$, the transpose of $W$, to be the $n \times m$ matrix whose $(i,\,j)$th entry is the $(j,\,i)$th entry of $W$; that is the matrix fow which
    \[
    (W ^ t)_{ij} = W_{ji}.
    \]
\end{definition}

A matrix which is equal to its transpose is said to be symmetric.

One way of describing or otherwise specifying the elements of a matrix is as follows. If $f(i,\,j)$ is some expression depending on $i$ and $j$ then $(f(i,\,j))_{m \times n}$ stands for the $m \times n$ matrix whose $(i,\,j)$th entry is $f(i,\,j)$. We often write an $m \times n$ matrix as $X = (x_{ij})_{m \times n}$, or $X = (x_{ij})$ if the size is obvious.

We write $O_{m \times n}$ for $(0)_{m \times n}$ that is the $m \times n$ matrix all of whose entries are zero, this is called the $m \times n$ zero matrix.

For $r$ and $s$ integers, we define $\delta_{r\,s}$ to be $1$ if $r = s$ and to be $0$ if $r \neq s$. (This is called the Kronecker delta.) The matrix $(\delta_{ij})_{m \times n}$ is denoted $I_n$ and is referred to as the $n \times n$ identity matrix.

In general, we write $M_{m \times n}(\R)$ or $M_{m, n}(\R)$ for the set of all real $m \times n$ matrices. We also write $M_n(\R)$ for the set of square matrices of size $n$, so $M_n(\R) = M_{n, n}(\R)$. $M_{m, 1}(\R)$ is the same set as $\R ^ m$.

\subsection{Matrix operations I: matrix addition and scalar multiplication}

\begin{enumerate}[label = (\roman*)]
    \item Matrix addition. This is a function
    \[
    M_{m \times n}(\R) \times M_{m \times n}(\R) \rightarrow M_{m \times n}(\R).
    \]
    For $X = (x_{ij})_{m \times n}$ and $Y = (y_{ij})_{m \times n}$ both $m \times n$ matrices, we define
    \[
    X + Y = (x_{ij})_{m \times n} + (y_{ij})_{m \times n} = (x_{ij} + y_{ij})_{m \times n},
    \]
    that is, matrix addition is defined by component wise addition. Matrix addition isn't defined when the matrices are of different sizes.
    \item Scalar multiplication. This is a function
    \[
    \R \times M_{m \times n}(\R) \rightarrow M_{m \times n}(\R).
    \]
    For $X = (x_{ij})_{m \times n}$ in $M_{m \times n}(\R)$ and $\lambda \in \R$ we set
    \[
    \lambda X = (\lambda x_{ij})_{m \times n}
    \]
    that is, we multiply every entry of the matrix by $\lambda$.
\end{enumerate}

We write $-X$ for $(-1)X$. If $X ^ t = -X$ we say that $X$ is skew-symmetric (or anti-symmetric).

\begin{proposition}[Properties of matrix addition and scalar multiplication]
    Matrix addition and scalar multiplication for $M_{m, n}(\R)$ satisfy exactly the same properties as vector addition and scalar multiplication for $\R ^ n$. That is, the operations satisfy the axioms of a (real) vector space.

    \textbf{Axioms for addition}
    \begin{enumerate}[label = (\roman*)]
        \item existence of additive identity: there is a matrix $O_{m \times n}$ in $M_{m, n}(\R)$ so that for all $X$ in $M_{m, n}(\R)$ we have $X + O_{m \times n} = X = O_{m \times n} + X$ 
        \item commutativity: $X + Y = Y + x$ for all $X$ and $Y$ in $M_{m, n}(\R)$
        \item existence of additive inverses: for each $X$ in $M_{m, n}(\R)$ there is an element $-X$ in $M_{m, n}(\R)$ such that $X + (-X) = O_{m \times n} = (-X) + X$
        \item associativity: $(X + Y) + Z = X + (Y + Z)$ for all $X, Y, Z \in M_{m, n}(\R)$
    \end{enumerate}
    (As before this makes $M_{m, n}(\R)$ an abelian group under addition)

    \textbf{Axioms for Scalar Multiplication}
    \begin{enumerate}[label = (\roman*)]
        \item $0X = O_{m \times n}$ for all $X$ in $M_{m, n}(\R)$
        \item $1X = X$ for all $X$ in $M_{m, n}(\R)$
        \item associativity: $(\lambda\mu)X = \lambda(\mu X)$ for all $\lambda, \mu \in \R$ and for all $X \in M_{m, n}(\R)$
        \item distributivity: $(\lambda + \mu)X = \lambda X + \mu X$ and $\lambda (X + Y) = \lambda X + \lambda Y$ for all real $\lambda, \mu$ and for all $X$ and $Y$ in $M_{m, n}(\R)$.
    \end{enumerate}
    \begin{proof}
        Do
    \end{proof}
\end{proposition}

\subsection{Matrix Operations II: Matrix multiplication}
Matrix multiplication: This is a function
\[
M_{m \times n}(\R) \times M_{n \times p}(\R) \rightarrow M_{m \times p}(\R).
\]
If $X = (x_{ij})_{m \times n}$ is $m \times n$ and $Y = (y_{ij})_{n \times p}$ is $n \times p$ (if the number of columns of $X$ equals the number of rows of $Y$) then we may form the product matrix $XY$. This is defined to be the $m \times p$ matrix whose $(i,\,j)$th element $(XY)_{ij}$ is
\[
(XY)_{ij} = x_{i\,1}y_{1\,j} + x_{i\,2}y_{2\,j} + \dotsi + x_{i\,n}x_{n\,j} = \sum_{k = 1}^{n}{x_{ik}y_{kj}}.
\]
If $\mbf{x ^ t}_{i}$ denotes the $i$-th row of $X$ and $\mbf{y}_j$ denotes the $j$-th column of $Y$ then $(XY)_{ij} = \mbf{x}_i \cdot \mbf{y}_j$ (scalar product of column vectors).

\begin{proposition}[Multiplication rules]
    Let $X$ and $X'$ be $m \times n$, $Y$ and $Y'$ be $n \times p$ and $Z$ be $p \times q$ matrices. Let $\lambda$ and $\lambda'$ be scalars. Then
    \begin{enumerate}[label = (\roman*)]
        \item $O_{p \times m}X = O_{p \times n}$ and $XO_{n \times q} = O_{m \times q}$.
        \item $I_{m}X = X = XI_n$.
        \item $\lambda(XY) = (\lambda X)Y = X(\lambda Y)$.
        \item associativity: $(XY)Z = X(YZ)$
        \item distributive rules:
        \begin{enumerate}[label = (\alph*)]
            \item $(X + X')Y = XY + X'Y$ and $X(Y + Y') = XY + XY'$.
            \item $\lambda(X + X') = \lambda X + \lambda X'$ and $(\lambda + \lambda')X = \lambda X + \lambda' X$.
        \end{enumerate}
    \end{enumerate}
    \begin{proof}
        For (iv), the $k$th entry in the $i$th row of $XY$ is given by $\sum_{r = 1}^{n}x_{ir}y_{rk}$, with $k = 1,\dotsc, p$. Hence the $(i,\,j)$th entry of $(XY)Z$ is given by
        \[
        ((XY)Z)_{ij} = (i\text{th row of }XY) \cdot (j\text{th column of } Z) = \sum_{k = 1}^{p}\left(\sum_{r = 1}^{n}x_{ir}y_{rk}\right)z_{kj}.
        \]
        On the other hand, the $r$th entry in the $j$th column of $YZ$ is given by $\sum_{k = 1}^{p}y_{rk}z_{kj}$, with $r = 1, \dotsc, n$. Hence the $(i,\,j)$th entry of $X(YZ)$ is given by
        \[
        (X(YZ))_{ij} = (i\text{th row of } X) \cdot (j\text{th column of } YZ) = \sum_{r = 1}^{n}x_{ir}\left(\sum_{k = 1}^{p}y_{rk}z_{kj}\right).
        \]
        Switching the order of summation using associativity of the usual multiplication given $(XY)Z = X(YZ)$.
        
        \textbf{Prove rest!}
    \end{proof}
\end{proposition}

\textbf{Warning}: We do not necessarily have $XY = YX$ in general.

\textbf{Warning}: It is possible to have $XY = O_{m \times p}$ even though both $X$ and $Y$ are non-zero.

\textbf{A useful trick for sometimes multiplying big matrices more easily}

Let $\begin{pmatrix}
    A & B \\ C & D
\end{pmatrix} \in M_{m, n}(\R)$ and $\begin{pmatrix}
    A' & B' \\ C' & D'
\end{pmatrix} \in M_{n, p}(\R)$ be two matrices given by blocks $A \in M_{m_1, n_1}(\R),\, B \in M_{m_1, n_2}(\R),\, C \in M_{m_2, n_1}(\R),\, D \in M_{m_2, n_2}(\R),\, A' \in M_{n_1, p_1}(\R),\, B' \in M_{n_1, p_2}(\R),\, C' \in M_{n_2, p_1}(\R),\, D' \in M_{n_2, p_2}(\R)$ (so-called block matrices) with $m_1 + m_2 = m$ and $n_1 + n_2 = n$ and $p_1 + p_2 = p$. It can easily shown that
\[
\begin{pmatrix}
    A & B \\ C & D
\end{pmatrix}
\begin{pmatrix}
    A' & B' \\ C' & D'
\end{pmatrix}
=
\begin{pmatrix}
    AA' + BC' & AB' + BD' \\ CA' + DC' & CB' DD'
\end{pmatrix} \in M_{m, p}(\R)
\]
that is, if matrix sizes match correctly the formula for the multiplication of block matrices is the same as one of "usual two by two matrices"

For example, if $A = \begin{pmatrix}
    0 & 1 \\ 0 & 0
\end{pmatrix}$(so that $A ^ 2 = O_{2 \times 2}$) then
\[
\begin{pmatrix}
    1 & 0 & 1 & 0 \\
    0 & 1 & 0 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 0 & 0 & 0
\end{pmatrix} ^ 2
=
\begin{pmatrix}
    I_2 & I_2 \\ A & A
\end{pmatrix} ^ 2
=
\begin{pmatrix}
    I & I \\ A & A
\end{pmatrix}
\begin{pmatrix}
    I & I \\ A & A
\end{pmatrix}
=
\begin{pmatrix}
    I + A & I + A \\ A + A ^ 2 & A + A ^ 2
\end{pmatrix}
=
\begin{pmatrix}
    1 & 1 & 1 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 0 & 0 & 0
\end{pmatrix}
\]

\subsection{Inverse matrix}

\begin{definition}
    Let $A$ be a square $(n\times n)$ matrix. We say $A$ is invertible (or non-singular) if there exists an $n \times n$ matrix $B$ such that
    \[
    AB = BA = I_n.
    \]
    If such a matrix $B$ exists we call $B$ the inverse of $A$ and we write $A ^ {-1}$ for this inverse. (there can only exist at most one inverse).
    If no such inverse $B$ exists for a matrix $A$, we say $A$ is non-invertible (or singular).
\end{definition}

\begin{proposition}[Properties of the inverse and transpose]
    \begin{enumerate}[label = (\roman*)]
        \item A matrix has at most one inverse.
        \item Assume $A$ and $B$ are invertible $n \times n$ matrices. Then the product $AB$ is invertible and
        \[
        (AB) ^ {-1} = B ^ {-1}A ^ {-1}.
        \]
        \item We have $(AB) ^ t = B ^ t A ^ t$ and $(\lambda A) ^ t = \lambda A ^ t$ for any matrix $A$ and any real constant $\lambda$.
        \item Assume $A$ is invertible. then the transpose $A ^ t$ is invertible and
        \[
        (A ^ t) ^ {-1} = (A ^ {-1}) ^ t.
        \]
        \item Let $A \in M_{n, n}(\R)$ and assume $BA = I_n$ for some $B \in M_{n, n}(\R)$. Then $AB = I_n$ as well as vice versa. So the "left inverse" is automatically the "right inverse".
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
            \item If $B'$ were a second inverse for $A$ then we would have
            \[
            B' = B'I_n = B'(AB) = (B'A)B = I_nB = B.
            \]
        \end{enumerate}
    \end{proof}
\end{proposition}

\begin{remark}
    We also have
    \[
    (X + Y) ^ t = X ^ t + Y ^ t,
    \]
    but it is not the case in general that
    \[
    (X + Y) ^ {-1} = X ^ {-1} + Y ^ {-1}.
    \]
\end{remark}

\end{document}