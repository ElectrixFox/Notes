\documentclass[10pt, a4paper]{article}
\usepackage{preamble}
\usepackage{systeme}

\setcounter{MaxMatrixCols}{20}
\newcommand{\mbf}[1]{\mathbf{#1}}

\title{Linear Algebra I \\
    \large Prereading}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{The vector space \(\R ^ n\)}
\begin{definition}[$n$-dimensional real space]
    The space $\R ^ n$ ($n$-dimensional real space) is the collection of all (column) vectors of the form
    \[
    \mathbf{x} =
    \begin{pmatrix}
        x_1 \\
        \vdots \\
        x_n
    \end{pmatrix}.
    \]
\end{definition}
Vectors are typically denoted by bold letters such as $\mathbf{x}$ and sometimes using underlined letters $\underline{x}$ to differentiate them from ordinary real numbers.

We write $\mathbf{0}$ for the zero-vector
\[
\mathbf{0} = \begin{pmatrix}
    0 \\
    \vdots \\
    0
\end{pmatrix}
\]
this is the point of origin of $\R ^ n$.

There are two important operations which can be performed on vectors.

\textbf{Addition}: for any vectors $\mathbf{v}$ and $\mathbf{w}$, we define
\[
\mathbf{v + w} =
\begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
+
\begin{pmatrix}
    w_1 \\
    \vdots \\
    w_n
\end{pmatrix}
=
\begin{pmatrix}
    v_1 + w_1 \\
    \vdots \\
    v_n + w_n
\end{pmatrix}
\]
this is component-wise addition.


\textbf{Scalar multiplication}: for any vector $\mathbf{v} \in \R ^ n$ and real number $\lambda \in \R$ we define
\[
\lambda\mathbf{v} =
\lambda\begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
=
\begin{pmatrix}
    \lambda v_1 \\
    \vdots \\
    \lambda v_n
\end{pmatrix}
\]
this is component-wise multiplication by $\lambda$.

\begin{proposition}
    These operations satisfy the axioms of a real vector space
\end{proposition}

{\large \textbf{Axioms for Addition}}
\begin{enumerate}[label = (\roman*)]
    \item existence of additive identity: there is a vector $\mathbf{0}$ in $\R ^ n$ so that $\mathbf{v} + \mathbf{0} = \mathbf{v} = \mathbf{0} + \mathbf{v}$ for all $\mathbf{v}$ in $\R ^ n$.
    \item commutativity: $\mbf{v + w = w + v}$ for all $\mathbf{v}$ and $\mathbf{w}$ in $\R ^ n$
    \item existence of additive inverses: for each $\mbf{v}$ in $\R ^ n$ there is an element $-\mbf{v}$ in $\R ^ n$ such that $\mbf{v + (-v) = 0 = (-v) + v}$
    \item associativity: $\mbf{u + (v + w) = (u + v) + w}$ for all $\mbf{u, v, w}$ in $\R ^ n$
\end{enumerate}
these axioms can be summarised by saying that $\R ^ n$ is an abelian group under addition.


{\large\textbf{Axioms for Scalar Multiplication}}
\begin{enumerate}[label = (\roman*)]
    \item multiplication by 0: $0\mbf{v = 0}$ for all $\mbf{v}$ in $\R ^ n$
    \item multiplication by 1: $1\mbf{v = v}$ for all $\mbf{v}$ in $\R ^ n$
    \item associativity: $(\lambda\mu)\mbf{v} = \lambda (\mu\mbf{v})\ \forall \lambda, \mu \in \R$ and $\forall \mbf{v} \in \R ^ n$
    \item distributivity: $(\lambda + \mu)\mbf{v} = \lambda\mbf{v} + \mu\mbf{v}$ and $\lambda (\mbf{v + w}) = \lambda\mbf{v} + \lambda\mbf{w}$ $\forall \lambda, \mu \in \R$ and $\forall \mbf{v, w} \in \R ^ n$
\end{enumerate}

\begin{proof}
    The above axioms ate trivially satisfied by $\R$ (the case $n = 1$), many of the proofs follow from this observation. Once we set
    \[
    \mbf{0} = \begin{pmatrix}
        0 \\
        \vdots \\
        0
    \end{pmatrix},
    \qquad
    -\mbf{v} = -\begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    =
    \begin{pmatrix}
        -v_1 \\
        \vdots \\
        -v_n
    \end{pmatrix}
    =
    (-1)\begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \]
    then the proofs are very straight forward
\end{proof}

\begin{definition}[Standard basis vectors]
    We set
    \[
    \mbf{e_1} = \begin{pmatrix}
        1 \\
        0 \\
        \vdots \\
        0
    \end{pmatrix},
    \ 
    \mbf{e_2} = \begin{pmatrix}
        0 \\
        1 \\
        \vdots \\
        0
    \end{pmatrix},
    \dotsc,\ 
    \mbf{e_n} = \begin{pmatrix}
        0 \\
        0 \\
        \vdots \\
        1
    \end{pmatrix}
    \]
    and call these vectors the standard basis vectors of $\R ^ n$.
\end{definition}
Sometimes the entries of a vector are referred to as the Cartesian coordinates of a point in space.

Addition can be thought of as an operation that inputs a pair of vectors, each from $\R ^ n$, and outputs a single vector in $\R ^ n$. In other words, addition is a function $\R ^ n \times \R ^ n \mapsto \R ^ n$. Similarly, scalar multiplication is a function $\R \times \R ^ n \mapsto \R ^ n$. The domains of each of these functions are called Cartesian products.

\subsection{The scalar product in $\R ^ n$}

\begin{definition}[Scalar product]
    The scalar (or dot) product is a function
    \[
    \R ^ n \times \R ^ n \mapsto \R
    \]
    defined as follows. For $\mbf{v, w}$ in $\R ^ n$ set
    \[
    \mbf{v \cdot w} =
    \begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        w_1 \\
        \vdots \\
        w_n
    \end{pmatrix}
    =
    v_1 w_1 + \cdots + v_n w_n.
    \]
    This is a real number, a scalar.
\end{definition}

The transpose $\mbf{v} ^ t$ of $\mbf{v}$ is defined to be the row vector with the same entries as the column vector $\mbf{v}$. So if
\[
\mbf{v} = \begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
\qquad\text{then}
\qquad\mbf{v} ^ t = (v_1,\ \dots,\ v_n).
\]

The dot product of $\mbf{v}$ and $\mbf{w}$ is then obtained by matrix multiplying $\mbf{v} ^ t$ and $\mbf{w}$.

\begin{proposition}[Properties of the scalar product]
\phantom{}
\begin{enumerate}[label = (\roman*)]
    \item Symmetry
    \[
    \mbf{u \cdot v = v \cdot u}\qquad\text{for all }\mbf{u, v}\text{ in } \R ^ n
    \]
    \item Linearity in first factor
    \begin{enumerate}[label = (\alph*)]
        \item $(\mbf{u} + \mbf{v}) \cdot \mbf{w} = \mbf{u} \cdot \mbf{w} + \mbf{v} \cdot \mbf{w}$\qquad for all $\mbf{u,\ v,\ w}$ in $\R ^ n$
        \item $(\lambda\mbf{u}) \cdot \mbf{v} = \lambda(\mbf{u} \cdot \mbf{v})$\qquad for all $\lambda \in \R$ and all $\mbf{u,\ v}$ in $\R ^ n$
    \end{enumerate}
    \item Linearity in the second factor
    \begin{enumerate}[label = (\alph*)]
        \item $\mbf{u} \cdot (\mbf{v} + \mbf{w}) = \mbf{u} \cdot \mbf{w} + \mbf{u} \cdot \mbf{v}$\qquad for all $\mbf{u,\ v,\ w}$ in $\R ^ n$
        \item $\mbf{u} \cdot (\lambda\mbf{v}) = \lambda(\mbf{u} \cdot \mbf{v})$\qquad for all $\lambda \in \R$ and all $\mbf{u,\ v}$ in $\R ^ n$
    \end{enumerate}
    \item Positivity
    
    $\mbf{v \cdot v} \geq 0$ for all $v$, and $\mbf{v \cdot v} = 0$ if and only if $\mbf{v = 0}$.
\end{enumerate}

\begin{proof}
    \phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item
        First we can determine some expressions for $\mbf{u\cdot v}$ and $\mbf{v\cdot u}$
        
        $\mbf{u\cdot v} = u_1 v_1 + u_2 v_2 + \dotsc + u_n v_n$ and

        $\mbf{v\cdot u} = v_1 u_1 + v_2 u_2 + \dotsc + v_n u_n$.

        Then by the commutativity of multiplication we have
        \begin{align*}
            \mbf{u\cdot v} &= u_1 v_1 + u_2 v_2 + \dotsc + u_n v_n \\
            &= v_1 u_1 + v_2 u_2 + \dotsc + v_n u_n \\
            &= \mbf{v \cdot u}
        \end{align*}
        which proves our goal as required
        
    \end{enumerate}
\end{proof}
\end{proposition}

\begin{definition}[Magnitude]
    Suppose
    \[
    \mbf{v} = \begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \]
    then $\mbf{v \cdot v} = v_1 ^ 2 + \dotsc + v_n ^ 2$ which is a non-negative real number. We define the magnitude (or length) of $\mbf{v}$, denoted $|\mbf{v}|$, by
    \[
    |\mbf{v}| = \sqrt{\mbf{v \cdot v}}.
    \]
    So long as the $n$ coordinate directions are mutually orthogonal, the magnitude
    \[
    |\mbf{v}| = \sqrt{\mbf{v \cdot v}} = \sqrt{x_1 ^ 2 + \cdots + v_n ^ 2}
    \]
    is the euclidean distance from the origin to the point with coordinates given by $\mbf{v}$. We say that a vector is non-trivial if it has positive length. It is easy to see that the only trivial vector in $\R ^ n$ is the zero vector $\mbf{0}$.
\end{definition}


We can describe the position of a point by what direction it lies and what distance away from us it is, using polar coordinates. \\


Using the length of a vector in $\R ^ 2$ we define polar co-ordinates on $\R ^ 2$ as follows. Let $\mbf{v} = \left(\frac{x}{y}\right)$ be a non-trivial vector with length $|\mbf{v}| = r > 0$. Let $\theta \in [0, 2\pi)$ be the angle at origin $\mbf{0}$ measured in an anticlockwise direction from the $x$-axis to $\mbf{v}$. By basic trigonometry
\[
\cos(\theta) = \frac{x}{r},\qquad\sin(\theta) = \frac{y}{r}.
\]
We define the polar coordinates of $\mbf{v \neq 0}$ to be the unique $(r, \theta)$ in $\R_+ \times [0, 2\pi)$ so that
\[
\mbf{v} = \begin{pmatrix}
    r\cos(\theta) \\
    r\sin(\theta)
\end{pmatrix}
=
r\begin{pmatrix}
    \cos(\theta)
    \sin(\theta)
\end{pmatrix}
\]

\begin{lemma}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ 2$ of length $r > 0 $ and $s > 0$ respectively. Let $\theta$ be the angle between them. Then
    \[
    \mbf{v \cdot w} = rs\cos(\theta).
    \]
    In particular, we have
    \[
    |\mbf{v \cdot w}| \leq |\mbf{v}||\mbf{w}|
    \]
    with equality if and only if $\mbf{v}$ and $\mbf{w}$ are collinear, i.e. are scalar multiples of each other.

    \begin{proof}
        Suppose that $\mbf{v}$ has polar coordinates $(r, \gamma)$ and $\mbf{w}$ has polar coordinates $(s, \rho)$. The angle between $\mbf{v}$ and $\mbf{w}$ is $\theta = |\gamma - \rho|$. Then
        \[
        \mbf{v} = \begin{pmatrix}
            r\cos\gamma \\
            r\sin\gamma
        \end{pmatrix},
        \qquad
        \mbf{w} = \begin{pmatrix}
            s\cos\rho \\
            s\sin\rho
        \end{pmatrix}.
        \]
        Applying the dot product using these definitions of $\mbf{v}$ and $\mbf{w}$ we obtain the following, applying trigonometric addition formula and the fact that cosine is even we get,
        \[
        \mbf{v \cdot w} =  rs\cos\gamma\cos\rho + rs\sin\gamma\sin\rho = rs(\cos(\gamma - \rho)) = rs\cos(|\gamma - \rho|) = rs\cos\theta.
        \]
    \end{proof}
\end{lemma}
Note that using this lemma we can obtain an expression for the angle between two vectors
\[
\cos(\theta) = \frac{\mbf{v \cdot w}}{|\mbf{v}||\mbf{w}|}
\]

\begin{theorem}[Cauchy-Schwarz inequality]
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ n$. Then
    \[
    |\mbf{v \cdot w}| \leq |\mbf{v}||\mbf{w}|
    \]
    with equality if and only if $\mbf{v}$ and $\mbf{w}$ are collinear

    \begin{proof}
        
    \end{proof}
\end{theorem}

\begin{definition}[Angle between two vectors]\label{def_linalg_vec_angbetvecs}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ n$ of length $r > 0$ and $s > 0$ respectively. Then we define the angle $\theta \in [0, \pi]$ between them by
    \[
    \cos(\theta) = \frac{\mbf{v \cdot w}}{rs}.
    \]
\end{definition}

We say that the vectors $\mbf{v}$ and $\mbf{w}$ are orthogonal (or perpendicular) if the angle between them is $\frac{\pi}{2}$. \\

\begin{corollary}
    The non-trivial vectors $\mbf{v}$ and $\mbf{w}$ in $\R ^ n$ are orthogonal if and only if we have $\mbf{v \cdot w} = 0$.

    \begin{proof}
        By \autoref{def_linalg_vec_angbetvecs} we have that\footnote{Since $\cos\theta$ can only be $0$ when $\theta = \frac{\pi}{2}$ then they have to be orthogonal and the converse is true too.}
        \[
        \mbf{v \cdot w} = 0 \iff \cos\theta = 0 \iff \theta = \frac{\pi}{2}.
        \]
    \end{proof}
\end{corollary}

\subsection{The vector product in $\R ^ 3$}
\begin{definition}[Vector product in $\R ^ 3$]
    The vector (or cross) product is a function
    \[
    \R ^ 3 \times \R ^ 3 \rightarrow \R ^ 3
    \]
    defined as follows. If $\mbf{v, w} \in \R ^ 3$, their cross product is given by
    \[
    \mbf{v \times w} =
    \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix}
    \times
    \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix}
    =
    \begin{pmatrix} v_2 w_3 - v_3 w_2 \\ v_3 w_1 - v_1 w_3 \\ v_1 w_2 - w_1 v_2 \end{pmatrix}
    \]
\end{definition}

\begin{example}
    \[
    \begin{pmatrix}
        1 \\ 2 \\ 3
    \end{pmatrix}
    \times
    \begin{pmatrix}
        3 \\ 2 \\ 1
    \end{pmatrix}
    = \begin{pmatrix}
        2 - 6 \\
        9 - 1 \\
        2 - 6
    \end{pmatrix}
    =
    \begin{pmatrix}
        -4 \\ 8 \\ -4
    \end{pmatrix}
    \]
\end{example}

\begin{proposition}[Properties of the cross product]
    \begin{enumerate}[label = (\roman*)]
        \item Anti-Symmetry:
        \[
        \mbf{u \times v = -v \times u}\quad\text{for all }\mbf{u, v} \in \R ^ 3
        \]
        \item Linearity in first factor
        \begin{enumerate}[label = (\arabic*)]
            \item $(\mbf{u + v}) \times \mbf{w} = \mbf{u \times w} + \mbf{v \times w}\quad\text{for all }\mbf{u, v, w} \in \R ^ 3$
            \item $(\lambda\mbf{u}) \times \mbf{v} = \lambda(\mbf{u \times v})\quad\text{for all } \lambda \in \R \text{ and all }\mbf{u, v} \in \R ^ 3$
        \end{enumerate}
        \item Linearity in the second factor
        \begin{enumerate}[label = (\arabic*)]
            \item $\mbf{u} \times (\mbf{v + w}) = \mbf{u \times w} + \mbf{u \times v}\quad\text{for all }\mbf{u, v, w} \in \R ^ 3$
            \item $\mbf{u} \times (\lambda\mbf{v}) = \lambda(\mbf{u \times v})\quad\text{for all } \lambda \in \R \text{ and all }\mbf{u, v} \in \R ^ 3$
        \end{enumerate}
        \item Orthogonality to the input
        \[
        \mbf{v} \cdot (\mbf{v \times w}) = \mbf{w} \cdot (\mbf{v \times w}) = 0\quad\text{for all vectors } \mbf{v} \text{ and } \mbf{w} \text{ in } \R ^ 3
        \]
    \end{enumerate}
\end{proposition}

\begin{lemma}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors of length $r > 0$ and $s > 0$ respectively. Let $\theta \in [0, \pi]$ be the angle between them. Then
    \[
    |\mbf{v \times w}| = rs\sin(\theta).
    \]
    In particular, we have
    \[
    \mbf{v \times w = 0}\quad\iff\quad\mbf{v}\text{ and }\mbf{w}\text{ are collinear}.
    \]

    \begin{proof}
        We have
        \begin{align*}
            \left|\begin{pmatrix}
                v_2 w_3 - v_3 w_2 \\
                v_3 w_1 - v_1 w_3 \\
                v_1 w_2 - w_1 v_2
            \end{pmatrix}\right| ^ 2
            &= (v_2w_3 - v_3w_2) ^ 2 + (v_3w_1 - v_1w_3) ^ 2 + (v_1w_2 - w_1v_2) ^ 2 \\
            &= (v_1 ^ 2 + v_2 ^ 2 + v_3 ^ 2)(w_1 ^ 2 + w_2 ^ 2 + w_3 ^ 2) - (v_1w_1 + v_2w_2 + v_3w_3) ^ 2 \\
            &= |\mbf{v}| ^ 2 |\mbf{w}| ^ 2 - (\mbf{v \cdot w}) ^ 2 \\
            &= |\mbf{v}| ^ 2 |\mbf{w}| ^ 2 - (|\mbf{v}||\mbf{w}|\cos(\theta)) ^ 2 \\
            &= r^2s^2(1 - \cos(\theta)) ^ 2 \\
            &= (rs\sin(\theta)) ^ 2.
        \end{align*}
        Since $\sin\theta \geq 0$ for $\theta \in [0, \pi]$, we are done.
    \end{proof}
\end{lemma}

\subsection{Planes and Lines in $\R ^ 3$}
\subsubsection{Planes}

\textbf{Parametric Form}

A plane $\Pi$ in $\R ^ 3$ can be described in parametric form as the collection of all points corresponding to vectors of the form
\[
\mbf{a} + \lambda\mbf{d_1} + \mu\mbf{d_2}
\]
where $\lambda$ and $\mu$ each run through $\R$. $\mbf{a}$ is any point on the plane and $\mbf{d_1}$ and $\mbf{d_2}$ are any two direction vectors for $\Pi$, which lie on the plane and are not collinear.

For example, if $\mbf{a, b}$ and $\mbf{c}$ are all points on $\Pi$ which are not collinear then we could take $\mbf{d_1 = b - a}$ and $\mbf{d_2 = c - a}$.

We write
\[
\Pi = \{\mbf{a} + \lambda\mbf{d_1} + \mu\mbf{d_2} : \lambda, \mu \in \R\}.
\]

\textbf{Using the normal vector}

We can describe a plane via its normal vector.
\[
\mbf{d}_1 \cdot (\mbf{d}_1 \times \mbf{d}_2) = 0 = \mbf{d}_2 \cdot (\mbf{d}_1 \times \mbf{d}_2)
\]
that is, $\mbf{d}_1 \times \mbf{d}_2$ is orthogonal to both $\mbf{d}_1$ and $\mbf{d}_2$.

Let us write $\mbf{n}$ for $\mbf{d}_1 \times \mbf{d}_2$. Additionally we can write the plane in terms of the dot product
\[
\mbf{n \cdot(x - a)} = 0.
\]
We can rearrange this to have the normal form of the plane $\Pi$: the plane $\Pi$ is the set of vectors satisfying
\[
\mbf{n \cdot x} = \ell
\]
where $\ell = \mbf{n \cdot a} \in \R$ since $\mbf{n \cdot a}$ doesn't depend on $\mbf{x}$ it is just a real number.


\textbf{Cartesian form}

A variation of the normal form, really just the same description but now written in Cartesian coordinates immediately follows. Set $\mbf{n} = \begin{pmatrix} a \\ b \\ c \end{pmatrix}$ and $\mbf{x} = \begin{pmatrix} x \\ y \\ z \end{pmatrix}$. Then the normal form above becomes the Cartesian (or the Cartesian form) of the plane:
\[
ax + by + cz = \ell
\]
Thus we have
\[
\Pi = \{\mbf{x} \in \R ^ 3 : \mbf{n \cdot x} = \ell\} = \left\{\begin{pmatrix} a \\ b \\ c \end{pmatrix}\,:\,ax + by + cz = \ell\,\right\}
\]

\subsubsection{Lines}
\textbf{Parametric form}

A line $L$ in $\R ^ 3$ can be described in parametric form as the collection of all points corresponding to vectors of the form
\[
\mbf{a} + t\mbf{d}
\]
where $t$ runs through $\R$.
\[
L = \{\mbf{a} + t\mbf{d} : t \in \R\}.
\]

If $\mbf{b \neq a}$ is another point on $L$, then we could take $\mbf{d = b - a}$.

\textbf{Cartesian form}

Suppose the line is given in parametric form by
\[
\begin{pmatrix}
    x \\ y \\ z
\end{pmatrix}
=
\begin{pmatrix}
    a_1 \\ a_2 \\ a_3
\end{pmatrix}
+
t\begin{pmatrix}
    d_1 \\ d_2 \\ d_3
\end{pmatrix}.
\]

If all $d_i \neq 0$ then we may solve for $t$ in each component and obtain
\[
\frac{x - a_1}{d_1} = \frac{y - a_2}{d_2} = \frac{z - a_3}{d_3}.
\]
If one of the $d_i = 0$, such as $d_3 = 0$ then we would get two equations
\[
\frac{x - a_1}{d_1} = \frac{y - a_2}{d_2}\qquad\text{and}\qquad z = a_3
\]

\textbf{Solution set for a pair of linear equations}
A pair of planes in $\R ^ 3$ is either parallel, in which case they have the same normal vector, or else they intersect in a line.
\begin{align*}
    ax + by + cz &= \ell \\
    dx + ey + fz &= m
\end{align*}
the either the two normal vectors
\[
\mbf{n}_1 = \begin{pmatrix} a \\ b \\ c \end{pmatrix}\quad\text{and}\quad\mbf{n}_2 = \begin{pmatrix} d \\ e \\ f \end{pmatrix}
\]
are multiples of each other or else the solution is a line.

\begin{definition}[Scalar triple product]
    The scalar triple product is a function
    \[
    \R ^ 3 \times \R ^ 3 \times \R ^ 3 \rightarrow \R.
    \]
    Given three vectors $\mbf{a, b, c}$ in $\R ^ 3$ we define their scalar triple product $[\mbf{a, b, c}]$ by
    \[
    [\mbf{a, b, c}] = \mbf{a} \cdot(\mbf{b} \times \mbf{c}).
    \]
\end{definition}

\begin{lemma}[Invariance under cyclic permutation]
    For any three vectors $\mbf{a, b, c}$ in $\R ^ 3$ we have
    \[
    [\mbf{a, b, c}] = [\mbf{b, c, a}] = [\mbf{c, a, b}] = -[\mbf{a, b, c}] = -[\mbf{b, c, a}] = -[\mbf{c, a, b}].
    \]
\end{lemma}

\section{Matrix algebra}

\subsection{What is a matrix?}

\begin{definition}[Matrix]
    A matrix is a rectangular array of numbers (or symbols, or functions) arranged into rows and columns.
\end{definition}

\textbf{Notation and Terminology:}
\begin{itemize}
    \item A matrix which has $m$ rows and $n$ columns is referred to as an $m \times n$ matrix. The individual entries in the matrix are sometimes referred to as its elements. The entry in the $i$th row and $j$th column of a matrix is referred to as the $(i,\,j)$th element of a matrix $M$, usually denoted $M_{ij}$.
    \item Matrices which consist of a single column are (column) vectors. Matrices which consist of a single row are row vectors.
\end{itemize}

\begin{definition}[Transpose]
    Given an $m \times n$ matrix $W$ we define $W ^ t$, the transpose of $W$, to be the $n \times m$ matrix whose $(i,\,j)$th entry is the $(j,\,i)$th entry of $W$; that is the matrix fow which
    \[
    (W ^ t)_{ij} = W_{ji}.
    \]
\end{definition}

A matrix which is equal to its transpose is said to be symmetric.

One way of describing or otherwise specifying the elements of a matrix is as follows. If $f(i,\,j)$ is some expression depending on $i$ and $j$ then $(f(i,\,j))_{m \times n}$ stands for the $m \times n$ matrix whose $(i,\,j)$th entry is $f(i,\,j)$. We often write an $m \times n$ matrix as $X = (x_{ij})_{m \times n}$, or $X = (x_{ij})$ if the size is obvious.

We write $O_{m \times n}$ for $(0)_{m \times n}$ that is the $m \times n$ matrix all of whose entries are zero, this is called the $m \times n$ zero matrix.

For $r$ and $s$ integers, we define $\delta_{r\,s}$ to be $1$ if $r = s$ and to be $0$ if $r \neq s$. (This is called the Kronecker delta.) The matrix $(\delta_{ij})_{m \times n}$ is denoted $I_n$ and is referred to as the $n \times n$ identity matrix.

In general, we write $M_{m \times n}(\R)$ or $M_{m, n}(\R)$ for the set of all real $m \times n$ matrices. We also write $M_n(\R)$ for the set of square matrices of size $n$, so $M_n(\R) = M_{n, n}(\R)$. $M_{m, 1}(\R)$ is the same set as $\R ^ m$.

\subsection{Matrix operations I: matrix addition and scalar multiplication}

\begin{enumerate}[label = (\roman*)]
    \item Matrix addition. This is a function
    \[
    M_{m \times n}(\R) \times M_{m \times n}(\R) \rightarrow M_{m \times n}(\R).
    \]
    For $X = (x_{ij})_{m \times n}$ and $Y = (y_{ij})_{m \times n}$ both $m \times n$ matrices, we define
    \[
    X + Y = (x_{ij})_{m \times n} + (y_{ij})_{m \times n} = (x_{ij} + y_{ij})_{m \times n},
    \]
    that is, matrix addition is defined by component wise addition. Matrix addition isn't defined when the matrices are of different sizes.
    \item Scalar multiplication. This is a function
    \[
    \R \times M_{m \times n}(\R) \rightarrow M_{m \times n}(\R).
    \]
    For $X = (x_{ij})_{m \times n}$ in $M_{m \times n}(\R)$ and $\lambda \in \R$ we set
    \[
    \lambda X = (\lambda x_{ij})_{m \times n}
    \]
    that is, we multiply every entry of the matrix by $\lambda$.
\end{enumerate}

We write $-X$ for $(-1)X$. If $X ^ t = -X$ we say that $X$ is skew-symmetric (or anti-symmetric).

\begin{proposition}[Properties of matrix addition and scalar multiplication]
    Matrix addition and scalar multiplication for $M_{m, n}(\R)$ satisfy exactly the same properties as vector addition and scalar multiplication for $\R ^ n$. That is, the operations satisfy the axioms of a (real) vector space.

    \textbf{Axioms for addition}
    \begin{enumerate}[label = (\roman*)]
        \item existence of additive identity: there is a matrix $O_{m \times n}$ in $M_{m, n}(\R)$ so that for all $X$ in $M_{m, n}(\R)$ we have $X + O_{m \times n} = X = O_{m \times n} + X$ 
        \item commutativity: $X + Y = Y + x$ for all $X$ and $Y$ in $M_{m, n}(\R)$
        \item existence of additive inverses: for each $X$ in $M_{m, n}(\R)$ there is an element $-X$ in $M_{m, n}(\R)$ such that $X + (-X) = O_{m \times n} = (-X) + X$
        \item associativity: $(X + Y) + Z = X + (Y + Z)$ for all $X, Y, Z \in M_{m, n}(\R)$
    \end{enumerate}
    (As before this makes $M_{m, n}(\R)$ an abelian group under addition)

    \textbf{Axioms for Scalar Multiplication}
    \begin{enumerate}[label = (\roman*)]
        \item $0X = O_{m \times n}$ for all $X$ in $M_{m, n}(\R)$
        \item $1X = X$ for all $X$ in $M_{m, n}(\R)$
        \item associativity: $(\lambda\mu)X = \lambda(\mu X)$ for all $\lambda, \mu \in \R$ and for all $X \in M_{m, n}(\R)$
        \item distributivity: $(\lambda + \mu)X = \lambda X + \mu X$ and $\lambda (X + Y) = \lambda X + \lambda Y$ for all real $\lambda, \mu$ and for all $X$ and $Y$ in $M_{m, n}(\R)$.
    \end{enumerate}
    \begin{proof}
        Do
    \end{proof}
\end{proposition}

\subsection{Matrix Operations II: Matrix multiplication}
Matrix multiplication: This is a function
\[
M_{m \times n}(\R) \times M_{n \times p}(\R) \rightarrow M_{m \times p}(\R).
\]
If $X = (x_{ij})_{m \times n}$ is $m \times n$ and $Y = (y_{ij})_{n \times p}$ is $n \times p$ (if the number of columns of $X$ equals the number of rows of $Y$) then we may form the product matrix $XY$. This is defined to be the $m \times p$ matrix whose $(i,\,j)$th element $(XY)_{ij}$ is
\[
(XY)_{ij} = x_{i\,1}y_{1\,j} + x_{i\,2}y_{2\,j} + \dotsi + x_{i\,n}x_{n\,j} = \sum_{k = 1}^{n}{x_{ik}y_{kj}}.
\]
If $\mbf{x ^ t}_{i}$ denotes the $i$-th row of $X$ and $\mbf{y}_j$ denotes the $j$-th column of $Y$ then $(XY)_{ij} = \mbf{x}_i \cdot \mbf{y}_j$ (scalar product of column vectors).

\begin{proposition}[Multiplication rules]
    Let $X$ and $X'$ be $m \times n$, $Y$ and $Y'$ be $n \times p$ and $Z$ be $p \times q$ matrices. Let $\lambda$ and $\lambda'$ be scalars. Then
    \begin{enumerate}[label = (\roman*)]
        \item $O_{p \times m}X = O_{p \times n}$ and $XO_{n \times q} = O_{m \times q}$.
        \item $I_{m}X = X = XI_n$.
        \item $\lambda(XY) = (\lambda X)Y = X(\lambda Y)$.
        \item associativity: $(XY)Z = X(YZ)$
        \item distributive rules:
        \begin{enumerate}[label = (\alph*)]
            \item $(X + X')Y = XY + X'Y$ and $X(Y + Y') = XY + XY'$.
            \item $\lambda(X + X') = \lambda X + \lambda X'$ and $(\lambda + \lambda')X = \lambda X + \lambda' X$.
        \end{enumerate}
    \end{enumerate}
    \begin{proof}
        For (iv), the $k$th entry in the $i$th row of $XY$ is given by $\sum_{r = 1}^{n}x_{ir}y_{rk}$, with $k = 1,\dotsc, p$. Hence the $(i,\,j)$th entry of $(XY)Z$ is given by
        \[
        ((XY)Z)_{ij} = (i\text{th row of }XY) \cdot (j\text{th column of } Z) = \sum_{k = 1}^{p}\left(\sum_{r = 1}^{n}x_{ir}y_{rk}\right)z_{kj}.
        \]
        On the other hand, the $r$th entry in the $j$th column of $YZ$ is given by $\sum_{k = 1}^{p}y_{rk}z_{kj}$, with $r = 1, \dotsc, n$. Hence the $(i,\,j)$th entry of $X(YZ)$ is given by
        \[
        (X(YZ))_{ij} = (i\text{th row of } X) \cdot (j\text{th column of } YZ) = \sum_{r = 1}^{n}x_{ir}\left(\sum_{k = 1}^{p}y_{rk}z_{kj}\right).
        \]
        Switching the order of summation using associativity of the usual multiplication given $(XY)Z = X(YZ)$.
        
        \textbf{Prove rest!}
    \end{proof}
\end{proposition}

\textbf{Warning}: We do not necessarily have $XY = YX$ in general.

\textbf{Warning}: It is possible to have $XY = O_{m \times p}$ even though both $X$ and $Y$ are non-zero.

\textbf{A useful trick for sometimes multiplying big matrices more easily}

Let $\begin{pmatrix}
    A & B \\ C & D
\end{pmatrix} \in M_{m, n}(\R)$ and $\begin{pmatrix}
    A' & B' \\ C' & D'
\end{pmatrix} \in M_{n, p}(\R)$ be two matrices given by blocks $A \in M_{m_1, n_1}(\R),\, B \in M_{m_1, n_2}(\R),\, C \in M_{m_2, n_1}(\R),\, D \in M_{m_2, n_2}(\R),\, A' \in M_{n_1, p_1}(\R),\, B' \in M_{n_1, p_2}(\R),\, C' \in M_{n_2, p_1}(\R),\, D' \in M_{n_2, p_2}(\R)$ (so-called block matrices) with $m_1 + m_2 = m$ and $n_1 + n_2 = n$ and $p_1 + p_2 = p$. It can easily shown that
\[
\begin{pmatrix}
    A & B \\ C & D
\end{pmatrix}
\begin{pmatrix}
    A' & B' \\ C' & D'
\end{pmatrix}
=
\begin{pmatrix}
    AA' + BC' & AB' + BD' \\ CA' + DC' & CB' DD'
\end{pmatrix} \in M_{m, p}(\R)
\]
that is, if matrix sizes match correctly the formula for the multiplication of block matrices is the same as one of "usual two by two matrices"

For example, if $A = \begin{pmatrix}
    0 & 1 \\ 0 & 0
\end{pmatrix}$(so that $A ^ 2 = O_{2 \times 2}$) then
\[
\begin{pmatrix}
    1 & 0 & 1 & 0 \\
    0 & 1 & 0 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 0 & 0 & 0
\end{pmatrix} ^ 2
=
\begin{pmatrix}
    I_2 & I_2 \\ A & A
\end{pmatrix} ^ 2
=
\begin{pmatrix}
    I & I \\ A & A
\end{pmatrix}
\begin{pmatrix}
    I & I \\ A & A
\end{pmatrix}
=
\begin{pmatrix}
    I + A & I + A \\ A + A ^ 2 & A + A ^ 2
\end{pmatrix}
=
\begin{pmatrix}
    1 & 1 & 1 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 0 & 0 & 0
\end{pmatrix}
\]

\subsection{Inverse matrix}

\begin{definition}
    Let $A$ be a square $(n\times n)$ matrix. We say $A$ is invertible (or non-singular) if there exists an $n \times n$ matrix $B$ such that
    \[
    AB = BA = I_n.
    \]
    If such a matrix $B$ exists we call $B$ the inverse of $A$ and we write $A ^ {-1}$ for this inverse. (there can only exist at most one inverse).
    If no such inverse $B$ exists for a matrix $A$, we say $A$ is non-invertible (or singular).
\end{definition}

\begin{proposition}[Properties of the inverse and transpose, label = prop:propofinvandtransp]\label{pre_linalg_prop_propofinvandtransp}
    \begin{enumerate}[label = (\roman*)]
        \item A matrix has at most one inverse.
        \item Assume $A$ and $B$ are invertible $n \times n$ matrices. Then the product $AB$ is invertible and
        \[
        (AB) ^ {-1} = B ^ {-1}A ^ {-1}.
        \]
        \item We have $(AB) ^ t = B ^ t A ^ t$ and $(\lambda A) ^ t = \lambda A ^ t$ for any matrix $A$ and any real constant $\lambda$.
        \item Assume $A$ is invertible. then the transpose $A ^ t$ is invertible and
        \[
        (A ^ t) ^ {-1} = (A ^ {-1}) ^ t.
        \]
        \item Let $A \in M_{n, n}(\R)$ and assume $BA = I_n$ for some $B \in M_{n, n}(\R)$. Then $AB = I_n$ as well as vice versa. So the "left inverse" is automatically the "right inverse".
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
            \item If $B'$ were a second inverse for $A$ then we would have
            \[
            B' = B'I_n = B'(AB) = (B'A)B = I_nB = B.
            \]
        \end{enumerate}
    \end{proof}
\end{proposition}

\begin{remark}
    We also have
    \[
    (X + Y) ^ t = X ^ t + Y ^ t,
    \]
    but it is not the case in general that
    \[
    (X + Y) ^ {-1} = X ^ {-1} + Y ^ {-1}.
    \]
\end{remark}

\newpage

\section{Gauss-Jordan elimination}

\subsection{Solving equations and Gauss-Jordan elimination}
We will turn to solving the general problem of solving $m$ linear equations in $n$ variables. To handle this, we need a systematic approach.

We begin by setting up our notation. We shall denote our variables by $x_1,\,x_2,\dotsc,x_n$, and consider the solution of the system of $m$ linear equations
\begin{align*}
    a_{1 1}x_{1} &+ a_{1 2}x_2 &+ \dotsi &+ a_{1 n}x_n &= b_1; \\ 
    a_{2 1}x_{1} &+ a_{2 2}x_2 &+ \dotsi &+ a_{2 n}x_n &= b_2; \\ 
    \vdots & \vdots & \ddots & \vdots &\vdots \vdots; \\ 
    a_{m 1}x_{1} &+ a_{m 2}x_2 &+ \dotsi &+ a_{m n}x_n &= b_m.
\end{align*}
For short we can write
\[
\sum_{j = 1}^{n}a_{ij}x_{j} = b_{i}\quad\text{for } i = 1,\dotsc,m.
\]
In vector notation we can write this using the dot product
\[
\mbf{a}_i \cdot \mbf{x} = \begin{pmatrix}
    a_{i 1} \\ \vdots \\ a_{i n}
\end{pmatrix}
\cdot
\begin{pmatrix}
    x_1 \\ \vdots \\ x_n
\end{pmatrix}
=
b_i\quad\text{for } i = 1, \dotsc, m.
\]
Each equation gives a so-called hyperplane with normal vector $\mbf{a}_i$.

To solve the equations we shall go through the following process.
\begin{itemize}
    \item First formalise the system of equations as a single equation of matrices, this is for notational convenience and systematises our process.
    \item Perform series of matrix operations (elementary row operations (ERO's)) that in effect change the original set of equations into a different set of linear equations, but ones that have the same solution set and are much easier to solve. This process is called Gauss-Jordan elimination, and the sort of matrix that represents the equations that are easier to solve is said to be in row reduced echelon form (RREF).
\end{itemize}

The first step we have essentially already done.
Using the notation from before,
the system of equations can be written as the matrix equation
\[
A\mbf{x} = \mbf{b}
\]
where $A$ is the matrix
\[
\begin{pmatrix}
    a_{1 1} & a_{1 2} & \dotsi & a_{1 n} \\
    a_{2 1} & a_{2 2} & \dotsi & a_{2 n} \\
    \vdots & \vdots & \phantom{} & \vdots \\
    a_{m 1} & a_{m 2} \dotsi a_{m n}
\end{pmatrix}
\]
and $\mbf{b}$ is the column vector
\[
\begin{pmatrix}
    b_1 \\ b_2 \\ \vdots \\ b_m
\end{pmatrix}.
\]

The augmented matrix associated with a linear system of equations is a rectangular array of numbers,
the coefficients of each equation are placed in order in each row,
followed by a vertical bar where the equal signs are with the $a_{i j}$ to the left,
and then the $b_i$ recorded to the right.
In other words
\[
\left(
\begin{array}{cccc|c}
    a_{1 1} & a_{1 2} & \dotsi & a_{1 n} & b_1 \\
    a_{2 1} & a_{2 2} & \dotsi & a_{2 n} & b_2 \\
    \vdots & \vdots & \phantom{} & \vdots & \vdots \\
    a_{m 1} & a_{m 2} & \dotsi & a_{m n} & b_m
\end{array}
\right).
\]
For example, the system
\begin{align*}
    3x_1 - 4x_2 + 3x_3 &= 1 \\
    2x_1 - x_2 + 2x_3 &= 2
\end{align*}
has augmented matrix $\left(
\begin{array}{ccc|c}
    3 & -4 & 3 & 1 \\
    2 & -1 & 2 & 2
\end{array}
\right).$

If as before we write the matrix
\[
\left(
\begin{array}{cccc|c}
    a_{1 1} & a_{1 2} & \dotsi & a_{1 n} & b_1 \\
    a_{2 1} & a_{2 2} & \dotsi & a_{2 n} & b_2 \\
    \vdots & \vdots & \phantom{} & \vdots & \vdots \\
    a_{m 1} & a_{m 2} & \dotsi & a_{m n} & b_m
\end{array}
\right),
\]
which we shall sometimes call the coefficient matrix of the linear system,
and as above write
\[
\mbf{b} = \begin{pmatrix}
    b_1 \\ \vdots \\ b_m
\end{pmatrix}
\quad\text{and}\quad\mbf{x} = \begin{pmatrix}
    x_1 \\ \vdots \\ x_n
\end{pmatrix},
\]
then the augmented matrix cab we written as $(A | \mbf{b})$.

We call a linear system homogenous if $\mbf{b} = \mbf{0}$,
and inhomogeneous if $\mbf{b} \neq \mbf{0}$.
Note that a homogeneous system always has at least one solution: $\mbf{x} =\mbf{0}$.

\begin{lemma}\label{linalg_lem_invprop}\phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item Let $A \in M_{n,\,n}(\R)$ be a square matrix and let $\mbf{v} \in \R ^ n$ be a non-zero vector such that $A\mbf{v} = \mbf{0}$. Then $A$ is singular.
        \item Let $A \in M_{n,\,n}(\R)$ be an invertible square matrix. Then the inhomogeneous system $A\mbf{x} = \mbf{b}$ has a single unique solution. In particular, the only solution to the homogeneous system $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$.
    \end{enumerate}
    \begin{proof}
        For (i), see example 2.4.2(iii) for an easily generalisable argument. For (ii), if $A$ is invertible then
        \[
        \mbf{x} = I_n\mbf{x} = (A ^ {-1} A)\mbf{x} = A ^ {-1}(A\mbf{x}) = A ^ {-1}\mbf{b}.
        \]
    \end{proof}
\end{lemma}

\subsection{Echelon form and its application to linear systems}

\begin{definition}
    A matrix of numbers is said to be in row reduced echelon form (RREF) if
    \begin{enumerate}[label = (\roman*)]
        \item The first (leftmost) non-zero entry in any non-zero row is a $1$ (called its leading $1$).
        \item If a row has its leading $1$ in the $j$th column then
        \begin{enumerate}[label = (\alph*)]
            \item all the other entries in the $j$th column are $0$; and
            \item the leading $1$s of subsequent rows are in columns to the right of the $j$th column.
        \end{enumerate}
    \item Any row(s) of zeros come after all the rows with non-zero entries.
    \end{enumerate}
\end{definition}

For example, the matrices
\[
\begin{pmatrix}
    1 & 4 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0
\end{pmatrix},
\qquad
\begin{pmatrix}
    0 & 1 & 0 & 0 & 2 \\ 0 & 0 & 1 & 0 & 3 \\ 0 & 0 & 0 & 1 & 4
\end{pmatrix}
\qquad\text{and}
\qquad
\begin{pmatrix}
    1 & 0 & 2 & 0 \\
    0 & 1 & 3 & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & 0 & 0
\end{pmatrix}
\]
are all in RREF.

In general, an RREF matrix looks as follows:
\[
\left(
\begin{array}{cccccccccccccccc}
    1 & * & \dotsi & * & 0 & * & \dotsi & * & 0 & * & \dotsi & * & 0 & * & \dotsi & * \\
    0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * & 0 & * & \dotsi & * & 0 & * & \dotsi & * \\
    0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * & 0 & * & \dotsi & * \\
    \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots \\
    0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & 1 & * & \dotsi & *
\end{array}
\right)
\]

\begin{example}
    Consider these three RREF's for a non-zero $2 \times 2$ matrix.
    \[
    (a): \begin{pmatrix}
        1 & \alpha \\ 0 & 0
    \end{pmatrix},\qquad
    (b): \begin{pmatrix}
        1 & 0 \\ 0 & 1
    \end{pmatrix},
    \qquad
    (c): \begin{pmatrix}
        0 & 1 \\ 0 & 0
    \end{pmatrix}.
    \]
    Here $\alpha$ is some scalar. Viewing these as the coefficient matrix for a linear system we have the following augmented matrices:
    \[
    (a): \begin{amatrix}{2}
        1 & \alpha & b_1 \\ 0 & 0 & b_2
    \end{amatrix},
    \qquad
    (b): \begin{amatrix}{2}
        1 & 0 & b_1 \\ 0 & 1 & b_2
    \end{amatrix},
    \qquad
    (c): \begin{amatrix}{2}
        0 & 1 & b_1 \\ 0 & 0 & b_2
    \end{amatrix}.
    \]
\end{example}

For $(a)$ and $(c)$ the correspond system has clearly only a solution if $b_2 = 0$ (because the second equation now reads $0x_1 + 0x_2 = 1$).
In these cases we obtain for the solution sets
\[
(a): x_1 = b_1 - \alpha x_2,\quad (b): x_1 = b_1,\,x_2 = b_2,\quad (c): x_2 = b_1
\]
note for $(b)$ we have only have a unique solution, while for $(a)$ and $(c)$ the solution set is a line.
More precisely, it is a non-horizontal line for $(a)$, and a horizontal one for $(c)$.

\subsection{The Elementary Row Operations and the big theorem}
The ERO's are simple operations you can do to a matrix to turn it into another matrix.
There are three fundamental operations, which are as follows,
\begin{enumerate}[label = (\alph*)]
    \item $P_{rs}$: switch row $r$ with row $s$;
    \item $M_{r}(\mbf{\lambda})$: multiple (all the entries in) row $r$ by the number $\lambda \neq 0$;
    \item $A_{rs}$: add (each entry in) row $r$ to (the corresponding entry in) row $s$.
    \item[(c')] $A_{rs}(\mbf{\lambda})$: add $\lambda$ times (each entry in) row $r$ to (the corresponding entry in) row $s$.
\end{enumerate}

Note that $A_{rs}(\lambda) = M_r\left(\frac{1}{\lambda}\right) \circ A_{rs} \circ M_r(\lambda)$,
read from right to left (in the same way as how $f \circ g$ means do $g$ then $f$).
\begin{example}
    \begin{align*}
    \begin{pmatrix}
        1 & 3 & 2 \\
        2 & 1 & -1 \\
        -1 & 1 & 2
    \end{pmatrix}
    &\xrightarrow{
    \begin{subarray}{c}
        A_{12}(-2) \\ A_{13}(1)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 3 & 2 \\
        0 & -5 & -5 \\
        0 & 4 & 4
    \end{pmatrix}
    &\xrightarrow{
    \begin{subarray}{c}
        M_2(-1 / 5) \\ M_3(1 / 4)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 3 & 2 \\
        0 & 1 & 1 \\
        0 & 1 & 1
    \end{pmatrix} 
    \\
    &\xrightarrow{
    \begin{subarray}{c}
        A_{23}(-1)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 3 & 2 \\
        0 & 1 & 1 \\
        0 & 0 & 0
    \end{pmatrix}
    &\xrightarrow{
    \begin{subarray}{c}
        A_{21}(-3)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 0 & -1 \\
        0 & 1 & 1 \\
        0 & 0 & 0
    \end{pmatrix}
    \end{align*}
\end{example}

\subsubsection{The basic routine}
\begin{enumerate}[label = (\alph*)]
    \item Find the first non-zero column (from the left).
    \item Go down the first non-zero column and find the first non-zero entry.
    \item If this is in the $r$-th row with $r \neq 1$ apply the elementary row operation $P_{1 r}$ to get a non-zero entry, $\lambda$ say, at the top of the first non-zero column.
    \item Apply the elementary row operation $M_1(1 / \lambda)$ to make the first non-zero entry of the first non-zero column $a$ $1$.
    \item Using elementary row operation of the form $A_{1 i}(\mu)$ clear all other non-zero entries in the first non-zero column.
\end{enumerate}

The resulting matrix is of the form
\[
\begin{pmatrix}
    1 & * & \dotsi & * \\
    0 & * & \dotsi & * \\
    \vdots & \vdots & \phantom{} & \vdots \\
    0 & * & \dotsi & *
\end{pmatrix}
\]
or, if the first non-zero column is not the first one,
\[
\begin{pmatrix}
    0 & \dotsi & 0 & 1 & * & \dotsi & * \\
    0 & \dotsi & 0 & 0 & * & \dotsi & * \\
    \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots \\
    0 & \dotsi & 0 & 0 & * & \dotsi & * \\
\end{pmatrix}
\]

\subsubsection{The algorithm}
Let $A$ be given $m \times n$ matrix.
The process of Gauss-Jordan elimination applied to $A$ is described by repeatedly applying the Basic Routine in stages.
(For simplicity of description we will assume that the first column of $A$ is non-zero).

\textbf{Stage 1}

Apply the basic routine to the given $m \times n$ matrix $A$.
This gives a new $m \times n$ matrix $A_1$ of the form:
\[
A_1 =
\begin{pmatrix}
    1 & * & \dotsi & * \\
    0 & * & \dotsi & * \\
    \vdots & \vdots & \phantom{} & \vdots \\
    0 & * & \dotsi & *
\end{pmatrix}
\]

\textbf{Stage 2}

If all of the remaining rows (below row $1$) are zero, it is easy to see that the matrix is now in RREF.
Otherwise:
\begin{enumerate}[label = (\roman*).]
    \item Apply the basic routine to the submatrix of $A_1$ obtained by ignoring the first row.
    This gives a new $m \times n$ matrix $\tilde{A}_2$ of the form:
    \[
    \begin{pmatrix}
        0 & \dotsi & 0 & 1 & * & \dotsi & * \\
        0 & \dotsi & 0 & 0 & * & \dotsi & * \\
        \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots \\
        0 & \dotsi & 0 & 0 & * & \dotsi & *
    \end{pmatrix}
    \]
    \item Apply an elementary row operation of the form $A_{2 1}(\mu)$,
    if necessary, to $\tilde{A}_2$ to make the entry above the first $1$ in the second row a zero.
    This gives a new $m \times n$ matrix $A_2$ of the form:
    \[
    A_2 = 
    \begin{pmatrix}
        1 & * & \dotsi & * & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & * & \dotsi & *
    \end{pmatrix}.
    \]
\end{enumerate}
Now continue inductively.
Assume stages $1$ to $k - 1$ have been completed (for $k - 1 < m$),
resulting in a matrix $A_{k - 1}$.

\textbf{Stage k (with $k \leq m$)}

If rows $k, k + 1, \dotsc, m$ of $A_{k - 1}$ are all zero then this matrix is in RREF and the algorithm terminates.
Similarly, if the leading $1$ in the $(k - 1)$th row of $A_{k - 1}$ lies in the $n$th column then it is easy to check that $A_{k - 1}$ is in RREF.
Otherwise:
\begin{enumerate}[label = (\roman*).]
    \item Apply the basic routine to the submatrix of $A_{k - 1}$ obtained by ignoring the first $k - 1$ rows.
    This gives a new $m \times n$ matrix $\tilde{A}_k$ of the form:
    \[
    \tilde{A}_k = \begin{pmatrix}
        1 & 0 & \dotsi & * & 0 & * & \dotsi & * & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & * & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 1 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & *
    \end{pmatrix}.
    \]
    \item Apply elementary row operations of the form $A_{k r}(\mu)\ (\text{for } r = 1, 2, \dotsc, k - 1)$,
    if necessary, to $\tilde{A}_k$; this ensures all remaining non-zero entries in the column containing the first $1$ in the $k$-th row equal to zero and gives a new $m \times n$ matrix $A_k$ of the form:
    \[
    \tilde{A}_k = \begin{pmatrix}
        1 & 0 & \dotsi & * & 0 & * & \dotsi & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 1 & * & \dotsi & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 1 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & *
    \end{pmatrix}.
    \]
    By induction, this process must terminate either before Stage $m$ or after Stage $m$.
    In the former case,
    the reason for early termination must have been that the matrix had reached RREF.
    In the latter case we must have reached a matrix $A_m$ with a leading $1$ on every row.
    It is then easy to check that $A_m$ is in RREF.
\end{enumerate}

\subsection{Using G-J to solve a system of linear equations}
Recall, we are considering the solution set of a general system of $m$ linear equations in $n$ variables.

Two such systems are said to be equivalent if they have the same solution set.
We say an augmented matrix is in row reduced echelon form if the associated coefficient matrix is in row reduced echelon form.
Applying the elementary row operations to the augmented matrix $(A | \mbf{b})$ allows us to pass to equivalent systems and eventually to a system for which it is easy to spot the solutions.
This is justified by
\begin{theorem}[label = thm:lintoech]\label{linalg_thm_lintoech}
    Applying ERO's to the augmented matrix of a linear system simply changes the system to an equivalent one; that is, the solution set remains unchanged.
\end{theorem}

If a row is of the form $(0\ \dotsi\ \,|\, k)$ for $k \neq 0$ this would mean that there are no solutions,
we call a system like this inconsistent.

A quick way to solve a linear system,
there is no need to run G-J fully.
One can do the following:
\begin{enumerate}[label = \arabic*) :]
    \item Perform ERO's to bring coefficient matrix $A$ into upper triangular form,
    that is, all the entries below the diagonal vanish.
    (Also not necessary to scale the "leading ones" to be actually equal to $1$.)
    \item Then go back to the linear system and use back substitution from bottom to top to solve the system.
\end{enumerate}

\begin{example}
    \[
    \sysdelim..\systeme{
        x_1 + 2x_2 + 3x_3 = 1,
        2x_1 + 3x_2 + 2x_3 = 1,
        3x_1 + 2x_2 + x_3 = 1.
        }
    \]
    We put this into matrix form then EROs to get it into upper triangular form
    \begin{align*}
        &\begin{amatrix}{3}
            1 & 2 & 3 & 1 \\
            2 & 3 & 2 & 1 \\
            3 & 2 & 1 & 1
        \end{amatrix}
        &\xrightarrow{\begin{subarray}{c}
        A_{12}(-2) \\ A_{13}(-3)
        \end{subarray}}
        &\begin{amatrix}{3}
            1 & 2 & 3 & 1 \\
            0 & -1 & -4 & -1 \\
            0 & -4 & -8 & -2
        \end{amatrix}
        \\
        \xrightarrow{A_{23}(-4)}
        &\begin{amatrix}{3}
            1 & 2 & 3 & 1 \\
            0 & -1 & -4 & -1 \\
            0 & 0 & 8 & 2
        \end{amatrix}.
    \end{align*}
    Putting this back into our equations gives us:
    
    $3$rd equation: $x_3 = \frac{1}{4}$;
    
    $2$nd equation: $x_2 = -4x_3 + 1 = -4\frac{1}{4} + 1 = 0$;
    
    $1$st equation: $x_1 = 1 - 2x_2 - 3x_3 = 1 - \frac{3}{4} = \frac{1}{4}$, i.e.,
    \[
    \begin{pmatrix}
        \frac{1}{4} \\
        0 \\
        \frac{1}{4}
    \end{pmatrix}.
    \]
\end{example}

\subsection{Inverse matrices and linear systems}
\begin{lemma}\label{linalg_lem_solsetsam}
    Consider an arbitrary $m \times n$ linear system $A\mbf{x} = \mbf{b}$.
    Let $B \in M_{m \times m}(\R)$ be an invertible matrix.
    Then the new system $(BA)\mbf{x} = B\mbf{b}$ has the same solution set as the original system.
    \begin{proof}
        We need to show that the two systems have the same solution set.
        First assume that $\mbf{v}$ is a solution to the system $A\mbf{x} = \mbf{b}$, assume $A\mbf{v} = \mbf{b}$.
        Then, $B\mbf{b} = B(A\mbf{v}) = (BA)\mbf{v}$ as required.
        Conversely, assume $(BA)\mbf{v} = B\mbf{b}$. Then
        \[
        \mbf{b} = I_m\mbf{b} = (B ^ {-1}B)\mbf{b} = B ^ {-1}(B\mbf{b}) = B ^ {-1}(BA\mbf{v}) = (B ^ {-1} B) A\mbf{v} = A\mbf{v},
        \]
        as required.
    \end{proof}
\end{lemma}

\begin{theorem}[continues = thm:lintoech]
    \begin{proof}
        Suppose we want to perform elementary row operations on a matrix with $m$ rows.
        Then we can do this using the following $m \times m$ matrices.
        \begin{enumerate}[label = (\alph*)]
            \item The row operation $P_{rs}$ can be performed by left multiplication by the $m \times m$ matrix,
            which we also call $P_{rs}$,
            obtained from the identity by swapping the $r$th and $s$th columns.
            That is $(p_{ij})_{m \times m}$ with $p_{ii} = 1$ when $i \neq r, s, p_{rr} = p_{ss} = 0; p_{ij} = 0$ when $i \neq j$ and $\{i, j\} \neq \{r, s\}, p_{rs} = p_{sr} = 1$. That is,
            \[
            P_{rs} = \begin{pmatrix}
                1 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                0 & 1 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 1 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 1 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 1
            \end{pmatrix}.
            \]
            One can check that we have $P_{rs} ^ {-1} = P_{rs}$ (swapping twice leaves you back where you started).
            \item
            The row operation $M_r(\lambda)$ can be performed by left multiplication by the $m \times m$ matrix,
            which we also call $M_r(\lambda)$,
            obtained from the identity by replacing the $r$th diagonal entry with $\lambda$.
            That is $(m_{ij})_{m \times m}$ with $m_{ii} = 1$ when $i \neq r$ and $m_{rr} = \lambda$; $m_{ij} = 0$ when $i \neq j$.
            That is,
            \[
            M_r(\lambda) = \begin{pmatrix}
                1 & 0 & \dotsi & 0 & \dotsi & 0 \\
                0 & 1 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & \lambda & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 1
            \end{pmatrix}.
            \]
            We have $M_r(\lambda) ^ {-1} = M_r\left(\frac{1}{\lambda}\right)$.
            \item The row operation $A_{rs}(\lambda)$ has be performed by left multiplication by the $m \times m$ matrix,
            which we also call $A_{rs}(\lambda)$,
            obtained from the identity matrix by replacing the entry in the $r$th column and $s$th row with $\lambda$.
            That is $(a_{ij})_{m \times m}$ where $a_{ii} = 1; a_{ij} = 0$ when $i \neq j$ and $(i, j) \neq (s, r);\; a_{sr} = \lambda$.
            That is,
            \[
            \begin{pmatrix}
                1 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                0 & 1 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 1 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & \lambda & \dotsi & 1 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 1
            \end{pmatrix}.
            \]
            We have $A_{rs}(\lambda) ^ {-1} = A_{rs}(-\lambda)$.
        \end{enumerate}
        We call any of these matrices an elementary matrix.
        Then if $E$ is an elementary $m \times m$ matrix the corresponding row operation is given by $EA\mbf{x} = E\mbf{b}$.
    \end{proof}
\end{theorem}

\begin{remark}
    Note that every elementary matrix is not only invertible,
    but the inverse of any elementary matrix is also an elementary matrix.
\end{remark}

\subsection{The fundamental theorem of invertible matrices}
\begin{lemma}\label{linalg_lem_sqmatrrefcond}
    Let $A \in M_{n \times n}(\R)$ be a square matrix.
    Then the following are (pairwise) equivalent,
    that is they are either all true or all false.
    \begin{enumerate}[label = (\roman*)]
        \item In each column of RREF of $A$ there exists a leading $1$.
        \item The RREF of $A$ is the identity matrix $I_n = \begin{pmatrix}
            1 & \phantom{} & \phantom{} \\
            \phantom{} & \ddots & \phantom{} \\
            \phantom{} & \phantom{} & 1
        \end{pmatrix}$.
        \item The only solution to the system $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$.
    \end{enumerate}
    \begin{proof}
        The equivalence of (i) and (ii) is clear:
        if the RREF is the identity then each column has a leading $1$.
        Conversely, if each column of the RREF has a leading $1$,
        then the first leading $1$ is in the $1, 1$-entry,
        the second must be in the $2, 2$-entry and so on.
        Then all the off-diagonal entries must be zero,
        that is, the RREF is indeed $I_n$.

        Next we show that (ii) implies (iii).
        So let's assume (ii).
        But by \autoref{linalg_lem_solsetsam} we can replace the equation $A\mbf{x} = \mbf{0}$ with the RREF of $A$,
        that is, we can consider $I_n\mbf{x} = \mbf{0}$,
        which means $\mbf{x} = \mbf{0}$.
        So (iii) holds.

        What remains is to show that (iii) implies (i).
        So let's assume (iii), that is,
        the only solution to the system $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$.
        Let's assume for a contradiction that the number of leading $1$'s in the RREF of $A$ is equal to $r < n$.
        This means that there are $n - r > 0$ free variables (one for each column without a leading $1$).
        To be explicit, after permutation of columns we can assume the RREF of $A$ looks as follows
        \[
        \begin{pmatrix}
            I_r & A'_{r, n - r} \\
            \mbf{0}_{n - r, r} & \mbf{0}_{n - r, n - r}
        \end{pmatrix}
        \]
        for some $r \times (n - r)$ matrix $A'$.
        But then any choice of real value for each of the free variables will yield a solution t the system $A\mbf{x} = \mbf{0}$.
        In particular there exists a non-trivial solution to the system.
        This is a contradiction, therefore there must have been $r = n$ leading $1$'s after all.
    \end{proof}
\end{lemma}

\begin{theorem}[Fundamental Theorem of Invertible Matrices]\label{linalg_thm_fundthmofinvertmatr}
    A square matrix $A \in M_{n \times n}(\R)$ is invertible if and only if any one of the conditions (i), (ii), (iii) of \autoref{linalg_lem_sqmatrrefcond} holds.
    \begin{proof}
        Suppose $A$ is invertible.
        Then by \autoref{linalg_lem_invprop}(ii) the only solution of $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$,
        which is \autoref{linalg_lem_sqmatrrefcond}(iii).

        Now assume that any one of (i), (ii), and (iii) hold.
        By (ii), there exist elementary matrices $E_1, E_2, \dotsc, E_\ell$ such that
        \[
        E_\ell E_{\ell - 1} \dotsi E_1 A = I_n.
        \]
        Thus $(E_\ell E_{\ell - 1} \dotsi E_1)$ is a left sided inverse of $A$.
        We must show it is a right sided inverse too.
        To do this, note that
        \begin{align*}
            AE_\ell E_{\ell - 1} \dotsi E_1 &= I_n A E_\ell E_{\ell - 1} \dotsi E_1 \\
            &= (E_1 ^ {-1} E_2 ^ {-1} \dotsi E_{\ell} ^ {-1}E_\ell E_{\ell - 1} \dotsi E_1)AE_\ell E_{\ell - 1} \dotsi E_1 \\
            &= E_1 ^ {-1} E_2 ^ {-1} \dotsi E_{\ell} ^ {-1}(E_\ell E_{\ell - 1} \dotsi E_1A)E_\ell E_{\ell - 1} \dotsi E_1 \\
            &= E_1 ^ {-1} E_2 ^ {-1} \dotsi E_{\ell} ^ {-1}E_\ell E_{\ell - 1} \dotsi E_1 = I_n.
        \end{align*}
        So $A$ is invertible with $A ^ {-1} = E_\ell E_{\ell - 1} \dotsi E_1$.
    \end{proof}
\end{theorem}
This proof immediately gives us
\begin{corollary}\label{pre_linalg_col_invisprodofele}
    Any invertible square matrix is the product of elementary matrices (and vice versa).
    \begin{proof}
        Simply notice that $E_1 ^ {-1}E_2 ^ {-1} \dotsi E_i ^ {-1}$ is an inverse of $A ^ {-1} = E_1E_2 \dotsi E_i$.
        But, so is the matrix $A$.
        So, by uniqueness of the inverses,
        we must have $A = E_1 ^ {-1} E_2 ^ {-1} \dotsi E_\ell ^ {-1}$.
    \end{proof}
\end{corollary}

\begin{remark}
    Note that the product above is not necessarily unique.
\end{remark}

\begin{remark}
    We can also now show that the "left inverse" of a square matrix $A$ is also the "right inverse" of $A$,
    that is if $BA = I_n$, then also $AB = I_n$
\end{remark}

\begin{proposition}[continues = prop:propofinvandtransp]
    \begin{proof}[Proof of (v)]
        Suppose that $BA = I_n$ and consider $A\mbf{x} = \mbf{0}$.
        Left multiplication by $B$ shows that the only solution is $\mbf{x} = \mbf{0}$.
        Hence $A$ is invertible by \autoref{linalg_thm_fundthmofinvertmatr}.
        In particular, $A$ has an inverse $A ^ {-1}$ and we have
        \[
        AB = ABAA ^ {-1} = A(BA)A ^ {-1} = AA ^ {-1} = I_n.
        \]
        By the uniqueness of inverses, this shows that $A ^ {-1} = B$.
    \end{proof}
\end{proposition}

\subsection{An algorithm to compute the inverse of a matrix}
We run G-J for the matrix $A$ and keep track of the elementary operations/matrices we used.
Concretely, we run G-J on the augmented or double matrix
\[
(A \,|\, I_n) =
\left(\hspace{-5pt}
\begin{array}{cccc|cccc}
     a_{1 1} & a_{1 2} & \dotsi & a_{1 n} & 1 & 0 & \dotsi & 0 \\
     a_{2 1} & a_{2 2} & \dotsi & a_{2 n} & 0 & 1 & \dotsi & 0 \\
     \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \ddots & \vdots \\
     a_{n 1} & a_{n 2} & \dotsi & a_{n n} & 0 & 0 & \dotsi & 1
\end{array}
\hspace{-5pt}\right).
\]
Now perform G-J as always and get (if $A$ is invertible)
\[
(I_n\,|\,E_\ell \dotsi E_1),
\]
and $A ^ {-1} = E_\ell \dotsi E_1$.
If $A$ is singular,
you will not obtain the identity matrix on the left hand side.

\newpage

\section{Determinants}
The determinant of a square matrix is a scalar.
The determinant of a matrix $A$ with real entries is a real number.
So we can think of this as a function
\[
\det : M_{n \times n}(\R) \rightarrow \R.
\]

\subsection{Small values of $n$}
We look at particular cases of the function
\[
\det : M_{n\times n}(\R) \rightarrow \R.
\]
$n = 1$:
The equation $ax = b$ has a unique solution if and only if $a \neq 0$.
We set
\[
\det(a) = a.
\]

$n = 2$:
We know $\begin{pmatrix}
    a & b \\ c & d
\end{pmatrix}$
is invertible if and only if $ad - bc \neq 0$.
So, set
\[
\det\begin{pmatrix}
    a & b \\ c & d
\end{pmatrix} = ad - bc.
\]
Then the matrix equation (system of two linear equations)

has a unique solution if and only if $\det\begin{pmatrix}
    a & b \\ c & d
\end{pmatrix} \neq 0$

\subsection{General definition of determinant}
We define the determinant of an $n \times n$ matrix inductively in terms of the determinants of certain $(n - 1)\times (n - 1)$ matrices.
Let $A = (a_{ij})_{n \times n}$
\begin{itemize}
    \item For integers $r$ and $s$ between $1$ and $n$ write $A_{r, s}$ for the $(n - 1)\times (n - 1)$ matrix obtained from $A$ be deleting its $r$th row and its $s$th column.
    Then $A_{r, s}$ is called the $(r, s)$th minor
    \footnote{The minor is called the 'unsigned cofactor'.
    We have defined the minor as simply a submatrix.}
    of $A$.
    \item The value of $\det(A_{r, s})$ i.e. the determinant of the $(r, s)$th minor of $A$,
    is called the $(r, s)$th unsigned cofactor of $A$ and the value $(-1) ^ {r + s}\det(A_{r, s})$ is called the $(r, s)$th (signed) cofactor of $A$
    \footnote{Some texts write 'adjunct' instead of 'cofactor'.}
    .
\end{itemize}

For any matrix $A \in M_{n \times n}(\R)$ the determinant of $A$ is the defined inductively via expansion by the first row,
in terms of the signed cofactors:
\[
\det A = \sum_{k = 1}^{n}(-1) ^ {1 + k}a_{1k}\det(A_1, k).
\]

An important observation is that we could have defined it via expansion by any given row.
\begin{lemma}\label{pre_linalg_lem_detanyrow}
    If $A$ is an $n \times n$ matrix then for any $1 \leq i \leq n$ we have
    \[
    \det(A) = \sum_{k = 1}^{n}(-1) ^ {i + k}a_{ik}\det(A_i, k).
    \]
    That is,
    the determinant formula remains valid via expansion along the $i$th row
    (note however the signs).
    \begin{proof}
        The proof is a very long (and involved) inductive one so$\dotsc$
    \end{proof}
\end{lemma}

\subsection{Fundamental properties}
In this section,
for notational convenience,
we write in terms of the row vector $\mbf{a}_r$ of an $n \times n$ matrix $A = (a_{ij})$ that is $\mbf{a}_r = (a_{r1}\; a_{r2}\; \dotsi \; a_{rn})$
\begin{enumerate}[label = (\roman*)]
    \item $\det I_n = 1$.
    \item If one multiplies
    (all the elements in)
    the $r$th row by some scalar $\lambda$
    (that is, if we apply the ERO $M_r(\lambda)$),
    then the determinant changes by precisely this factor:
    \[
    \det(M_r(\lambda)A) = \det\begin{pmatrix}
        \mbf{a} \\ \vdots \\ \lambda\mbf{a}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix} = \lambda\det A.
    \]
    \item For any row vector $\mbf{b}_r$ we have
    \[
    \det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_r + \mbf{b}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix} =
    \det \begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}
    +
    \det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{b}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}.
    \]
    Note that (ii) and (iii) can be summarised as linearity in each row.
    \item Anti-symmetry in rows.
    The determinant switches signs if one swaps a pair of rows (i.e. if one applies the ERO $P_{rs}$);
    that is,
    \[
    \det(A) = \det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}
    = -\det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}
    =
    -\det(P_{rs}A).
    \]
    \item $\det A = 0$ if $A$ has two rows that are equal
    (or even two rows that are collinear - follows simply by property (ii)).
    \item $\det A = 0$ if $A$ has a row of zeros.
    \item The determinant does not change under the ERO $A_{rs}(\lambda)$; that is,
    \[
    \det(A_{rs}(\lambda)A) = \det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s + \lambda\mbf{a}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}
    =
    \det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}
    = \det A.
    \]
    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
            \item We perform induction on $n$.
            The case $n = 1$ is obvious:
            $\det(1) = 1$.
            So now assume the claim holds for some $n \geq 1$,
            that $\det I_n = 1$.
            We need to establish the claim for $n + 1$.
            But now by the definition of the determinant
            (say, by expanding by the first row)
            we immediately see $\det(I_{n + 1}) = 1\cdot\det(I_n) = 1 \cdot 1 = 1$,
            as desired.
            \item This is straightforward,
            by expansion along the $r$th row (by \autoref{pre_linalg_lem_detanyrow}):
            \[
            \det(M_r(\lambda)A) = \sum_{k = 1}^{n}(-1) ^ {r + k}(\lambda a_{rk})\det(A_r, k) = \lambda\sum_{k = 1}^{n}(-1) ^ {r + k}a_{rk}\det(A_{r, k}) = \lambda\det A.
            \]
            \item is similar to (ii),
            by expansion along the $r$th row.
            \item \phantom{}
            \item This follows directly from (iv):
            Assume row $r$ is repeated in row $s$ then
            \[
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r + \mbf{b}_r \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            =
            -\det\left(P_{rs}\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r + \mbf{b}_r \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}\right)
            =
            -\det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r + \mbf{b}_r \\ \vdots \\ \mbf{a}_n
            \end{pmatrix},
            \]
            and so the value of the determinant must be zero.
            \item This follows directly from (iii) by writing $\mbf{0} = \mbf{a}_r - \mbf{a}_r$ (or from (ii) by writing $\mbf{0} = 0 \cdot \mbf{a}_r$ for some vector $\mbf{a}_r$).
            Indeed,
            if the row of zeros appears in row $r$ then we have
            \[
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_{r - 1} \\ \mbf{0} \\ \mbf{a}_{r + 1} \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            =
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_{r - 1} \\ \mbf{a}_r - \mbf{a}_r \\ \mbf{a}_{r + 1} \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            =
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_{r - 1} \\ \mbf{a}_r \\ \mbf{a}_{r + 1} \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            -
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_{r - 1} \\ \mbf{a}_r \\ \mbf{a}_{r + 1} \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            = 0.
            \]
            \item This follows from (ii), (iii), and (iv):
            \[
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s + \lambda \mbf{a}_r \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            =
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            +
            \lambda
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            =
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}.
            \]
        \end{enumerate}
    \end{proof}
\end{enumerate}

\subsection{Uniqueness of the determinant}
We will now prove that properties (i)-(vii) completely characterise the determinant.
We could define the function
\[
\det : M_{n \times n}(\R) \rightarrow \R
\]
as the unique function that satisfies these four properties.
\begin{theorem}\label{pre_linalg_thm_uniquedet}
    Let $f: M_n(\R) \rightarrow \R$ be a function that satisfies properties (i)-(iv).
    Then
    \[
    f(A) = \det(A).
    \]
    In other words,
    the determinant is the only function with the properties (i)-(vii).
    \begin{proof}
        Apply Gauss-Jordan to the matrix $A$ to get the row reduced echelon form $A'$.
        As $\det$ satisfies properties (ii), (iv) and (vii) and we are assuming $f$ does too,
        during this process $\det$ and $f$ change value in exactly the same way.
        So
        \begin{align*}
            \det A' &= \lambda_1\dotsi\lambda_k\det A \\
            f(A') &= \lambda_1\dotsi\lambda_kf(A)
        \end{align*}
        for some non-zero scalars $\lambda_i$.
        But $A'$ is either equal to $I_n$ in which case $f(A') = \det A' = 1$ (by property (i)),
        or it contains a zero row in which case $f(A') = \det A' = 0$ by property (vi).
        So in either case $f(A') = \det A'$ and we obtain
        \[
        \lambda_1\dotsi\lambda_kf(A) = \lambda_1\dotsi\lambda_k\det A,
        \]
        which gives $f(A) = \det A$ since $\lambda_i \neq 0$ for all $i$.
    \end{proof}
\end{theorem}

\subsection{Determinant calculations}
Properties (i)-(vii) tell us how the determinant changes under the G-J algorithm,
and hence gives us a way to compute the determinant.
There are two basic cases.
\begin{enumerate}[label = (\Roman*)]
    \item The RREF $A'$ of $A$ contains a zero row.
    Then $\det(A) = \det(A') = 0$.
    \item The RREF $A'$ of $A$ is the identity matrix.
    Assume that we applied the operation $P_{rs}$ of switching rows $\ell$ times and applied the scaling operations $M_{r_1}(\lambda_1), M_{r_2}(\lambda_2), \dotsc, M_{r_k}(\lambda_k)$ at some point during the algorithm.
    Then
    \[
    1 = \det I_n = \det A' = (-1) ^ \ell \lambda_1 \dotsi \lambda_k\det A.
    \]
    So
    \[
    \det(A) = (-1) ^ \ell \lambda_1 ^ {-1} \dotsi \lambda_k ^ {-1}.
    \]
\end{enumerate}
\begin{proposition}
    Let $A$ be an upper (or lower) triangular $n \times n$ matrix with diagonal entries $a_{11}, a_{22}, \dotsc, a_{nn}$.
    Then
    \[
    \det(A) = \prod_{k = 1}^{n}a_{kk} = a_{11} \cdot a_{22} \dotsi a_{nn}.
    \]
    \begin{proof}
        
    \end{proof}
\end{proposition}

\subsection{Several Big Theorems}
\begin{theorem}\label{pre_linalg_thm_invifdetnonzero}
    The $n \times n$ matrix $A$ has an inverse if and only if $\det(A) \neq 0$.
    \begin{proof}
        By definition we already know this for the cases $n = 1$,
        $n = 2$ and $n = 3$.
        For $n > 3$ we apply G-J to the matrix $A$ to get to the RREF $A'$.
        As in the proof of \autoref{pre_linalg_thm_uniquedet},
        during this process $\det$ changes in a clear way using properties (ii), (iv), and (vii).
        We get
        \[
        \det A' = \lambda_1\dotsi\lambda_k\det A
        \]
        for some non-zero scalars $\lambda_i$/
        But then
        \[
        \det(A) \neq 0 \iff \det(A') \neq 0 \iff A' = I_n \iff A \text{ is non-singular}.
        \]
    \end{proof}
\end{theorem}

\begin{theorem}\label{pre_linalg_thm_detsplits}
    Let $A$ and $B$ be $n \times n$ matrices.
    Then
    \[
    \det(AB) = \det(A)\det(B).
    \]
    \begin{proof}
        We apply G-J to the matrix $A$ to get to the RREF $A'$.
        As before there are some non-zero scalars $\lambda_i$ such that
        \[
        \det A' = \lambda_1\dotsi\lambda_k\det A \text{ so that } \det A = \lambda_1 ^ {-1} \dotsi\lambda_k ^ {-1}\det A'.
        \]
        Applying the same ERO's to the matrix $AB$ we obtain the matrix $A'B$ (ERO's are given by left multiplication of elementary matrices) and hence also
        \[
        \det(A'B) = \lambda_1\dotsi\lambda_k\det(AB).
        \]
        We now have two cases:
        \begin{enumerate}[label = (\roman*)]
            \item 
            $A' = I_n$.
            Then
            \[
            \det(AB) = \lambda_1 ^ {-1} \dotsi \lambda_k ^ {-1}\det(B) = \det(A)\det(B),
            \]
            as desired.
            \item $A'$ contains a zero row;
            so $\det A = \det A' = 0$.
            But then the product $A'B$ contains also a zero row which gives $\det(A'B) = 0$.
            Since $\det(A'B)$ and $\det(AB)$ only differ by non-zero scalars,
            we get $\det(AB) = 0$.
            But we also have $\det(A)\det(B) = 0$ since $\det A = 0$.
        \end{enumerate}
    \end{proof}
\end{theorem}

\begin{theorem}
    We have
    \[
    \det(A ^ t) = \det(A).
    \]
    \begin{proof}
        Firstly,
        \autoref{pre_linalg_prop_propofinvandtransp}(iv) implies that either both $A$ and $A ^ t$ are invertible,
        or neither are.
        In the case $\det A = 0$ we have by \autoref{pre_linalg_thm_invifdetnonzero} that $A$ is singular,
        and so $A ^ t$ is also singular.
        Applying \autoref{pre_linalg_thm_invifdetnonzero} a second time shows that $\det(A ^ t) = 0$.
        If $\det A \neq 0$ then $A$ is invertible.
        But then \autoref{pre_linalg_col_invisprodofele} tells us we may write it as a product of EROs,
        say $A = E_1E_2\dotsi E_\ell$.
        By combining determinant properties (ii),
        (iv) and (vii) it is easy to check that $\det(P_{rs}) = -1$,
        $\det(M_r(\lambda)) = \lambda$ and $\det(A_{rs}(\lambda)) = 1$ for every $r$ and $s$.
        Combining this with the observation that
        \[
        (P_{rs}) ^ t = P_{rs},\quad(M_{r}(\lambda)) ^ t = M_r(\lambda)\quad(A_{rs}(\lambda)) ^ t = A_{sr}(\lambda),
        \]
        it follows that $\det(E ^ t) = \det(E)$ for every elementary matrix $E$.
        Thus,
        by \autoref{pre_linalg_prop_propofinvandtransp}(iii) and \autoref{pre_linalg_thm_detsplits} we have
        \begin{align*}
            \det(A ^ t) &= \det((E_1\dotsi E_\ell) ^ t) &= \det(E_\ell ^ t \dotsi E_1 ^ t) &= \det(E_\ell ^ t)\dotsi\det(E_1 ^ t) \\
            &= \det(E_\ell)\dotsi\det(E_1) &= \det(E_1)\dotsi\det(E_\ell) &= \det(E_1 \dotsi E_\ell) &= \det A.
        \end{align*}
    \end{proof}
\end{theorem}

\begin{corollary}
    If $A$ is not singular,
    then
    \[
    \det(A ^ {-1}) = \det((A ^ t) ^ {-1}) = \frac{1}{\det(A)}.
    \]
    \begin{proof}
        Simply notice that $1 = \det(I_n) = \det(A ^ {-1}A) = \det(A ^ {-1})\det(A)$,
        and similarly for the transpose (using \autoref{pre_linalg_prop_propofinvandtransp}(iv)).
    \end{proof}
\end{corollary}

\subsection{Determinants and inverses; Cramer's Rule}
\begin{definition}
    Given a square matrix $A$,
    we define the adjoint matrix $\Adj(A)$ by
    \[
    (\Adj(A))_{ij} = (-1) ^ {i + j}\det(A_{j, i}).
    \]
    The adjoint is the transpose of the matrix of signed cofactors.
\end{definition}

\begin{proposition}
    Let $A \in M_n(\R)$ and $\Adj(A)$ be its adjoint.
    Then
    \[
    A \cdot \Adj(A) = \det(A) \cdot I_n = \Adj(A) \cdot A.
    \]
    \begin{proof}
        We first prove $A \cdot \Adj(A) = \det(A) \cdot I_n$.
        The $(r, s)$-entry of $A \cdot \Adj(A)$ is given by
        \[
        \sum_{k = 1}^{n}a_{rk}(\Adj(A))_{ks} = \sum_{k = 1}^{n}a_{rk}(-1) ^ {k + s}\det(A_{s, k}).
        \]
        For $r = s$ this is the expansion of $\det A$ by the $s$-th row.
        While for $r \neq s$,
        this is the expansion of the determinant of $A$ by the $s$-th row with row $s$ replaced by the $r$th row $\mbf{a}_r$.
        So we are looking at
        \[
        \det\begin{pmatrix}
            \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_n
        \end{pmatrix} = 0.
        \]
        Hence in summary
        \[
        (r, s)-\text{entry of } A\Adj(A) = \begin{cases}
            \det A &\text{if } r = s \\
            0 &\text{if } r \neq s
        \end{cases};
        \]
        i.e., $A \cdot \Adj(A) = (\det A)I_n$.
        This is enough to conclude that $A ^ {-1} = \frac{1}{\det(A)}\Adj(A)$ and the second equation,
        that $\Adj(A) \cdot A = (\det A)I_n$ follows from the uniqueness of inverses.
    \end{proof}
\end{proposition}

\begin{theorem}[Inverse by adjoint matrix]\label{pre_linalg_thm_invbyadjmatr}
    Let $A \in M_n(\R)$ be non-singular.
    Then the inverse $A ^ {-1}$ is given by
    \[
    A ^ {-1} = \frac{1}{\det A}\Adj(A)
    \]
\end{theorem}

\begin{theorem}[Cramer's rule]\label{pre_linalg_thm_cramersrulepre_linalg_thm_cramersrule}
    Let $A\mbf{x} = \mbf{b}$ be a linear system with a non-singular matrix $A \in M_n(\R)$.
    The unique solution $\begin{pmatrix}
        x_1 \\ \vdots \\ x_n
    \end{pmatrix}$ is given by
    \[
    x_i = \frac{\det(\mbf{a}_1, \dotsc, \mbf{a}_{i - 1}, \mbf{b}, \mbf{a}_{i + 1}, \dotsc, \mbf{a}_n)}{\det A}.
    \]
    Here $\mbf{a}_\ell$ denotes (for a change) the $\ell$-th column vector of $A$.
\end{theorem}

\begin{remark}
    For practical computations \autoref{pre_linalg_thm_invbyadjmatr} and \autoref{pre_linalg_thm_cramersrule} are rather useless.
    But for theoretical considerations this is very useful.
    For example,
    since the determinant is a polynomial in the entries of the matrix $A$,
    the entries $A ^ {-1}$
    (and also the solution to $A\mbf{x} = \mbf{b}$)
    depend continuously on the entries of $A$.
\end{remark}

\subsection{Determinants, area and volume}

\textbf{Area of parallelograms}

Consider the pair of vectors
\[
\mbf{v} = \begin{pmatrix} a \\ b \end{pmatrix},
\,\mbf{w} = \begin{pmatrix} c \\ d \end{pmatrix}.
\]
Now form the parallelogram $P$ with vertices $0, \mbf{v}, \mbf{v + w}, \mbf{w}$.
Elementary trigonometry tells us
\begin{equation}
    \mathrm{area}(P) = |\mbf{v}| \cdot |\mbf{w}| \cdot \sin\theta,
\end{equation}
where $\theta \in [0, \pi]$ is the angle between $\mbf{v}$ and $\mbf{w}$.
Recall for two vectors $\mbf{x, y} \in \R ^ 3$,
we have
\begin{equation}
    |\mbf{x \times y}| = |\mbf{x}||\mbf{y}|\sin\theta.
\end{equation}
We use this to rewrite (1) in the following way.
We define two auxiliary vectors $\Tilde{\mbf{v}} = \begin{pmatrix}
    a \\ b \\ 0
\end{pmatrix}$ and $\Tilde{\mbf{w}} = \begin{pmatrix}
    c \\ d \\ 0
\end{pmatrix}$ in $\R ^ 3$.
We then have
\[
\Tilde{\mbf{v}} \times \Tilde{\mbf{w}} = \begin{pmatrix}
    0 \\ 0 \\ ad - bc
\end{pmatrix}
\]
So (2) becomes
\[
|ad - bc| = |\Tilde{\mbf{v}}||\Tilde{\mbf{w}}|\sin\theta = |\mbf{v}||\mbf{w}|\sin\theta.
\]
Combining this with (1) we have shown
\begin{theorem}
    For $\mbf{v, w} \in \R ^ 2$ two vectors in the plane,
    form the parallelogram $P$ with vertices $\mbf{0, v, w}$ and $\mbf{v + w}$.
    Then
    \[
    \mathrm{area}(P) = |\det(\mbf{v, w})|.
    \]
\end{theorem}

\textbf{Area of triangles}

The area of a triangle with vertices $\mbf{a, b}$ and $\mbf{c}$ is half the area of the parallelogram with vertices $\mbf{a}$,
$\mbf{a + (b - a)}, \mbf{a + (c - a)} = \mbf{c}$ and $\mbf{a + (b - a) + (c - a)} = \mbf{b + c - a}$.
In other words,
the area of this triangle is given by:
\begin{proposition}
    The triangle with vertices $\mbf{a, b}$ and $\mbf{c}$ has area
    \[
    \frac{1}{2}|\det(\mbf{b - a, c - a})|.
    \]
\end{proposition}

\textbf{Volume of parallelepipeds}

A parallelepiped $P$ is a three dimensional polyhedron with six quadrilateral faces where opposite faces are parallel.
A parallelepiped has twelve edges.
These come in three families of four where all the vectors in each family are parallel;
call a vector from the first family $\mbf{u}$,
one from the second $\mbf{v}$ and one from the third $\mbf{w}$
(we then say that $P$ is spanned by $\mbf{u, v}$ and $\mbf{w}$).
The parallelepiped $P$ in $\R ^ 3$ spanned by the vectors $\mbf{u, v, w}$ hence has vertices $\mbf{0, u, v, w, u + v, u + w, v + w, u + v + w}$.
We have
\[
\mathrm{vol}(P) = \mathrm{area}(\text{base}) \times \text{height}.
\]
As base we take the parallelogram spanned by $\mbf{v}$ and $\mbf{w}$.
Hence, as before
\[
\mathrm{area}(\text{base}) = |\mbf{v \times w}|.
\]
For the height $h$ we have
\[
h = |\mbf{u}| \cdot |\sin(\mathrm{angle}(\mbf{u}, \text{base}))| = |\mbf{u}| \cdot |\cos(\mathrm{angle}(\mbf{u}, \text{normal vector to base}))|.
\]
A normal vector to the base is of course $\mbf{v \times w}$.
Hence by definition of the angle between two vectors we see
\[
|\cos(\mathrm{angle}(\mbf{u}, \mbf{v} \times \mbf{w}))| = \frac{|\mbf{u} \cdot (\mbf{v} \times \mbf{w})}{|\mbf{u}| \cdot |\mbf{v} \times \mbf{w}|} = \frac{|\det(\mbf{u}, \mbf{v}, \mbf{w})|}{|\mbf{u}| \cdot |\mbf{v} \times \mbf{w}|}.
\]
Combining all of this we obtain
\begin{proposition}
    The volume of the parallelepiped $P$ in $\R ^ 3$ spanned by the vectors $\mbf{u, v, w}$ is given by
    \[
    \mathrm{vol}(P) = |\det(\mbf{u, v, w})|.
    \]
\end{proposition}

\textbf{Other polyhedra}

Similar reasoning shows that the volume of a tetrahedron $T$ is one sixth of the determinant of the matrix whose columns are the vectors along edges of $T$ based at the same vertex.

\textbf{General $n$}

To define this phenomenon in higher dimensions we first need to have an idea what we mean by an $n$-dimension volume.
The $n$-dimension box spanned by $a_1\mbf{e}_1, a_2\mbf{e}_2, \dotsc, a_n\mbf{e}_n$ we define to have volume $|a_1|\dotsi|a_n|$.
Note that $\left|\begin{pmatrix}
    a_1 & \phantom{} & \phantom{} \\ \phantom{} & \ddots & \phantom{} \\ \phantom{} & \phantom{} & a_n
\end{pmatrix}\right| = |a_1| \dotsi |a_n|$.
With this definition one can show that the volume of the $n$-dimensional parallelepiped $P$ spanned by vectors $\mbf{v}_1, \dotsc, \mbf{v}_n$ is given by
\[
\mathrm{vol}(P) = |\det(\mbf{v}_1, \dotsc, \mbf{v}_n)|.
\]

\newpage

\section{Spanning sets and linear independence in $\R ^ n$}
\subsection{Subspaces of $\R ^ n$}
\begin{definition}[Vector subspace of $\R ^ n$]
    A (non-empty) subset $U$ of $\R ^ n$ is called a vector subspace (or linear subspace) of $\R ^ n$ if it satisfies the following three conditions:
    \begin{enumerate}[label = (\roman*)]
        \item closure under addition:
        if $\mbf{u}_1$ and $\mbf{u}_2$ are in $U$,
        then $\mbf{u}_1 + \mbf{u}_2$ is also in $U$;
        \item closure under scalar multiplication:
        if $\mbf{u}$ is in $U$ and $\lambda$ is in $\R$,
        then $\lambda\mbf{u}$ is in $U$;
        \item existence of an origin:
        the vector $\mbf{0}$ is in $U$.
    \end{enumerate}
\end{definition}
\textbf{Simply},
a vector subspace is just a non-empty subset of $\R ^ n$ that satisfies some conditions.

\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item  $U = \{\mbf{0}\}$ is clearly a vector subspace;
        it is called the trivial vector subspace.
        \item Equally clearly $\R ^ n$ is a vector subspace of itself.
        \item Let $L_0$ be a line in $\R ^ n$ through the origin $\mbf{0}$ with direction vector $\mbf{d}$.
        Then any point on $L_0$ has the form $\mbf{x} = \mbf{0} + \lambda\mbf{d} = \lambda\mbf{d}$.
        Clearly $L$ satisfies the three conditions above and so is a vector subspace of $\R ^ n$.
        If the line does not pass through $\mbf{0}$ then it is not a vector subspace
        (it is called an affine line,
        which is a special case of an affine subspace).
        \item Let $\Pi_0$ be the plane in $\R ^ n$ through the origin $\mbf{0}$ with direction vectors $\mbf{d}_1$ and $\mbf{d}_2$ (not collinear).
        Then we have
        \[
        \Pi_0 = \{\lambda_1\mbf{d}_1 + \lambda_2\mbf{d}_2\,:\, \lambda_1, \lambda_2 \in \R\},
        \]
        it is clear that $\Pi_0$ satisfies the three conditions above and so is a vector subspace.
        If the plane does not pass through $\mbf{0}$,
        then it is not a vector subspace.
        (It is called an affine plane,
        which is a special case of an affine subspace).
    \end{enumerate}
\end{example}

\begin{proposition}
    The solution set of an arbitrary $m \times n$ homogeneous linear system $A\mbf{x} = \mbf{0}$ is a vector subspace of $\R ^ n$.
    \begin{proof}
        Let $\mbf{u}_1$ and $\mbf{u}_2$ be two solutions to the linear system and $\lambda \in \R$.
        Then
        \[
        A(\mbf{u}_1 + \mbf{u}_2) = A\mbf{u}_1 + A\mbf{u}_2 = \mbf{0} + \mbf{0} = \mbf{0}\quad\text{and}\quad A(\lambda\mbf{u}_1) = \lambda(A\mbf{u}_1) = \lambda\mbf{0} = \mbf{0},
        \]
        as claimed.
        Of course we also have $A\mbf{0} = \mbf{0}$.
    \end{proof}
\end{proposition}

Note that the solution set of an inhomogeneous system $A\mbf{x} = \mbf{b}$
(where $\mbf{b} \neq \mbf{0}$)
is not a vector subspace since it does not contain the zero vector.
Instead it forms an affine subspace of $\R ^ n$;
that is,
a subset of $\R ^ n$ of the form
\[
\mbf{c} + U = \{\mbf{v} \in \R ^ n\,:\,\mbf{v} = \mbf{c} + \mbf{u}\text{ with }\mbf{u} \in U\},
\]
where $U$ is a vector subspace.

\subsection{Spanning sets}
Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be vectors in $\R ^ n$.
A linear combination of these vectors is a vector $\mbf{u}$ of the form
\[
\mbf{u} = \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k
\]
for some (real) scalars $\lambda_1, \dotsc, \lambda_k$.

\begin{definition}[Linear span]
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ vectors in $\R ^ n$.
    Their (linear) span is the subset $U$ of $\R ^ n$ consisting of all linear combinations of these vectors.
    So
    \[
    U = \{\lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k\quad\lambda_1, \dotsc, \lambda_k \in \R\}.
    \]
    Conversely,
    we say that such a $U$ is spanned by $\mbf{u}_1, \dotsc, \mbf{u}_k$ and write $U = \mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k)$.
    Equivalently,
    $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ is called a spanning set of $U$.
\end{definition}
Another simplification of this is to consider that the span is just a subset of $\R ^ n$ containing all of the combinations of the vectors.
So for a set of vectors to span a subspace $U$,
everything in $U$ must be able to be written as a combination of the vectors which span $U$.

\begin{proposition}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k \in \R ^ n$ be $k$ vectors,
    and let $A = (\mbf{u}_1, \dotsc, \mbf{u}_k) \in M_{n, k}(\R)$ be the associated matrix of column vectors.
    Then we have
    \[
    \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k = A\mbf{\lambda},
    \]
    where $\mbf{\lambda} = \begin{pmatrix}
        \lambda_1 \\ \vdots \\ \lambda_k
    \end{pmatrix} \in \R ^ k$ is the column vector of the $\lambda_j$.
    Moreover,
    we have
    \[
    \mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k) = \{\mbf{u} \in \R ^ n\,:\,\text{The system } A\mbf{\lambda} = \mbf{u}\text{ has a solution}\} = \{A\mbf{\lambda}:\mbf{\lambda} \in \R ^ k\}.
    \]
\end{proposition}

\begin{lemma}
    The linear span $U$ of any collection $\mbf{u}_1, \dotsc, \mbf{u}_k$ of vectors is a vector subspace of $\R ^ n$.
    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
            \item If $\mbf{u} = \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k$ and $\mbf{v} = \mu_1\mbf{u}_1 + \dotsi + \mu_k\mbf{u}_k$ then
            \[
            \mbf{u} + \mbf{v} = (\lambda_1 + \mu_1)\mbf{u}_1 + \dotsi + (\lambda_k + \mu_k)\mbf{u}_k \in U.
            \]
            \item If $\mbf{u} = \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k$ and $\lambda \in \R$,
            then
            \[
            \lambda\mbf{u} = (\lambda\lambda_1)\mbf{u}_1 + \dotsi + (\lambda\lambda_k)\mbf{u}_k \in U.
            \]
            \item Clearly $\mbf{0} = 0\mbf{u}_1 + \dotsi + 0\mbf{u}_k$ is in $U$.
        \end{enumerate}
    \end{proof}
\end{lemma}

\begin{lemma}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k \in \R ^ n$ and assume that one of the vectors,
    say $\mbf{u}_1$,
    can be expressed as a linear combination of the others.
    Then
    \[
    \mathrm{span}(\mbf{u}_1, \mbf{u}_2, \dotsc, \mbf{u}_k) = \mathrm{span}(\mbf{u}_2, \dotsc, \mbf{u}_k).
    \]
    \begin{proof}
        For $\supseteq$:
        This is obvious since
        \[
        \lambda_2\mbf{u}_2 + \dotsi + \lambda_k\mbf{u}_k = 0\mbf{u}_1 + \lambda_2\mbf{u}_2 + \dotsi + \lambda_k\mbf{u}_k,
        \]
        and so any element in the span of $\mbf{u}_2, \dotsc, \mbf{u}_k$ is also in the span of $\mbf{u}_1, \mbf{u}_2, \dotsc, \mbf{u}_k$.
        $\subseteq$:
        As $\mbf{u}_1$ can be written in the span of the others,
        let us write $\mbf{u}_1 = \mu_2\mbf{u}_2 + \dotsi + \mu_k\mbf{u}_k$ for some $\mu_j$.
        Then
        \[
        \lambda_1\mbf{u}_1 + \lambda_2\mbf{u}_2 + \dotsi + \lambda_k\mbf{u}_k = (\lambda_2 + \lambda_1\mu_2)\mbf{u}_2 \dotsi + (\lambda_k + \lambda_1\mu_k)\mbf{u}_k.
        \]
        So an element in the span of $\mbf{u}_1, \mbf{u}_2, \dotsc, \mbf{u}_k$ is also in the span of $\mbf{u}_2, \dotsc, \mbf{u}_k$.
    \end{proof}
\end{lemma}
This lemma tells us that the span of a set of vectors is equal to the span of a smaller set of vectors which doesn't include the vectors which can be made up of the other vectors.
Effectively,
I believe this is saying that we can keep reducing the spanning set until we get a smallest spanning set where each of the vectors cannot be written as a combination of the others.

\subsection{Linear independence}

\begin{definition}[Linear independence]
    The vectors $\mbf{u}_1, \dotsc, \mbf{u}_k$ are said to be linearly independent if the only solution to the equation
    \[
    \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k = \mbf{0}\quad\text{if}\quad\lambda_1 = \lambda_2 = \dotsi = \lambda_k = 0.
    \]
\end{definition}
This definition is saying that you cannot make one vector from the other.
In other words,
$\mbf{v}$ cannot be made from $\lambda\mbf{u}$ for some $\lambda \in \R$.
Here is a more informative example
\[
\begin{pmatrix}
    2 \\ 2 \\ 2
\end{pmatrix}\text{ is not linearly independent to }
\begin{pmatrix}
    1 \\ 1 \\ 1
\end{pmatrix}.
\]


\begin{corollary}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ vectors in $\R ^ n$ and let $A = (\mbf{u}_1, \dotsc, \mbf{u}_k) \in M_{n, k}(\R)$ be the associated matrix of column vectors.
    Then
    \[
    \mbf{u}_1, \dotsc, \mbf{u}_k\text{ are linear independent} \iff \text{The system } A\mbf{\lambda} = \mbf{0}\text{ has no non-trivial solution}.
    \]
\end{corollary}
If the vectors $\mbf{u}_1, \dotsc, \mbf{u}_k$ are not linearly independent we say that they are linearly dependent.

\begin{lemma}
    A pair of (non-zero) vectors $\mbf{u}, \mbf{v}$ in $\R ^ n$ are linearly dependent if and only if there is $\lambda \in \R$ so that $\mbf{v} = \lambda\mbf{u}$.
    In particular,
    any two (non-zero) non-collinear vectors are always linear independent.
    \begin{proof}
        $\Leftarrow$:
        If $\mbf{v} = \lambda\mbf{u}$ then we can rewrite this as $1\mbf{v} + (-\lambda)\mbf{u} = \mbf{0}$.
        So $\mbf{u}$,
        and $\mbf{v}$ are linearly dependent.
        $\Rightarrow$:
        Conversely if they are linearly dependent there are $\lambda_1, \lambda_2$ (not both zero, say $\lambda_2 \neq 0$) so that $\lambda_1\mbf{u} + \lambda_2\mbf{v} = \mbf{0}$.
        Then
        $\mbf{v} = \frac{-\lambda_1}{\lambda_2}\mbf{u}$.
    \end{proof}
\end{lemma}

The following proposition says that for a set of linearly independent vectors,
if we can choose a vector,
say $\mbf{u}$,
such that $\mbf{u}$ is not in the set of their combinations then the vectors that span this are linearly independent to $\mbf{u}$.

\begin{proposition}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ linear independent vectors in $\R ^ n$ and let $\mbf{u} \in \R ^ n$ such that
    \[
    \mbf{u} \notin \mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k).
    \]
    Then
    \[
    \mbf{u}_1, \dotsc, \mbf{u}_k, \mbf{u}\text{ are linear independent}.
    \]
    \begin{proof}
        Consider the equation
        \[
        \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k + \alpha\mbf{u} = \mbf{0}.
        \]
        We need to show $\lambda_1 = \dotsi = \lambda_k = \alpha = 0$ is the only solution.
        We have two cases:
        \begin{enumerate}[label = \arabic*)]
            \item $\alpha = 0$.
            But then we have $\lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k = \mbf{0}$,
            which has only the trivial solution since the $\mbf{u}_1, \dotsc, \mbf{u}_k$ are linear independent.
            \item $\alpha \neq 0$.
            But then we can write $\mbf{u} = -\frac{1}{\alpha}(\lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k)$.
            But this is impossible since $\mbf{u}$ is not in $\mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k)$.
        \end{enumerate}
    \end{proof}
\end{proposition}

\newpage

\section{Real vector spaces}

\subsection{Vector spaces and subspaces}
A (real) vector space is a non-empty set\footnote{This could be for any type,
not just vectors or reals,
I know,
it's a bit bloody confusing.}
$V$ together with two operations:
\begin{enumerate}[label = (\roman*)]
    \item Addition:
    This is a function
    \[
    V \times V \rightarrow V
    \]
    (Given any $\mbf{u}$ and $\mbf{v}$ in $V$,
    we write their sum as $\mbf{u + v}$);
    \item Scalar multiplication:
    This is a function
    \[
    \R \times V \rightarrow V
    \]
    (Given $\mbf{u} \in V$ and $\lambda \in \R$ we write the multiplication of $\mbf{u}$ by $\lambda$ as $\lambda\mbf{u}$).
\end{enumerate}

These operations must additionally satisfy the axioms of a real vector space:

Axioms for addition:
\begin{enumerate}[label = (\roman*)]
    \item Existence of additive identity:
    there is a vector $\mbf{0}$ in $V$ so that $\mbf{v + 0} = \mbf{v} = \mbf{0 + v}$ for all $\mbf{v}$ in $V$;
    \item Commutativity:
    $\mbf{v + w} = \mbf{w + v}$ for all $\mbf{v}$ and $\mbf{w}$ in $V$;
    \item Existence of additive inverses:
    for each $\mbf{v}$ in $V$ there is an element $-\mbf{v}$ in $V$ such that $\mbf{v} + (-\mbf{v}) = \mbf{0} = (-\mbf{v}) + \mbf{v}$;
    \item Associativity:
    $\mbf{u} + (\mbf{v + w}) = (\mbf{u + v}) + \mbf{w}$ for all $\mbf{u, v, w} \in V$.
\end{enumerate}

Axioms for scalar multiplication:
\begin{enumerate}[label = (\roman*)]
    \item $0\mbf{v} = \mbf{0}$ for all $\mbf{v}$ in $V$;
    \item $1\mbf{v} = \mbf{v}$ for all $\mbf{v}$ in $V$;
    \item Associativity:
    $(\lambda\mu)\mbf{v} = \lambda(\mu\mbf{v})$ for all $\lambda, \mu$ in $\R$ and for all $\mbf{v}$ in $V$;
    \item Distributivity:
    For all $\lambda, \mu \in \R$ and for all $\mbf{v, w} \in V$ we have
    \begin{enumerate}[label = \alph*)]
        \item $(\lambda + \mu)\mbf{v} = \lambda\mbf{v} + \mu\mbf{v}$;
        \item $\lambda(\mbf{v + w}) = \lambda\mbf{v} + \lambda\mbf{w}$.
    \end{enumerate}
\end{enumerate}

We refer to elements of a vector space as vectors\footnote{Big shock.}.

\begin{remark}
    Instead of $\R$ one can also allow other scalars such that complex numbers $\C$ or the rationals $\Q$.
    Then,
    for example,
    $\C ^ n$ is a vector space over $\C$ and $\Q ^ n$ is a vector space over $\Q$.
    In fact
    (almost)
    everything in these notes holds if we replace $\R$ with $\Q$ or $\C$.
    In general,
    one considers vector spaces over a field $F$,
    where $F$ plays the role of $\R, \C$ or $\Q$.
    (Very roughly,
    a field is a set of scalars,
    where we have nice addition and multiplication,
    and in particular every non-zero scalar has a multiplicative inverse).
\end{remark}

\begin{definition}[Vector subspace]
    A vector subspace of a vector space $V$ is a non-empty subset that is itself a vector space with respect to the same addition and multiplication operations of $V$.
\end{definition}
This is just a subset of a vector space $V$,
don't forget that this can still be for any type of object not just our typical $\R ^ n$ vectors\footnote{Why would we make it so easy to understand by just thinking about $\R ^ n$.}.

\begin{proposition}[Subspace criterion]
    A non-empty subset $U$ of a vector space $V$ is a vector subspace of $V$ if and only if it satisfies the following three conditions:
    \begin{enumerate}[label = (\roman*)]
        \item closure under addition:
        if $\mbf{u}_1$ and $\mbf{u}_2$ are in $U$,
        then $\mbf{u}_1 + \mbf{u}_2$ is also in $U$;
        \item closure under scalar multiplication:
        if $\mbf{u}$ is in $U$ and $\lambda$ is in $\R$,
        then $\lambda\mbf{u}$ is in $U$;
        \item existence of an origin:
        the vector $\mbf{0}$ is in $U$.
    \end{enumerate}
\end{proposition}

\subsection{Linear independence, spanning sets, bases, and dimension}
The notations for linear combination, linear span, spanning sets, and linear independence for a general vector space $V$ are defined exactly in the same way as for $\R ^ n$.
\begin{definition}[Basis]
    A (finite) basis for a vector space $V$ is a finite subset of vectors $\{\mbf{v}_1, \mbf{v}_2, \dotsc, \mbf{v}_k\}$ in $V$ such that
    \begin{enumerate}[label = (\roman*)]
        \item the vectors $\mbf{v}_1, \mbf{v}_2, \dotsc, \mbf{v}_k$ are linearly independent,
        and
        \item $V = \mathrm{span}(\mbf{v}_1, \mbf{v}_2, \dotsc, \mbf{v}_k)$.
    \end{enumerate}
\end{definition}

\textbf{Assorted Lemmas}:
\begin{lemma}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ vectors in a vector space $V$.
    Then $\mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k)$ is a vector subspace of $V$.
\end{lemma}
\begin{lemma}\label{pre_linalg_lem_lincombeqspanof}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be vectors in a vector space $V$ and assume that one of the vectors,
    say $\mbf{u}_1$,
    can be expressed as a linear combination of the others.
    Then
    \[
    \mathrm{span}(\mbf{u}_1, \mbf{u}_2, \dotsc, \mbf{u}_k) = \mathrm{span}(\mbf{u}_2, \dotsc, \mbf{u}_k).
    \]
\end{lemma}
\begin{lemma}
    Suppose that the set $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ is not linearly independent.
    Then for some $1 \leq i \leq k$,
    we have that
    \[
    \mbf{u}_i \in \mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_{i - 1}, \mbf{u}_{i + 1}, \dotsc, \mbf{u}_k).
    \]
    \begin{proof}
        Since $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ are not linearly independent there is some linear dependence relation
        \[
        \sum_{j = 1}^{k}\lambda_j\mbf{u}_j = \mbf{0}
        \]
        with at least one of the $\lambda_j$ non zero.
        Say that $\lambda_i \neq 0$.
        Then the linear dependence relation may be rearranged to read
        \[
        \mbf{u}_i = \sum_{j = 1}^{i - 1}\frac{\lambda_j}{-\lambda_i}\mbf{u}_j + \sum_{j = i + 1}^{k}\frac{\lambda_j}{-\lambda_i}\mbf{u}_j.
        \]
        The equivalence of the spans then follows from \autoref{pre_linalg_lem_lincombeqspanof}.
    \end{proof}
\end{lemma}

\begin{lemma}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ linearly independent vectors in a vector space $V$ and let $\mbf{u} \in V$ such that
    \[
    \mbf{u} \notin \mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k).
    \]
    Then
    \[
    \mbf{u}_1, \dotsc, \mbf{u}_k, \mbf{u}\text{ are linearly independent}.
    \]
\end{lemma}

\begin{theorem}[Existence of finite basis]
    Every vector space that is spanned by finitely many vectors has a basis.
\end{theorem}

\begin{theorem}["What is a basis?"]
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be vectors in a vector space $V$.
    Then the following statements are
    (pairwise)
    equivalent:
    \begin{enumerate}[label = (\roman*)]
        \item The vectors $\mbf{u}_1, \dotsc, \mbf{u}_k$ form a basis of $V$.
        \item The vectors $\mbf{u}_1, \dotsc, \mbf{u}_k$ form a maximal linear independent set in $V$,
        i.e., one cannot add another vector in $V$ to the $\mbf{u}_i$,
        and still obtain a linearly independent set.
        \item Any vector $\mbf{u}$ in $V$ can be written uniquely as
        \[
        \mbf{u} = \lambda_1\mbf{u}_1 + \lambda_2\mbf{u}_2 + \dotsi + \lambda_k\mbf{u}_k.
        \]
        \item The vectors $\mbf{u}_1, \dotsc, \mbf{u}_j$ form a minimal spanning set of $V$;
        i.e., one cannot remove any of the $\mbf{u}_i$ and still have a spanning set of $V$.
    \end{enumerate}
    \begin{proof}
        \textbf{Add proof}.
    \end{proof}
\end{theorem}

We call a vector space finite dimensional if it has a finite basis.

For example,
we have seen that $\{1, x, x ^ 2, \dotsc, x ^ n\}$ is a basis for $\R[x]_n$ and is therefore finite dimensional.

\begin{theorem}[Steinitz Exchange Theorem]
    Let $S = \{\mbf{v}_1, \dotsc, \mbf{v}_k\}$ be a spanning set of a vector space $V = \mathrm{span}(S)$.
    Let $\{\mbf{u}_1, \dotsc, \mbf{u}_{\ell}\} \subseteq V$ be a linearly independent set.

    Then there exist $\ell$ distinct vectors $\mbf{v}_{i_1}, \mbf{v}_{i_2}, \dotsc, \mbf{v}_{i_{\ell}} \in S$ such that for all $j$ with $0 \leq j \leq \ell$ we have
    \[
    (S \setminus \{\mbf{v}_{i_1}, \dotsc, \mbf{v}_{i_{\ell}}\}) \cup \{\mbf{u}_1, \dotsc, \mbf{u}_j\}
    \]
    is a spanning set for $V$.
    \begin{proof}
        \textbf{Add proof}.
    \end{proof}
\end{theorem}

\begin{corollary}
    In the situation above $k \geq \ell$.
    In other words,
    a spanning set for a vector space is always at least as big as any linearly independent set within the vector space.
\end{corollary}

\begin{corollary}
    Let $V$ be a finite-dimensional vector space.
    Then every basis for $V$ has the same number of elements.
    \begin{proof}
        \textbf{Add proof}.
    \end{proof}
\end{corollary}

\begin{definition}[Dimension]
    Let $V$ be a vector space that is generated by finitely many vectors.
    The dimension,
    $\dim{(V)}$,
    of $V$ is defined to be the cardinality of any basis for $V$.
\end{definition}
This definition is best understood through the following examples.
\begin{example}
    The vector space $\R[x]_n$ has basis $1, x, \dotsc, x ^ n$.
    Hence
    \[
    \dim{\R[x]_n} = n + 1.
    \]

    Additionally,
    the vector space $M_{m, n}(\R)$ has a basis of matrices $E_{rs}$
    (for $1 \leq r \leq m$ and $1 \leq s \leq n$)
    and so
    \[
    \dim{M_{m, n}(\R)} = m \cdot n.
    \]
\end{example}

\begin{theorem}
    Let $V$ be a finite dimensional vector space and $U$ a vector subspace of $V$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item $U$ is also finite-dimensional;
        \item $\dim{(U)} \leq \dim{(V)}$;
        \item $\dim{(U)} = \dim{(V)} \iff U = V$.
    \end{enumerate}
    \begin{proof}
        \textbf{Add proof}.
    \end{proof}
\end{theorem}

\begin{remark}
    Not every vector space is finite-dimensional.
\end{remark}
This remark is shown through the following example.
\begin{example}
    The vector space $\R[x]$ of all polynomials has the infinite basis $S = \{1, x, x ^ 2, x ^ 3, \dotsc\}$.
    Every element in $\R[x]$ is the unique linear combination of finitely many elements in $S$.
\end{example}

The following theorem provides us with a list of useful observations.
\begin{theorem}
    Let $V$ be a vector space of dimension $k$ and let $\mbf{v}_1, \dotsc, \mbf{v}_{\ell}$ be $\ell$ vectors in $V$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item If $\ell > k$,
        then the $\mbf{v}_1, \dotsc, \mbf{v}_{\ell}$ are linearly dependent.
        \item If $\ell < k$,
        then the $\mbf{v}_1, \dotsc, \mbf{v}_{\ell}$ do not span $V$.
        \itme If $k = \ell$,
        then the following statements are (pairwise) equivalent
        \begin{enumerate}[label = (\alph*)]
            \item The vectors $\mbf{v}_1, \dotsc, \mbf{v}_k$ form a basis of $V$.
            \item The vectors $\mbf{v}_1, \dotsc, \mbf{v}_k$ are linear independent.
            \item The vectors $\mbf{v}_1, \dotsc, \mbf{v}_k$ span $V$.
        \end{enumerate}
    \end{enumerate}
    \begin{proof}
        \textbf{Insert proof}.
    \end{proof}
\end{theorem}

\subsection{Bases and dimension in $\R ^ n$}
\begin{remark}
    Let $U$ be a vector subspace of $\R ^ n$.
    Then,
    of course,
    a basis for $U$ is any linearly independent set of vectors that span $U$.
\end{remark}

\begin{theorem}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ vectors in $\R ^ n$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item If $k > n$,
        then the $\mbf{u}_1, \dotsc, \mbf{u}_k$ are linearly dependent.
        \item If $k < n$,
        then the $\mbf{u}_1, \dotsc, \mbf{u}_k$ do not span $\R ^ n$.
        \item If $k = n$,
        then
        \begin{align*}
            \mbf{u}_1, \dotsc, \mbf{u}_n\text{ form a basis of } \R ^ n &\iff\text{The matrix $(\mbf{u}_1, \dotsc, \mbf{u}_n)$ is non singular} \\
            &\iff \det(\mbf{u}_1, \dotsc, \mbf{u}_n) \neq 0.
        \end{align*}
        So,
        the vectors $\mbf{u}_1, \dotsc, \mbf{u}_k$ form a basis for $\R ^ n$ if and only if $k = n$ and the square matrix $(\mbf{u}_1, \dotsc, \mbf{u}_k)$ is non-singular.
    \end{enumerate}
    \begin{proof}
        \textbf{Add proof}.
    \end{proof}
\end{theorem}

\begin{definition}
    Let $A \in M_{n \times k}(\R)$ be a $n \times k$ matrix.
\end{definition}






















\end{document}