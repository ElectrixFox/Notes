\documentclass[10pt, a4paper]{article}
\usepackage{preamble}
\usepackage{systeme}

\title{Linear Algebra I \\
    \large Prereading}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{The vector space \texorpdfstring{$\R ^ n$}{}}
\begin{definition}[$n$-dimensional real space]
    The space $\R ^ n$ ($n$-dimensional real space) is the collection of all (column) vectors of the form
    \[
    \mathbf{x} =
    \begin{pmatrix}
        x_1 \\
        \vdots \\
        x_n
    \end{pmatrix}.
    \]
\end{definition}
Vectors are typically denoted by bold letters such as $\mathbf{x}$ and sometimes using underlined letters $\underline{x}$ to differentiate them from ordinary real numbers.

We write $\mathbf{0}$ for the zero-vector
\[
\mathbf{0} = \begin{pmatrix}
    0 \\
    \vdots \\
    0
\end{pmatrix}
\]
this is the point of origin of $\R ^ n$.

There are two important operations which can be performed on vectors.

\textbf{Addition}: for any vectors $\mathbf{v}$ and $\mathbf{w}$, we define
\[
\mathbf{v + w} =
\begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
+
\begin{pmatrix}
    w_1 \\
    \vdots \\
    w_n
\end{pmatrix}
=
\begin{pmatrix}
    v_1 + w_1 \\
    \vdots \\
    v_n + w_n
\end{pmatrix}
\]
this is component-wise addition.


\textbf{Scalar multiplication}: for any vector $\mathbf{v} \in \R ^ n$ and real number $\lambda \in \R$ we define
\[
\lambda\mathbf{v} =
\lambda\begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
=
\begin{pmatrix}
    \lambda v_1 \\
    \vdots \\
    \lambda v_n
\end{pmatrix}
\]
this is component-wise multiplication by $\lambda$.

\begin{proposition}
    These operations satisfy the axioms of a real vector space
\end{proposition}

{\large \textbf{Axioms for Addition}}
\begin{enumerate}[label = (\roman*)]
    \item existence of additive identity: there is a vector $\mathbf{0}$ in $\R ^ n$ so that $\mathbf{v} + \mathbf{0} = \mathbf{v} = \mathbf{0} + \mathbf{v}$ for all $\mathbf{v}$ in $\R ^ n$.
    \item commutativity: $\mbf{v + w = w + v}$ for all $\mathbf{v}$ and $\mathbf{w}$ in $\R ^ n$
    \item existence of additive inverses: for each $\mbf{v}$ in $\R ^ n$ there is an element $-\mbf{v}$ in $\R ^ n$ such that $\mbf{v + (-v) = 0 = (-v) + v}$
    \item associativity: $\mbf{u + (v + w) = (u + v) + w}$ for all $\mbf{u, v, w}$ in $\R ^ n$
\end{enumerate}
these axioms can be summarised by saying that $\R ^ n$ is an abelian group under addition.


{\large\textbf{Axioms for Scalar Multiplication}}
\begin{enumerate}[label = (\roman*)]
    \item multiplication by 0: $0\mbf{v = 0}$ for all $\mbf{v}$ in $\R ^ n$
    \item multiplication by 1: $1\mbf{v = v}$ for all $\mbf{v}$ in $\R ^ n$
    \item associativity: $(\lambda\mu)\mbf{v} = \lambda (\mu\mbf{v})\ \forall \lambda, \mu \in \R$ and $\forall \mbf{v} \in \R ^ n$
    \item distributivity: $(\lambda + \mu)\mbf{v} = \lambda\mbf{v} + \mu\mbf{v}$ and $\lambda (\mbf{v + w}) = \lambda\mbf{v} + \lambda\mbf{w}$ $\forall \lambda, \mu \in \R$ and $\forall \mbf{v, w} \in \R ^ n$
\end{enumerate}

\begin{proof}
    The above axioms ate trivially satisfied by $\R$ (the case $n = 1$), many of the proofs follow from this observation. Once we set
    \[
    \mbf{0} = \begin{pmatrix}
        0 \\
        \vdots \\
        0
    \end{pmatrix},
    \qquad
    -\mbf{v} = -\begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    =
    \begin{pmatrix}
        -v_1 \\
        \vdots \\
        -v_n
    \end{pmatrix}
    =
    (-1)\begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \]
    then the proofs are very straight forward
\end{proof}

\begin{definition}[Standard basis vectors]
    We set
    \[
    \mbf{e_1} = \begin{pmatrix}
        1 \\
        0 \\
        \vdots \\
        0
    \end{pmatrix},
    \ 
    \mbf{e_2} = \begin{pmatrix}
        0 \\
        1 \\
        \vdots \\
        0
    \end{pmatrix},
    \dotsc,\ 
    \mbf{e_n} = \begin{pmatrix}
        0 \\
        0 \\
        \vdots \\
        1
    \end{pmatrix}
    \]
    and call these vectors the standard basis vectors of $\R ^ n$.
\end{definition}
Sometimes the entries of a vector are referred to as the Cartesian coordinates of a point in space.

Addition can be thought of as an operation that inputs a pair of vectors, each from $\R ^ n$, and outputs a single vector in $\R ^ n$. In other words, addition is a function $\R ^ n \times \R ^ n \mapsto \R ^ n$. Similarly, scalar multiplication is a function $\R \times \R ^ n \mapsto \R ^ n$. The domains of each of these functions are called Cartesian products.

\subsection{The scalar product in \texorpdfstring{$\R ^ n$}{}}

\begin{definition}[Scalar product]
    The scalar (or dot) product is a function
    \[
    \R ^ n \times \R ^ n \mapsto \R
    \]
    defined as follows. For $\mbf{v, w}$ in $\R ^ n$ set
    \[
    \mbf{v \cdot w} =
    \begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        w_1 \\
        \vdots \\
        w_n
    \end{pmatrix}
    =
    v_1 w_1 + \cdots + v_n w_n.
    \]
    This is a real number, a scalar.
\end{definition}

The transpose $\mbf{v} ^ t$ of $\mbf{v}$ is defined to be the row vector with the same entries as the column vector $\mbf{v}$. So if
\[
\mbf{v} = \begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
\qquad\text{then}
\qquad\mbf{v} ^ t = (v_1,\ \dots,\ v_n).
\]

The dot product of $\mbf{v}$ and $\mbf{w}$ is then obtained by matrix multiplying $\mbf{v} ^ t$ and $\mbf{w}$.

\begin{proposition}[Properties of the scalar product]
\phantom{}
\begin{enumerate}[label = (\roman*)]
    \item Symmetry
    \[
    \mbf{u \cdot v = v \cdot u}\qquad\text{for all }\mbf{u, v}\text{ in } \R ^ n
    \]
    \item Linearity in first factor
    \begin{enumerate}[label = (\alph*)]
        \item $(\mbf{u} + \mbf{v}) \cdot \mbf{w} = \mbf{u} \cdot \mbf{w} + \mbf{v} \cdot \mbf{w}$\qquad for all $\mbf{u,\ v,\ w}$ in $\R ^ n$
        \item $(\lambda\mbf{u}) \cdot \mbf{v} = \lambda(\mbf{u} \cdot \mbf{v})$\qquad for all $\lambda \in \R$ and all $\mbf{u,\ v}$ in $\R ^ n$
    \end{enumerate}
    \item Linearity in the second factor
    \begin{enumerate}[label = (\alph*)]
        \item $\mbf{u} \cdot (\mbf{v} + \mbf{w}) = \mbf{u} \cdot \mbf{w} + \mbf{u} \cdot \mbf{v}$\qquad for all $\mbf{u,\ v,\ w}$ in $\R ^ n$
        \item $\mbf{u} \cdot (\lambda\mbf{v}) = \lambda(\mbf{u} \cdot \mbf{v})$\qquad for all $\lambda \in \R$ and all $\mbf{u,\ v}$ in $\R ^ n$
    \end{enumerate}
    \item Positivity
    
    $\mbf{v \cdot v} \geq 0$ for all $v$, and $\mbf{v \cdot v} = 0$ if and only if $\mbf{v = 0}$.
\end{enumerate}

\begin{proof}
    \phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item
        First we can determine some expressions for $\mbf{u\cdot v}$ and $\mbf{v\cdot u}$
        
        $\mbf{u\cdot v} = u_1 v_1 + u_2 v_2 + \dotsc + u_n v_n$ and

        $\mbf{v\cdot u} = v_1 u_1 + v_2 u_2 + \dotsc + v_n u_n$.

        Then by the commutativity of multiplication we have
        \begin{align*}
            \mbf{u\cdot v} &= u_1 v_1 + u_2 v_2 + \dotsc + u_n v_n \\
            &= v_1 u_1 + v_2 u_2 + \dotsc + v_n u_n \\
            &= \mbf{v \cdot u}
        \end{align*}
        which proves our goal as required
        
    \end{enumerate}
\end{proof}
\end{proposition}

\begin{definition}[Magnitude]
    Suppose
    \[
    \mbf{v} = \begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \]
    then $\mbf{v \cdot v} = v_1 ^ 2 + \dotsc + v_n ^ 2$ which is a non-negative real number. We define the magnitude (or length) of $\mbf{v}$, denoted $|\mbf{v}|$, by
    \[
    |\mbf{v}| = \sqrt{\mbf{v \cdot v}}.
    \]
    So long as the $n$ coordinate directions are mutually orthogonal, the magnitude
    \[
    |\mbf{v}| = \sqrt{\mbf{v \cdot v}} = \sqrt{x_1 ^ 2 + \cdots + v_n ^ 2}
    \]
    is the euclidean distance from the origin to the point with coordinates given by $\mbf{v}$. We say that a vector is non-trivial if it has positive length. It is easy to see that the only trivial vector in $\R ^ n$ is the zero vector $\mbf{0}$.
\end{definition}


We can describe the position of a point by what direction it lies and what distance away from us it is, using polar coordinates. \\


Using the length of a vector in $\R ^ 2$ we define polar co-ordinates on $\R ^ 2$ as follows. Let $\mbf{v} = \left(\frac{x}{y}\right)$ be a non-trivial vector with length $|\mbf{v}| = r > 0$. Let $\theta \in [0, 2\pi)$ be the angle at origin $\mbf{0}$ measured in an anticlockwise direction from the $x$-axis to $\mbf{v}$. By basic trigonometry
\[
\cos(\theta) = \frac{x}{r},\qquad\sin(\theta) = \frac{y}{r}.
\]
We define the polar coordinates of $\mbf{v \neq 0}$ to be the unique $(r, \theta)$ in $\R_+ \times [0, 2\pi)$ so that
\[
\mbf{v} = \begin{pmatrix}
    r\cos(\theta) \\
    r\sin(\theta)
\end{pmatrix}
=
r\begin{pmatrix}
    \cos(\theta)
    \sin(\theta)
\end{pmatrix}
\]

\begin{lemma}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ 2$ of length $r > 0 $ and $s > 0$ respectively. Let $\theta$ be the angle between them. Then
    \[
    \mbf{v \cdot w} = rs\cos(\theta).
    \]
    In particular, we have
    \[
    |\mbf{v \cdot w}| \leq |\mbf{v}||\mbf{w}|
    \]
    with equality if and only if $\mbf{v}$ and $\mbf{w}$ are collinear, i.e. are scalar multiples of each other.

    \begin{proof}
        Suppose that $\mbf{v}$ has polar coordinates $(r, \gamma)$ and $\mbf{w}$ has polar coordinates $(s, \rho)$. The angle between $\mbf{v}$ and $\mbf{w}$ is $\theta = |\gamma - \rho|$. Then
        \[
        \mbf{v} = \begin{pmatrix}
            r\cos\gamma \\
            r\sin\gamma
        \end{pmatrix},
        \qquad
        \mbf{w} = \begin{pmatrix}
            s\cos\rho \\
            s\sin\rho
        \end{pmatrix}.
        \]
        Applying the dot product using these definitions of $\mbf{v}$ and $\mbf{w}$ we obtain the following, applying trigonometric addition formula and the fact that cosine is even we get,
        \[
        \mbf{v \cdot w} =  rs\cos\gamma\cos\rho + rs\sin\gamma\sin\rho = rs(\cos(\gamma - \rho)) = rs\cos(|\gamma - \rho|) = rs\cos\theta.
        \]
    \end{proof}
\end{lemma}
Note that using this lemma we can obtain an expression for the angle between two vectors
\[
\cos(\theta) = \frac{\mbf{v \cdot w}}{|\mbf{v}||\mbf{w}|}
\]

\begin{theorem}[Cauchy-Schwarz inequality]
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ n$. Then
    \[
    |\mbf{v \cdot w}| \leq |\mbf{v}||\mbf{w}|
    \]
    with equality if and only if $\mbf{v}$ and $\mbf{w}$ are collinear

    \begin{proof}
        
    \end{proof}
\end{theorem}

\begin{definition}[Angle between two vectors]\label{def:linalg:vec:angbetvecs}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ n$ of length $r > 0$ and $s > 0$ respectively. Then we define the angle $\theta \in [0, \pi]$ between them by
    \[
    \cos(\theta) = \frac{\mbf{v \cdot w}}{rs}.
    \]
\end{definition}

We say that the vectors $\mbf{v}$ and $\mbf{w}$ are orthogonal (or perpendicular) if the angle between them is $\frac{\pi}{2}$. \\

\begin{corollary}
    The non-trivial vectors $\mbf{v}$ and $\mbf{w}$ in $\R ^ n$ are orthogonal if and only if we have $\mbf{v \cdot w} = 0$.

    \begin{proof}
        By \autoref{def:linalg:vec:angbetvecs} we have that\footnote{Since $\cos\theta$ can only be $0$ when $\theta = \frac{\pi}{2}$ then they have to be orthogonal and the converse is true too.}
        \[
        \mbf{v \cdot w} = 0 \iff \cos\theta = 0 \iff \theta = \frac{\pi}{2}.
        \]
    \end{proof}
\end{corollary}

\subsection{The vector product in \texorpdfstring{$\R ^ 3$}{}}
\begin{definition}[Vector product in $\R ^ 3$]
    The vector (or cross) product is a function
    \[
    \R ^ 3 \times \R ^ 3 \rightarrow \R ^ 3
    \]
    defined as follows. If $\mbf{v, w} \in \R ^ 3$, their cross product is given by
    \[
    \mbf{v \times w} =
    \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix}
    \times
    \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix}
    =
    \begin{pmatrix} v_2 w_3 - v_3 w_2 \\ v_3 w_1 - v_1 w_3 \\ v_1 w_2 - w_1 v_2 \end{pmatrix}
    \]
\end{definition}

\begin{example}
    \[
    \begin{pmatrix}
        1 \\ 2 \\ 3
    \end{pmatrix}
    \times
    \begin{pmatrix}
        3 \\ 2 \\ 1
    \end{pmatrix}
    = \begin{pmatrix}
        2 - 6 \\
        9 - 1 \\
        2 - 6
    \end{pmatrix}
    =
    \begin{pmatrix}
        -4 \\ 8 \\ -4
    \end{pmatrix}
    \]
\end{example}

\begin{proposition}[Properties of the cross product]
    \begin{enumerate}[label = (\roman*)]
        \item Anti-Symmetry:
        \[
        \mbf{u \times v = -v \times u}\quad\text{for all }\mbf{u, v} \in \R ^ 3
        \]
        \item Linearity in first factor
        \begin{enumerate}[label = (\arabic*)]
            \item $(\mbf{u + v}) \times \mbf{w} = \mbf{u \times w} + \mbf{v \times w}\quad\text{for all }\mbf{u, v, w} \in \R ^ 3$
            \item $(\lambda\mbf{u}) \times \mbf{v} = \lambda(\mbf{u \times v})\quad\text{for all } \lambda \in \R \text{ and all }\mbf{u, v} \in \R ^ 3$
        \end{enumerate}
        \item Linearity in the second factor
        \begin{enumerate}[label = (\arabic*)]
            \item $\mbf{u} \times (\mbf{v + w}) = \mbf{u \times w} + \mbf{u \times v}\quad\text{for all }\mbf{u, v, w} \in \R ^ 3$
            \item $\mbf{u} \times (\lambda\mbf{v}) = \lambda(\mbf{u \times v})\quad\text{for all } \lambda \in \R \text{ and all }\mbf{u, v} \in \R ^ 3$
        \end{enumerate}
        \item Orthogonality to the input
        \[
        \mbf{v} \cdot (\mbf{v \times w}) = \mbf{w} \cdot (\mbf{v \times w}) = 0\quad\text{for all vectors } \mbf{v} \text{ and } \mbf{w} \text{ in } \R ^ 3
        \]
    \end{enumerate}
\end{proposition}

\begin{lemma}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors of length $r > 0$ and $s > 0$ respectively. Let $\theta \in [0, \pi]$ be the angle between them. Then
    \[
    |\mbf{v \times w}| = rs\sin(\theta).
    \]
    In particular, we have
    \[
    \mbf{v \times w = 0}\quad\iff\quad\mbf{v}\text{ and }\mbf{w}\text{ are collinear}.
    \]

    \begin{proof}
        We have
        \begin{align*}
            \left|\begin{pmatrix}
                v_2 w_3 - v_3 w_2 \\
                v_3 w_1 - v_1 w_3 \\
                v_1 w_2 - w_1 v_2
            \end{pmatrix}\right| ^ 2
            &= (v_2w_3 - v_3w_2) ^ 2 + (v_3w_1 - v_1w_3) ^ 2 + (v_1w_2 - w_1v_2) ^ 2 \\
            &= (v_1 ^ 2 + v_2 ^ 2 + v_3 ^ 2)(w_1 ^ 2 + w_2 ^ 2 + w_3 ^ 2) - (v_1w_1 + v_2w_2 + v_3w_3) ^ 2 \\
            &= |\mbf{v}| ^ 2 |\mbf{w}| ^ 2 - (\mbf{v \cdot w}) ^ 2 \\
            &= |\mbf{v}| ^ 2 |\mbf{w}| ^ 2 - (|\mbf{v}||\mbf{w}|\cos(\theta)) ^ 2 \\
            &= r^2s^2(1 - \cos(\theta)) ^ 2 \\
            &= (rs\sin(\theta)) ^ 2.
        \end{align*}
        Since $\sin\theta \geq 0$ for $\theta \in [0, \pi]$, we are done.
    \end{proof}
\end{lemma}

\subsection{Planes and Lines in \texorpdfstring{$\R ^ 3$}{}}
\subsubsection{Planes}

\textbf{Parametric Form}

A plane $\Pi$ in $\R ^ 3$ can be described in parametric form as the collection of all points corresponding to vectors of the form
\[
\mbf{a} + \lambda\mbf{d_1} + \mu\mbf{d_2}
\]
where $\lambda$ and $\mu$ each run through $\R$. $\mbf{a}$ is any point on the plane and $\mbf{d_1}$ and $\mbf{d_2}$ are any two direction vectors for $\Pi$, which lie on the plane and are not collinear.

For example, if $\mbf{a, b}$ and $\mbf{c}$ are all points on $\Pi$ which are not collinear then we could take $\mbf{d_1 = b - a}$ and $\mbf{d_2 = c - a}$.

We write
\[
\Pi = \{\mbf{a} + \lambda\mbf{d_1} + \mu\mbf{d_2} : \lambda, \mu \in \R\}.
\]

\textbf{Using the normal vector}

We can describe a plane via its normal vector.
\[
\mbf{d}_1 \cdot (\mbf{d}_1 \times \mbf{d}_2) = 0 = \mbf{d}_2 \cdot (\mbf{d}_1 \times \mbf{d}_2)
\]
that is, $\mbf{d}_1 \times \mbf{d}_2$ is orthogonal to both $\mbf{d}_1$ and $\mbf{d}_2$.

Let us write $\mbf{n}$ for $\mbf{d}_1 \times \mbf{d}_2$. Additionally we can write the plane in terms of the dot product
\[
\mbf{n \cdot(x - a)} = 0.
\]
We can rearrange this to have the normal form of the plane $\Pi$: the plane $\Pi$ is the set of vectors satisfying
\[
\mbf{n \cdot x} = \ell
\]
where $\ell = \mbf{n \cdot a} \in \R$ since $\mbf{n \cdot a}$ doesn't depend on $\mbf{x}$ it is just a real number.


\textbf{Cartesian form}

A variation of the normal form, really just the same description but now written in Cartesian coordinates immediately follows. Set $\mbf{n} = \begin{pmatrix} a \\ b \\ c \end{pmatrix}$ and $\mbf{x} = \begin{pmatrix} x \\ y \\ z \end{pmatrix}$. Then the normal form above becomes the Cartesian (or the Cartesian form) of the plane:
\[
ax + by + cz = \ell
\]
Thus we have
\[
\Pi = \{\mbf{x} \in \R ^ 3 : \mbf{n \cdot x} = \ell\} = \left\{\begin{pmatrix} a \\ b \\ c \end{pmatrix}\,:\,ax + by + cz = \ell\,\right\}
\]

\subsubsection{Lines}
\textbf{Parametric form}

A line $L$ in $\R ^ 3$ can be described in parametric form as the collection of all points corresponding to vectors of the form
\[
\mbf{a} + t\mbf{d}
\]
where $t$ runs through $\R$.
\[
L = \{\mbf{a} + t\mbf{d} : t \in \R\}.
\]

If $\mbf{b \neq a}$ is another point on $L$, then we could take $\mbf{d = b - a}$.

\textbf{Cartesian form}

Suppose the line is given in parametric form by
\[
\begin{pmatrix}
    x \\ y \\ z
\end{pmatrix}
=
\begin{pmatrix}
    a_1 \\ a_2 \\ a_3
\end{pmatrix}
+
t\begin{pmatrix}
    d_1 \\ d_2 \\ d_3
\end{pmatrix}.
\]

If all $d_i \neq 0$ then we may solve for $t$ in each component and obtain
\[
\frac{x - a_1}{d_1} = \frac{y - a_2}{d_2} = \frac{z - a_3}{d_3}.
\]
If one of the $d_i = 0$, such as $d_3 = 0$ then we would get two equations
\[
\frac{x - a_1}{d_1} = \frac{y - a_2}{d_2}\qquad\text{and}\qquad z = a_3
\]

\textbf{Solution set for a pair of linear equations}
A pair of planes in $\R ^ 3$ is either parallel, in which case they have the same normal vector, or else they intersect in a line.
\begin{align*}
    ax + by + cz &= \ell \\
    dx + ey + fz &= m
\end{align*}
the either the two normal vectors
\[
\mbf{n}_1 = \begin{pmatrix} a \\ b \\ c \end{pmatrix}\quad\text{and}\quad\mbf{n}_2 = \begin{pmatrix} d \\ e \\ f \end{pmatrix}
\]
are multiples of each other or else the solution is a line.

\begin{definition}[Scalar triple product]
    The scalar triple product is a function
    \[
    \R ^ 3 \times \R ^ 3 \times \R ^ 3 \rightarrow \R.
    \]
    Given three vectors $\mbf{a, b, c}$ in $\R ^ 3$ we define their scalar triple product $[\mbf{a, b, c}]$ by
    \[
    [\mbf{a, b, c}] = \mbf{a} \cdot(\mbf{b} \times \mbf{c}).
    \]
\end{definition}

\begin{lemma}[Invariance under cyclic permutation]
    For any three vectors $\mbf{a, b, c}$ in $\R ^ 3$ we have
    \[
    [\mbf{a, b, c}] = [\mbf{b, c, a}] = [\mbf{c, a, b}] = -[\mbf{a, b, c}] = -[\mbf{b, c, a}] = -[\mbf{c, a, b}].
    \]
\end{lemma}

\section{Matrix algebra}

\subsection{What is a matrix?}

\begin{definition}[Matrix]
    A matrix is a rectangular array of numbers (or symbols, or functions) arranged into rows and columns.
\end{definition}

\textbf{Notation and Terminology:}
\begin{itemize}
    \item A matrix which has $m$ rows and $n$ columns is referred to as an $m \times n$ matrix. The individual entries in the matrix are sometimes referred to as its elements. The entry in the $i$th row and $j$th column of a matrix is referred to as the $(i,\,j)$th element of a matrix $M$, usually denoted $M_{ij}$.
    \item Matrices which consist of a single column are (column) vectors. Matrices which consist of a single row are row vectors.
\end{itemize}

\begin{definition}[Transpose]
    Given an $m \times n$ matrix $W$ we define $W ^ t$, the transpose of $W$, to be the $n \times m$ matrix whose $(i,\,j)$th entry is the $(j,\,i)$th entry of $W$; that is the matrix for which
    \[
    (W ^ t)_{ij} = W_{ji}.
    \]
\end{definition}

A matrix which is equal to its transpose is said to be symmetric.

One way of describing or otherwise specifying the elements of a matrix is as follows. If $f(i,\,j)$ is some expression depending on $i$ and $j$ then $(f(i,\,j))_{m \times n}$ stands for the $m \times n$ matrix whose $(i,\,j)$th entry is $f(i,\,j)$. We often write an $m \times n$ matrix as $X = (x_{ij})_{m \times n}$, or $X = (x_{ij})$ if the size is obvious.

We write $O_{m \times n}$ for $(0)_{m \times n}$ that is the $m \times n$ matrix all of whose entries are zero, this is called the $m \times n$ zero matrix.

For $r$ and $s$ integers, we define $\delta_{r\,s}$ to be $1$ if $r = s$ and to be $0$ if $r \neq s$. (This is called the Kronecker delta.) The matrix $(\delta_{ij})_{m \times n}$ is denoted $I_n$ and is referred to as the $n \times n$ identity matrix.

In general, we write $M_{m \times n}(\R)$ or $M_{m, n}(\R)$ for the set of all real $m \times n$ matrices. We also write $M_n(\R)$ for the set of square matrices of size $n$, so $M_n(\R) = M_{n, n}(\R)$. $M_{m, 1}(\R)$ is the same set as $\R ^ m$.

\subsection{Matrix operations I: matrix addition and scalar multiplication}

\begin{enumerate}[label = (\roman*)]
    \item Matrix addition. This is a function
    \[
    M_{m \times n}(\R) \times M_{m \times n}(\R) \rightarrow M_{m \times n}(\R).
    \]
    For $X = (x_{ij})_{m \times n}$ and $Y = (y_{ij})_{m \times n}$ both $m \times n$ matrices, we define
    \[
    X + Y = (x_{ij})_{m \times n} + (y_{ij})_{m \times n} = (x_{ij} + y_{ij})_{m \times n},
    \]
    that is, matrix addition is defined by component wise addition. Matrix addition isn't defined when the matrices are of different sizes.
    \item Scalar multiplication. This is a function
    \[
    \R \times M_{m \times n}(\R) \rightarrow M_{m \times n}(\R).
    \]
    For $X = (x_{ij})_{m \times n}$ in $M_{m \times n}(\R)$ and $\lambda \in \R$ we set
    \[
    \lambda X = (\lambda x_{ij})_{m \times n}
    \]
    that is, we multiply every entry of the matrix by $\lambda$.
\end{enumerate}

We write $-X$ for $(-1)X$. If $X ^ t = -X$ we say that $X$ is skew-symmetric (or anti-symmetric).

\begin{proposition}[Properties of matrix addition and scalar multiplication]
    Matrix addition and scalar multiplication for $M_{m, n}(\R)$ satisfy exactly the same properties as vector addition and scalar multiplication for $\R ^ n$. That is, the operations satisfy the axioms of a (real) vector space.

    \textbf{Axioms for addition}
    \begin{enumerate}[label = (\roman*)]
        \item existence of additive identity: there is a matrix $O_{m \times n}$ in $M_{m, n}(\R)$ so that for all $X$ in $M_{m, n}(\R)$ we have $X + O_{m \times n} = X = O_{m \times n} + X$ 
        \item commutativity: $X + Y = Y + x$ for all $X$ and $Y$ in $M_{m, n}(\R)$
        \item existence of additive inverses: for each $X$ in $M_{m, n}(\R)$ there is an element $-X$ in $M_{m, n}(\R)$ such that $X + (-X) = O_{m \times n} = (-X) + X$
        \item associativity: $(X + Y) + Z = X + (Y + Z)$ for all $X, Y, Z \in M_{m, n}(\R)$
    \end{enumerate}
    (As before this makes $M_{m, n}(\R)$ an abelian group under addition)

    \textbf{Axioms for Scalar Multiplication}
    \begin{enumerate}[label = (\roman*)]
        \item $0X = O_{m \times n}$ for all $X$ in $M_{m, n}(\R)$
        \item $1X = X$ for all $X$ in $M_{m, n}(\R)$
        \item associativity: $(\lambda\mu)X = \lambda(\mu X)$ for all $\lambda, \mu \in \R$ and for all $X \in M_{m, n}(\R)$
        \item distributivity: $(\lambda + \mu)X = \lambda X + \mu X$ and $\lambda (X + Y) = \lambda X + \lambda Y$ for all real $\lambda, \mu$ and for all $X$ and $Y$ in $M_{m, n}(\R)$.
    \end{enumerate}
    \begin{proof}
        Do
    \end{proof}
\end{proposition}

\subsection{Matrix Operations II: Matrix multiplication}
Matrix multiplication: This is a function
\[
M_{m \times n}(\R) \times M_{n \times p}(\R) \rightarrow M_{m \times p}(\R).
\]
If $X = (x_{ij})_{m \times n}$ is $m \times n$ and $Y = (y_{ij})_{n \times p}$ is $n \times p$ (if the number of columns of $X$ equals the number of rows of $Y$) then we may form the product matrix $XY$. This is defined to be the $m \times p$ matrix whose $(i,\,j)$th element $(XY)_{ij}$ is
\[
(XY)_{ij} = x_{i\,1}y_{1\,j} + x_{i\,2}y_{2\,j} + \dotsi + x_{i\,n}x_{n\,j} = \sum_{k = 1}^{n}{x_{ik}y_{kj}}.
\]
If $\mbf{x ^ t}_{i}$ denotes the $i$-th row of $X$ and $\mbf{y}_j$ denotes the $j$-th column of $Y$ then $(XY)_{ij} = \mbf{x}_i \cdot \mbf{y}_j$ (scalar product of column vectors).

\begin{proposition}[Multiplication rules]
    Let $X$ and $X'$ be $m \times n$, $Y$ and $Y'$ be $n \times p$ and $Z$ be $p \times q$ matrices. Let $\lambda$ and $\lambda'$ be scalars. Then
    \begin{enumerate}[label = (\roman*)]
        \item $O_{p \times m}X = O_{p \times n}$ and $XO_{n \times q} = O_{m \times q}$.
        \item $I_{m}X = X = XI_n$.
        \item $\lambda(XY) = (\lambda X)Y = X(\lambda Y)$.
        \item associativity: $(XY)Z = X(YZ)$
        \item distributive rules:
        \begin{enumerate}[label = (\alph*)]
            \item $(X + X')Y = XY + X'Y$ and $X(Y + Y') = XY + XY'$.
            \item $\lambda(X + X') = \lambda X + \lambda X'$ and $(\lambda + \lambda')X = \lambda X + \lambda' X$.
        \end{enumerate}
    \end{enumerate}
    \begin{proof}
        For (iv), the $k$th entry in the $i$th row of $XY$ is given by $\sum_{r = 1}^{n}x_{ir}y_{rk}$, with $k = 1,\dotsc, p$. Hence the $(i,\,j)$th entry of $(XY)Z$ is given by
        \[
        ((XY)Z)_{ij} = (i\text{th row of }XY) \cdot (j\text{th column of } Z) = \sum_{k = 1}^{p}\left(\sum_{r = 1}^{n}x_{ir}y_{rk}\right)z_{kj}.
        \]
        On the other hand, the $r$th entry in the $j$th column of $YZ$ is given by $\sum_{k = 1}^{p}y_{rk}z_{kj}$, with $r = 1, \dotsc, n$. Hence the $(i,\,j)$th entry of $X(YZ)$ is given by
        \[
        (X(YZ))_{ij} = (i\text{th row of } X) \cdot (j\text{th column of } YZ) = \sum_{r = 1}^{n}x_{ir}\left(\sum_{k = 1}^{p}y_{rk}z_{kj}\right).
        \]
        Switching the order of summation using associativity of the usual multiplication given $(XY)Z = X(YZ)$.
        
        \textbf{Prove rest!}
    \end{proof}
\end{proposition}

\textbf{Warning}: We do not necessarily have $XY = YX$ in general.

\textbf{Warning}: It is possible to have $XY = O_{m \times p}$ even though both $X$ and $Y$ are non-zero.

\textbf{A useful trick for sometimes multiplying big matrices more easily}

Let $\begin{pmatrix}
    A & B \\ C & D
\end{pmatrix} \in M_{m, n}(\R)$ and $\begin{pmatrix}
    A' & B' \\ C' & D'
\end{pmatrix} \in M_{n, p}(\R)$ be two matrices given by blocks $A \in M_{m_1, n_1}(\R),\, B \in M_{m_1, n_2}(\R),\, C \in M_{m_2, n_1}(\R),\, D \in M_{m_2, n_2}(\R),\, A' \in M_{n_1, p_1}(\R),\, B' \in M_{n_1, p_2}(\R),\, C' \in M_{n_2, p_1}(\R),\, D' \in M_{n_2, p_2}(\R)$ (so-called block matrices) with $m_1 + m_2 = m$ and $n_1 + n_2 = n$ and $p_1 + p_2 = p$. It can easily shown that
\[
\begin{pmatrix}
    A & B \\ C & D
\end{pmatrix}
\begin{pmatrix}
    A' & B' \\ C' & D'
\end{pmatrix}
=
\begin{pmatrix}
    AA' + BC' & AB' + BD' \\ CA' + DC' & CB' DD'
\end{pmatrix} \in M_{m, p}(\R)
\]
that is, if matrix sizes match correctly the formula for the multiplication of block matrices is the same as one of "usual two by two matrices"

For example, if $A = \begin{pmatrix}
    0 & 1 \\ 0 & 0
\end{pmatrix}$(so that $A ^ 2 = O_{2 \times 2}$) then
\[
\begin{pmatrix}
    1 & 0 & 1 & 0 \\
    0 & 1 & 0 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 0 & 0 & 0
\end{pmatrix} ^ 2
=
\begin{pmatrix}
    I_2 & I_2 \\ A & A
\end{pmatrix} ^ 2
=
\begin{pmatrix}
    I & I \\ A & A
\end{pmatrix}
\begin{pmatrix}
    I & I \\ A & A
\end{pmatrix}
=
\begin{pmatrix}
    I + A & I + A \\ A + A ^ 2 & A + A ^ 2
\end{pmatrix}
=
\begin{pmatrix}
    1 & 1 & 1 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 0 & 0 & 0
\end{pmatrix}
\]

\subsection{Inverse matrix}

\begin{definition}
    Let $A$ be a square $(n\times n)$ matrix. We say $A$ is invertible (or non-singular) if there exists an $n \times n$ matrix $B$ such that
    \[
    AB = BA = I_n.
    \]
    If such a matrix $B$ exists we call $B$ the inverse of $A$ and we write $A ^ {-1}$ for this inverse. (there can only exist at most one inverse).
    If no such inverse $B$ exists for a matrix $A$, we say $A$ is non-invertible (or singular).
\end{definition}

\begin{proposition}[Properties of the inverse and transpose]\label{pre:linalg:prop:propofinvandtransp}
    \begin{enumerate}[label = (\roman*)]
        \item A matrix has at most one inverse.
        \item Assume $A$ and $B$ are invertible $n \times n$ matrices. Then the product $AB$ is invertible and
        \[
        (AB) ^ {-1} = B ^ {-1}A ^ {-1}.
        \]
        \item We have $(AB) ^ t = B ^ t A ^ t$ and $(\lambda A) ^ t = \lambda A ^ t$ for any matrix $A$ and any real constant $\lambda$.
        \item Assume $A$ is invertible. then the transpose $A ^ t$ is invertible and
        \[
        (A ^ t) ^ {-1} = (A ^ {-1}) ^ t.
        \]
        \item Let $A \in M_{n, n}(\R)$ and assume $BA = I_n$ for some $B \in M_{n, n}(\R)$. Then $AB = I_n$ as well as vice versa. So the "left inverse" is automatically the "right inverse".
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
            \item If $B'$ were a second inverse for $A$ then we would have
            \[
            B' = B'I_n = B'(AB) = (B'A)B = I_nB = B.
            \]
        \end{enumerate}
    \end{proof}
\end{proposition}

\begin{remark}
    We also have
    \[
    (X + Y) ^ t = X ^ t + Y ^ t,
    \]
    but it is not the case in general that
    \[
    (X + Y) ^ {-1} = X ^ {-1} + Y ^ {-1}.
    \]
\end{remark}

\newpage

\section{Gauss-Jordan elimination}

\subsection{Solving equations and Gauss-Jordan elimination}
We will turn to solving the general problem of solving $m$ linear equations in $n$ variables. To handle this, we need a systematic approach.

We begin by setting up our notation. We shall denote our variables by $x_1,\,x_2,\dotsc,x_n$, and consider the solution of the system of $m$ linear equations
\begin{align*}
    a_{1 1}x_{1} &+ a_{1 2}x_2 &+ \dotsi &+ a_{1 n}x_n &= b_1; \\ 
    a_{2 1}x_{1} &+ a_{2 2}x_2 &+ \dotsi &+ a_{2 n}x_n &= b_2; \\ 
    \vdots & \vdots & \ddots & \vdots &\vdots \vdots; \\ 
    a_{m 1}x_{1} &+ a_{m 2}x_2 &+ \dotsi &+ a_{m n}x_n &= b_m.
\end{align*}
For short we can write
\[
\sum_{j = 1}^{n}a_{ij}x_{j} = b_{i}\quad\text{for } i = 1,\dotsc,m.
\]
In vector notation we can write this using the dot product
\[
\mbf{a}_i \cdot \mbf{x} = \begin{pmatrix}
    a_{i 1} \\ \vdots \\ a_{i n}
\end{pmatrix}
\cdot
\begin{pmatrix}
    x_1 \\ \vdots \\ x_n
\end{pmatrix}
=
b_i\quad\text{for } i = 1, \dotsc, m.
\]
Each equation gives a so-called hyperplane with normal vector $\mbf{a}_i$.

To solve the equations we shall go through the following process.
\begin{itemize}
    \item First formalise the system of equations as a single equation of matrices, this is for notational convenience and systematises our process.
    \item Perform series of matrix operations (elementary row operations (ERO's)) that in effect change the original set of equations into a different set of linear equations, but ones that have the same solution set and are much easier to solve. This process is called Gauss-Jordan elimination, and the sort of matrix that represents the equations that are easier to solve is said to be in row reduced echelon form (RREF).
\end{itemize}

The first step we have essentially already done.
Using the notation from before,
the system of equations can be written as the matrix equation
\[
A\mbf{x} = \mbf{b}
\]
where $A$ is the matrix
\[
\begin{pmatrix}
    a_{1 1} & a_{1 2} & \dotsi & a_{1 n} \\
    a_{2 1} & a_{2 2} & \dotsi & a_{2 n} \\
    \vdots & \vdots & \phantom{} & \vdots \\
    a_{m 1} & a_{m 2} \dotsi a_{m n}
\end{pmatrix}
\]
and $\mbf{b}$ is the column vector
\[
\begin{pmatrix}
    b_1 \\ b_2 \\ \vdots \\ b_m
\end{pmatrix}.
\]

The augmented matrix associated with a linear system of equations is a rectangular array of numbers,
the coefficients of each equation are placed in order in each row,
followed by a vertical bar where the equal signs are with the $a_{i j}$ to the left,
and then the $b_i$ recorded to the right.
In other words
\[
\left(
\begin{array}{cccc|c}
    a_{1 1} & a_{1 2} & \dotsi & a_{1 n} & b_1 \\
    a_{2 1} & a_{2 2} & \dotsi & a_{2 n} & b_2 \\
    \vdots & \vdots & \phantom{} & \vdots & \vdots \\
    a_{m 1} & a_{m 2} & \dotsi & a_{m n} & b_m
\end{array}
\right).
\]
For example, the system
\begin{align*}
    3x_1 - 4x_2 + 3x_3 &= 1 \\
    2x_1 - x_2 + 2x_3 &= 2
\end{align*}
has augmented matrix $\left(
\begin{array}{ccc|c}
    3 & -4 & 3 & 1 \\
    2 & -1 & 2 & 2
\end{array}
\right).$

If as before we write the matrix
\[
\left(
\begin{array}{cccc|c}
    a_{1 1} & a_{1 2} & \dotsi & a_{1 n} & b_1 \\
    a_{2 1} & a_{2 2} & \dotsi & a_{2 n} & b_2 \\
    \vdots & \vdots & \phantom{} & \vdots & \vdots \\
    a_{m 1} & a_{m 2} & \dotsi & a_{m n} & b_m
\end{array}
\right),
\]
which we shall sometimes call the coefficient matrix of the linear system,
and as above write
\[
\mbf{b} = \begin{pmatrix}
    b_1 \\ \vdots \\ b_m
\end{pmatrix}
\quad\text{and}\quad\mbf{x} = \begin{pmatrix}
    x_1 \\ \vdots \\ x_n
\end{pmatrix},
\]
then the augmented matrix cab we written as $(A | \mbf{b})$.

We call a linear system homogenous if $\mbf{b} = \mbf{0}$,
and inhomogeneous if $\mbf{b} \neq \mbf{0}$.
Note that a homogeneous system always has at least one solution: $\mbf{x} =\mbf{0}$.

\begin{lemma}\label{linalg:lem:invprop}\phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item Let $A \in M_{n,\,n}(\R)$ be a square matrix and let $\mbf{v} \in \R ^ n$ be a non-zero vector such that $A\mbf{v} = \mbf{0}$. Then $A$ is singular.
        \item Let $A \in M_{n,\,n}(\R)$ be an invertible square matrix. Then the inhomogeneous system $A\mbf{x} = \mbf{b}$ has a single unique solution. In particular, the only solution to the homogeneous system $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$.
    \end{enumerate}
    \begin{proof}
        For (i), see example 2.4.2(iii) for an easily generalisable argument. For (ii), if $A$ is invertible then
        \[
        \mbf{x} = I_n\mbf{x} = (A ^ {-1} A)\mbf{x} = A ^ {-1}(A\mbf{x}) = A ^ {-1}\mbf{b}.
        \]
    \end{proof}
\end{lemma}

\subsection{Echelon form and its application to linear systems}

\begin{definition}
    A matrix of numbers is said to be in row reduced echelon form (RREF) if
    \begin{enumerate}[label = (\roman*)]
        \item The first (leftmost) non-zero entry in any non-zero row is a $1$ (called its leading $1$).
        \item If a row has its leading $1$ in the $j$th column then
        \begin{enumerate}[label = (\alph*)]
            \item all the other entries in the $j$th column are $0$; and
            \item the leading $1$s of subsequent rows are in columns to the right of the $j$th column.
        \end{enumerate}
    \item Any row(s) of zeros come after all the rows with non-zero entries.
    \end{enumerate}
\end{definition}

For example, the matrices
\[
\begin{pmatrix}
    1 & 4 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0
\end{pmatrix},
\qquad
\begin{pmatrix}
    0 & 1 & 0 & 0 & 2 \\ 0 & 0 & 1 & 0 & 3 \\ 0 & 0 & 0 & 1 & 4
\end{pmatrix}
\qquad\text{and}
\qquad
\begin{pmatrix}
    1 & 0 & 2 & 0 \\
    0 & 1 & 3 & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & 0 & 0
\end{pmatrix}
\]
are all in RREF.

In general, an RREF matrix looks as follows:
\[
\left(
\begin{array}{cccccccccccccccc}
    1 & * & \dotsi & * & 0 & * & \dotsi & * & 0 & * & \dotsi & * & 0 & * & \dotsi & * \\
    0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * & 0 & * & \dotsi & * & 0 & * & \dotsi & * \\
    0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * & 0 & * & \dotsi & * \\
    \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots \\
    0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & 1 & * & \dotsi & *
\end{array}
\right)
\]

\begin{example}
    Consider these three RREF's for a non-zero $2 \times 2$ matrix.
    \[
    (a): \begin{pmatrix}
        1 & \alpha \\ 0 & 0
    \end{pmatrix},\qquad
    (b): \begin{pmatrix}
        1 & 0 \\ 0 & 1
    \end{pmatrix},
    \qquad
    (c): \begin{pmatrix}
        0 & 1 \\ 0 & 0
    \end{pmatrix}.
    \]
    Here $\alpha$ is some scalar. Viewing these as the coefficient matrix for a linear system we have the following augmented matrices:
    \[
    (a): \begin{amatrix}{2}
        1 & \alpha & b_1 \\ 0 & 0 & b_2
    \end{amatrix},
    \qquad
    (b): \begin{amatrix}{2}
        1 & 0 & b_1 \\ 0 & 1 & b_2
    \end{amatrix},
    \qquad
    (c): \begin{amatrix}{2}
        0 & 1 & b_1 \\ 0 & 0 & b_2
    \end{amatrix}.
    \]
\end{example}

For $(a)$ and $(c)$ the correspond system has clearly only a solution if $b_2 = 0$ (because the second equation now reads $0x_1 + 0x_2 = 1$).
In these cases we obtain for the solution sets
\[
(a): x_1 = b_1 - \alpha x_2,\quad (b): x_1 = b_1,\,x_2 = b_2,\quad (c): x_2 = b_1
\]
note for $(b)$ we have only have a unique solution, while for $(a)$ and $(c)$ the solution set is a line.
More precisely, it is a non-horizontal line for $(a)$, and a horizontal one for $(c)$.

\subsection{The Elementary Row Operations and the big theorem}
The ERO's are simple operations you can do to a matrix to turn it into another matrix.
There are three fundamental operations, which are as follows,
\begin{enumerate}[label = (\alph*)]
    \item $P_{rs}$: switch row $r$ with row $s$;
    \item $M_{r}(\pmb{\lambda})$: multiple (all the entries in) row $r$ by the number $\lambda \neq 0$;
    \item $A_{rs}$: add (each entry in) row $r$ to (the corresponding entry in) row $s$.
    \item[(c')] $A_{rs}(\pmb{\lambda})$: add $\lambda$ times (each entry in) row $r$ to (the corresponding entry in) row $s$.
\end{enumerate}

Note that $A_{rs}(\lambda) = M_r\left(\frac{1}{\lambda}\right) \circ A_{rs} \circ M_r(\lambda)$,
read from right to left (in the same way as how $f \circ g$ means do $g$ then $f$).
\begin{example}
    \begin{align*}
    \begin{pmatrix}
        1 & 3 & 2 \\
        2 & 1 & -1 \\
        -1 & 1 & 2
    \end{pmatrix}
    &\xrightarrow{
    \begin{subarray}{c}
        A_{12}(-2) \\ A_{13}(1)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 3 & 2 \\
        0 & -5 & -5 \\
        0 & 4 & 4
    \end{pmatrix}
    &\xrightarrow{
    \begin{subarray}{c}
        M_2(-1 / 5) \\ M_3(1 / 4)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 3 & 2 \\
        0 & 1 & 1 \\
        0 & 1 & 1
    \end{pmatrix} 
    \\
    &\xrightarrow{
    \begin{subarray}{c}
        A_{23}(-1)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 3 & 2 \\
        0 & 1 & 1 \\
        0 & 0 & 0
    \end{pmatrix}
    &\xrightarrow{
    \begin{subarray}{c}
        A_{21}(-3)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 0 & -1 \\
        0 & 1 & 1 \\
        0 & 0 & 0
    \end{pmatrix}
    \end{align*}
\end{example}

\subsubsection{The basic routine}
\begin{enumerate}[label = (\alph*)]
    \item Find the first non-zero column (from the left).
    \item Go down the first non-zero column and find the first non-zero entry.
    \item If this is in the $r$-th row with $r \neq 1$ apply the elementary row operation $P_{1 r}$ to get a non-zero entry, $\lambda$ say, at the top of the first non-zero column.
    \item Apply the elementary row operation $M_1(1 / \lambda)$ to make the first non-zero entry of the first non-zero column $a$ $1$.
    \item Using elementary row operation of the form $A_{1 i}(\mu)$ clear all other non-zero entries in the first non-zero column.
\end{enumerate}

The resulting matrix is of the form
\[
\begin{pmatrix}
    1 & * & \dotsi & * \\
    0 & * & \dotsi & * \\
    \vdots & \vdots & \phantom{} & \vdots \\
    0 & * & \dotsi & *
\end{pmatrix}
\]
or, if the first non-zero column is not the first one,
\[
\begin{pmatrix}
    0 & \dotsi & 0 & 1 & * & \dotsi & * \\
    0 & \dotsi & 0 & 0 & * & \dotsi & * \\
    \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots \\
    0 & \dotsi & 0 & 0 & * & \dotsi & * \\
\end{pmatrix}
\]

\subsubsection{The algorithm}
Let $A$ be given $m \times n$ matrix.
The process of Gauss-Jordan elimination applied to $A$ is described by repeatedly applying the Basic Routine in stages.
(For simplicity of description we will assume that the first column of $A$ is non-zero).

\textbf{Stage 1}

Apply the basic routine to the given $m \times n$ matrix $A$.
This gives a new $m \times n$ matrix $A_1$ of the form:
\[
A_1 =
\begin{pmatrix}
    1 & * & \dotsi & * \\
    0 & * & \dotsi & * \\
    \vdots & \vdots & \phantom{} & \vdots \\
    0 & * & \dotsi & *
\end{pmatrix}
\]

\textbf{Stage 2}

If all of the remaining rows (below row $1$) are zero, it is easy to see that the matrix is now in RREF.
Otherwise:
\begin{enumerate}[label = (\roman*).]
    \item Apply the basic routine to the submatrix of $A_1$ obtained by ignoring the first row.
    This gives a new $m \times n$ matrix $\tilde{A}_2$ of the form:
    \[
    \begin{pmatrix}
        0 & \dotsi & 0 & 1 & * & \dotsi & * \\
        0 & \dotsi & 0 & 0 & * & \dotsi & * \\
        \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots \\
        0 & \dotsi & 0 & 0 & * & \dotsi & *
    \end{pmatrix}
    \]
    \item Apply an elementary row operation of the form $A_{2 1}(\mu)$,
    if necessary, to $\tilde{A}_2$ to make the entry above the first $1$ in the second row a zero.
    This gives a new $m \times n$ matrix $A_2$ of the form:
    \[
    A_2 = 
    \begin{pmatrix}
        1 & * & \dotsi & * & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & * & \dotsi & *
    \end{pmatrix}.
    \]
\end{enumerate}
Now continue inductively.
Assume stages $1$ to $k - 1$ have been completed (for $k - 1 < m$),
resulting in a matrix $A_{k - 1}$.

\textbf{Stage k (with $k \leq m$)}

If rows $k, k + 1, \dotsc, m$ of $A_{k - 1}$ are all zero then this matrix is in RREF and the algorithm terminates.
Similarly, if the leading $1$ in the $(k - 1)$th row of $A_{k - 1}$ lies in the $n$th column then it is easy to check that $A_{k - 1}$ is in RREF.
Otherwise:
\begin{enumerate}[label = (\roman*).]
    \item Apply the basic routine to the submatrix of $A_{k - 1}$ obtained by ignoring the first $k - 1$ rows.
    This gives a new $m \times n$ matrix $\tilde{A}_k$ of the form:
    \[
    \tilde{A}_k = \begin{pmatrix}
        1 & 0 & \dotsi & * & 0 & * & \dotsi & * & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & * & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 1 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & *
    \end{pmatrix}.
    \]
    \item Apply elementary row operations of the form $A_{k r}(\mu)\ (\text{for } r = 1, 2, \dotsc, k - 1)$,
    if necessary, to $\tilde{A}_k$; this ensures all remaining non-zero entries in the column containing the first $1$ in the $k$-th row equal to zero and gives a new $m \times n$ matrix $A_k$ of the form:
    \[
    \tilde{A}_k = \begin{pmatrix}
        1 & 0 & \dotsi & * & 0 & * & \dotsi & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 1 & * & \dotsi & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 1 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & *
    \end{pmatrix}.
    \]
    By induction, this process must terminate either before Stage $m$ or after Stage $m$.
    In the former case,
    the reason for early termination must have been that the matrix had reached RREF.
    In the latter case we must have reached a matrix $A_m$ with a leading $1$ on every row.
    It is then easy to check that $A_m$ is in RREF.
\end{enumerate}

\subsection{Using G-J to solve a system of linear equations}
Recall, we are considering the solution set of a general system of $m$ linear equations in $n$ variables.

Two such systems are said to be equivalent if they have the same solution set.
We say an augmented matrix is in row reduced echelon form if the associated coefficient matrix is in row reduced echelon form.
Applying the elementary row operations to the augmented matrix $(A | \mbf{b})$ allows us to pass to equivalent systems and eventually to a system for which it is easy to spot the solutions.
This is justified by
\begin{theorem}\label{pre:linalg:thm:lintoech}
    Applying ERO's to the augmented matrix of a linear system simply changes the system to an equivalent one; that is, the solution set remains unchanged.
\end{theorem}

If a row is of the form $(0\ \dotsi\ \,|\, k)$ for $k \neq 0$ this would mean that there are no solutions,
we call a system like this inconsistent.

A quick way to solve a linear system,
there is no need to run G-J fully.
One can do the following:
\begin{enumerate}[label = \arabic*) :]
    \item Perform ERO's to bring coefficient matrix $A$ into upper triangular form,
    that is, all the entries below the diagonal vanish.
    (Also not necessary to scale the "leading ones" to be actually equal to $1$.)
    \item Then go back to the linear system and use back substitution from bottom to top to solve the system.
\end{enumerate}

\begin{example}
    \[
    \sysdelim..\systeme{
        x_1 + 2x_2 + 3x_3 = 1,
        2x_1 + 3x_2 + 2x_3 = 1,
        3x_1 + 2x_2 + x_3 = 1.
        }
    \]
    We put this into matrix form then EROs to get it into upper triangular form
    \begin{align*}
        &\begin{amatrix}{3}
            1 & 2 & 3 & 1 \\
            2 & 3 & 2 & 1 \\
            3 & 2 & 1 & 1
        \end{amatrix}
        &\xrightarrow{\begin{subarray}{c}
        A_{12}(-2) \\ A_{13}(-3)
        \end{subarray}}
        &\begin{amatrix}{3}
            1 & 2 & 3 & 1 \\
            0 & -1 & -4 & -1 \\
            0 & -4 & -8 & -2
        \end{amatrix}
        \\
        \xrightarrow{A_{23}(-4)}
        &\begin{amatrix}{3}
            1 & 2 & 3 & 1 \\
            0 & -1 & -4 & -1 \\
            0 & 0 & 8 & 2
        \end{amatrix}.
    \end{align*}
    Putting this back into our equations gives us:
    
    $3$rd equation: $x_3 = \frac{1}{4}$;
    
    $2$nd equation: $x_2 = -4x_3 + 1 = -4\frac{1}{4} + 1 = 0$;
    
    $1$st equation: $x_1 = 1 - 2x_2 - 3x_3 = 1 - \frac{3}{4} = \frac{1}{4}$, i.e.,
    \[
    \begin{pmatrix}
        \frac{1}{4} \\
        0 \\
        \frac{1}{4}
    \end{pmatrix}.
    \]
\end{example}

\subsection{Inverse matrices and linear systems}
\begin{lemma}\label{pre:linalg:lem:solsetsam}
    Consider an arbitrary $m \times n$ linear system $A\mbf{x} = \mbf{b}$.
    Let $B \in M_{m \times m}(\R)$ be an invertible matrix.
    Then the new system $(BA)\mbf{x} = B\mbf{b}$ has the same solution set as the original system.
    \begin{proof}
        We need to show that the two systems have the same solution set.
        First assume that $\mbf{v}$ is a solution to the system $A\mbf{x} = \mbf{b}$, assume $A\mbf{v} = \mbf{b}$.
        Then, $B\mbf{b} = B(A\mbf{v}) = (BA)\mbf{v}$ as required.
        Conversely, assume $(BA)\mbf{v} = B\mbf{b}$. Then
        \[
        \mbf{b} = I_m\mbf{b} = (B ^ {-1}B)\mbf{b} = B ^ {-1}(B\mbf{b}) = B ^ {-1}(BA\mbf{v}) = (B ^ {-1} B) A\mbf{v} = A\mbf{v},
        \]
        as required.
    \end{proof}
\end{lemma}

\begin{theorem}[continues = pre:linalg:thm:lintoech]
    \begin{proof}
        Suppose we want to perform elementary row operations on a matrix with $m$ rows.
        Then we can do this using the following $m \times m$ matrices.
        \begin{enumerate}[label = (\alph*)]
            \item The row operation $P_{rs}$ can be performed by left multiplication by the $m \times m$ matrix,
            which we also call $P_{rs}$,
            obtained from the identity by swapping the $r$th and $s$th columns.
            That is $(p_{ij})_{m \times m}$ with $p_{ii} = 1$ when $i \neq r, s, p_{rr} = p_{ss} = 0; p_{ij} = 0$ when $i \neq j$ and $\{i, j\} \neq \{r, s\}, p_{rs} = p_{sr} = 1$. That is,
            \[
            P_{rs} = \begin{pmatrix}
                1 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                0 & 1 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 1 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 1 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 1
            \end{pmatrix}.
            \]
            One can check that we have $P_{rs} ^ {-1} = P_{rs}$ (swapping twice leaves you back where you started).
            \item
            The row operation $M_r(\lambda)$ can be performed by left multiplication by the $m \times m$ matrix,
            which we also call $M_r(\lambda)$,
            obtained from the identity by replacing the $r$th diagonal entry with $\lambda$.
            That is $(m_{ij})_{m \times m}$ with $m_{ii} = 1$ when $i \neq r$ and $m_{rr} = \lambda$; $m_{ij} = 0$ when $i \neq j$.
            That is,
            \[
            M_r(\lambda) = \begin{pmatrix}
                1 & 0 & \dotsi & 0 & \dotsi & 0 \\
                0 & 1 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & \lambda & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 1
            \end{pmatrix}.
            \]
            We have $M_r(\lambda) ^ {-1} = M_r\left(\frac{1}{\lambda}\right)$.
            \item The row operation $A_{rs}(\lambda)$ has be performed by left multiplication by the $m \times m$ matrix,
            which we also call $A_{rs}(\lambda)$,
            obtained from the identity matrix by replacing the entry in the $r$th column and $s$th row with $\lambda$.
            That is $(a_{ij})_{m \times m}$ where $a_{ii} = 1; a_{ij} = 0$ when $i \neq j$ and $(i, j) \neq (s, r);\; a_{sr} = \lambda$.
            That is,
            \[
            \begin{pmatrix}
                1 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                0 & 1 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 1 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & \lambda & \dotsi & 1 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 1
            \end{pmatrix}.
            \]
            We have $A_{rs}(\lambda) ^ {-1} = A_{rs}(-\lambda)$.
        \end{enumerate}
        We call any of these matrices an elementary matrix.
        Then if $E$ is an elementary $m \times m$ matrix the corresponding row operation is given by $EA\mbf{x} = E\mbf{b}$.
    \end{proof}
\end{theorem}

\begin{remark}
    Note that every elementary matrix is not only invertible,
    but the inverse of any elementary matrix is also an elementary matrix.
\end{remark}

\subsection{The fundamental theorem of invertible matrices}
\begin{lemma}\label{linalg:lem:sqmatrrefcond}
    Let $A \in M_{n \times n}(\R)$ be a square matrix.
    Then the following are (pairwise) equivalent,
    that is they are either all true or all false.
    \begin{enumerate}[label = (\roman*)]
        \item In each column of RREF of $A$ there exists a leading $1$.
        \item The RREF of $A$ is the identity matrix $I_n = \begin{pmatrix}
            1 & \phantom{} & \phantom{} \\
            \phantom{} & \ddots & \phantom{} \\
            \phantom{} & \phantom{} & 1
        \end{pmatrix}$.
        \item The only solution to the system $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$.
    \end{enumerate}
    \begin{proof}
        The equivalence of (i) and (ii) is clear:
        if the RREF is the identity then each column has a leading $1$.
        Conversely, if each column of the RREF has a leading $1$,
        then the first leading $1$ is in the $1, 1$-entry,
        the second must be in the $2, 2$-entry and so on.
        Then all the off-diagonal entries must be zero,
        that is, the RREF is indeed $I_n$.

        Next we show that (ii) implies (iii).
        So let's assume (ii).
        But by \autoref{pre:linalg:lem:solsetsam} we can replace the equation $A\mbf{x} = \mbf{0}$ with the RREF of $A$,
        that is, we can consider $I_n\mbf{x} = \mbf{0}$,
        which means $\mbf{x} = \mbf{0}$.
        So (iii) holds.

        What remains is to show that (iii) implies (i).
        So let's assume (iii), that is,
        the only solution to the system $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$.
        Let's assume for a contradiction that the number of leading $1$'s in the RREF of $A$ is equal to $r < n$.
        This means that there are $n - r > 0$ free variables (one for each column without a leading $1$).
        To be explicit, after permutation of columns we can assume the RREF of $A$ looks as follows
        \[
        \begin{pmatrix}
            I_r & A'_{r, n - r} \\
            \mbf{0}_{n - r, r} & \mbf{0}_{n - r, n - r}
        \end{pmatrix}
        \]
        for some $r \times (n - r)$ matrix $A'$.
        But then any choice of real value for each of the free variables will yield a solution t the system $A\mbf{x} = \mbf{0}$.
        In particular there exists a non-trivial solution to the system.
        This is a contradiction, therefore there must have been $r = n$ leading $1$'s after all.
    \end{proof}
\end{lemma}

\begin{theorem}[Fundamental Theorem of Invertible Matrices]\label{linalg:thm:fundthmofinvertmatr}
    A square matrix $A \in M_{n \times n}(\R)$ is invertible if and only if any one of the conditions (i), (ii), (iii) of \autoref{linalg:lem:sqmatrrefcond} holds.
    \begin{proof}
        Suppose $A$ is invertible.
        Then by \autoref{linalg:lem:invprop}(ii) the only solution of $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$,
        which is \autoref{linalg:lem:sqmatrrefcond}(iii).

        Now assume that any one of (i), (ii), and (iii) hold.
        By (ii), there exist elementary matrices $E_1, E_2, \dotsc, E_\ell$ such that
        \[
        E_\ell E_{\ell - 1} \dotsi E_1 A = I_n.
        \]
        Thus $(E_\ell E_{\ell - 1} \dotsi E_1)$ is a left sided inverse of $A$.
        We must show it is a right sided inverse too.
        To do this, note that
        \begin{align*}
            AE_\ell E_{\ell - 1} \dotsi E_1 &= I_n A E_\ell E_{\ell - 1} \dotsi E_1 \\
            &= (E_1 ^ {-1} E_2 ^ {-1} \dotsi E_{\ell} ^ {-1}E_\ell E_{\ell - 1} \dotsi E_1)AE_\ell E_{\ell - 1} \dotsi E_1 \\
            &= E_1 ^ {-1} E_2 ^ {-1} \dotsi E_{\ell} ^ {-1}(E_\ell E_{\ell - 1} \dotsi E_1A)E_\ell E_{\ell - 1} \dotsi E_1 \\
            &= E_1 ^ {-1} E_2 ^ {-1} \dotsi E_{\ell} ^ {-1}E_\ell E_{\ell - 1} \dotsi E_1 = I_n.
        \end{align*}
        So $A$ is invertible with $A ^ {-1} = E_\ell E_{\ell - 1} \dotsi E_1$.
    \end{proof}
\end{theorem}
This proof immediately gives us
\begin{corollary}\label{pre:linalg:col:invisprodofele}
    Any invertible square matrix is the product of elementary matrices (and vice versa).
    \begin{proof}
        Simply notice that $E_1 ^ {-1}E_2 ^ {-1} \dotsi E_i ^ {-1}$ is an inverse of $A ^ {-1} = E_1E_2 \dotsi E_i$.
        But, so is the matrix $A$.
        So, by uniqueness of the inverses,
        we must have $A = E_1 ^ {-1} E_2 ^ {-1} \dotsi E_\ell ^ {-1}$.
    \end{proof}
\end{corollary}

\begin{remark}
    Note that the product above is not necessarily unique.
\end{remark}

\begin{remark}
    We can also now show that the "left inverse" of a square matrix $A$ is also the "right inverse" of $A$,
    that is if $BA = I_n$, then also $AB = I_n$
\end{remark}

\begin{proposition}[continues = pre:linalg:prop:propofinvandtransp]
    \begin{proof}[Proof of (v)]
        Suppose that $BA = I_n$ and consider $A\mbf{x} = \mbf{0}$.
        Left multiplication by $B$ shows that the only solution is $\mbf{x} = \mbf{0}$.
        Hence $A$ is invertible by \autoref{linalg:thm:fundthmofinvertmatr}.
        In particular, $A$ has an inverse $A ^ {-1}$ and we have
        \[
        AB = ABAA ^ {-1} = A(BA)A ^ {-1} = AA ^ {-1} = I_n.
        \]
        By the uniqueness of inverses, this shows that $A ^ {-1} = B$.
    \end{proof}
\end{proposition}

\subsection{An algorithm to compute the inverse of a matrix}
We run G-J for the matrix $A$ and keep track of the elementary operations/matrices we used.
Concretely, we run G-J on the augmented or double matrix
\[
(A \,|\, I_n) =
\left(\hspace{-5pt}
\begin{array}{cccc|cccc}
     a_{1 1} & a_{1 2} & \dotsi & a_{1 n} & 1 & 0 & \dotsi & 0 \\
     a_{2 1} & a_{2 2} & \dotsi & a_{2 n} & 0 & 1 & \dotsi & 0 \\
     \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \ddots & \vdots \\
     a_{n 1} & a_{n 2} & \dotsi & a_{n n} & 0 & 0 & \dotsi & 1
\end{array}
\hspace{-5pt}\right).
\]
Now perform G-J as always and get (if $A$ is invertible)
\[
(I_n\,|\,E_\ell \dotsi E_1),
\]
and $A ^ {-1} = E_\ell \dotsi E_1$.
If $A$ is singular,
you will not obtain the identity matrix on the left hand side.

\newpage

\section{Determinants}
The determinant of a square matrix is a scalar.
The determinant of a matrix $A$ with real entries is a real number.
So we can think of this as a function
\[
\det : M_{n \times n}(\R) \rightarrow \R.
\]

\subsection{Small values of \texorpdfstring{$n$}{}}
We look at particular cases of the function
\[
\det : M_{n\times n}(\R) \rightarrow \R.
\]
$n = 1$:
The equation $ax = b$ has a unique solution if and only if $a \neq 0$.
We set
\[
\det(a) = a.
\]

$n = 2$:
We know $\begin{pmatrix}
    a & b \\ c & d
\end{pmatrix}$
is invertible if and only if $ad - bc \neq 0$.
So, set
\[
\det\begin{pmatrix}
    a & b \\ c & d
\end{pmatrix} = ad - bc.
\]
Then the matrix equation (system of two linear equations)

has a unique solution if and only if $\det\begin{pmatrix}
    a & b \\ c & d
\end{pmatrix} \neq 0$

\subsection{General definition of determinant}
We define the determinant of an $n \times n$ matrix inductively in terms of the determinants of certain $(n - 1)\times (n - 1)$ matrices.
Let $A = (a_{ij})_{n \times n}$
\begin{itemize}
    \item For integers $r$ and $s$ between $1$ and $n$ write $A_{r, s}$ for the $(n - 1)\times (n - 1)$ matrix obtained from $A$ be deleting its $r$th row and its $s$th column.
    Then $A_{r, s}$ is called the $(r, s)$th minor
    \footnote{The minor is called the 'unsigned cofactor'.
    We have defined the minor as simply a submatrix.}
    of $A$.
    \item The value of $\det(A_{r, s})$ i.e. the determinant of the $(r, s)$th minor of $A$,
    is called the $(r, s)$th unsigned cofactor of $A$ and the value $(-1) ^ {r + s}\det(A_{r, s})$ is called the $(r, s)$th (signed) cofactor of $A$
    \footnote{Some texts write 'adjunct' instead of 'cofactor'.}
    .
\end{itemize}

For any matrix $A \in M_{n \times n}(\R)$ the determinant of $A$ is the defined inductively via expansion by the first row,
in terms of the signed cofactors:
\[
\det A = \sum_{k = 1}^{n}(-1) ^ {1 + k}a_{1k}\det(A_1, k).
\]

An important observation is that we could have defined it via expansion by any given row.
\begin{lemma}\label{pre:linalg:lem:detanyrow}
    If $A$ is an $n \times n$ matrix then for any $1 \leq i \leq n$ we have
    \[
    \det(A) = \sum_{k = 1}^{n}(-1) ^ {i + k}a_{ik}\det(A_i, k).
    \]
    That is,
    the determinant formula remains valid via expansion along the $i$th row
    (note however the signs).
    \begin{proof}
        The proof is a very long (and involved) inductive one so$\dotsc$
    \end{proof}
\end{lemma}

\subsection{Fundamental properties}
In this section,
for notational convenience,
we write in terms of the row vector $\mbf{a}_r$ of an $n \times n$ matrix $A = (a_{ij})$ that is $\mbf{a}_r = (a_{r1}\; a_{r2}\; \dotsi \; a_{rn})$
\begin{enumerate}[label = (\roman*)]
    \item $\det I_n = 1$.
    \item If one multiplies
    (all the elements in)
    the $r$th row by some scalar $\lambda$
    (that is, if we apply the ERO $M_r(\lambda)$),
    then the determinant changes by precisely this factor:
    \[
    \det(M_r(\lambda)A) = \det\begin{pmatrix}
        \mbf{a} \\ \vdots \\ \lambda\mbf{a}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix} = \lambda\det A.
    \]
    \item For any row vector $\mbf{b}_r$ we have
    \[
    \det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_r + \mbf{b}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix} =
    \det \begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}
    +
    \det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{b}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}.
    \]
    Note that (ii) and (iii) can be summarised as linearity in each row.
    \item Anti-symmetry in rows.
    The determinant switches signs if one swaps a pair of rows (i.e. if one applies the ERO $P_{rs}$);
    that is,
    \[
    \det(A) = \det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}
    = -\det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}
    =
    -\det(P_{rs}A).
    \]
    \item $\det A = 0$ if $A$ has two rows that are equal
    (or even two rows that are collinear - follows simply by property (ii)).
    \item $\det A = 0$ if $A$ has a row of zeros.
    \item The determinant does not change under the ERO $A_{rs}(\lambda)$; that is,
    \[
    \det(A_{rs}(\lambda)A) = \det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s + \lambda\mbf{a}_r \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}
    =
    \det\begin{pmatrix}
        \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_n
    \end{pmatrix}
    = \det A.
    \]
    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
            \item We perform induction on $n$.
            The case $n = 1$ is obvious:
            $\det(1) = 1$.
            So now assume the claim holds for some $n \geq 1$,
            that $\det I_n = 1$.
            We need to establish the claim for $n + 1$.
            But now by the definition of the determinant
            (say, by expanding by the first row)
            we immediately see $\det(I_{n + 1}) = 1\cdot\det(I_n) = 1 \cdot 1 = 1$,
            as desired.
            \item This is straightforward,
            by expansion along the $r$th row (by \autoref{pre:linalg:lem:detanyrow}):
            \[
            \det(M_r(\lambda)A) = \sum_{k = 1}^{n}(-1) ^ {r + k}(\lambda a_{rk})\det(A_r, k) = \lambda\sum_{k = 1}^{n}(-1) ^ {r + k}a_{rk}\det(A_{r, k}) = \lambda\det A.
            \]
            \item is similar to (ii),
            by expansion along the $r$th row.
            \item \phantom{}
            \item This follows directly from (iv):
            Assume row $r$ is repeated in row $s$ then
            \[
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r + \mbf{b}_r \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            =
            -\det\left(P_{rs}\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r + \mbf{b}_r \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}\right)
            =
            -\det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r + \mbf{b}_r \\ \vdots \\ \mbf{a}_n
            \end{pmatrix},
            \]
            and so the value of the determinant must be zero.
            \item This follows directly from (iii) by writing $\mbf{0} = \mbf{a}_r - \mbf{a}_r$ (or from (ii) by writing $\mbf{0} = 0 \cdot \mbf{a}_r$ for some vector $\mbf{a}_r$).
            Indeed,
            if the row of zeros appears in row $r$ then we have
            \[
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_{r - 1} \\ \mbf{0} \\ \mbf{a}_{r + 1} \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            =
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_{r - 1} \\ \mbf{a}_r - \mbf{a}_r \\ \mbf{a}_{r + 1} \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            =
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_{r - 1} \\ \mbf{a}_r \\ \mbf{a}_{r + 1} \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            -
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_{r - 1} \\ \mbf{a}_r \\ \mbf{a}_{r + 1} \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            = 0.
            \]
            \item This follows from (ii), (iii), and (iv):
            \[
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s + \lambda \mbf{a}_r \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            =
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            +
            \lambda
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}
            =
            \det\begin{pmatrix}
                \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_n
            \end{pmatrix}.
            \]
        \end{enumerate}
    \end{proof}
\end{enumerate}

\subsection{Uniqueness of the determinant}
We will now prove that properties (i)-(vii) completely characterise the determinant.
We could define the function
\[
\det : M_{n \times n}(\R) \rightarrow \R
\]
as the unique function that satisfies these four properties.
\begin{theorem}\label{pre:linalg:thm:uniquedet}
    Let $f: M_n(\R) \rightarrow \R$ be a function that satisfies properties (i)-(iv).
    Then
    \[
    f(A) = \det(A).
    \]
    In other words,
    the determinant is the only function with the properties (i)-(vii).
    \begin{proof}
        Apply Gauss-Jordan to the matrix $A$ to get the row reduced echelon form $A'$.
        As $\det$ satisfies properties (ii), (iv) and (vii) and we are assuming $f$ does too,
        during this process $\det$ and $f$ change value in exactly the same way.
        So
        \begin{align*}
            \det A' &= \lambda_1\dotsi\lambda_k\det A \\
            f(A') &= \lambda_1\dotsi\lambda_kf(A)
        \end{align*}
        for some non-zero scalars $\lambda_i$.
        But $A'$ is either equal to $I_n$ in which case $f(A') = \det A' = 1$ (by property (i)),
        or it contains a zero row in which case $f(A') = \det A' = 0$ by property (vi).
        So in either case $f(A') = \det A'$ and we obtain
        \[
        \lambda_1\dotsi\lambda_kf(A) = \lambda_1\dotsi\lambda_k\det A,
        \]
        which gives $f(A) = \det A$ since $\lambda_i \neq 0$ for all $i$.
    \end{proof}
\end{theorem}

\subsection{Determinant calculations}
Properties (i)-(vii) tell us how the determinant changes under the G-J algorithm,
and hence gives us a way to compute the determinant.
There are two basic cases.
\begin{enumerate}[label = (\Roman*)]
    \item The RREF $A'$ of $A$ contains a zero row.
    Then $\det(A) = \det(A') = 0$.
    \item The RREF $A'$ of $A$ is the identity matrix.
    Assume that we applied the operation $P_{rs}$ of switching rows $\ell$ times and applied the scaling operations $M_{r_1}(\lambda_1), M_{r_2}(\lambda_2), \dotsc, M_{r_k}(\lambda_k)$ at some point during the algorithm.
    Then
    \[
    1 = \det I_n = \det A' = (-1) ^ \ell \lambda_1 \dotsi \lambda_k\det A.
    \]
    So
    \[
    \det(A) = (-1) ^ \ell \lambda_1 ^ {-1} \dotsi \lambda_k ^ {-1}.
    \]
\end{enumerate}
\begin{proposition}
    Let $A$ be an upper (or lower) triangular $n \times n$ matrix with diagonal entries $a_{11}, a_{22}, \dotsc, a_{nn}$.
    Then
    \[
    \det(A) = \prod_{k = 1}^{n}a_{kk} = a_{11} \cdot a_{22} \dotsi a_{nn}.
    \]
    \begin{proof}
        
    \end{proof}
\end{proposition}

\subsection{Several Big Theorems}
\begin{theorem}\label{pre:linalg:thm:invifdetnonzero}
    The $n \times n$ matrix $A$ has an inverse if and only if $\det(A) \neq 0$.
    \begin{proof}
        By definition we already know this for the cases $n = 1$,
        $n = 2$ and $n = 3$.
        For $n > 3$ we apply G-J to the matrix $A$ to get to the RREF $A'$.
        As in the proof of \autoref{pre:linalg:thm:uniquedet},
        during this process $\det$ changes in a clear way using properties (ii), (iv), and (vii).
        We get
        \[
        \det A' = \lambda_1\dotsi\lambda_k\det A
        \]
        for some non-zero scalars $\lambda_i$/
        But then
        \[
        \det(A) \neq 0 \iff \det(A') \neq 0 \iff A' = I_n \iff A \text{ is non-singular}.
        \]
    \end{proof}
\end{theorem}

\begin{theorem}\label{pre:linalg:thm:detsplits}
    Let $A$ and $B$ be $n \times n$ matrices.
    Then
    \[
    \det(AB) = \det(A)\det(B).
    \]
    \begin{proof}
        We apply G-J to the matrix $A$ to get to the RREF $A'$.
        As before there are some non-zero scalars $\lambda_i$ such that
        \[
        \det A' = \lambda_1\dotsi\lambda_k\det A \text{ so that } \det A = \lambda_1 ^ {-1} \dotsi\lambda_k ^ {-1}\det A'.
        \]
        Applying the same ERO's to the matrix $AB$ we obtain the matrix $A'B$ (ERO's are given by left multiplication of elementary matrices) and hence also
        \[
        \det(A'B) = \lambda_1\dotsi\lambda_k\det(AB).
        \]
        We now have two cases:
        \begin{enumerate}[label = (\roman*)]
            \item 
            $A' = I_n$.
            Then
            \[
            \det(AB) = \lambda_1 ^ {-1} \dotsi \lambda_k ^ {-1}\det(B) = \det(A)\det(B),
            \]
            as desired.
            \item $A'$ contains a zero row;
            so $\det A = \det A' = 0$.
            But then the product $A'B$ contains also a zero row which gives $\det(A'B) = 0$.
            Since $\det(A'B)$ and $\det(AB)$ only differ by non-zero scalars,
            we get $\det(AB) = 0$.
            But we also have $\det(A)\det(B) = 0$ since $\det A = 0$.
        \end{enumerate}
    \end{proof}
\end{theorem}

\begin{theorem}
    We have
    \[
    \det(A ^ t) = \det(A).
    \]
    \begin{proof}
        Firstly,
        \autoref{pre:linalg:prop:propofinvandtransp}(iv) implies that either both $A$ and $A ^ t$ are invertible,
        or neither are.
        In the case $\det A = 0$ we have by \autoref{pre:linalg:thm:invifdetnonzero} that $A$ is singular,
        and so $A ^ t$ is also singular.
        Applying \autoref{pre:linalg:thm:invifdetnonzero} a second time shows that $\det(A ^ t) = 0$.
        If $\det A \neq 0$ then $A$ is invertible.
        But then \autoref{pre:linalg:col:invisprodofele} tells us we may write it as a product of EROs,
        say $A = E_1E_2\dotsi E_\ell$.
        By combining determinant properties (ii),
        (iv) and (vii) it is easy to check that $\det(P_{rs}) = -1$,
        $\det(M_r(\lambda)) = \lambda$ and $\det(A_{rs}(\lambda)) = 1$ for every $r$ and $s$.
        Combining this with the observation that
        \[
        (P_{rs}) ^ t = P_{rs},\quad(M_{r}(\lambda)) ^ t = M_r(\lambda)\quad(A_{rs}(\lambda)) ^ t = A_{sr}(\lambda),
        \]
        it follows that $\det(E ^ t) = \det(E)$ for every elementary matrix $E$.
        Thus,
        by \autoref{pre:linalg:prop:propofinvandtransp}(iii) and \autoref{pre:linalg:thm:detsplits} we have
        \begin{align*}
            \det(A ^ t) &= \det((E_1\dotsi E_\ell) ^ t) &= \det(E_\ell ^ t \dotsi E_1 ^ t) &= \det(E_\ell ^ t)\dotsi\det(E_1 ^ t) \\
            &= \det(E_\ell)\dotsi\det(E_1) &= \det(E_1)\dotsi\det(E_\ell) &= \det(E_1 \dotsi E_\ell) &= \det A.
        \end{align*}
    \end{proof}
\end{theorem}

\begin{corollary}
    If $A$ is not singular,
    then
    \[
    \det(A ^ {-1}) = \det((A ^ t) ^ {-1}) = \frac{1}{\det(A)}.
    \]
    \begin{proof}
        Simply notice that $1 = \det(I_n) = \det(A ^ {-1}A) = \det(A ^ {-1})\det(A)$,
        and similarly for the transpose (using \autoref{pre:linalg:prop:propofinvandtransp}(iv)).
    \end{proof}
\end{corollary}

\subsection{Determinants and inverses; Cramer's Rule}
\begin{definition}
    Given a square matrix $A$,
    we define the adjoint matrix $\Adj(A)$ by
    \[
    (\Adj(A))_{ij} = (-1) ^ {i + j}\det(A_{j, i}).
    \]
\end{definition}

\begin{center}
\fbox{
\begin{minipage}{0.8\textwidth}
The adjoint is the transpose of the matrix of signed cofactors.
\end{minipage}
}
\end{center}
\hfill

\begin{proposition}
    Let $A \in M_n(\R)$ and $\Adj(A)$ be its adjoint.
    Then
    \[
    A \cdot \Adj(A) = \det(A) \cdot I_n = \Adj(A) \cdot A.
    \]
    \begin{proof}
        We first prove $A \cdot \Adj(A) = \det(A) \cdot I_n$.
        The $(r, s)$-entry of $A \cdot \Adj(A)$ is given by
        \[
        \sum_{k = 1}^{n}a_{rk}(\Adj(A))_{ks} = \sum_{k = 1}^{n}a_{rk}(-1) ^ {k + s}\det(A_{s, k}).
        \]
        For $r = s$ this is the expansion of $\det A$ by the $s$-th row.
        While for $r \neq s$,
        this is the expansion of the determinant of $A$ by the $s$-th row with row $s$ replaced by the $r$th row $\mbf{a}_r$.
        So we are looking at
        \[
        \det\begin{pmatrix}
            \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_n
        \end{pmatrix} = 0.
        \]
        Hence in summary
        \[
        (r, s)-\text{entry of } A\Adj(A) = \begin{cases}
            \det A &\text{if } r = s \\
            0 &\text{if } r \neq s
        \end{cases};
        \]
        i.e., $A \cdot \Adj(A) = (\det A)I_n$.
        This is enough to conclude that $A ^ {-1} = \frac{1}{\det(A)}\Adj(A)$ and the second equation,
        that $\Adj(A) \cdot A = (\det A)I_n$ follows from the uniqueness of inverses.
    \end{proof}
\end{proposition}

\begin{theorem}[Inverse by adjoint matrix]\label{pre:linalg:thm:invbyadjmatr}
    Let $A \in M_n(\R)$ be non-singular.
    Then the inverse $A ^ {-1}$ is given by
    \[
    A ^ {-1} = \frac{1}{\det A}\Adj(A).
    \]
\end{theorem}

\begin{theorem}[Cramer's rule]\label{pre:linalg:thm:cramersrule}
    Let $A\mbf{x} = \mbf{b}$ be a linear system with a non-singular matrix $A \in M_n(\R)$.
    The unique solution $\begin{pmatrix}
        x_1 \\ \vdots \\ x_n
    \end{pmatrix}$ is given by
    \[
    x_i = \frac{\det(\mbf{a}_1, \dotsc, \mbf{a}_{i - 1}, \mbf{b}, \mbf{a}_{i + 1}, \dotsc, \mbf{a}_n)}{\det A}.
    \]
    Here $\mbf{a}_\ell$ denotes (for a change) the $\ell$-th column vector of $A$.
\end{theorem}

\begin{remark}
    For practical computations \autoref{pre:linalg:thm:invbyadjmatr} and \autoref{pre:linalg:thm:cramersrule} are rather useless.
    But for theoretical considerations this is very useful.
    For example,
    since the determinant is a polynomial in the entries of the matrix $A$,
    the entries $A ^ {-1}$
    (and also the solution to $A\mbf{x} = \mbf{b}$)
    depend continuously on the entries of $A$.
\end{remark}

\subsection{Determinants, area and volume}

\textbf{Area of parallelograms}

Consider the pair of vectors
\[
\mbf{v} = \begin{pmatrix} a \\ b \end{pmatrix},
\,\mbf{w} = \begin{pmatrix} c \\ d \end{pmatrix}.
\]
Now form the parallelogram $P$ with vertices $0, \mbf{v}, \mbf{v + w}, \mbf{w}$.
Elementary trigonometry tells us
\begin{equation}
    \mathrm{area}(P) = |\mbf{v}| \cdot |\mbf{w}| \cdot \sin\theta,
\end{equation}
where $\theta \in [0, \pi]$ is the angle between $\mbf{v}$ and $\mbf{w}$.
Recall for two vectors $\mbf{x, y} \in \R ^ 3$,
we have
\begin{equation}
    |\mbf{x \times y}| = |\mbf{x}||\mbf{y}|\sin\theta.
\end{equation}
We use this to rewrite (1) in the following way.
We define two auxiliary vectors $\Tilde{\mbf{v}} = \begin{pmatrix}
    a \\ b \\ 0
\end{pmatrix}$ and $\Tilde{\mbf{w}} = \begin{pmatrix}
    c \\ d \\ 0
\end{pmatrix}$ in $\R ^ 3$.
We then have
\[
\Tilde{\mbf{v}} \times \Tilde{\mbf{w}} = \begin{pmatrix}
    0 \\ 0 \\ ad - bc
\end{pmatrix}
\]
So (2) becomes
\[
|ad - bc| = |\Tilde{\mbf{v}}||\Tilde{\mbf{w}}|\sin\theta = |\mbf{v}||\mbf{w}|\sin\theta.
\]
Combining this with (1) we have shown
\begin{theorem}
    For $\mbf{v, w} \in \R ^ 2$ two vectors in the plane,
    form the parallelogram $P$ with vertices $\mbf{0, v, w}$ and $\mbf{v + w}$.
    Then
    \[
    \mathrm{area}(P) = |\det(\mbf{v, w})|.
    \]
\end{theorem}

\textbf{Area of triangles}

The area of a triangle with vertices $\mbf{a, b}$ and $\mbf{c}$ is half the area of the parallelogram with vertices $\mbf{a}$,
$\mbf{a + (b - a)}, \mbf{a + (c - a)} = \mbf{c}$ and $\mbf{a + (b - a) + (c - a)} = \mbf{b + c - a}$.
In other words,
the area of this triangle is given by:
\begin{proposition}
    The triangle with vertices $\mbf{a, b}$ and $\mbf{c}$ has area
    \[
    \frac{1}{2}|\det(\mbf{b - a, c - a})|.
    \]
\end{proposition}

\textbf{Volume of parallelepipeds}

A parallelepiped $P$ is a three dimensional polyhedron with six quadrilateral faces where opposite faces are parallel.
A parallelepiped has twelve edges.
These come in three families of four where all the vectors in each family are parallel;
call a vector from the first family $\mbf{u}$,
one from the second $\mbf{v}$ and one from the third $\mbf{w}$
(we then say that $P$ is spanned by $\mbf{u, v}$ and $\mbf{w}$).
The parallelepiped $P$ in $\R ^ 3$ spanned by the vectors $\mbf{u, v, w}$ hence has vertices $\mbf{0, u, v, w, u + v, u + w, v + w, u + v + w}$.
We have
\[
\mathrm{vol}(P) = \mathrm{area}(\text{base}) \times \text{height}.
\]
As base we take the parallelogram spanned by $\mbf{v}$ and $\mbf{w}$.
Hence, as before
\[
\mathrm{area}(\text{base}) = |\mbf{v \times w}|.
\]
For the height $h$ we have
\[
h = |\mbf{u}| \cdot |\sin(\mathrm{angle}(\mbf{u}, \text{base}))| = |\mbf{u}| \cdot |\cos(\mathrm{angle}(\mbf{u}, \text{normal vector to base}))|.
\]
A normal vector to the base is of course $\mbf{v \times w}$.
Hence by definition of the angle between two vectors we see
\[
|\cos(\mathrm{angle}(\mbf{u}, \mbf{v} \times \mbf{w}))| = \frac{|\mbf{u} \cdot (\mbf{v} \times \mbf{w})}{|\mbf{u}| \cdot |\mbf{v} \times \mbf{w}|} = \frac{|\det(\mbf{u}, \mbf{v}, \mbf{w})|}{|\mbf{u}| \cdot |\mbf{v} \times \mbf{w}|}.
\]
Combining all of this we obtain
\begin{proposition}
    The volume of the parallelepiped $P$ in $\R ^ 3$ spanned by the vectors $\mbf{u, v, w}$ is given by
    \[
    \mathrm{vol}(P) = |\det(\mbf{u, v, w})|.
    \]
\end{proposition}

\textbf{Other polyhedra}

Similar reasoning shows that the volume of a tetrahedron $T$ is one sixth of the determinant of the matrix whose columns are the vectors along edges of $T$ based at the same vertex.

\textbf{General $n$}

To define this phenomenon in higher dimensions we first need to have an idea what we mean by an $n$-dimension volume.
The $n$-dimension box spanned by $a_1\mbf{e}_1, a_2\mbf{e}_2, \dotsc, a_n\mbf{e}_n$ we define to have volume $|a_1|\dotsi|a_n|$.
Note that $\left|\begin{pmatrix}
    a_1 & \phantom{} & \phantom{} \\ \phantom{} & \ddots & \phantom{} \\ \phantom{} & \phantom{} & a_n
\end{pmatrix}\right| = |a_1| \dotsi |a_n|$.
With this definition one can show that the volume of the $n$-dimensional parallelepiped $P$ spanned by vectors $\mbf{v}_1, \dotsc, \mbf{v}_n$ is given by
\[
\mathrm{vol}(P) = |\det(\mbf{v}_1, \dotsc, \mbf{v}_n)|.
\]

\newpage

\section{Spanning sets and linear independence in \texorpdfstring{$\R ^ n$}{}}
\subsection{Subspaces of \texorpdfstring{$\R ^ n$}{}}
\begin{definition}[Vector subspace of $\R ^ n$]
    A (non-empty) subset $U$ of $\R ^ n$ is called a vector subspace (or linear subspace) of $\R ^ n$ if it satisfies the following three conditions:
    \begin{enumerate}[label = (\roman*)]
        \item closure under addition:
        if $\mbf{u}_1$ and $\mbf{u}_2$ are in $U$,
        then $\mbf{u}_1 + \mbf{u}_2$ is also in $U$;
        \item closure under scalar multiplication:
        if $\mbf{u}$ is in $U$ and $\lambda$ is in $\R$,
        then $\lambda\mbf{u}$ is in $U$;
        \item existence of an origin:
        the vector $\mbf{0}$ is in $U$.
    \end{enumerate}
\end{definition}
\textbf{Simply},
a vector subspace is just a non-empty subset of $\R ^ n$ that satisfies some conditions.

\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item  $U = \{\mbf{0}\}$ is clearly a vector subspace;
        it is called the trivial vector subspace.
        \item Equally clearly $\R ^ n$ is a vector subspace of itself.
        \item Let $L_0$ be a line in $\R ^ n$ through the origin $\mbf{0}$ with direction vector $\mbf{d}$.
        Then any point on $L_0$ has the form $\mbf{x} = \mbf{0} + \lambda\mbf{d} = \lambda\mbf{d}$.
        Clearly $L$ satisfies the three conditions above and so is a vector subspace of $\R ^ n$.
        If the line does not pass through $\mbf{0}$ then it is not a vector subspace
        (it is called an affine line,
        which is a special case of an affine subspace).
        \item Let $\Pi_0$ be the plane in $\R ^ n$ through the origin $\mbf{0}$ with direction vectors $\mbf{d}_1$ and $\mbf{d}_2$ (not collinear).
        Then we have
        \[
        \Pi_0 = \{\lambda_1\mbf{d}_1 + \lambda_2\mbf{d}_2\,:\, \lambda_1, \lambda_2 \in \R\},
        \]
        it is clear that $\Pi_0$ satisfies the three conditions above and so is a vector subspace.
        If the plane does not pass through $\mbf{0}$,
        then it is not a vector subspace.
        (It is called an affine plane,
        which is a special case of an affine subspace).
    \end{enumerate}
\end{example}

\begin{proposition}
    The solution set of an arbitrary $m \times n$ homogeneous linear system $A\mbf{x} = \mbf{0}$ is a vector subspace of $\R ^ n$.
    \begin{proof}
        Let $\mbf{u}_1$ and $\mbf{u}_2$ be two solutions to the linear system and $\lambda \in \R$.
        Then
        \[
        A(\mbf{u}_1 + \mbf{u}_2) = A\mbf{u}_1 + A\mbf{u}_2 = \mbf{0} + \mbf{0} = \mbf{0}\quad\text{and}\quad A(\lambda\mbf{u}_1) = \lambda(A\mbf{u}_1) = \lambda\mbf{0} = \mbf{0},
        \]
        as claimed.
        Of course we also have $A\mbf{0} = \mbf{0}$.
    \end{proof}
\end{proposition}

Note that the solution set of an inhomogeneous system $A\mbf{x} = \mbf{b}$
(where $\mbf{b} \neq \mbf{0}$)
is not a vector subspace since it does not contain the zero vector.
Instead it forms an affine subspace of $\R ^ n$;
that is,
a subset of $\R ^ n$ of the form
\[
\mbf{c} + U = \{\mbf{v} \in \R ^ n\,:\,\mbf{v} = \mbf{c} + \mbf{u}\text{ with }\mbf{u} \in U\},
\]
where $U$ is a vector subspace.
\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
The above is quite important as it has some relevance to some other definitions we will be making,
specifically this includes linear independence and span.
\end{minipage}
}
\end{center}
\hfill

\subsection{Spanning sets}
Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be vectors in $\R ^ n$.
A linear combination of these vectors is a vector $\mbf{u}$ of the form
\[
\mbf{u} = \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k
\]
for some (real) scalars $\lambda_1, \dotsc, \lambda_k$.

\begin{definition}[Linear span]
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ vectors in $\R ^ n$.
    Their (linear) span is the subset $U$ of $\R ^ n$ consisting of all linear combinations of these vectors.
    So
    \[
    U = \{\lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k\quad\lambda_1, \dotsc, \lambda_k \in \R\}.
    \]
    Conversely,
    we say that such a $U$ is spanned by $\mbf{u}_1, \dotsc, \mbf{u}_k$ and write $U = \mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k)$.
    Equivalently,
    $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ is called a spanning set of $U$.
\end{definition}

\textbf{Alternatively},
another simplification of this is to consider that the span is just a subset of $\R ^ n$ containing all of the combinations of the vectors.
So for a set of vectors to span a subspace $U$,
everything in $U$ must be able to be written as a combination of the vectors which span $U$.
\medskip

\begin{proposition}\label{pre:linalg:prop:spanissolutionset}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k \in \R ^ n$ be $k$ vectors,
    and let $A = (\mbf{u}_1, \dotsc, \mbf{u}_k) \in M_{n, k}(\R)$ be the associated matrix of column vectors.
    Then we have
    \[
    \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k = A\pmb{\lambda},
    \]
    where $\pmb{\lambda} = \begin{pmatrix}
        \lambda_1 \\ \vdots \\ \lambda_k
    \end{pmatrix} \in \R ^ k$ is the column vector of the $\lambda_j$.
    Moreover,
    we have
    \[
    \mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k) = \{\mbf{u} \in \R ^ n\,:\,\text{The system } A\pmb{\lambda} = \mbf{u}\text{ has a solution}\} = \{A\pmb{\lambda}:\pmb{\lambda} \in \R ^ k\}.
    \]
\end{proposition}
The above sort of tells us that we can rewrite the span of a collection of vectors in terms of matrices.

\begin{lemma}
    The linear span $U$ of any collection $\mbf{u}_1, \dotsc, \mbf{u}_k$ of vectors is a vector subspace of $\R ^ n$.
    \begin{proof}
        We check the conditions of a vector space:
        \begin{enumerate}[label = (\roman*)]
            \item If $\mbf{u} = \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k$ and $\mbf{v} = \mu_1\mbf{u}_1 + \dotsi + \mu_k\mbf{u}_k$ then
            \[
            \mbf{u} + \mbf{v} = (\lambda_1 + \mu_1)\mbf{u}_1 + \dotsi + (\lambda_k + \mu_k)\mbf{u}_k \in U.
            \]
            \item If $\mbf{u} = \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k$ and $\lambda \in \R$,
            then
            \[
            \lambda\mbf{u} = (\lambda\lambda_1)\mbf{u}_1 + \dotsi + (\lambda\lambda_k)\mbf{u}_k \in U.
            \]
            \item Clearly $\mbf{0} = 0\mbf{u}_1 + \dotsi + 0\mbf{u}_k$ is in $U$.
        \end{enumerate}
    \end{proof}
\end{lemma}

\begin{lemma}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k \in \R ^ n$ and assume that one of the vectors,
    say $\mbf{u}_1$,
    can be expressed as a linear combination of the others.
    Then
    \[
    \mathrm{span}(\mbf{u}_1, \mbf{u}_2, \dotsc, \mbf{u}_k) = \mathrm{span}(\mbf{u}_2, \dotsc, \mbf{u}_k).
    \]
    \begin{proof}
        For $\supseteq$:
        This is obvious since
        \[
        \lambda_2\mbf{u}_2 + \dotsi + \lambda_k\mbf{u}_k = 0\mbf{u}_1 + \lambda_2\mbf{u}_2 + \dotsi + \lambda_k\mbf{u}_k,
        \]
        and so any element in the span of $\mbf{u}_2, \dotsc, \mbf{u}_k$ is also in the span of $\mbf{u}_1, \mbf{u}_2, \dotsc, \mbf{u}_k$.
        $\subseteq$:
        As $\mbf{u}_1$ can be written in the span of the others,
        let us write $\mbf{u}_1 = \mu_2\mbf{u}_2 + \dotsi + \mu_k\mbf{u}_k$ for some $\mu_j$.
        Then
        \[
        \lambda_1\mbf{u}_1 + \lambda_2\mbf{u}_2 + \dotsi + \lambda_k\mbf{u}_k = (\lambda_2 + \lambda_1\mu_2)\mbf{u}_2 \dotsi + (\lambda_k + \lambda_1\mu_k)\mbf{u}_k.
        \]
        So an element in the span of $\mbf{u}_1, \mbf{u}_2, \dotsc, \mbf{u}_k$ is also in the span of $\mbf{u}_2, \dotsc, \mbf{u}_k$.
    \end{proof}
\end{lemma}

\textbf{Idea:}
This lemma tells us that the span of a set of vectors is equal to the span of a smaller set of vectors which doesn't include the vectors which can be made up of the other vectors.
Effectively,
I believe this is saying that we can keep reducing the spanning set until we get a smallest spanning set where each of the vectors cannot be written as a combination of the others.

\subsection{Linear independence}

\begin{definition}[Linear independence]
    The vectors $\mbf{u}_1, \dotsc, \mbf{u}_k$ are said to be linearly independent if the only solution to the equation
    \[
    \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k = \mbf{0}\quad\text{if}\quad\lambda_1 = \lambda_2 = \dotsi = \lambda_k = 0.
    \]
\end{definition}

\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
This definition is saying that you cannot make one vector from the other.
In other words,
$\mbf{v}$ cannot be made from $\lambda\mbf{u}$ for some $\lambda \in \R$.
Here is a more informative example
\[
\begin{pmatrix}
    2 \\ 2 \\ 2
\end{pmatrix}\text{ is not linearly independent to }
\begin{pmatrix}
    1 \\ 1 \\ 1
\end{pmatrix}.
\]
\end{minipage}
}
\end{center}
\hfill


\begin{corollary}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ vectors in $\R ^ n$ and let $A = (\mbf{u}_1, \dotsc, \mbf{u}_k) \in M_{n, k}(\R)$ be the associated matrix of column vectors.
    Then
    \[
    \mbf{u}_1, \dotsc, \mbf{u}_k\text{ are linear independent} \iff \text{The system } A\pmb{\lambda} = \mbf{0}\text{ has no non-trivial\footnotemark solution}.
    \]
    \footnotetext{Has a solution where $\pmb{\lambda} \neq 0$.}
    \begin{proof}
        We can see this from \autoref{pre:linalg:prop:spanissolutionset}.
        
        \textit{As this states that $\pmb{\lambda} = \mbf{0}$ which is the same as $\lambda_1 = \lambda_2 = \dotsi = \lambda_k$.}
    \end{proof}
\end{corollary}
\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
The above corollary is very important,
it states that if we cannot solve the matrix equation $A\pmb{\lambda} = \mbf{0}$ for non-trivial solutions then $\mbf{u}_1, \dotsc, \mbf{u}_k$ are linearly independent.

\textit{I.e. we can use G-J to solve the matrix equation $A\pmb{\lambda} = \mbf{0}$ and if we do not get $A$
(after G-J)
in a form which shows $\lambda_1 = \dotsi = \lambda_k = 0$ then we can say they are not linearly independent}.
\end{minipage}
}
\end{center}
\hfill


If the vectors $\mbf{u}_1, \dotsc, \mbf{u}_k$ are not linearly independent we say that they are linearly dependent.

\begin{lemma}
    A pair of (non-zero) vectors $\mbf{u}, \mbf{v}$ in $\R ^ n$ are linearly dependent if and only if there is $\lambda \in \R$ so that $\mbf{v} = \lambda\mbf{u}$.
    In particular,
    any two (non-zero) non-collinear vectors are always linear independent.
    \begin{proof}
        $\Leftarrow$:
        If $\mbf{v} = \lambda\mbf{u}$ then we can rewrite this as $1\mbf{v} + (-\lambda)\mbf{u} = \mbf{0}$.
        So $\mbf{u}$,
        and $\mbf{v}$ are linearly dependent.
        $\Rightarrow$:
        Conversely if they are linearly dependent there are $\lambda_1, \lambda_2$ (not both zero, say $\lambda_2 \neq 0$) so that $\lambda_1\mbf{u} + \lambda_2\mbf{v} = \mbf{0}$.
        Then
        $\mbf{v} = \frac{-\lambda_1}{\lambda_2}\mbf{u}$.
    \end{proof}
\end{lemma}
The above is simply saying if we can make one vector from another then they are not linearly independent,
specifically if they are non-collinear then they are always linearly independent.

The following proposition says that for a set of linearly independent vectors,
if we can choose a vector,
say $\mbf{u}$,
such that $\mbf{u}$ is not in the set of their combinations
(so we cannot make $\mbf{u}$ from the vectors in the set)
then the vectors that span this are linearly independent to $\mbf{u}$.

\begin{proposition}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ linear independent vectors in $\R ^ n$ and let $\mbf{u} \in \R ^ n$ such that
    \[
    \mbf{u} \notin \mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k).
    \]
    Then
    \[
    \mbf{u}_1, \dotsc, \mbf{u}_k, \mbf{u}\text{ are linear independent}.
    \]
    \begin{proof}
        Consider the equation
        \[
        \lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k + \alpha\mbf{u} = \mbf{0}.
        \]
        We need to show $\lambda_1 = \dotsi = \lambda_k = \alpha = 0$ is the only solution.
        We have two cases:
        \begin{enumerate}[label = \arabic*)]
            \item $\alpha = 0$.
            But then we have $\lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k = \mbf{0}$,
            which has only the trivial solution since the $\mbf{u}_1, \dotsc, \mbf{u}_k$ are linear independent.
            \item $\alpha \neq 0$.
            But then we can write $\mbf{u} = -\frac{1}{\alpha}(\lambda_1\mbf{u}_1 + \dotsi + \lambda_k\mbf{u}_k)$.
            But this is impossible since $\mbf{u}$ is not in $\mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k)$.
        \end{enumerate}
    \end{proof}
\end{proposition}

\newpage

\section{Real vector spaces}

\subsection{Vector spaces and subspaces}
A (real) vector space is a non-empty set\footnote{This could be for any type,
not just vectors or reals,
I know,
it's a bit bloody confusing.}
$V$ together with two operations:
\begin{enumerate}[label = (\roman*)]
    \item Addition:
    This is a function
    \[
    V \times V \rightarrow V
    \]
    (Given any $\mbf{u}$ and $\mbf{v}$ in $V$,
    we write their sum as $\mbf{u + v}$);
    \item Scalar multiplication:
    This is a function
    \[
    \R \times V \rightarrow V
    \]
    (Given $\mbf{u} \in V$ and $\lambda \in \R$ we write the multiplication of $\mbf{u}$ by $\lambda$ as $\lambda\mbf{u}$).
\end{enumerate}

These operations must additionally satisfy the axioms of a real vector space:

Axioms for addition:
\begin{enumerate}[label = (\roman*)]
    \item Existence of additive identity:
    there is a vector $\mbf{0}$ in $V$ so that $\mbf{v + 0} = \mbf{v} = \mbf{0 + v}$ for all $\mbf{v}$ in $V$;
    \item Commutativity:
    $\mbf{v + w} = \mbf{w + v}$ for all $\mbf{v}$ and $\mbf{w}$ in $V$;
    \item Existence of additive inverses:
    for each $\mbf{v}$ in $V$ there is an element $-\mbf{v}$ in $V$ such that $\mbf{v} + (-\mbf{v}) = \mbf{0} = (-\mbf{v}) + \mbf{v}$;
    \item Associativity:
    $\mbf{u} + (\mbf{v + w}) = (\mbf{u + v}) + \mbf{w}$ for all $\mbf{u, v, w} \in V$.
\end{enumerate}

Axioms for scalar multiplication:
\begin{enumerate}[label = (\roman*)]
    \item $0\mbf{v} = \mbf{0}$ for all $\mbf{v}$ in $V$;
    \item $1\mbf{v} = \mbf{v}$ for all $\mbf{v}$ in $V$;
    \item Associativity:
    $(\lambda\mu)\mbf{v} = \lambda(\mu\mbf{v})$ for all $\lambda, \mu$ in $\R$ and for all $\mbf{v}$ in $V$;
    \item Distributivity:
    For all $\lambda, \mu \in \R$ and for all $\mbf{v, w} \in V$ we have
    \begin{enumerate}[label = \alph*)]
        \item $(\lambda + \mu)\mbf{v} = \lambda\mbf{v} + \mu\mbf{v}$;
        \item $\lambda(\mbf{v + w}) = \lambda\mbf{v} + \lambda\mbf{w}$.
    \end{enumerate}
\end{enumerate}

We refer to elements of a vector space as vectors\footnote{Big shock.}.

\begin{remark}
    Instead of $\R$ one can also allow other scalars such that complex numbers $\C$ or the rationals $\Q$.
    Then,
    for example,
    $\C ^ n$ is a vector space over $\C$ and $\Q ^ n$ is a vector space over $\Q$.
    In fact
    (almost)
    everything in these notes holds if we replace $\R$ with $\Q$ or $\C$.
    In general,
    one considers vector spaces over a field $F$,
    where $F$ plays the role of $\R, \C$ or $\Q$.
    (Very roughly,
    a field is a set of scalars,
    where we have nice addition and multiplication,
    and in particular every non-zero scalar has a multiplicative inverse).
\end{remark}

\begin{definition}[Vector subspace]
    A vector subspace of a vector space $V$ is a non-empty subset that is itself a vector space with respect to the same addition and multiplication operations of $V$.
\end{definition}
This is just a subset of a vector space $V$,
don't forget that this can still be for any type of object not just our typical $\R ^ n$ vectors\footnote{Why would we make it so easy to understand by just thinking about $\R ^ n$.}.

\begin{proposition}[Subspace criterion]
    A non-empty subset $U$ of a vector space $V$ is a vector subspace of $V$ if and only if it satisfies the following three conditions:
    \begin{enumerate}[label = (\roman*)]
        \item closure under addition:
        if $\mbf{u}_1$ and $\mbf{u}_2$ are in $U$,
        then $\mbf{u}_1 + \mbf{u}_2$ is also in $U$;
        \item closure under scalar multiplication:
        if $\mbf{u}$ is in $U$ and $\lambda$ is in $\R$,
        then $\lambda\mbf{u}$ is in $U$;
        \item existence of an origin:
        the vector $\mbf{0}$ is in $U$.
    \end{enumerate}
\end{proposition}

\subsection{Linear independence, spanning sets, bases, and dimension}
The notations for linear combination, linear span, spanning sets, and linear independence for a general vector space $V$ are defined exactly in the same way as for $\R ^ n$.
\begin{definition}[Basis]
    A (finite) basis for a vector space $V$ is a finite subset of vectors $\{\mbf{v}_1, \mbf{v}_2, \dotsc, \mbf{v}_k\}$ in $V$ such that
    \begin{enumerate}[label = (\roman*)]
        \item the vectors $\mbf{v}_1, \mbf{v}_2, \dotsc, \mbf{v}_k$ are linearly independent,
        and
        \item $V = \mathrm{span}(\mbf{v}_1, \mbf{v}_2, \dotsc, \mbf{v}_k)$.
    \end{enumerate}
\end{definition}

\textbf{Assorted Lemmas}:
\begin{lemma}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ vectors in a vector space $V$.
    Then $\mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k)$ is a vector subspace of $V$.
\end{lemma}
\begin{lemma}\label{pre:linalg:lem:lincombeqspanof}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be vectors in a vector space $V$ and assume that one of the vectors,
    say $\mbf{u}_1$,
    can be expressed as a linear combination of the others.
    Then
    \[
    \mathrm{span}(\mbf{u}_1, \mbf{u}_2, \dotsc, \mbf{u}_k) = \mathrm{span}(\mbf{u}_2, \dotsc, \mbf{u}_k).
    \]
\end{lemma}
\begin{lemma}
    Suppose that the set $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ is not linearly independent.
    Then for some $1 \leq i \leq k$,
    we have that
    \[
    \mbf{u}_i \in \mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_{i - 1}, \mbf{u}_{i + 1}, \dotsc, \mbf{u}_k).
    \]
    \begin{proof}
        Since $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ are not linearly independent there is some linear dependence relation
        \[
        \sum_{j = 1}^{k}\lambda_j\mbf{u}_j = \mbf{0}
        \]
        with at least one of the $\lambda_j$ non zero.
        Say that $\lambda_i \neq 0$.
        Then the linear dependence relation may be rearranged to read
        \[
        \mbf{u}_i = \sum_{j = 1}^{i - 1}\frac{\lambda_j}{-\lambda_i}\mbf{u}_j + \sum_{j = i + 1}^{k}\frac{\lambda_j}{-\lambda_i}\mbf{u}_j.
        \]
        The equivalence of the spans then follows from \autoref{pre:linalg:lem:lincombeqspanof}.
    \end{proof}
\end{lemma}

\begin{lemma}\label{pre:linalg:lem:ifnotinspanofvecspacethenlinindep}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ linearly independent vectors in a vector space $V$ and let $\mbf{u} \in V$ such that
    \[
    \mbf{u} \notin \mathrm{span}(\mbf{u}_1, \dotsc, \mbf{u}_k).
    \]
    Then
    \[
    \mbf{u}_1, \dotsc, \mbf{u}_k, \mbf{u}\text{ are linearly independent}.
    \]
\end{lemma}

\begin{theorem}[Existence of finite basis]
    Every vector space that is spanned by finitely many vectors has a basis.
\end{theorem}

\begin{theorem}["What is a basis?"]\label{pre:linalg:thm:whatisabasis}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be vectors in a vector space $V$.
    Then the following statements are
    (pairwise)
    equivalent:
    \begin{enumerate}[label = (\roman*)]
        \item The vectors $\mbf{u}_1, \dotsc, \mbf{u}_k$ form a basis of $V$.
        \item The vectors $\mbf{u}_1, \dotsc, \mbf{u}_k$ form a maximal linear independent set in $V$,
        i.e., one cannot add another vector in $V$ to the $\mbf{u}_i$,
        and still obtain a linearly independent set.
        \item Any vector $\mbf{u}$ in $V$ can be written uniquely as
        \[
        \mbf{u} = \lambda_1\mbf{u}_1 + \lambda_2\mbf{u}_2 + \dotsi + \lambda_k\mbf{u}_k.
        \]
        \item The vectors $\mbf{u}_1, \dotsc, \mbf{u}_j$ form a minimal spanning set of $V$;
        i.e., one cannot remove any of the $\mbf{u}_i$ and still have a spanning set of $V$.
    \end{enumerate}
    \begin{proof}
        \textbf{Add proof}.
    \end{proof}
\end{theorem}

We call a vector space finite dimensional if it has a finite basis.

For example,
we have seen that $\{1, x, x ^ 2, \dotsc, x ^ n\}$ is a basis for $\R[x]_n$ and is therefore finite dimensional.

\begin{theorem}[Steinitz Exchange Theorem]
    Let $S = \{\mbf{v}_1, \dotsc, \mbf{v}_k\}$ be a spanning set of a vector space $V = \mathrm{span}(S)$.
    Let $\{\mbf{u}_1, \dotsc, \mbf{u}_{\ell}\} \subseteq V$ be a linearly independent set.

    Then there exist $\ell$ distinct vectors $\mbf{v}_{i_1}, \mbf{v}_{i_2}, \dotsc, \mbf{v}_{i_{\ell}} \in S$ such that for all $j$ with $0 \leq j \leq \ell$ we have
    \[
    (S \setminus \{\mbf{v}_{i_1}, \dotsc, \mbf{v}_{i_{\ell}}\}) \cup \{\mbf{u}_1, \dotsc, \mbf{u}_j\}
    \]
    is a spanning set for $V$.
    \begin{proof}
        \textbf{Add proof}.
    \end{proof}
\end{theorem}

\begin{corollary}
    In the situation above $k \geq \ell$.
    In other words,
    a spanning set for a vector space is always at least as big as any linearly independent set within the vector space.
\end{corollary}

\begin{corollary}
    Let $V$ be a finite-dimensional vector space.
    Then every basis for $V$ has the same number of elements.
    \begin{proof}
        \textbf{Add proof}.
    \end{proof}
\end{corollary}

\begin{definition}[Dimension]
    Let $V$ be a vector space that is generated by finitely many vectors.
    The dimension,
    $\dim{(V)}$,
    of $V$ is defined to be the cardinality of any basis for $V$.
\end{definition}
This definition is best understood through the following examples.
\begin{example}
    The vector space $\R[x]_n$ has basis $1, x, \dotsc, x ^ n$.
    Hence
    \[
    \dim{\R[x]_n} = n + 1.
    \]

    Additionally,
    the vector space $M_{m, n}(\R)$ has a basis of matrices $E_{rs}$
    (for $1 \leq r \leq m$ and $1 \leq s \leq n$)
    and so
    \[
    \dim{M_{m, n}(\R)} = m \cdot n.
    \]
\end{example}

\begin{theorem}\label{pre:linalg:thm:finitedimensionalvecspacegivesdimresults}
    Let $V$ be a finite dimensional vector space and $U$ a vector subspace of $V$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item $U$ is also finite-dimensional;
        \item $\dim{(U)} \leq \dim{(V)}$;
        \item $\dim{(U)} = \dim{(V)} \iff U = V$.
    \end{enumerate}
    \begin{proof}
        \textbf{Add proof}.
    \end{proof}
\end{theorem}

\begin{remark}
    Not every vector space is finite-dimensional.
\end{remark}
This remark is shown through the following example.
\begin{example}
    The vector space $\R[x]$ of all polynomials has the infinite basis $S = \{1, x, x ^ 2, x ^ 3, \dotsc\}$.
    Every element in $\R[x]$ is the unique linear combination of finitely many elements in $S$.
\end{example}

The following theorem provides us with a list of useful observations.
\begin{theorem}
    Let $V$ be a vector space of dimension $k$ and let $\mbf{v}_1, \dotsc, \mbf{v}_{\ell}$ be $\ell$ vectors in $V$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item If $\ell > k$,
        then the $\mbf{v}_1, \dotsc, \mbf{v}_{\ell}$ are linearly dependent.
        \item If $\ell < k$,
        then the $\mbf{v}_1, \dotsc, \mbf{v}_{\ell}$ do not span $V$.
        \item If $k = \ell$,
        then the following statements are (pairwise) equivalent
        \begin{enumerate}[label = (\alph*)]
            \item The vectors $\mbf{v}_1, \dotsc, \mbf{v}_k$ form a basis of $V$.
            \item The vectors $\mbf{v}_1, \dotsc, \mbf{v}_k$ are linear independent.
            \item The vectors $\mbf{v}_1, \dotsc, \mbf{v}_k$ span $V$.
        \end{enumerate}
    \end{enumerate}
    \begin{proof}
        \textbf{Insert proof}.
    \end{proof}
\end{theorem}

\subsection{Bases and dimension in \texorpdfstring{$\R ^ n$}{}}
\begin{remark}
    Let $U$ be a vector subspace of $\R ^ n$.
    Then,
    of course,
    a basis for $U$ is any linearly independent set of vectors that span $U$.
\end{remark}

\begin{theorem}
    Let $\mbf{u}_1, \dotsc, \mbf{u}_k$ be $k$ vectors in $\R ^ n$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item If $k > n$,
        then the $\mbf{u}_1, \dotsc, \mbf{u}_k$ are linearly dependent.
        \item If $k < n$,
        then the $\mbf{u}_1, \dotsc, \mbf{u}_k$ do not span $\R ^ n$.
        \item If $k = n$,
        then
        \begin{align*}
            \mbf{u}_1, \dotsc, \mbf{u}_n\text{ form a basis of } \R ^ n &\iff\text{The matrix $(\mbf{u}_1, \dotsc, \mbf{u}_n)$ is non singular} \\
            &\iff \det(\mbf{u}_1, \dotsc, \mbf{u}_n) \neq 0.
        \end{align*}
        So,
        the vectors $\mbf{u}_1, \dotsc, \mbf{u}_k$ form a basis for $\R ^ n$ if and only if $k = n$ and the square matrix $(\mbf{u}_1, \dotsc, \mbf{u}_k)$ is non-singular.
    \end{enumerate}
    \begin{proof}
        \textbf{Add proof}.
    \end{proof}
\end{theorem}

\begin{definition}
    Let $A \in M_{n \times k}(\R)$ be a $n \times k$ matrix.
    \begin{enumerate}[label = (\roman*)]
        \item The column space of $A$ is the span of the columns of $A$.
        The column rank of $A$ is the dimension of the column space of $A$.

        The column space is the subspace of all $\mbf{b} \in \R ^ n$ such that $A\mbf{x} = \mbf{b}$ has a solution.

        \item The row space of $A$ is the span of the rows of $A$.
        The row rank of $A$ is the dimension of the row space of $A$.

        The row space is the subspace of all $c \in \R ^ k$ such that $A ^ t\mbf{y} = \mbf{c}$ has a solution.

        \item The nullspace
        (or kernal)
        of $A$ is the solution space to the homogeneous system $A\mbf{x} = \mbf{0}$.
        The nullity of $A$ is the dimension of the nullspace.
    \end{enumerate}
\end{definition}

\begin{lemma}\label{pre:linalg:lem:linindepvecmulmatrisindep}
    Let $\mbf{v}_1, \dotsc, \mbf{v}_k$ be linear independent vectors in $\R ^ n$ and let $P \in M_n(\R)$ be an invertible matrix.
    Then the vectors $P\mbf{v}_1, \dotsc,P\mbf{v}_k$ are linear independent.
    \begin{proof}
        Consider the equation $\lambda_1(P\mbf{v}_1) + \dotsi + \lambda_k(P\mbf{v}_k) = \mbf{0}$.
        By linearity of matrix multiplication this is equivalent to the equation $P(\lambda_1\mbf{v}_1 + \dotsi + \lambda_k\mbf{v}_k) = \mbf{0}$,
        and multiplying with $P ^ {-1}$ gives $\lambda_1\mbf{v}_1 + \dotsi + \lambda_k\mbf{v}_k = P ^ {-1}\mbf{0} = \mbf{0}$.
        But the $\mbf{v}_i$ are linear independent,
        so we have $\lambda_1 = \dotsi = \lambda_k = 0$.
        Thus the vectors $P\mbf{v}_1, \dotsc, P\mbf{v}_k$ are linear independent.
    \end{proof}
\end{lemma}

\begin{proposition}\label{pre:linalg:prop:invmatrsameranksandnul}
    Let $A \in M_{n \times k}(\R)$ and let $P \in M_{n}(\R)$ be an invertible matrix.
    Then $A$ and $PA$ have the same column rank,
    row rank,
    and nullity.
    In particular,
    elementary row operations do not change column rank,
    row rank,
    and nullity.
    More precisely,
    \begin{enumerate}
        \item $A$ and $PA$ have the same row space.
        In particular,
        ERO's do not change the row space.
        \item $PA$ may have a different column space than $A$,
        but the dimension stays the same.
        In particular,
        ERO's do not change the dimension of the column space.
        \item $A$ and $PA$ have the same nullsapce.
        In particular,
        ERO's do not change the nullspace.
    \end{enumerate}

    \begin{proof}
        Since every invertible matrix is a product of elementary matrices,
        it suffices to consider an elementary matrix $E$.
        \begin{enumerate}[label = (\roman*)]
            \item After applying an ERO the new rows are a linear combination of the old ones
            (think about what each ERO does).
            This could make the row space smaller
            (conceivably).
            but not bigger.
            However,
            the process is reversible
            (instead consider the same argument with $E ^ {-1}$),
            so it must be that the space didn't change.
            \item Denote the column space of $A$ by $U$ and let $\mbf{u}_1, \dotsc, \mbf{u}_{\ell}$ be a basis of $U$.
            But $A\mbf{x} = \mbf{b}$ if and only if $(EA)\mbf{x} = E\mbf{b}$
            (since applying an ERO does not change the solution set as per \autoref{pre:linalg:lem:solsetsam}).
            This shows that the column space of $EA$ is equal to $EU := \{E\mbf{u} : \mbf{u} \in U\}$.

            We claim $E\mbf{u}_1, \dotsc, E\mbf{u}_{\ell}$ is a basis of $EU$,
            and this will show the dimension of the two spaces $U$ and $EU$ is the same.
            To do this we need to show the vectors form a linearly independent spanning set of $EU$.
            By \autoref{pre:linalg:lem:linindepvecmulmatrisindep} we know $E\mbf{u}_1, \dotsc, E\mbf{v}_{\ell}$ are linearly independent.
            For spanning,
            let $\mbf{v} \in EU$ be given by $\mbf{v} = E\mbf{u}$ for some $\mbf{u} \in U$.
            We can write $\mbf{u} = \lambda_1\mbf{u}_1 + \dotsi + \lambda_{\ell}\mbf{u}_{\ell}$ for some $\lambda_i \in \R$.
            But then $\mbf{v} = E\mbf{u} = E(\lambda_1\mbf{u}_1 + \dotsi + \lambda_{\ell}\mbf{u}_{\ell}) = \lambda_1(E\mbf{u}_1) + \dotsi + \lambda_{\ell}(E\mbf{u}_{\ell})$.
            \item This follows immediately from \autoref{pre:linalg:lem:solsetsam}.
        \end{enumerate}
    \end{proof}
\end{proposition}

\begin{theorem}[Rank Theorem]\label{pre:linalg:thm:ranktheorem}
    The row rank of a matrix is equal to its column rank.
\end{theorem}

\begin{definition}[Rank]
    We define the rank of a matrix $A$ to be its $\text{row rank} = \text{column rank}$.
    We write $\mathrm{rank}(A) = \mathrm{rk}(A)$.
\end{definition}

\begin{theorem}[Rank-Nullity Theorem]
    Let $A \in M_{n \times k}(\R)$ be an $n \times k$ matrix.
    Then
    \[
    \mathrm{rk}(A) + \mathrm{nullity}(A) = k.
    \]
    \begin{proof}
        We prove \autoref{pre:linalg:thm:ranktheorem} and this theorem together.
        By \autoref{pre:linalg:prop:invmatrsameranksandnul} we can switch to the RREF $A'$ of $A$.
        After switching some columns if necessary we can assume it looks as follows:
        \[
        A' = \left(\hspace{-5pt}\begin{array}{c|c}
             I_r & C  \\
             \hline
             \mbf{0}_{(n - r)\times r} & \mbf{0}_{(n - r) \times (k - r)}
        \end{array}\hspace{-5pt}\right).
        \]
        For some $r \times (k - r)$ matrix $C$.
        But the first $r$ columns are linear independent
        (they are simply the first $r$ standard basis vectors of $\R ^ n$),
        and the other columns are in their span.
        Thus,
        $\mathrm{span}(\text{column space of } A') = \mathrm{span}\{\mbf{e}_1, \dotsc, \mbf{e}_r\}$ and
        \[
        \text{column rank of } A' = r.
        \]
        But also
        \[
        \text{row rank of } A' = r,
        \]
        since the first $r$ rows are linearly independent in $\R ^ k$
        (since the $1$'s in the $I_r$ are in different slots),
        and the other rows are zero.
        Finally,
        upon taking $x_{r + 1}, \dotsc, x_k$ as free variables,
        we see that a general vector in the nullspace of $A'$
        (that is,
        a solution $\mbf{x}$ to the system $A'\mbf{x} = \mbf{0}$)
        is given by
        \[
        \mbf{x} = x_{r + 1}\begin{pmatrix}
            -\mbf{c}_1 \\
            - - - \\
            1 \\
            0 \\
            \vdots \\
            0
        \end{pmatrix}
        + x_{r + 2}\begin{pmatrix}
            -\mbf{c}_2 \\
            - - - \\
            0 \\
            1 \\
            \vdots \\
            0
        \end{pmatrix}
        +
        \dotsi
        +
        x_k\begin{pmatrix}
            -\mbf{c}_{k - r} \\
            - - - \\
            0 \\
            0 \\
            \vdots \\
            1
        \end{pmatrix},
        \]
        where $\mbf{c}_i \in \R ^ r$ denotes the $i$-th column of $C$.
        The spanning vectors are clearly linear independent and so
        \[
        \mathrm{nullity}(A') = k - r.
        \]
    \end{proof}
\end{theorem}

\begin{corollary}
    For any matrix $A \in M_{n \times k}(\R)$ we have
    \begin{enumerate}[label = (\roman*)]
        \item The rank of $A$ is the number of leading $1$'s in the RREF of $A$.
        \item The nullity of $A$ is the number columns in the RREF of $A$ without a leading $1$;
        i.e.,
        the number of free variables in the solution set to $A\mbf{x} = \mbf{0}$.
    \end{enumerate}
\end{corollary}

\subsection{Sums and intersections of vector subspaces}
Let $V$ be a vector space and let $U, W$ be subspaces.
Then define
\begin{align*}
    U + W &:= \{\mbf{v} \in V\,:\, \mbf{v} = \mbf{u + w}\text{ for some }\mbf{u} \in U, \mbf{w} \in W\}, \\
    U \cap W &:= \{\mbf{v} \in V\,:\, \mbf{v} \in U\text{ and } \mbf{v} \in W\}.
\end{align*}

Both $U + W$ and $U \cap W$ are vector subspaces of $V$.

\begin{proposition}\label{pre:linalg:prop:sumofdimsissumofsumandintersection}
    Let $U$ and $W$ be finite-dimensional vector subspaces of a vector space $V$.
    Then
    \[
    \dim(U) + \dim(W) = \dim(U + W) + \dim(U \cap W).
    \]
    \begin{proof}
        Let $\{\mbf{v}_1, \dotsc, \mbf{v}_r\}$ be a basis for $U \cap W$
        (we know such a basis exists by \autoref{pre:linalg:thm:finitedimensionalvecspacegivesdimresults}).
        Using \autoref{pre:linalg:lem:ifnotinspanofvecspacethenlinindep} multiple times,
        we may repeatedly add vectors in $U$ to this set
        (keeping it linearly independent)
        until we span $U$.
        Do the same for $W$.
        Then,
        we have extended the basis $\{\mbf{v}_1, \dotsc, \mbf{v}_r\}$ to bases of $U$ and $W$ given by
        \[
        \{\mbf{v}_1, \dotsc, \mbf{v}_r, \mbf{u}_{r + 1}, \dotsc, \mbf{u}_s\},\quad\{\mbf{v}_1, \dotsc, \mbf{v}_r, \mbf{w}_{r + 1}, \dotsc, \mbf{w}_t\}
        \]
        respectively.
        Then we claim that
        \[
        B = \{\mbf{v}_1, \dotsc, \mbf{v}_r, \mbf{u}_{r + 1}, \dotsc, \mbf{u}_s, \mbf{w}_{r + 1}, \dotsc, \mbf{w}_t\}
        \]
        is a basis for $U + W$.
        If this claim is true we have
        \[
        \dim(U) + \dim(W) = s + t = (s + t - r) + r = \dim(U + W) + \dim(U \cap W).
        \]
        In order to prove this claim,
        observe that $B$ spans $U + W$,
        all we need to do now is establish linear independence.
        If $s = 0$ or $t = 0$ it is trivial.
        Assume $s \geq 1$ and $t \geq 1$.
        Now consider
        \[
        \mbf{0} = \lambda_1\mbf{v}_1 + \dotsi + \lambda_r\mbf{v}_r + \mu_{r + 1}\mbf{u}_{r + 1} + \dotsi + \mu_s\mbf{u}_s + \gamma_{r + 1}\mbf{w}_{r + 1} + \dotsi + \gamma_t\mbf{w}_t.
        \]
        Suppose at least one of the $\gamma_j$ is non-zero.
        This gives
        \[
        \lambda_1\mbf{v}_1 + \dotsi + \lambda_r\mbf{v}_r + \mu_{r + 1}\mbf{u}_{r + 1} + \dotsi + \mu_s\mbf{u}_s + = -(\gamma_{r + 1}\mbf{w}_{r + 1} + \dotsi + \gamma_t\mbf{w}_t).
        \]
        The left hand side is in $U$ and the right hand side is in $W \setminus (U \cap W)$.
        But,
        the intersection of these two sets is empty,
        so we have a contradiction.
        Thus $\gamma_{r + 1} = \dotsi = \gamma_t = 0$.
        Since the vectors
        \[
        \{\mbf{v}_1, \dotsc, \mbf{v}_r, \mbf{u}_{r + 1}, \dotsc, \mbf{u}_s\}
        \]
        are linearly independent we must also have $\lambda_1 = \dotsi = \lambda_r = \mu_{r + 1} = \dotsi = \mu_s = 0$.
        Thus the vectors $B$ are linearly independent.
    \end{proof}
\end{proposition}

\begin{definition}[Direct Sum]
    Suppose that $U$ and $W$ are vector subspaces of $V$ with
    \begin{enumerate}[label = (\roman*)]
        \item $U + W = V$;
        \item $U \cap W = \{\mbf{0}\}$.
    \end{enumerate}
    Then,
    we say $V$ is the direct sum of $U$ and $W$ and write $V = U \oplus W$.
\end{definition}

\begin{proposition}
    Let $U$ and $W$ be two subspaces of a vector space $V$.
    Then the following statements are pairwise equivalent:
    \begin{enumerate}[label = (\roman*)]
        \item $V = U \oplus W$;
        \item If $\{\mbf{u}_1, \dotsc, \mbf{u}_s\}$ is a basis for $U$ and $\{\mbf{w}_1, \dotsc, \mbf{w}_t\}$ is a basis for $W$,
        then $\{\mbf{u}_1, \dotsc, \mbf{u}_s, \mbf{w}_1, \dotsc, \mbf{w}_t\}$ is a basis for $V$;
        \item $V = U + W$ and $\dim{V} = \dim{U} + \dim{W}$;
        \item Every $\mbf{v} \in V$ can be written uniquely as $\mbf{v} = \mbf{u} + \mbf{w}$ where $\mbf{u} \in U$ and $\mbf{w} \in W$.
    \end{enumerate}
    \begin{proof}
        $(i) \iff (ii) \iff (iii)$ follows from \autoref{pre:linalg:prop:sumofdimsissumofsumandintersection} and its proof.
        For $(i) \iff (iv)$ see problem sheet $9$.
    \end{proof}
\end{proposition}

\subsection{Coordinates}
If $B = \{\mbf{v}_1, \dotsc, \mbf{v}_n\}$ is a basis for $V$ and if $\mbf{u}$ is any point of $V$ then
(by \autoref{pre:linalg:thm:whatisabasis})
we can uniquely express $\mbf{u}$ as
\[
\mbf{u} = x_1\mbf{v}_1 + \dotsi + x_n\mbf{v}_n.
\]
We call the real numbers $(x_1, \dotsc, x_n)$ the coordinates of $\mbf{u}$ with respect to this basis.

The coordinates are simply a collection of real numbers,
if we wished we could write them as a column vector.
We obtain a map,
which we call the coordinate map,
\begin{align*}
    \Phi = \Phi_B : &V \rightarrow &\R ^ n \\
    &\mbf{u} \mapsto \Phi(\mbf{u}) &:=\begin{pmatrix}
        x_1 \\ \vdots \\ x_n
    \end{pmatrix}.
\end{align*}
It is clear that the coordinate map depends on the choice of the basis $B$.

For a column vector $\mbf{x} = \begin{pmatrix}
    x_1 \\ \vdots \\ x_n
\end{pmatrix} \in \R ^ n$,
we have that $(x_1, \dotsc, x_n)$ are the coordinates of $\mbf{x}$ with respect to the standard basis $\{\mbf{e}_1, \dotsc, \mbf{e}_n\}$.

\begin{lemma}
    Consider a basis $\mbf{v}_1, \mbf{v}_2, \dotsc, \mbf{v}_n$ of $\R ^ n$,
    and let $P = (\mbf{v}_1, \mbf{v}_2, \dotsc, \mbf{v}_n)$ be the associated matrix of column vectors.
    Then the coordinates $(x_1, \dotsc, x_n)$ of $\mbf{u}$ with respect to $\mbf{v}_1, \mbf{v}_2, \dotsc, \mbf{v}_n$ are given by
    \[
    \begin{pmatrix}
        x_1 \\ \vdots \\ x_n
    \end{pmatrix} = P ^ {-1}\mbf{u}.
    \]
    Note that if $\{\mbf{v}_1, \mbf{v}_2, \dotsc, \mbf{v}_n\} = \{\mbf{e}_1, \dotsc, \mbf{e}_n\}$ then $P = I_n$.
\end{lemma}

\newpage

\section{Linear maps}

\subsection{Definition and examples of linear maps}

\begin{definition}[Linear map]
    Let $V$ and $W$ be two vector spaces,
    with addition denoted by $+_{V}$ and $+_{W}$ respectively.
    A linear map $T : V \rightarrow W$ is a function satisfying the following properties
    \begin{enumerate}[label = (\roman*)]
        \item for all $\mbf{u}$ and $\mbf{v}$ in $V$ we have $T(\mbf{u} +_{V} \mbf{v}) = T(\mbf{u}) +_{W} T(\mbf{v})$;
        \item for all $\mbf{v}$ in $V$ and all $\lambda \in \R$ we have $T(\lambda\mbf{v}) = \lambda T(\mbf{v})$.
    \end{enumerate}
\end{definition}

A linear map is sometimes referred to as a linear transformation or a linear mapping.

\begin{example}
    The map $T$ given by
    \[
    T\begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix}
    =
    \begin{pmatrix}
        3x - y + z \\ z \\ 0
    \end{pmatrix}
    =
    \begin{pmatrix}
        3 & -1 & 1 \\ 0 & 0 & 1 \\ 0 & 0 & 0
    \end{pmatrix}
    \begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix}
    \]
    is a linear map $\R ^ 3 \rightarrow \R ^ 3$
\end{example}

\begin{example}
    The map $T$ given by
    \[
    T\begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix}
    =
    \begin{pmatrix}
        x + 1 \\ x \\ y
    \end{pmatrix}
    =
    \begin{pmatrix}
        1 & 0 & 0 \\ 1 & 0 & 0 \\ 0 & 1 & 0
    \end{pmatrix}
    \begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix}
    +
    \begin{pmatrix}
        1 \\ 0 \\ 0
    \end{pmatrix}
    \]
    is not linear,
    since $T\begin{pmatrix}
        0 \\ 0 \\ 0
    \end{pmatrix} = \begin{pmatrix}
        1 \\ 0 \\ 0
    \end{pmatrix} \neq \mbf{0}$.

    It is however called an affine map,
    it is of the form $A\mbf{x} + \mbf{b}$.
\end{example}

\begin{lemma}
    Let $\{\mbf{v}_1, \dotsc, \mbf{v}_n\}$ be a basis of a vector space $V$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item Any linear map $T : V \rightarrow W$ of vector spaces is determined by its values $T(\mbf{v}_1), \dotsc, T(\mbf{v}_n)$ on the basis vectors of $V$.
        \item Conversely,
        given arbitrary vectors $\mbf{w}_1, \dotsc, \mbf{w}_n$ in $W$,
        then there exists a linear map $T : V \rightarrow W$ such that
        \[
        T(\mbf{v}_1) = \mbf{w}_1, \dotsc, T(\mbf{v}_n) = \mbf{w}_n.
        \]
    \end{enumerate}
    \begin{proof}
        
    \end{proof}
\end{lemma}

\begin{lemma}
    A map $T : \R ^ n \rightarrow \R ^ m$ is linear if and only if there is a $m \times n$ matrix $A$ such that
    \[
    T(\mbf{x}) = A\mbf{x}.
    \]
    Moreover,
    $A$ is given by
    \[
    A = (T(\mbf{e}_1), T(\mbf{e}_2), \dotsi, T(\mbf{e}_n)).
    \]
    So the $i$th column is the image of the standard basis vector $\mbf{e}_i$.
\end{lemma}

We can see that the only linear maps between Euclidean spaces are the matrices.

\subsection{Properties of linear maps}
\begin{lemma}
    Let $T : V \rightarrow W$ and $S : W \rightarrow U$ be two linear maps of vector spaces.
    Then the composition $S \circ T : V \rightarrow U$ is a linear map.
    In particular,
    if $T : \R ^ n \rightarrow \R ^ m$ and $S : \R ^ m \rightarrow \R ^ k$ are linear maps given by $T(\mbf{x}) = A\mbf{x}$ and $S(\mbf{y}) = B\mbf{y}$,
    then the composition $S \circ T : \R ^ n \rightarrow \R ^ k$ is again linear,
    and moreover we have
    \[
    (S \circ T)(\mbf{x}) = (BA)\mbf{x}.
    \]
    So the composition of linear maps corresponds to matrix multiplication.
\end{lemma}

\begin{definition}[Isomorphism]
    Let $T : V \rightarrow W$ be a bijective linear map of vector spaces
    (that is,
    a linear map which is one-to-one and onto).
    Then we call $T$ an isomorphism and call $V$ and $W$ isomorphic.
    We write $V \cong W$.
\end{definition}

\begin{lemma}
    Let $T : V \rightarrow W$ be an isomorphism of vector spaces.
    Then the inverse map $T ^ {-1} : W \rightarrow V$ is also a linear map,
    in fact an isomorphism.
    For the linear map $T : \R ^ n \rightarrow \R ^ n$ given by the matrix $A \in M_n(\R)$,
    we have that $T$ is an isomorphism if and only if $A$ is invertible.
    In that case,
    we have
    \[
    T ^ {-1}(\mbf{y}) = A ^ {-1}\mbf{y}.
    \]
\end{lemma}

\begin{lemma}
    Let $V$ b a vector space of dimension $n$.
    Then the coordinate map $\Phi : V \rightarrow \R ^ n$ is an isomorphism.
    In particular,
    every finite dimensional vector space is isomorphic to the Euclidean space of the same dimension.
\end{lemma}

\begin{theorem}
    Let $V$ be a vector space of dimension $n$ and let $W$ be a vector space of dimension $m$.
    Let $\phi : V \rightarrow \R ^ n$ and $\psi : W \rightarrow \R ^ m$ be two coordinate maps with respect to the choice of some bases $V$ and $W$ respectively.
    Let $T : V \rightarrow W$ be a linear map.
    Then there exists a $m \times n$ matrix $A$ so that for all $\mbf{v} \in V$ we have
    \[
    \phi(T(\mbf{v})) = A\phi(\mbf{v}).
    \]
    We say $T$ is represented by $A$.
    (The matrix $A$ depends on the choice of the bases made.)
\end{theorem}

\subsection{Image, kernel, rank and nullity}
\begin{definition}[Image and kernel]
    Let $V$ and $W$ be vector spaces and $T : V \rightarrow W$ be a linear map.
    We define the image of $T$ by
    \[
    \mathrm{im}(T) = \{\mbf{w} \in W : \text{there exists $\mbf{v} \in V$ so that } \mbf{w} = T(\mbf{v})\}.
    \]
    We define the kernel of $T$ by
    \[
    \ker(T) = \{\mbf{v} \in V : T(\mbf{v}) = \mbf{0}\}.
    \]
\end{definition}

\begin{lemma}
    Let $T : V \rightarrow W$ be a linear map of vector spaces.
    Then $\mathrm{im}(T)$ is a vector subspace of $W$ and $\ker(T)$ is a vector subspace of $V$.
\end{lemma}

\begin{lemma}
    Let $T : V \rightarrow W$ be a linear map of vector spaces,
    and let $\{\mbf{v}_1, \dotsc, \mbf{v}_n\}$ be a basis for $V$.
    Then the image $\mathrm{im}(T)$ is spanned by $T(\mbf{v}_1), \dotsc, T(\mbf{v}_n)$.
    \begin{proof}
        Let $\mbf{w} = T(\mbf{u})$ be in the image of $T$.
        We can write $\mbf{u} = \lambda_1\mbf{V}_1 + \dotsi + \lambda_n\mbf{v}_n$,
        and then we see immediately
        \[
        \mbf{w} = T(\mbf{u}) = T(\lambda_1\mbf{v}_1 + \dotsi + \lambda_n\mbf{v}_n) = \lambda_1T(\mbf{v}_1) + \dotsi + \lambda_nT(\mbf{v}_n).
        \]
        So $\mbf{w}$ is in the span of $T(\mbf{v}_1), \dotsc, T(\mbf{v}_n)$.
    \end{proof}
\end{lemma}

\begin{proposition}
    Let $T : V \rightarrow W$ be a linear map of vector spaces.
    Then the following statements are
    (pairwise)
    equivalent:
    \begin{enumerate}[label = (\roman*)]
        \item $\ker(T) = \{\mbf{0}\}$;
        \item $T$ is one-to-one
        (that is,
        it is injective);
        \item If $\mbf{v}_1, \dotsc, \mbf{v}_k$ are linear independent vectors in $V$,
        then the images $T(\mbf{v}_1), \dotsc, T(\mbf{v}_k)$ are linearly independent in $W$.
    \end{enumerate}
\end{proposition}

\begin{corollary}
    Let $T : V \rightarrow W$ be a linear map of vector spaces with $\ker(T) = \{\mbf{0}\}$.
    Assume $\dim{V} = n$ with a basis $\{\mbf{v}_1, \dotsc, \mbf{v}_n\}$.
    Then the images $\{T(\mbf{v}_1), \dotsc, T(\mbf{v}_n)\}$ are a basis for $\mathrm{im}(T)$.
    In particular,
    we have $\dim(\mathrm{im}(T)) = \dim(V)$.
\end{corollary}

\begin{definition}[Rank and nullity of a linear map]
    Let $V$ and $W$ be vector spaces and $T : V \rightarrow W$ be a linear map.
    We define the rank of $T$ to be
    \[
    \mathrm{rk}(T) = \dim(\mathrm{im}(T)).
    \]
    We define the nullity of $T$ to be
    \[
    n(T) = \dim(\ker(T)).
    \]
\end{definition}

\begin{proposition}
    Let $T : V \rightarrow W$ be a linear map of finite-dimensional vector spaces and let $\phi : V' \rightarrow V$ and $\psi : W \rightarrow W'$ be two isomorphisms.
    Then
    \[
    \mathrm{rk}(\psi \circ T \circ \phi) = \mathrm{rk}(T)\quad\text{and}\quad n(\psi \circ T \circ \phi) = n(T).
    \]
\end{proposition}

\begin{theorem}[Rank-Nullity Theorem for linear maps]
    Let $V$ and $W$ be two finite-dimensional vector spaces and $T : V \rightarrow W$ be a linear map.
    Then
    \[
    \dim(V) = \mathrm{rk}(T) + n(T).
    \]
\end{theorem}

\begin{remark}
    Alternatively,
    we could argue directly.
    Let $\mbf{u}_1, \dotsc, \mbf{u}_r$ be a basis for $\ker(T)$ and let $\mbf{w}_1, \dotsc, \mbf{w}_s$ be a basis for $\mathrm{im}(T)$.
    By the definition of $\mathrm{im}(T)$ there exist $\mbf{v}_1, \dotsc, \mbf{v}_s$ with $T(\mbf{v}_i) = \mbf{w}_i$ for $1 \leq i \leq s$.
    Now we claim that $\mbf{u}_1, \dotsc, \mbf{u}_r, \mbf{v}_1, \dotsc, \mbf{v}_s$ is a basis for $V$,
    which will prove the result.
\end{remark}

\begin{theorem}
    For two finite-dimensional vector spaces $V$ and $W$ we have
    \[
    V \cong W \iff \dim{V} = \dim{W}.
    \]
\end{theorem}

\subsection{Change of basis and of coordinates}

\begin{theorem}
    Let $T : \R ^ n \rightarrow \R ^ m$ be a linear map.
    Suppose that with respect to the standard bases $T$ is represented by the matrix $A$.
    Let $S = \{\mbf{v}_1, \dotsc, \mbf{v}_n\}$ be any basis of $\R ^ n$ and let $P = (\mbf{v}_1, \dotsi, \mbf{v}_n)$.
    Let $S' = \{\mbf{w}_1, \dotsc, \mbf{w}_m\}$ be any basis of $\R ^ m$ and let $Q = (\mbf{w}_1, \dotsc, \mbf{w}_m)$.
    Then with respect to the bases $S$ of $\R ^ n$ and $S'$ of $\R ^ m$ the transformation $T$ is represented by the matrix $B = Q ^ {-1}AP$.
\end{theorem}

\begin{corollary}
    Suppose that $\{\mbf{v}_1, \dotsc, \mbf{v}_n\}$ and $\{\mbf{v}'_1, \dotsc, \mbf{v}'_n\}$ are any bases of $\R ^ n$.
    Let $P$ be a matrix so that $\mbf{v}'_j = P\mbf{v}_j$.
    Suppose that $\{\mbf{w}_1, \dotsc, \mbf{w}_m\}$ and $\{\mbf{w}'_1, \dotsc, \mbf{w}'_m\}$ are any bases of $\R ^ m$.
    Let $Q$ be a matrix so that $\mbf{w}'_j = Q\mbf{w}_j$.

    Let $T : \R ^ n \rightarrow \R ^ m$ be a linear map.
    Suppose that with respect to the bases $\{\mbf{v}_1, \dotsc, \mbf{v}_n\}$ and $\{\mbf{w}_1, \dotsc, \mbf{w}_m\}$ the map $T$ is represented by the matrix $A$.
    Then with respect to the bases $\{\mbf{v}'_1, \dotsc, \mbf{v}'_n\}$ and $\{\mbf{w}'_1, \dotsc, \mbf{w}'_m\}$ the map $T$ is represented by the matrix $B = Q ^ {-1}AP$.
\end{corollary}

\newpage

\section{Eigenvalues, Eigenvectors, and Diagonalisation}

\subsection{Prologue}

\subsection{Eigenvalues and eigenvectors}

\begin{definition}
    Let $A$ be a square matrix.
    A non-zero $n$-vector is an eigenvector of $A$ with eigenvalue $\lambda \in \C$,
    if
    \[
    A\mbf{v} = \lambda\mbf{v}.
    \]
\end{definition}
From this we can see
\[
\mbf{0} = A\mbf{v} - \lambda\mbf{v} = (A - \lambda I)\mbf{v}.
\]
By the fundamental lemma of invertible matrices we have the existence for such a $\mbf{v}$ for a given $\lambda$ is equivalent to the matrix $A - \lambda I$ being non-invertible.
Hence the set of eigenvalues $\lambda$ is the set of solutions to
\[
\det(A - \lambda I) = 0.
\]

We define $p_A(t) = \det(A - tI)$ to be the characteristic polynomial of $A$.

\begin{example}
    If $A$ is a diagonal matrix,
    then the eigenvalues of $A$ are precisely the entries on the diagonal.
    For example,
    \[
    A = \begin{pmatrix}
        -2 & 0 & 0 \\
        0 & 3 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \]
    has characteristic polynomial $p_A(t) = -(t + 2)(t - 3)(t - 1)$,
    so the eigenvalues are $\lambda = -2, 3, 1$.
\end{example}

To find the corresponding \textbf{eigenvectors},
we solve
\[
(A - \lambda I)\mbf{v} = \mbf{0}.
\]

\textbf{Multiple eigenvalues}.
If we have a polynomial $p(t)$ with degree $N$,
the following decomposition holds:
\[
p(t) = c(t - \lambda_1) ^ {k_1} \cdot (t - \lambda_2) ^ {k_2} \cdot \dotsc \cdot (t - \lambda_p) ^ {k_p},
\]
with $c \in \C$ is non-zero.
The roots,
$\lambda \in \C$,
are all distinct,
i.e. $\lambda_i \neq \lambda_j$ for all $i \neq j$.

\begin{definition}
    The algebraic multiplicity of the root $\lambda_i$ is given by $k_i \in \N$,
    with $k_i \neq 0$,
    and $\sum_{i = 1}^{p}k_i = N$.
\end{definition}

\begin{remark}
    If an eigenvalue $\lambda$ has multiplicity $k \neq 0 \in \N$ it means that the characteristic polynomial takes the form
    \[
    p_A(t) = (t - \lambda) ^ kQ(t),
    \]
    where the polynomial $Q(t)$ has degree $N - k$ and it is non-vanishing on $\lambda$,
    i.e. $Q(\lambda) \neq 0$.
\end{remark}

\begin{proposition}
    There are $p$ linearly independent eigenvectors corresponding to $\lambda$,
    where $1 \leq p \leq k$.
\end{proposition}

\begin{definition}
    The eigenspace $V_{\lambda}$ is the $p$-dimensional vector space spanned by these eigenvectors.
    Since the eigenspace $V_{\lambda}$ is generated by all the eigenvectors associated with the eigenvalue $\lambda$ we can also write it as
    \[
    V_{\lambda} = \ker(A - \lambda I),
    \]
    where the kernel,
    $\ker$,
    or a linear operator $B$ is a subspace of the full vector space $V$ given by
    \[
    \ker{B} = \left\{\mbf{v} \in V\mmid B\mbf{v} = 0\right\}.
    \]
\end{definition}

\begin{definition}
    We call $p = \dim{V_{\lambda}}$ the geometric multiplicity of $\lambda$.
\end{definition}

\subsection{The Cayley-Hamilton theorem}
\begin{theorem}[Cayley-Hamilton]
    Let $B$ be a square matrix with characteristic polynomial $p_B(t)$.
    Then $p_B(B) = 0$,
    as a matrix equation.
    \begin{proof}[Partial proof]
        Let us show this first for $2 \times 2$ matrices by direct calculation.
        Let
        \[
        B = \begin{pmatrix}
            a & b \\ c & d
        \end{pmatrix}.
        \]
        Then $p_B(t) = t ^ 2 - (a + d)t + (ad - bc)$,
        and
        \[
        p_B(B) = \begin{pmatrix}
            a & b \\ c & d
        \end{pmatrix}\begin{pmatrix}
            a & b \\ c & d
        \end{pmatrix} - (a + d)\begin{pmatrix}
            a & b \\ c & d
        \end{pmatrix} + (ad - bc)\begin{pmatrix}
            1 & 0 \\ 0 & 1
        \end{pmatrix} = \begin{pmatrix}
            0 & 0 \\
            0 & 0
        \end{pmatrix}.
        \]
    \end{proof}
    \begin{proof}
        The matrix $B$ defines a linear transformation $T$.
        Let us change basis to a flag basis for $T$,
        i.e.
        an ordered basis $\{\mbf{v}_1, \dotsc, \mbf{v}_N\}$ such that $B$ in this basis becomes upper triangular
        \[
        A = P ^ {-1}BP = \begin{pmatrix}
            a_{11} & a_{12} & \dotsi & a_{1N} \\
            0 & a_{22} & \dotsi & a_{2N} \\
            \vdots & \ddots & \ddots & \vdots \\
            0 & \dotsi & 0 & a_{NN}
        \end{pmatrix}
        \]
        where $P$ is the change of basis matrix.
        We use the result that for any linear transformation $T$ we can always find such a change of basis.

        The characteristic polynomial does not change under changes of basis
        (see the proposition below),
        so the characteristic polynomial of the matrix in the new basis,
        which takes the form
        \[
        p_A(t) = (a_{11} - t)(a_{22} - t) \dotsi (a_{NN} - t),
        \]
        is the same as the original characteristic polynomial $p_B(t)$.
        Also,
        \begin{align*}
            p_A(A) &= (a_{11} - A)(a_{22} - A) \dotsi (a_{NN} - A) \\
            &= (P ^ {-1}a_{11}P - P ^ {-1}BP)(P ^ {-1}a_{22}P - P ^ {-1}BP) \dotsi (P ^ {-1}a_{NN}P - P ^ {-1}BP) \\
            &= P ^ {-1}(a_{11} - B)(a_{22} - B) \dotsi (a_{NN} - B)P \\
            &= P ^ {-1}p_B(B)P
        \end{align*}
        so $p_A(A)$ will vanish if and only if $p_B(B)$ vanishes.
        We will now show that $p_A(A) = 0$.
        To prove this matrix equation we will show for all $1 \leq i \leq N$ that $\forall \mbf{v} \in V_i := \mathrm{span}(\mbf{v}_1, \dotsc, \mbf{v}_i)$ we have $p_i(A)\mbf{v} = 0$,
        with
        \[
        p_i(A) = (A - a_{11}I)(A - a_{22}I) \dotsi (A - a_{ii}I).
        \]
        The case $i = N$ is the statement we are after,
        since in this case $p_N(A) = p_A(A)$ and $V_N$ is the full vector space.
        We do this using induction on $i$.
        For $i = 1$ we have that $V_1 = \mathrm{span}(\mbf{v}_1)$ and $p_1 = (A - a_{11}I)$.
        Clearly,
        from the upper triangular form of $A$,
        \[
        A\mbf{v}_1 = a_{11}\mbf{v}_1
        \]
        so for $i = 1$ indeed $p_1\mbf{v} = 0$ for all $\mbf{v} \in V_1$.
        Let us assume that the inductive hypothesis is valid up to $i - 1$,
        we will show that it also holds for $i$.
        To prove it we note that $\forall \mbf{v} \in V_i$,
        we can write
        \[
        \mbf{v} = \alpha\mbf{v}_i + \mbf{w},
        \]
        with $\alpha \in \C$ and $\mbf{w} \in V_{i - 1}$.
        By linearity
        (and noting that $p_i = p_{i - 1}(A - a_{ii}I) = (A - a_{ii})p_{i - 1}$)
        \[
        p_i\mbf{v} = \alpha p_{i - 1}(A - a_{ii}I)\mbf{v}_i + (A - a_{ii})p_{i - 1}\mbf{w}
        \]
        but $p_{i - 1}\mbf{w} = 0$ by the inductive hypothesis,
        so
        \[
        p_i\mbf{v} = \alpha p_{i - 1}(A - a_{ii}I)\mbf{v}_i.
        \]
        Now,
        because of the upper triangular form,
        if we apply $A$ to $\mbf{v}_i$ we obtain
        \[
        A\mbf{v}_i = a_{ii}\mbf{v}_i
        \]
        with $\mbf{w}_{i - 1} \in V_{i - 1}$.
        Therefore
        \[
        p_{i - 1}(A - a_{ii}I)\mbf{v}_i = p_{i - 1}\mbf{w}_{i - 1} = 0
        \]
        concluding our theorem.
    \end{proof}
\end{theorem}

\subsection{The similarity relation}
\begin{definition}
    An equivalence relation,
    $\sim$,
    on a set $X$,
    is a binary relation with the following properties:
    \begin{enumerate}[label = (\roman*)]
        \item $a \sim a$
        (reflexivity).
        
        \item $a \sim b \implies b \sim a$
        (symmetry).

        \item If $a \sim b$ and $b \sim c \implies a \sim c$
        (transitivity).
    \end{enumerate}
    $\forall a, b, c \in X$.
\end{definition}

\begin{definition}
    We say that two square matrices $A$ and $B$
    (of the same size)
    are similar,
    written $A \sim B$,
    if there exists an invertible matrix $M$ such that $A = MBM ^ {-1}$.
\end{definition}

\begin{proposition}
    Similarity is an equivalence relation.
\end{proposition}

The set of all $n \times n$ matrices gets partitioned into equivalence classes
\[
\left[A\right] = \left\{B \mmid B \sim A, \text{ equivalently } B = MAM ^ {-1}\text{ for some } M \in \mathrm{GL}(n, \R)\right\}.
\]
We have that $\mathrm{GL}(n, \R)$ denotes the set of invertible $n \times n$ matrices with real coefficients,
i.e.
\[
\mathrm{GL}(n, \R) = \{M \in M_n(\R)\text{ s.t. } \det{M} \neq 0\}.
\]
This set is usually called General Linear group.
Changes of basis matrices are elements of the general linear group.

\begin{proposition}
    Similar matrices have the same eigenvalues.
\end{proposition}

\subsection{Diagonalisation by change of basis}

\begin{definition}
    A square matrix $A$ is said to be diagonalisable if $A$ is similar to a diagonal matrix.
\end{definition}

\begin{proposition}
    The $n \times n$ matrix $A$ is diagonalisable if and only if it has $n$ linearly independent eigenvectors.
\end{proposition}

\begin{example}
    If possible,
    diagonalise the matrix
    \[
    A = \begin{pmatrix}
        -1 & 4 \\ -2 & 5
    \end{pmatrix}.
    \]
    \begin{solution}
        $p(t) = t ^ 2 - 4t + 3 = (t - 1)(t - 3)$,
        so we get eigenvalues $\lambda_1 = 1, \lambda_2 = 3$.

        Now we need to get a similar matrix,
        this will have the same eigenvalues.
        Thus

        $\lambda = 1$
        \[
        \begin{pmatrix}
            0 \\ 0
        \end{pmatrix}
        = \mbf{0} = (A - \lambda I)\mbf{v} = (A - I)\mbf{v} = \begin{pmatrix}
            -2 & 4 \\ -2 & 4
        \end{pmatrix}\begin{pmatrix}
            v_1 \\ v_2
        \end{pmatrix}
        \]
        solving this gives the condition $-2v_1 + 4v_2 = 0$,
        taking $v_2 = 1$ gives us $v_1 = 2$,
        so
        \[
        \mbf{v}_1 = \begin{pmatrix}
            2 \\ 1
        \end{pmatrix}.
        \]
        
        $\lambda = 3$
        \[
        \begin{pmatrix}
            0 \\ 0
        \end{pmatrix}
        = \mbf{0} = (A - \lambda I)\mbf{v} = (A - 3I)\mbf{v} = \begin{pmatrix}
            -4 & 4 \\ -2 & 2
        \end{pmatrix}\begin{pmatrix}
            v_1 \\ v_2
        \end{pmatrix}
        \]
        which gives $v_1 = v_2$,
        taking $v_2 = 1$ gives $v_1 = 1$,
        hence
        \[
        \mbf{v}_2 = \begin{pmatrix}
            1 \\ 1
        \end{pmatrix}
        \]

        We can set
        \[
        M = \begin{pmatrix}
            2 & 1 \\ 1 & 1
        \end{pmatrix},\quad
        M ^ {-1} =\begin{pmatrix}
            1 & -1 \\ -1 & 2
        \end{pmatrix},
        \]
        then
        \[
        M ^ {-1}AM = \begin{pmatrix}
            1 & 0 \\ 0 & 3
        \end{pmatrix}.
        \]
    \end{solution}
\end{example}

\begin{remark}
    We can actually check our answer without computing $M ^ {-1}$,
    we just have to check that $AM = MD$,
    with $D$ being the diagonal matrix of eigenvalues
    (in the right order).
\end{remark}

\begin{example}
    Solve the system of ODEs
    \begin{align*}
        \dot{x} &= -x + 4y, \\
        \dot{y} &= -2x + 5y,
    \end{align*}
    where $x = x(t)$,
    $y = y(t)$ and $\dot{x} = \frac{dx(t)}{dt}$,
    similarly $\dot{y} = \frac{dy(t)}{dt}$.
    \begin{solution}
        If we diagonalise the system this is the same thing as changing basis.
        If $\mbf{v}(t)$ denotes the column vector with entries $(x(t), y(t))$,
        put $\mbf{w}(t) = M ^ {-1}\mbf{v}(t)$.
        Then $\mbf{w}(t)$ satisfies the system $\dot{\mbf{w}}(t) = D\mbf{w}(t)$,
        where $D = \mathrm{diag}(1, 3)$.

        The solution is $w_1(t) = c_1e ^ t$,
        $w_2(t) = c_2e ^ {3t}$,
        with $c_1$ and $c_2$ constant;
        and then using $\mbf{v}(t) = M\mbf{w}(t)$ gives
        \begin{align*}
            x(t) &= 2c_1e ^ t + c_2e ^ {3t}, \\
            y(t) &= c_1e ^ t + c_"e ^ {3t}.
        \end{align*}
    \end{solution}
\end{example}

\begin{proposition}
    Eigenvectors that correspond to distinct eigenvalues of $A$ are linearly independent.
    In particular,
    if all eigenvalues of $A$ are distinct,
    then $A$ is diagonalisable.
\end{proposition}

\begin{example}
    Let us try to diagonalise
    \[
    A = \begin{pmatrix}
        -2 & - 1 \\ 5 & 2
    \end{pmatrix}.
    \]
    \begin{solution}
        $p(t) = t ^ 2 + 1$,
        so the eigenvalues are $\lambda_1 = i, \lambda_2 = -i$.
        Then we get
        
        \textit{Using the fact that $A$ is similar to a diagonal matrix and that similar matrices have the same eigenvalues,
        so $A \sim B$ with $B = MAM ^ {-1}$ is equivalent to $M ^ {-1}BM = A$}.
        \[
        M = \begin{pmatrix}
            -2 + i & -2 - i \\ 5 & 5
        \end{pmatrix},
        \qquad
        M ^ {-1} = \frac{1}{10}\begin{pmatrix}
            -5i & 1 - 2i \\ 5i & 1 + 2i
        \end{pmatrix},
        \]
        and
        \[
        M ^ {-1}AM = \begin{pmatrix}
            i & 0 \\ 0 & -i
        \end{pmatrix}.
        \]
    \end{solution}
\end{example}

\begin{example}
    If
    \[
    A = \begin{pmatrix}
        -1 & 4 \\ -2 & 5
    \end{pmatrix},
    \]
    derive a formula for $A ^ n$.
    \begin{solution}
        Note that $A = MDM ^ {-1}$,
        $D$ is the diagonal matrix,
        so after we get that $p_A(t) = (t - 3)(t - 1)$ and $\lambda_1 = 1, \lambda_2 = 3$,
        we can see that
        \[
        D = \begin{pmatrix}
            1 & 0 \\
            0 & 3
        \end{pmatrix},
        \]
        Finding the eigenvectors using the usual method we get
        \[
        M = \begin{pmatrix}
            2 & 1 \\
            1 & 1
        \end{pmatrix}, \qquad M ^ {-1} = \begin{pmatrix}
            1 & -1 \\
            -1 & 2
        \end{pmatrix}.
        \]
        From here we can see that
        \begin{align*}
            A = MDM ^ {-1} \iff A ^ n &= (MDM ^ {-1})(MDM ^ {-1}) \dotsi (MDM ^ {-1}) \\
            &= (M)D(M ^ {-1}M)D(M ^ {-1}M)\dotsi (M ^ {-1}M)D(M ^ {-1}) \\
            &= MD ^ {n}M ^ {-1} \\
            &= \begin{pmatrix}
                2 - 3 ^ n & 2 \cdot 3 ^ n - 2 \\
                1 - 3 ^ n & 2 \cdot 3 ^ n - 1
            \end{pmatrix}.
        \end{align*}
    \end{solution}
\end{example}

\newpage

\section{Inner-product spaces}

\subsection{Real bilinear forms and inner products}
The dot product is linear in both "entries"
(we will say bilinear),
i.e.
\begin{align*}
    (\alpha\mbf{v}_1 + \beta\mbf{v}_2) \cdot \mbf{w} &= \alpha\mbf{v}_1 \cdot \mbf{w} + \beta\mbf{v}_2 \cdot \mbf{w} \\
    \mbf{v} \cdot (\alpha\mbf{w}_1 + \beta\mbf{w}_2) &= \alpha\mbf{v}\cdot\mbf{w}_1 + \beta\mbf{v}\cdot \mbf{w}_2.
\end{align*}

\begin{definition}
    A bilinear form $B$ on a real vector space $V$ is a mapping $B : V \times V \rightarrow \R$ assigning to each pair of vectors $\mbf{u}, \mbf{v} \in V$ a real number $B(\mbf{u}, \mbf{v})$ satisfying the following for all $\mbf{u}, \mbf{v}, \mbf{w} \in V$ and all $\lambda \in \R$:
    \begin{enumerate}[label = (\roman*)]
        \item $B(\mbf{u} + \mbf{v}, \mbf{w}) = B(\mbf{u}, \mbf{w}) + B(\mbf{v}, \mbf{w})$,
        
        \item $B(\mbf{w}, \mbf{u} + \mbf{v}) = B(\mbf{w}, \mbf{u}) + B(\mbf{w}, \mbf{v})$,
        
        \item $B(\lambda\mbf{u}, \mbf{v}) = B(\mbf{u}, \lambda\mbf{v}) = \lambda B(\mbf{u}, \mbf{v})$.
    \end{enumerate}
\end{definition}

\begin{definition}
    An inner product on a real vector space $V$ is a map $(\cdot,\cdot) : V \times V \mapsto \R$ which assigns to each pair of vectors $\mbf{u}, \mbf{v} \in V$ a real number $(\mbf{u}, \mbf{v})$,
    satisfying the following:
    for all $\mbf{u}, \mbf{v}, \mbf{w} \in V$ and all $\lambda \in \R$
    \begin{enumerate}[label = (\roman*)]
        \item $(\mbf{u} + \mbf{v}, \mbf{w}) = (\mbf{u}, \mbf{w}) + (\mbf{v}, \mbf{w})$,
        
        \item $(\lambda\mbf{u}, \mbf{v}) = \lambda (\mbf{u}, \mbf{v})$;

        \item $(\mbf{u}, \mbf{v}) = (\mbf{v}, \mbf{u})$
        (symmetry);

        \item $(\mbf{v}, \mbf{v}) \geq 0$ with $(\mbf{v}, \mbf{v}) = 0 \iff \mbf{v} = 0$
        (positivity).
    \end{enumerate}
\end{definition}

\begin{definition}
    A real inner-product space is a real vector space equipped with an inner product.
\end{definition}

\begin{example}
    $V = \R ^ n$ with the standard Euclidean inner product
    (dot product)
    \[
    (\mbf{u}, \mbf{v}) = \mbf{u} \cdot \mbf{v} = u_1v_1 + \dotsi + u_nv_n.
    \]
\end{example}

\begin{example}
    On $V = \R ^ 2$,
    is the expression $(\mbf{x}, \mbf{y}) = x_1y_1 + x_1y_2 + x_2y_2$ an inner product?

    \begin{solution}
        No as $\mbf{x} = (1, 1)$ and $\mbf{y} = (1, 0)$,
        then $(\mbf{x}, \mbf{y}) = 1$,
        and $(\mbf{y}, \mbf{x}) = 2$.
        Hence this does not satisfy symmetry.
    \end{solution}
\end{example}

\begin{example}
    On $V = \R ^ 2$,
    is the expression $(\mbf{x}, \mbf{y}) = x_1y_1 + x_1y_2 + x_2y_1 + x_2y_2$ an inner product?

    \begin{solution}
        Both symmetry and bilinearity hold,
        however positivity does not.
        Look at $(\mbf{x}, \mbf{x}) = (x_1 + x_2) ^ 2$,
        so we can just have $x_1 = -x_2$ or vice versa,
        for example $\mbf{x} = (2, -2)$,
        $(\mbf{x}, \mbf{x}) = (2 - 2) ^ 2 = 0$ but $\mbf{x} \neq 0$.
    \end{solution}
\end{example}

\begin{definition}
    We refer to the matrix $A_{ij} \coloneqq B(\mbf{v}_j, \mbf{v}_i)$ as the matrix form of $B$ in the basis $\{\mbf{v}_i\}$.
\end{definition}

\begin{definition}
    We say that an $n \times n$ matrix $B$ is symmetric if
    \[
    B ^ t = B,
    \]
    and anti-symmetric
    (or skew-symmetric)
    if
    \[
    B ^ t = -B.
    \]
\end{definition}

\begin{definition}
    Every matrix $B$ can be decomposed into its symmetric part $B_{+}$ and anti-symmetric part $B_{-}$ simply via
    \[
    B = \frac{B + B ^ {t}}{2} + \frac{B - B ^ {t}}{2} = B_{+} + B_{-},
    \]
    where $B_{+} \coloneqq \frac{B + B ^ {t}}{2}$ and $B_{-} \coloneqq \frac{B - B ^ {t}}{2}$.
    Clearly $B_{+}$ is a symmetric matrix and $B_{-}$ is an anti-symmetric matrix.
\end{definition}

\begin{definition}
    A symmetric $n \times n$ matrix $B$ is positive-definite if
    \[
    \mbf{v} ^ tB\mbf{v} > 0
    \]
    for all non-zero vectors $\mbf{v} \in \R ^ n$.
\end{definition}

\begin{proposition}
    A symmetric matrix $B$ is positive-definite if and only if its eigenvalues are all positive.

    \begin{proof}
        "$\implies$".
        For each eigenvalue $\lambda_i$ choose an eigenvector $\mbf{v}_i$ with eigenvalue $\lambda_i$.
        For a positive-definite matrix we then have
        \[
        0 < \mbf{v}_i ^ tB\mbf{v}_i = \lambda_i\mbf{v}_i ^ t\mbf{v}_i = \lambda_i \mbf{v}_i \cdot \mbf{v}_i.
        \]
        But $\mbf{v}_i \cdot \mbf{v}_i > 0$,
        so we conclude that $\lambda_i > 0$.

        "$\impliedby$".
        Given a real $n \times n$ symmetric matrix $B$,
        we can form a basis for $\R ^ n$ formed by eigenvectors $\mbf{v}_i$ of $B$,
        with eigenvalue $\lambda_i$,
        and such that $\mbf{v}_i \cdot \mbf{v}_j = 0$ for $i \neq j$.
        If we express an arbitrary non-zero vector $\mbf{v}$ in this basis as $\mbf{v} = \sum_{i = 1}^{n}c_i\mbf{v}_i$
        (with at least one $c_i \neq 0$) we have
        \begin{align*}
            \mbf{v} ^ tB\mbf{v} &= \left(\sum_{i = 1}^{n}c_i\mbf{v}_i\right)\cdot \left(B\sum_{j = 1}^{n}c_j\mbf{v}_j\right) \\
            &= \left(\sum_{i = 1}^{n}c_i\mbf{v}_i\right)\cdot \left(\sum_{j = 1}^{n}c_j\lambda_j\mbf{v}_j\right) \\
            &= \sum_{i = 1}^{n}c_i ^ 2(\mbf{v}_i \cdot \mbf{v}_i)\lambda_i.
        \end{align*}
        where in going from the second to the third lines we have used $\mbf{v}_i \cdot \mbf{v}_j = 0$ for $i \neq j$.
        The last line is greater than $0$ if all the eigenvalues are positive.
    \end{proof}
\end{proposition}

\begin{proposition}
    An alternative,
    equivalent,
    criterion for a real symmetric matrix $B$ to be positive-definite is that all $n$ of its upper-left square submatrices,
    i.e.
    the upper left $1$-by-$1$ corner of $B$,
    the upper left $2$-by-$2$ corner of $B$,
    etc,
    $B$ itself,
    have positive determinant.

    This is called Sylvester's criterion,
    and these subdeterminants are called the leading principle minors of $B$.
\end{proposition}

\begin{example}
    We have
    \[
    B = \begin{pmatrix}
        1 & 1 \\
        1 & 1
    \end{pmatrix},
    \]
    is this positive-definite?

    \begin{solution}
        No,
        the subdeterminants are
        \[
        \det\begin{pmatrix}
            1
        \end{pmatrix} = 1\qquad\text{and}\qquad\det{B} = \det\begin{pmatrix}
            1 & 1 \\ 1 & 1
        \end{pmatrix} = 1 - 1 = 0
        \]
        so $B$ is not positive-definite by the previous proposition.
    \end{solution}
\end{example}

\begin{proposition}
    Consider a bilinear form $B$ on $\R ^ n$,
    and denote by $A$ its matrix form for some given choice of basis for $\R ^ n$.
    $B$ defines an inner product on $\R ^ n$ if and only if its matrix representation $A$ is a positive-definite symmetric matrix.

    \begin{proof}
        
    \end{proof}
\end{proposition}

\subsection{Hermitian inner products}
\begin{definition}
    A complex
    (or hermitian)
    inner product on a complex vector space $V$ assigns to each pair of vectors $\mbf{u}, \mbf{v} \in V$ a complex number $\langle \mbf{u}, \mbf{v}\rangle$,
    satisfying the following for all $\mbf{u}, \mbf{v}, \mbf{w} \in V$ and all $\lambda \in \C$:
    \begin{enumerate}[label = (\roman*)]
        \item $\langle\mbf{u} + \mbf{v}, \mbf{w}\rangle = \langle\mbf{u}, \mbf{w}\rangle + \langle\mbf{v}, \mbf{w}\rangle$;

        \item $\langle\lambda\mbf{u}, \mbf{v}\rangle = \lambda\langle\mbf{u}, \mbf{v}\rangle$;

        \item $\langle\mbf{u}, \mbf{v}\rangle = \overline{\langle \mbf{v}, \mbf{u}\rangle}$
        (hermiticity);

        \item $\langle \mbf{v}, \mbf{v}\rangle \geq 0$ with $\langle\mbf{v}, \mbf{v}\rangle = 0 \iff \mbf{v} = 0$
        (positivity).
    \end{enumerate}
\end{definition}
\begin{remark}
    We will use $\overline{z}$ to denote the complex conjugate of the complex number $z$.
\end{remark}

\begin{example}
    $V = \C ^ 2$ and $\langle\mbf{z}, \mbf{w}\rangle = 3z_1\overline{w}_1 + 2z_2\overline{w}_2 + iz_1\overline{w}_2 - iz_2\overline{w}_1$.
    Hermiticity is satisfied,
    the linearity properties are satisfied since the right hand side is linear in $z_1, z_2$.
    For positivity,
    we have
    \begin{align*}
        \langle\mbf{z}, \mbf{z}\rangle &= 3z_1\overline{z}_1 + 2z_2\overline{z}_2 + iz_1\overline{z}_2 - iz_2\overline{z}_1 \\
        &= 2|z_1| ^ 2 + |z_2| ^ 2 + |z_1 - iz_2| ^ 2 \\
        &\geq 0,
    \end{align*}
    with equality if and only if $z_1 = z_2 = 0$.
    So positivity is satisfied as well.
\end{example}

\begin{example}
    $V = \C ^ 2$ and $\langle\mbf{z}, \mbf{w}\rangle = z_1\overline{w}_1 + iz_1\overline{w}_2 - iz_2\overline{w}_1 + z_2\overline{w}_2$ satisfies linearity and hermiticity,
    but positivity fails,
    since $\langle\mbf{z}, \mbf{z}\rangle = |z_1 - iz_2| ^ 2$ which is zero for $\mbf{z} = (1, -i)$.
\end{example}

\textbf{Matrix version of the hermitian product}

\begin{definition}
    The Hermitian transpose
    (or complex-conjugate transpose)
    $B ^ {*}$ of a matrix $B$ is the matrix with components
    \[
    (B ^ {*})_{ij} = \overline{B_{{ji}}}.
    \]
\end{definition}

\begin{remark}
    If we denote by $\overline{B}$ the matrix such that
    \[
    (\overline{B})_{ij} = \overline{B_{ij}}
    \]
    (that is,
    the matrix whose entries are the complex conjugates of those of $B$)
    then
    \[
    B ^ {*} = (\overline{B}) ^ t = \overline{B ^ t}.
    \]
\end{remark}

\begin{remark}
    If $A$ and $B$ are matrices,
    then $(AB) ^ {*} = \overline{(AB) ^ t} = \overline{B ^ tA ^ t} = B ^ {*}A ^ {*}$.
\end{remark}

\begin{proposition}
    If $V = \C ^ n$ with basis $\{\mbf{v}_i\}$,
    then an inner product on $V$ is realised by a $n \times n$ matrix $B$ as
    \[
    \langle\mbf{u}, \mbf{w}\rangle = \mbf{y} ^ {*}B\mbf{x}
    \]
    where $\mbf{x}$ and $\mbf{y}$ are the vector of components of $\mbf{u}$ and $\mbf{w}$ in the $\{\mbf{v}_i\}$ basis
    (so $\mbf{u} = x_1\mbf{v}_1 + \dotsc + x_n\mbf{v}_n$ and $\mbf{w} = y_1\mbf{v}_1 + \dotsc + y_n\mbf{v}_n$) and $B$ is the matrix with components
    \[
    B_{ij} = \langle\mbf{v}_j, \mbf{v}_i\rangle.
    \]

    \begin{proof}
        The argument is analogous to the one in the real case.
        That $B$ has the components given in the proposition follows from expanding $\mbf{u}, \mbf{w} \in V$ in the $\{\mbf{v}_i\}$ basis,
        and using
        (sesqui)linearity.
        That the resulting matrix $B$ is hermitian and positive-definite follows from the definitions below.
    \end{proof}
\end{proposition}

\begin{definition}
    We say that a matrix $B$ is hermitian if $B ^ {*} = B$.
    Similarly $B$ is an anti-hermitian matrix if $B ^ {*} = -B$.
\end{definition}

\begin{proposition}
    $A$ is Hermitian if and only if $\mbf{y} ^ {*}A\mbf{x} = (A\mbf{y}) * \mbf{x}$ for all $\mbf{x}, \mbf{y} \in \C ^ n$.
    If we choose the standard inner product on $\C ^ n$,
    this can be written as $\langle A\mbf{x}, \mbf{y}\rangle = \langle \mbf{x}, A\mbf{y} \rangle$ for all $\mbf{x}, \mbf{y} \in \C ^ n$.

    \begin{proof}
        This follows by choosing $\mbf{x}$ and $\mbf{y}$ the standard basis vectors of $\C ^ n$.
        For instance,
        choosing $\mbf{x} = \mbf{e}_i$ and $\mbf{y} = \mbf{e}_j$ we have
        \[
        \langle\mbf{e}_i, A\mbf{e}_j\rangle = \langle \mbf{e}_i, \sum_{k = 1}^{n}A_{kj}\mbf{e}_k\rangle = \overline{A_{ij}}
        \]
        and similarly $\langle A\mbf{e}_i, \mbf{e}_j\rangle = A_{ji}$.
    \end{proof}
\end{proposition}

\begin{definition}
    We say that a hermitian matrix $B$ is positive-definite if
    \[
    \mbf{v} ^ {*}B\mbf{v} > 0
    \]
    for all non-zero $\mbf{v} \in \C ^ n$.
\end{definition}

\begin{proposition}
    A hermitian matrix has real eigenvalues.
    
    \begin{proof}
    \end{proof}
\end{proposition}

\begin{proposition}
    A hermitian matrix is positive-definite if and only if its eigenvalues are all positive.

    \begin{proof}
        The proof is entirely analogous to the one in the real case,
        just by replacing $\mbf{v} ^ t$ by $\mbf{v} ^ {*}$ everywhere.
    \end{proof}
\end{proposition}

\begin{proposition}[Sylvester's criterion]
    A hermitian matrix $B$ is positive-definite if and only if the determinants of all of its leading minors are positive.
\end{proposition}

\subsection{Norms, orthogonality and the Cauchy-Schwarz inequality}

\begin{definition}
    Given a vector space $V$ over $\R$
    (or $\C$),
    a norm on $V$ is a function usually denoted by $\|\cdot\| : V \rightarrow \R$ with the following properties
    \begin{enumerate}[label = (\roman*)]
        \item $\|a\mbf{v}\| = |a|\|\mbf{v}\|$
        (absolute homogeneity),

        \item $\|\mbf{v} + \mbf{u}\| \leq \|\mbf{v}\| + \|\mbf{u}\|$
        (triangle inequality or subadditivity),

        \item If $\|\mbf{v}\| = 0$ then $\mbf{v} = \mbf{0}$
        (separation of points),
    \end{enumerate}
    for all $a \in \R$
    (or $\C$)
    and all $\mbf{v}, \mbf{u} \in V$.
\end{definition}

\begin{definition}
    If $(V, (\,,\,))$ is a real inner product space,
    then we define the norm $\|\mbf{v}\|$ of a vector $\mbf{v} \in V$ by
    \[
    \|\mbf{v}\| \coloneqq \sqrt{(\mbf{v}, \mbf{v})}.
    \]
\end{definition}

\begin{definition}
    A vector $\mbf{v} \in V$ is a unit vector if $\|\mbf{v}\| = 1$.
\end{definition}

\begin{definition}
    Two vectors $\mbf{u}$ and $\mbf{v}$ are orthogonal if $(\mbf{u}, \mbf{v}) = 0$.
\end{definition}

\begin{definition}
    An orthonormal basis $\{\mbf{u}_1, \dotsc, \mbf{u}_n\}$ of $V$ is a basis consisting of mutually orthogonal unit vectors.
    For the bases $\{\mbf{u}_1, \dotsc, \mbf{u}_n\}$ to be orthonormal we then must have
    \[
    (\mbf{u}_i, \mbf{u}_i) = \|\mbf{u}_i\| ^ 2 = 1, \text{ for } i = 1, \dotsc, n,\qquad(\mbf{u}_i, \mbf{u}_j) = 0, \text{ when } i \neq j,
    \]
    these conditions can be written in the compact form
    \[
    (\mbf{u}_i, \mbf{u}_j) = \delta_{ij},
    \]
    with $\delta_{ij}$ denoting Kronecker's delta.
\end{definition}

\begin{theorem}[The real Cauchy-Schwarz inequality]\label{pre:linalg:thm:cauchyschwarz}
    We have
    \[
    (\mbf{u}, \mbf{v}) ^ 2 \leq (\mbf{u}, \mbf{u})(\mbf{v}, \mbf{v})
    \]
    with equality if and only if $\mbf{u}$ and $\mbf{v}$ are linearly dependent.
    Equivalently,
    $|(\mbf{u}, \mbf{v})| \leq \|\mbf{u}\|\|\mbf{v}\|$.

    \begin{proof}
        For $\mbf{u} = \mbf{0}$ the proof is trivial,
        suppose $\mbf{u} \neq \mbf{0}$.
        For all real numbers $x$ we have
        \[
        \|x\mbf{u} - \mbf{v}\| ^ 2 = x ^ 2\|\mbf{u}\| ^ 2 - 2x(\mbf{u}, \mbf{v}) + \|\mbf{v}\| ^ 2 \geq 0,
        \]
        with equality if and only if $x\mbf{u} - \mbf{v} = 0$.

        If $\mbf{u}, \mbf{v}$ are linearly independent then there is no $x$ such that $x\mbf{u} = \mbf{v}$ then the quadratic equation in $x$ we just formed would have a discriminant less than zero
        \[
        (\mbf{u}, \mbf{v}) ^ 2 - \|\mbf{u}\|\|\mbf{v}\| ^ 2 < 0.
        \]
        Alternatively,
        if they are linearly dependent,
        from linearity of the inner product we can see that
        \[
        (\mbf{u}, \mbf{v}) ^ 2 - \|\mbf{u}\| ^ 2\|\mbf{v}\| ^ 2 = 0.
        \]
    \end{proof}
\end{theorem}

\begin{corollary}[The triangle inequality]
    For all $\mbf{u}, \mbf{v} \in V$,
    we have
    \[
    \|\mbf{u} + \mbf{v}\| \leq \|\mbf{u}\| + \|\mbf{v}\|.
    \]

    \begin{proof}
        \begin{align*}
            (\|\mbf{u}\| + \|\mbf{v}\|) ^ 2 - \|\mbf{u} + \mbf{v}\| ^ 2 &= \|\mbf{u}\| ^ 2 + 2\|\mbf{u}\|\|\mbf{v}\| + \|\mbf{v}\| ^ 2 - \|\mbf{u}\| ^ 2 - 2(\mbf{u}, \mbf{v}) - \|\mbf{v}\| ^ 2 \\
            &= 2(\|\mbf{u}\|\|\mbf{v}\| - (\mbf{u}, \mbf{v})) \\
            &\geq 0.
        \end{align*}
    \end{proof}
\end{corollary}

\begin{corollary}
    For a real inner-product space,
    the angle $\theta \in [0, \pi]$ between $\mbf{u}$ and $\mbf{v}$,
    given by $(\mbf{u}, \mbf{v}) = \|\mbf{u}\|\|\mbf{v}\|\cos{\theta}$,
    is well-defined.

    \begin{proof}
        By Cauchy-Schwarz
        (\autoref{pre:linalg:thm:cauchyschwarz})
        we know that $-\|\mbf{u}\|\|\mbf{v}\| \leq (\mbf{u}, \mbf{v}) \leq \|\mbf{u}\|\|\mbf{v}\|$.
        Additionally,
        if $\mbf{u}$ and $\mbf{v}$ are orthogonal then $(\mbf{u}, \mbf{v}) = 0$ so the angle between them is $\\theta = pm\pi / 2$.
        If they are parallel,
        linearly dependent with $\mbf{v} = x\mbf{u}$ with $x > 0$,
        the angle is $\theta = 0$,
        if they are anti-parallel,
        linearly dependent with $\mbf{v} = x\mbf{u}$ with $x < 0$,
        the angle is $\theta = \pi$.
    \end{proof}
\end{corollary}

\textbf{The norm induced by a Hermitian inner product}

\begin{definition}
    Let $(V, \langle\,,\,\rangle)$ be a Hermitian inner product space,
    then we define the norm $\|\mbf{v}\|$ of a vector $\mbf{v} \in V$ by
    \[
    \|\mbf{v}\| \coloneqq \sqrt{\langle\mbf{v}, \mbf{v}\rangle}.
    \]
\end{definition}

\begin{definition}
    A vector $\mbf{v} \in V$ is a unit vector if $\|\mbf{v}\| = 1$.
\end{definition}

\begin{definition}
    Two vectors $\mbf{u}$ and $\mbf{v}$ are orthogonal if $\langle\mbf{u}, \mbf{v}\rangle = 0$.
\end{definition}

\begin{theorem}[The complex Cauchy-Schwarz inequality]
    Let $(V, \langle\,,\,\rangle)$ be a Hermitian inner product space,
    then for all vectors $\mbf{u}, \mbf{v} \in V$ we have
    \[
    |\langle\mbf{u}, \mbf{v}\rangle| ^ 2 \leq \|\mbf{u}\| ^ 2\|\mbf{v}\| ^ 2,
    \]
    with equality if and only if $\mbf{u}$ and $\mbf{v}$ are linearly dependent.

    \begin{proof}
        Assume $\mbf{u} \neq 0$ and $\langle\mbf{u}, \mbf{v}\rangle \neq 0$,
        otherwise the statement is trivial and let us consider the following quadratic expression in $x \in \C$
        \[
        \|x\mbf{u} - \mbf{v}\| ^ 2 = |x| ^ 2\|\mbf{u}\| ^ 2 - 2\mathrm{Re}(x\langle\mbf{u}, \mbf{v}\rangle) + \|\mbf{v}\| ^ 2 \geq 0,
        \]
        with equality if and only if $x\mbf{u} - \mbf{v} = 0$.

        Since this expression must be non-negative for all $x \in \C$ we can choose in particular
        \[
        x = \lambda\frac{|\langle\mbf{u}, \mbf{v}\rangle|}{\langle\mbf{u}, \mbf{v}\rangle},
        \]
        with $\lambda \in \R$.

        The above inequality becomes
        \[
        \lambda ^ 2\|\mbf{u}\| ^ 2 - 2\lambda|\langle\mbf{u}, \mbf{v}\rangle| + \|\mbf{v}\| ^ 2 \geq 0,
        \]
        at this point we can apply the same argument used in the real Cauchy-Schwarz inequality
        (\autoref{pre:linalg:thm:cauchyschwarz})
    \end{proof}
\end{theorem}

\subsection{The Gram-Schmidt procedure}
Our goal of this procedure is to form an orthonormal set of vectors in an inner-product space.
Given any set of linearly independent vectors we can form an orthonormal set of vectors.
Let $\mbf{v}_1, \dotsc, \mbf{v}_k$ be linearly independent vectors in an inner-product space $V$.
these span a $k$-dimensional subspace $U$ of $V$.
Our task is to construct an orthonormal basis $\mbf{u}_1, \dotsc, \mbf{u}_k$ for $U$,
The procedure is as follows.

\textbf{The Gram-Schmidt procedure}:
\begin{enumerate}[label = (\roman*)]
    \item Define $\mbf{u}_1 = \frac{\mbf{v}_1}{\|\mbf{v}_1\|}$.
    Then $\mbf{u}_1$ is a unit vector,
    and $\mathrm{span}\{\mbf{u}_1\} = \mathrm{span}\{\mbf{v}_1\}$.

    \item Define $\Bar{\mbf{v}_2} = \mbf{v}_2 - (\mbf{v}_2, \mbf{u}_1)\mbf{u}_1$.
    Then $(\mbf{u}_1, \Bar{\mbf{v}_2}) = 0$ and $\Bar{\mbf{v}_2} \neq 0$,
    since $\mbf{v}_2 \notin \mathrm{span}\{\mbf{u}_1\}$.
    Now define $\mbf{u}_2 = \frac{\mbf{v}_1}{\|\mbf{v}_1\|}$.
    Then $\mbf{u}_1, \mbf{u}_2$ are mutually orthogonal unit vectors,
    and $\mathrm{span}\{\mbf{u}_1, \mbf{u}_2\} = \mathrm{span}\{\mbf{v}_1, \mbf{v}_2\}$
    (because clearly $u_1, u_2 \in \mathrm{span}(v_1, v_2)$ and also $v_1, v_2 \in \mathrm{span}(u_1, u_2)$).

    \item Suppose now that mutually orthogonal unit vectors $\mbf{u}_1, \dotsc, \mbf{u}_r$ have been found with $\mathrm{span}\{\mbf{u}_1, \dotsc, \mbf{u}_r\} = \mathrm{span}\{\mbf{v}_1, \dotsc, \mbf{v}_r\}$ for some $r$ with $1 \leq r \leq k$.
    If $r < k$,
    then define
    \[
    \Bar{\mbf{v}_{r + 1}} = \mbf{v}_{r + 1} - (\mbf{v}_{r + 1}, \mbf{u}_1)\mbf{u}_1 - \dotsc - (\mbf{v}_{r + 1}, \mbf{u}_r)\mbf{u}_r.
    \]
    Then
    \[
    (\Bar{\mbf{v}_{r + 1}}, \mbf{u}_1) = \dotsi = (\Bar{\mbf{v}_{r + 1}}, \mbf{u}_r) = 0,
    \]
    and $\Bar{\mbf{v}_{r + 1}} \neq 0$ since $\mbf{v}_{r + 1} \notin \mathrm{span}\{\mbf{u}_1, \dotsc, \mbf{u}_r\} = \mathrm{span}\{\mbf{v}_1, \dotsc, \mbf{v}_r\}$.
    Define $\mbf{u}_{r + 1} = \frac{\Bar{\mbf{v}_{r + 1}}}{\|\Bar{\mbf{v}_{r + 1}}\|}$.
    Then $\mbf{u}_1, \dotsc, \mbf{u}_{r + 1}$ are mutually orthogonal unit vectors,
    and $\mathrm{span}\{\mbf{u}_1, \dotsc, \mbf{u}_{r + 1}\} = \mathrm{span}\{\mbf{v}_1, \dotsc, \mbf{v}_{r + 1}\}$
\end{enumerate}

\begin{example}
    In $\R ^ 3$ with the standard inner product,
    apply Gram-Schmidt to
    \[
    \mbf{v}_1 = (1, 1, 0)\qquad\mbf{v}_2 = (1, 0, 1)\qquad\mbf{v}_3 = (0, 1, 1).
    \]

    \begin{solution}
        \begin{align*}
        \mbf{u}_1 &= \frac{1}{\sqrt{2}}\begin{pmatrix}
            1 \\ 0 \\ 0
        \end{pmatrix}, \\
        \Bar{\mbf{v}_2} &= \mbf{v}_2 - (\mbf{v}_2, \mbf{u}_1)\mbf{u}_1 = \begin{pmatrix}
            1 \\ 0 \\ 1
        \end{pmatrix}
        - \frac{1}{2}\begin{pmatrix}
            1 \\ 1 \\ 0
        \end{pmatrix} = \frac{1}{2}\begin{pmatrix}
            1 \\ -1 \\ 2
        \end{pmatrix} \\ 
        \mbf{u}_2 &= \frac{1}{6}\begin{pmatrix}
            1 \\ -1 \\ 2
        \end{pmatrix}, \\
        \Bar{\mbf{v}_3} &= \mbf{v}_3 - (\mbf{v}_3, \mbf{u}_1)\mbf{u}_1 - (\mbf{v}_3, \mbf{u}_2)\mbf{u}_2 \\
        &= \begin{pmatrix}
            0 \\ 1 \\ 1
        \end{pmatrix} - \frac{1}{2}\begin{pmatrix}
            1 \\ 1 \\ 0
        \end{pmatrix} - \frac{1}{6}\begin{pmatrix}
            1 \\ -1 \\ 2
        \end{pmatrix} = \frac{2}{3}\begin{pmatrix}
            -1 \\ 1 \\ 1
        \end{pmatrix}, \\
        \mbf{u}_3 &= \frac{1}{\sqrt{3}}\begin{pmatrix}
            -1 \\ 1 \\ 1
        \end{pmatrix}.
        \end{align*}
    \end{solution}
\end{example}

\subsection{Orthogonal complement and orthogonal projection}
Let us explore some consequences of orthogonality between vectors in a vector space $V$.

\begin{definition}
    Let $U$ be a vector subspace of an inner product space $V$.
    Then the orthogonal complement of $U$ is the set
    \[
    U ^ {\perp} = \{\mbf{v} \in V\,:\, (\mbf{u}, \mbf{v}) = 0\text{ for all } \mbf{u} \in U\},
    \]
    of vectors in $V$ which are orthogonal to all the vectors in $U$.
\end{definition}

\begin{remark}
    $U ^ {\perp}$ is a vector subspace of $V$.
\end{remark}

\begin{definition}
    Given any vector $\mbf{v} \in V$,
    there is a unique vector $\mbf{u} \in U$ called the orthogonal projection of $\mbf{v}$ onto $U$
    (at least if $U$ is finite-dimensional),
    and one may find this $\mbf{u}$ explicitly as follows.
\end{definition}

\begin{proposition}
    If $U$ is finite-dimensional,
    then there always exists a unique decomposition $\mbf{v} = \mbf{u} + \Tilde{\mbf{u}}$,
    where $\mbf{u} \in U$ and $\Tilde{\mbf{u}} \in U ^ {\perp}$.
    so we can write $V = U \oplus U ^ {\perp}$.

    \begin{proof}
        Let $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ be an orthonormal basis of $U$,
        and set
        \[
        \mbf{u} = (\mbf{v}, \mbf{u}_1)\mbf{u}_1 + \dotsc + (\mbf{v}, \mbf{u}_k)\mbf{u}_k,\qquad\Tilde{\mbf{u}} = \mbf{v} - \mbf{u}.
        \]
        Clearly $\mbf{u} \in U$;
        and for each $\mbf{u}_i$ note that $(\mbf{u}, \mbf{u}_i) = (\mbf{v}, \mbf{u}_i) \cdot (\mbf{u}_i, \mbf{u}_i) = (\mbf{v}, \mbf{u}_i)$.
        So for each $\mbf{u}_i$ we have
        \begin{align*}
            (\Tilde{\mbf{u}}, \mbf{u}_i) &= (\mbf{v} - \mbf{u}, \mbf{u}_i) \\
            &= (\mbf{v}, \mbf{u}_i) - (\mbf{u}, \mbf{u}_i) \\
            &= (\mbf{v}, \mbf{u}_i) - (\mbf{v}, \mbf{u}_i) \\
            &= 0.
        \end{align*}
        Hence $\Tilde{\mbf{u}} \in U ^ {\perp}$.
        This shows that $V = U + U ^ {\perp}$.
        To show that $V = U \oplus U ^ {\perp}$ we need to show additionally that $U \cap U ^ {\perp} = \{\mbf{0}\}$.
        Take $\mbf{w} = w_1\mbf{u}_1 + \dotsc + w_k\mbf{u}_k$.
        Clearly $\mbf{w} \in U$,
        and let us assume that also $\mbf{w} \in U ^ {\perp}$.
        In this case,
        by the definition of $U ^ {\perp}$,
        we must have that $(\mbf{w}, \mbf{u}_i) = 0$ for all $i \in 1, \dotsc, k$.
        Since the $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ form an orthonormal basis,
        this implies
        (by linearity of the inner product)
        $w_i = 0$ for all $i$,
        and therefore $\mbf{w} = \mbf{0}$.
        To show uniqueness,
        assume that there is a second decomposition $\mbf{v} = \mbf{u} + \Tilde{\mbf{u}} = \mbf{u}' + \Tilde{\mbf{u}}'$,
        with $\mbf{u}' \in U$ and $\Tilde{\mbf{u}} \in U ^ {\perp}$.
        This implies $\mbf{u}' - \mbf{u} = \Tilde{\mbf{u}} - \Tilde{\mbf{u}}'$.
        The left hand side of this equality lives in $U$,
        while the right hand side lives in $U ^ {\perp}$.
        Since $U \cap U ^ {\perp} = \{\mbf{0}\}$ this implies $\mbf{u}' - \mbf{u} = \mbf{0}$ and $\Tilde{\mbf{u}} - \Tilde{\mbf{u}}' = \mbf{0}$,
        so the decomposition is indeed unique.
    \end{proof}
\end{proposition}

\begin{definition}
    Given a finite-dimensional vector subspace $U$ of the inner product space $(V, (\,,\,))$ we can consider the orthogonal projection operator $P$ onto $U$,
    defined by
    \[
    P(\mbf{v}) \coloneqq (\mbf{v}, \mbf{u}_1)\mbf{u}_1 + \dotsc + (\mbf{v}, \mbf{u}_k)\mbf{u}_k,
    \]
    where $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ is an orthonormal basis of $U$.
\end{definition}

We can assume that the inner product space $V$ has finite dimension $\dim{V} = n$,
then the matrix form of the projection onto $U$ takes a simple form if we use the correct basis.
We known $V = U \oplus U ^ {\perp}$,
hence we can use Gram-Schmidt to obtain a basis $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ for $U$,
made of orthonormal vectors,
with $k = \dim{U}$,
similarly a basis $\{\mbf{v}_1, \dotsc, \mbf{v}_{n - k}\}$ for $U ^ {\perp}$,
with $n - k = \dim{U ^ {\perp}}$.
This means that the set $\{\mbf{u}_1, \dotsc, \mbf{u}_k, \mbf{v}_1, \dotsc, \mbf{v}_{n - k}\}$ forms an orthonormal basis for $V$.
The orthogonal projection onto $U$ can be easily evaluated on these basis elements:
\begin{align*}
    P(\mbf{u}_i) = \mbf{u}_i, && \forall i = 1, \dotsc, k && \text{while} && P(\mbf{v}_j) = 0, && \forall j = 1, \dotsc, n - k,
\end{align*}
so $P = \mathrm{diag}(1, \dotsc, 1, 0, \dotsc, 0)$ where the number of $1$s on the diagonal is $k = \dim{U}$,
while the number of $0$s on the diagonal is $n - k = \dim{U ^ {\perp}}$.

\begin{proposition}
    This projection operator satisfies the following properties:
    \begin{enumerate}[label = (\roman*)]
        \item $P$ is a linear operator $P : V \rightarrow V$;

        \item the image of $P$ is $\mathrm{Im}\,P = U$,
        while its kernel is $\ker{P} = U ^ {\perp}$;

        \item $P$ is idempotent,
        i.e. $P ^ 2 = P$;

        \item $P$ is restricted to $U$ is the identity operator,
        which we usually write as $P|_{U} = I$,
        i.e. $P(\mbf{u}) = \mbf{u}$ for all $\mbf{u} \in U$;

        \item $(I - P)P = P(I - P) = 0$.
    \end{enumerate}
\end{proposition}

\begin{remark}
    The last equation tells us that if we project first a vector $\mbf{v}$ onto $U$,
    then take the component of this projection orthogonal to $U$,
    or if we take the component of a vector $\mbf{v}$ orthogonal to $U$ and then project it onto $U$,
    we always find the zero vector.
\end{remark}

\begin{remark}
    From the idempotency condition it is easy to see the eigenvalues of $P$.
    Let $\mbf{v} \neq \mbf{0}$ be an eigenvector of $P$,
    with eigenvalue $\lambda$,
    i.e. $P\mbf{v} = \lambda\mbf{v}$,
    since $P ^ 2 = P$
    \[
    \lambda\mbf{v} = P\mbf{v} = P ^ 2\mbf{v} = \lambda ^ 2\mbf{v},
    \]
    meaning that the only possible eigenvalues are $\lambda = 0, 1$ since $\mbf{v} \neq \mbf{0}$.
\end{remark}

In a sense,
the projection $P(\mbf{v})$ of a vector $\mbf{v}$ onto the vector subspace $U$,
is the vector $P(\mbf{v}) = \mbf{u} \in U$ which is
"nearest"
to $\mbf{v}$.

\begin{proposition}
    Let $U$ be a finite-dimensional subspace of an inner product space $V$,
    and let $\mbf{v}_0 \in V$.
    If $\mbf{u}_0$ is the orthogonal projection of $\mbf{v}_0$ onto $U$,
    then for all $\mbf{u} \in U$ we have
    \[
    \|\mbf{u} - \mbf{v}_0\| \geq \|\mbf{u}_0 - \mbf{v}_0\|,
    \]
    with equality if and only if $\mbf{u} = \mbf{u}_0$.
    Thus $\mbf{u}_0$ gives the nearest point on $U$ to $\mbf{v}_0$.

    \begin{proof}
        Write $\mbf{v}_0 = \mbf{u}_0 + \Tilde{\mbf{u}}_0$.
        Then
        \begin{align*}
            \|\mbf{v}_0 - \mbf{u}\| ^ 2 &= \|\mbf{v}_0 - \mbf{u}_0 + \mbf{u}_0 - \mbf{u}\| ^ 2 \\
            &= \|\mbf{v}_0 - \mbf{u}_0\| ^ 2 + \|\mbf{u}_0 - \mbf{u}\| ^ 2
        \end{align*}
        by Pythagoras,
        since $\mbf{v}_0 - \mbf{u}_0 = \Tilde{\mbf{u}}_0 \in U ^ {\perp}$ and $\mbf{u}_0 - \mbf{u} \in U$.
        Thus
        \[
        \|\mbf{v}_0 - \mbf{u}\| ^ 2 \geq \|\mbf{v}_0 - \mbf{u}_0\| ^ 2,
        \]
        with equality if and only if $\|\mbf{u}_0 - \mbf{u}\| ^ 2 = 0$.
    \end{proof}
\end{proposition}

\begin{remark}
    In some contexts,
    $\mbf{u}_0$ is known as the least-squares approximation to $\mbf{v}_0$.
\end{remark}

\begin{proposition}[Bessel's Inequality]
    Let $V$ be an inner product space and $U$ a finite-dimensional subspace.
    If $\mbf{v} \in V$,
    and $\mbf{u} \in U$ is the orthogonal projection of $\mbf{v}$ onto $U$,
    then $\|\mbf{u}\| ^ 2 \leq \|\mbf{v}\| ^ 2$.
    In particular if $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ is an orthonormal basis for $U$ and $\mbf{u} = \lambda_1\mbf{u}_1 + \dotsc + \lambda__k\mbf{u}_k$ then
    \[
    \sum_{i = 1}^{k}\lambda_i ^ 2 \leq \|\mbf{v}\| ^ 2.
    \]
    
    \begin{proof}
        Writing $\mbf{v} = \mbf{u} + \Tilde{\mbf{u}}$ as usual,
        we have
        \[
        \sum_{i = 1}^{k}\lambda_i ^ 2 = \|\mbf{u}\| ^ 2 \leq \|\mbf{u}\| ^ 2 + \|\Tilde{\mbf{u}}\| ^ 2 = \|\mbf{v}\| ^ 2.
        \]
        The fact that $\|\mbf{u}\| ^ 2 = (\mbf{u}, \mbf{u}) = \sum_{i = 1}^{k}\lambda_i ^ 2$ follows from bilinearity of the inner product together with orthonormality of the $\{\mbf{u}_1, \dotsc, \mbf{u}_k\}$ basis.
    \end{proof}
\end{proposition}

\subsection{Orthogonal and unitary diagonalisation}
\begin{definition}
    A real $n \times n$ matrix $M$ is orthogonal if
    \[
    M ^ tM = MM ^ t = I,
    \]
    or equivalently if $M ^ {-1} = M ^ t$.
\end{definition}

\begin{definition}
    The set of $n \times n$ orthogonal matrices is known as the orthogonal group,
    and is denoted by
    \[
    O(n) \coloneqq \{M \in \mathrm{GL}(n, \R) \text{ such that } M ^ tM = MM ^ t = I\}.
    \]
\end{definition}

\begin{remark}
    Since $\det(M) = \det(M ^ t)$,
    orthogonal matrices must have $\det{M} = \pm 1$.
\end{remark}

\begin{definition}
    The subset of all the orthogonal matrices with determinant $+1$ is known as the special orthogonal group,
    and is denoted by
    \[
    \mathrm{SO}(n) \coloneqq \{M \in O(n)\text{ such that } \det{M} = +1\}.
    \]
\end{definition}

\begin{definition}
    A complex $n \times n$ matrix $U$ is called unitary if
    \[
    UU ^ {*} = U ^ {*}U = I,
    \]
    or equivalently if $U ^ {-1} = U ^ {*}$.
\end{definition}

\begin{definition}
    The set of $n \times n$ unitary matrices is known as the unitary group,
    and is usually denoted by
    \[
    O(n) \coloneqq \{U \in \mathrm{GL}(n, \C)\text{ such that } U ^ {*}U = UU ^ {*} = I\}.
    \]
\end{definition}

\begin{remark}
    $\det(U ^ {*}) = \overline{\det(U)}$,
    so a unitary matrix has
    \[
    |\det(U)| ^ 2 = \det(U)\det(U ^ {*}) = \det(UU ^ {*}) = 1.
    \]
\end{remark}

\begin{definition}
    The subset of all unitary matrices with determinant $+1$ known as the special unitary group,
    and is usually denoted by
    \[
    \mathrm{SU}(n) \coloneqq \{M \in U(n)\text{ such that } \det(M) = +1\}.
    \]
\end{definition}

\begin{proposition}
    A matrix is orthogonal
    (or unitary)
    if and only if its columns form an orthonormal set of vectors,
    using the standard inner product on $\R ^ n$
    (or $\C ^ n$).

    \begin{proof}
        Let us prove the complex case
        (the real case follows from taking the special case in which all the vectors are real).

        Think of a complex matrix $U$ as a matrix of column vectors $(\mbf{u}_1, \dotsc, \mbf{u}_n)$.
        Then matrix $U ^ {*}$ is then the matrix of row vectors $(\mbf{u}_1 ^ {*}, \dotsc, \mbf{u}_n ^ {*})$.
        By the usual rules of matrix multiplication we have that $\delta_{ij} = (U ^ {*}U)_{ij} = \langle\mbf{u}_j, \mbf{u}_i\rangle$ with the standard hermitian inner product in $\C ^ n$.
        These are the entries of the identity matrix if and only if the $\mbf{u}_i$ are orthonormal.
    \end{proof}
\end{proposition}

\begin{example}
    \[
    \frac{1}{\sqrt{2}}\begin{pmatrix}
        1 & -1 \\ 1 & 1
    \end{pmatrix}
    \]
    is orthogonal but not symmetric.
\end{example}

\begin{example}
    \[
    \frac{1}{\sqrt{2}}\begin{pmatrix}
        1 & -i \\ -i & 1
    \end{pmatrix}
    \]
    is unitary but not hermitian.
\end{example}

\begin{proposition}
    Let $A$ be a complex Hermitian
    (or real symmetric)
    $n \times n$ matrix.
    Then the eigenvalues of $A$ are real.

    \begin{proof}
        Let $\lambda$ be an eigenvalue of $A$,
        so $A\mbf{x} = \lambda\mbf{x}$ for some $\mbf{x} \neq \mbf{0}$.
        Then we have
        \begin{align*}
            \mbf{x} ^ {*}A\mbf{x} &= \lambda\mbf{x} ^ {*}\mbf{x}, \\
            \mbf{x} ^ {*}A ^ {*}\mbf{x} &= \Bar{\lambda}\mbf{x} ^ {*}\mbf{x} & (\text{conjugate transpose}) \\
            \mbf{x} ^ {*}A\mbf{x} &= \Bar{\lambda}\mbf{x} ^ {*}\mbf{x} & (\text{$A = A ^ {*}$ as $A$ is hermitian}) \\
            \lambda\mbf{x} ^ {*}\mbf{x} &= \Bar{\lambda}\mbf{x} ^ {*}\mbf{x} & (\text{using $A\mbf{x} = \lambda\mbf{x}$}) \\
            \lambda &= \Bar{\lambda} & (\mbf{x} ^ {*}\mbf{x} > 0).
        \end{align*}
    \end{proof}
\end{proposition}

\begin{proposition}
    Let $A$ be a complex Hermitian
    (or real symmetric)
    $n \times n$ matrix.
    Eigenvectors corresponding to different eigenvalues of $A$ are mutually orthogonal using the standard inner product in $\C ^ n$
    (or $\R ^ n$).

    \begin{proof}
        Suppose that $\lambda, \mu$ are eigenvalues
        (which are real,
        as we have just shown),
        with $\lambda \neq \mu$.
        Then for some $\mbf{x}, \mbf{y}$ we have
        \begin{align*}
            A\mbf{x} = \lambda\mbf{x}, \mbf{x} \neq \mbf{0}, & A\mbf{y} = \mu\mbf{y}, \mbf{y} \neq \mbf{0}
        \end{align*}
        and
        \begin{align*}
            \mbf{x} ^ {*}A\mbf{y} &= \mu\mbf{x} ^ {*}\mbf{y}, \\
            \mbf{y} ^ {*}A\mbf{x} &= \lambda\mbf{y} ^ {*}\mbf{x}, \\
            \mbf{x} ^ {*}A\mbf{y} &= \overline{\mbf{y} ^ {*}A ^ {*}\mbf{x}} = \overline{\mbf{y} ^ {*}A\mbf{x}} = \overline{\mbf{y} ^ {*}\lambda\mbf{x}} = \Bar{\lambda}\mbf{x} ^ {*}\mbf{y} = \lambda\mbf{x} ^ {*}\mbf{y}, \\
            \text{so } (\lambda - \mu)\mbf{x} ^ {*}\mbf{y} &= 0, \\
            \mbf{x} ^ {*}\mbf{y} = 0
        \end{align*}
        and thus $\mbf{x} \perp \mbf{y}$.
    \end{proof}
\end{proposition}















\end{document}