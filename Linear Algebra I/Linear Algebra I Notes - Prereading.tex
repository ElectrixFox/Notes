\documentclass[10pt, a4paper]{article}
\usepackage{preamble}
\usepackage{systeme}

\setcounter{MaxMatrixCols}{20}
\newcommand{\mbf}[1]{\mathbf{#1}}

\title{Linear Algebra I \\
    \large Prereading}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{The vector space \(\R ^ n\)}
\begin{definition}[$n$-dimensional real space]
    The space $\R ^ n$ ($n$-dimensional real space) is the collection of all (column) vectors of the form
    \[
    \mathbf{x} =
    \begin{pmatrix}
        x_1 \\
        \vdots \\
        x_n
    \end{pmatrix}.
    \]
\end{definition}
Vectors are typically denoted by bold letters such as $\mathbf{x}$ and sometimes using underlined letters $\underline{x}$ to differentiate them from ordinary real numbers.

We write $\mathbf{0}$ for the zero-vector
\[
\mathbf{0} = \begin{pmatrix}
    0 \\
    \vdots \\
    0
\end{pmatrix}
\]
this is the point of origin of $\R ^ n$.

There are two important operations which can be performed on vectors.

\textbf{Addition}: for any vectors $\mathbf{v}$ and $\mathbf{w}$, we define
\[
\mathbf{v + w} =
\begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
+
\begin{pmatrix}
    w_1 \\
    \vdots \\
    w_n
\end{pmatrix}
=
\begin{pmatrix}
    v_1 + w_1 \\
    \vdots \\
    v_n + w_n
\end{pmatrix}
\]
this is component-wise addition.


\textbf{Scalar multiplication}: for any vector $\mathbf{v} \in \R ^ n$ and real number $\lambda \in \R$ we define
\[
\lambda\mathbf{v} =
\lambda\begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
=
\begin{pmatrix}
    \lambda v_1 \\
    \vdots \\
    \lambda v_n
\end{pmatrix}
\]
this is component-wise multiplication by $\lambda$.

\begin{proposition}
    These operations satisfy the axioms of a real vector space
\end{proposition}

{\large \textbf{Axioms for Addition}}
\begin{enumerate}[label = (\roman*)]
    \item existence of additive identity: there is a vector $\mathbf{0}$ in $\R ^ n$ so that $\mathbf{v} + \mathbf{0} = \mathbf{v} = \mathbf{0} + \mathbf{v}$ for all $\mathbf{v}$ in $\R ^ n$.
    \item commutativity: $\mbf{v + w = w + v}$ for all $\mathbf{v}$ and $\mathbf{w}$ in $\R ^ n$
    \item existence of additive inverses: for each $\mbf{v}$ in $\R ^ n$ there is an element $-\mbf{v}$ in $\R ^ n$ such that $\mbf{v + (-v) = 0 = (-v) + v}$
    \item associativity: $\mbf{u + (v + w) = (u + v) + w}$ for all $\mbf{u, v, w}$ in $\R ^ n$
\end{enumerate}
these axioms can be summarised by saying that $\R ^ n$ is an abelian group under addition.


{\large\textbf{Axioms for Scalar Multiplication}}
\begin{enumerate}[label = (\roman*)]
    \item multiplication by 0: $0\mbf{v = 0}$ for all $\mbf{v}$ in $\R ^ n$
    \item multiplication by 1: $1\mbf{v = v}$ for all $\mbf{v}$ in $\R ^ n$
    \item associativity: $(\lambda\mu)\mbf{v} = \lambda (\mu\mbf{v})\ \forall \lambda, \mu \in \R$ and $\forall \mbf{v} \in \R ^ n$
    \item distributivity: $(\lambda + \mu)\mbf{v} = \lambda\mbf{v} + \mu\mbf{v}$ and $\lambda (\mbf{v + w}) = \lambda\mbf{v} + \lambda\mbf{w}$ $\forall \lambda, \mu \in \R$ and $\forall \mbf{v, w} \in \R ^ n$
\end{enumerate}

\begin{proof}
    The above axioms ate trivially satisfied by $\R$ (the case $n = 1$), many of the proofs follow from this observation. Once we set
    \[
    \mbf{0} = \begin{pmatrix}
        0 \\
        \vdots \\
        0
    \end{pmatrix},
    \qquad
    -\mbf{v} = -\begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    =
    \begin{pmatrix}
        -v_1 \\
        \vdots \\
        -v_n
    \end{pmatrix}
    =
    (-1)\begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \]
    then the proofs are very straight forward
\end{proof}

\begin{definition}[Standard basis vectors]
    We set
    \[
    \mbf{e_1} = \begin{pmatrix}
        1 \\
        0 \\
        \vdots \\
        0
    \end{pmatrix},
    \ 
    \mbf{e_2} = \begin{pmatrix}
        0 \\
        1 \\
        \vdots \\
        0
    \end{pmatrix},
    \dotsc,\ 
    \mbf{e_n} = \begin{pmatrix}
        0 \\
        0 \\
        \vdots \\
        1
    \end{pmatrix}
    \]
    and call these vectors the standard basis vectors of $\R ^ n$.
\end{definition}
Sometimes the entries of a vector are referred to as the Cartesian coordinates of a point in space.

Addition can be thought of as an operation that inputs a pair of vectors, each from $\R ^ n$, and outputs a single vector in $\R ^ n$. In other words, addition is a function $\R ^ n \times \R ^ n \mapsto \R ^ n$. Similarly, scalar multiplication is a function $\R \times \R ^ n \mapsto \R ^ n$. The domains of each of these functions are called Cartesian products.

\subsection{The scalar product in $\R ^ n$}

\begin{definition}[Scalar product]
    The scalar (or dot) product is a function
    \[
    \R ^ n \times \R ^ n \mapsto \R
    \]
    defined as follows. For $\mbf{v, w}$ in $\R ^ n$ set
    \[
    \mbf{v \cdot w} =
    \begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        w_1 \\
        \vdots \\
        w_n
    \end{pmatrix}
    =
    v_1 w_1 + \cdots + v_n w_n.
    \]
    This is a real number, a scalar.
\end{definition}

The transpose $\mbf{v} ^ t$ of $\mbf{v}$ is defined to be the row vector with the same entries as the column vector $\mbf{v}$. So if
\[
\mbf{v} = \begin{pmatrix}
    v_1 \\
    \vdots \\
    v_n
\end{pmatrix}
\qquad\text{then}
\qquad\mbf{v} ^ t = (v_1,\ \dots,\ v_n).
\]

The dot product of $\mbf{v}$ and $\mbf{w}$ is then obtained by matrix multiplying $\mbf{v} ^ t$ and $\mbf{w}$.

\begin{proposition}[Properties of the scalar product]
\phantom{}
\begin{enumerate}[label = (\roman*)]
    \item Symmetry
    \[
    \mbf{u \cdot v = v \cdot u}\qquad\text{for all }\mbf{u, v}\text{ in } \R ^ n
    \]
    \item Linearity in first factor
    \begin{enumerate}[label = (\alph*)]
        \item $(\mbf{u} + \mbf{v}) \cdot \mbf{w} = \mbf{u} \cdot \mbf{w} + \mbf{v} \cdot \mbf{w}$\qquad for all $\mbf{u,\ v,\ w}$ in $\R ^ n$
        \item $(\lambda\mbf{u}) \cdot \mbf{v} = \lambda(\mbf{u} \cdot \mbf{v})$\qquad for all $\lambda \in \R$ and all $\mbf{u,\ v}$ in $\R ^ n$
    \end{enumerate}
    \item Linearity in the second factor
    \begin{enumerate}[label = (\alph*)]
        \item $\mbf{u} \cdot (\mbf{v} + \mbf{w}) = \mbf{u} \cdot \mbf{w} + \mbf{u} \cdot \mbf{v}$\qquad for all $\mbf{u,\ v,\ w}$ in $\R ^ n$
        \item $\mbf{u} \cdot (\lambda\mbf{v}) = \lambda(\mbf{u} \cdot \mbf{v})$\qquad for all $\lambda \in \R$ and all $\mbf{u,\ v}$ in $\R ^ n$
    \end{enumerate}
    \item Positivity
    
    $\mbf{v \cdot v} \geq 0$ for all $v$, and $\mbf{v \cdot v} = 0$ if and only if $\mbf{v = 0}$.
\end{enumerate}

\begin{proof}
    \phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item
        First we can determine some expressions for $\mbf{u\cdot v}$ and $\mbf{v\cdot u}$
        
        $\mbf{u\cdot v} = u_1 v_1 + u_2 v_2 + \dotsc + u_n v_n$ and

        $\mbf{v\cdot u} = v_1 u_1 + v_2 u_2 + \dotsc + v_n u_n$.

        Then by the commutativity of multiplication we have
        \begin{align*}
            \mbf{u\cdot v} &= u_1 v_1 + u_2 v_2 + \dotsc + u_n v_n \\
            &= v_1 u_1 + v_2 u_2 + \dotsc + v_n u_n \\
            &= \mbf{v \cdot u}
        \end{align*}
        which proves our goal as required
        
    \end{enumerate}
\end{proof}
\end{proposition}

\begin{definition}[Magnitude]
    Suppose
    \[
    \mbf{v} = \begin{pmatrix}
        v_1 \\
        \vdots \\
        v_n
    \end{pmatrix}
    \]
    then $\mbf{v \cdot v} = v_1 ^ 2 + \dotsc + v_n ^ 2$ which is a non-negative real number. We define the magnitude (or length) of $\mbf{v}$, denoted $|\mbf{v}|$, by
    \[
    |\mbf{v}| = \sqrt{\mbf{v \cdot v}}.
    \]
    So long as the $n$ coordinate directions are mutually orthogonal, the magnitude
    \[
    |\mbf{v}| = \sqrt{\mbf{v \cdot v}} = \sqrt{x_1 ^ 2 + \cdots + v_n ^ 2}
    \]
    is the euclidean distance from the origin to the point with coordinates given by $\mbf{v}$. We say that a vector is non-trivial if it has positive length. It is easy to see that the only trivial vector in $\R ^ n$ is the zero vector $\mbf{0}$.
\end{definition}


We can describe the position of a point by what direction it lies and what distance away from us it is, using polar coordinates. \\


Using the length of a vector in $\R ^ 2$ we define polar co-ordinates on $\R ^ 2$ as follows. Let $\mbf{v} = \left(\frac{x}{y}\right)$ be a non-trivial vector with length $|\mbf{v}| = r > 0$. Let $\theta \in [0, 2\pi)$ be the angle at origin $\mbf{0}$ measured in an anticlockwise direction from the $x$-axis to $\mbf{v}$. By basic trigonometry
\[
\cos(\theta) = \frac{x}{r},\qquad\sin(\theta) = \frac{y}{r}.
\]
We define the polar coordinates of $\mbf{v \neq 0}$ to be the unique $(r, \theta)$ in $\R_+ \times [0, 2\pi)$ so that
\[
\mbf{v} = \begin{pmatrix}
    r\cos(\theta) \\
    r\sin(\theta)
\end{pmatrix}
=
r\begin{pmatrix}
    \cos(\theta)
    \sin(\theta)
\end{pmatrix}
\]

\begin{lemma}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ 2$ of length $r > 0 $ and $s > 0$ respectively. Let $\theta$ be the angle between them. Then
    \[
    \mbf{v \cdot w} = rs\cos(\theta).
    \]
    In particular, we have
    \[
    |\mbf{v \cdot w}| \leq |\mbf{v}||\mbf{w}|
    \]
    with equality if and only if $\mbf{v}$ and $\mbf{w}$ are collinear, i.e. are scalar multiples of each other.

    \begin{proof}
        Suppose that $\mbf{v}$ has polar coordinates $(r, \gamma)$ and $\mbf{w}$ has polar coordinates $(s, \rho)$. The angle between $\mbf{v}$ and $\mbf{w}$ is $\theta = |\gamma - \rho|$. Then
        \[
        \mbf{v} = \begin{pmatrix}
            r\cos\gamma \\
            r\sin\gamma
        \end{pmatrix},
        \qquad
        \mbf{w} = \begin{pmatrix}
            s\cos\rho \\
            s\sin\rho
        \end{pmatrix}.
        \]
        Applying the dot product using these definitions of $\mbf{v}$ and $\mbf{w}$ we obtain the following, applying trigonometric addition formula and the fact that cosine is even we get,
        \[
        \mbf{v \cdot w} =  rs\cos\gamma\cos\rho + rs\sin\gamma\sin\rho = rs(\cos(\gamma - \rho)) = rs\cos(|\gamma - \rho|) = rs\cos\theta.
        \]
    \end{proof}
\end{lemma}
Note that using this lemma we can obtain an expression for the angle between two vectors
\[
\cos(\theta) = \frac{\mbf{v \cdot w}}{|\mbf{v}||\mbf{w}|}
\]

\begin{theorem}[Cauchy-Schwarz inequality]
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ n$. Then
    \[
    |\mbf{v \cdot w}| \leq |\mbf{v}||\mbf{w}|
    \]
    with equality if and only if $\mbf{v}$ and $\mbf{w}$ are collinear

    \begin{proof}
        
    \end{proof}
\end{theorem}

\begin{definition}[Angle between two vectors]\label{def_linalg_vec_angbetvecs}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ n$ of length $r > 0$ and $s > 0$ respectively. Then we define the angle $\theta \in [0, \pi]$ between them by
    \[
    \cos(\theta) = \frac{\mbf{v \cdot w}}{rs}.
    \]
\end{definition}

We say that the vectors $\mbf{v}$ and $\mbf{w}$ are orthogonal (or perpendicular) if the angle between them is $\frac{\pi}{2}$. \\

\begin{corollary}
    The non-trivial vectors $\mbf{v}$ and $\mbf{w}$ in $\R ^ n$ are orthogonal if and only if we have $\mbf{v \cdot w} = 0$.

    \begin{proof}
        By \autoref{def_linalg_vec_angbetvecs} we have that\footnote{Since $\cos\theta$ can only be $0$ when $\theta = \frac{\pi}{2}$ then they have to be orthogonal and the converse is true too.}
        \[
        \mbf{v \cdot w} = 0 \iff \cos\theta = 0 \iff \theta = \frac{\pi}{2}.
        \]
    \end{proof}
\end{corollary}

\subsection{The vector product in $\R ^ 3$}
\begin{definition}[Vector product in $\R ^ 3$]
    The vector (or cross) product is a function
    \[
    \R ^ 3 \times \R ^ 3 \rightarrow \R ^ 3
    \]
    defined as follows. If $\mbf{v, w} \in \R ^ 3$, their cross product is given by
    \[
    \mbf{v \times w} =
    \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix}
    \times
    \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix}
    =
    \begin{pmatrix} v_2 w_3 - v_3 w_2 \\ v_3 w_1 - v_1 w_3 \\ v_1 w_2 - w_1 v_2 \end{pmatrix}
    \]
\end{definition}

\begin{example}
    \[
    \begin{pmatrix}
        1 \\ 2 \\ 3
    \end{pmatrix}
    \times
    \begin{pmatrix}
        3 \\ 2 \\ 1
    \end{pmatrix}
    = \begin{pmatrix}
        2 - 6 \\
        9 - 1 \\
        2 - 6
    \end{pmatrix}
    =
    \begin{pmatrix}
        -4 \\ 8 \\ -4
    \end{pmatrix}
    \]
\end{example}

\begin{proposition}[Properties of the cross product]
    \begin{enumerate}[label = (\roman*)]
        \item Anti-Symmetry:
        \[
        \mbf{u \times v = -v \times u}\quad\text{for all }\mbf{u, v} \in \R ^ 3
        \]
        \item Linearity in first factor
        \begin{enumerate}[label = (\arabic*)]
            \item $(\mbf{u + v}) \times \mbf{w} = \mbf{u \times w} + \mbf{v \times w}\quad\text{for all }\mbf{u, v, w} \in \R ^ 3$
            \item $(\lambda\mbf{u}) \times \mbf{v} = \lambda(\mbf{u \times v})\quad\text{for all } \lambda \in \R \text{ and all }\mbf{u, v} \in \R ^ 3$
        \end{enumerate}
        \item Linearity in the second factor
        \begin{enumerate}[label = (\arabic*)]
            \item $\mbf{u} \times (\mbf{v + w}) = \mbf{u \times w} + \mbf{u \times v}\quad\text{for all }\mbf{u, v, w} \in \R ^ 3$
            \item $\mbf{u} \times (\lambda\mbf{v}) = \lambda(\mbf{u \times v})\quad\text{for all } \lambda \in \R \text{ and all }\mbf{u, v} \in \R ^ 3$
        \end{enumerate}
        \item Orthogonality to the input
        \[
        \mbf{v} \cdot (\mbf{v \times w}) = \mbf{w} \cdot (\mbf{v \times w}) = 0\quad\text{for all vectors } \mbf{v} \text{ and } \mbf{w} \text{ in } \R ^ 3
        \]
    \end{enumerate}
\end{proposition}

\begin{lemma}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors of length $r > 0$ and $s > 0$ respectively. Let $\theta \in [0, \pi]$ be the angle between them. Then
    \[
    |\mbf{v \times w}| = rs\sin(\theta).
    \]
    In particular, we have
    \[
    \mbf{v \times w = 0}\quad\iff\quad\mbf{v}\text{ and }\mbf{w}\text{ are collinear}.
    \]

    \begin{proof}
        We have
        \begin{align*}
            \left|\begin{pmatrix}
                v_2 w_3 - v_3 w_2 \\
                v_3 w_1 - v_1 w_3 \\
                v_1 w_2 - w_1 v_2
            \end{pmatrix}\right| ^ 2
            &= (v_2w_3 - v_3w_2) ^ 2 + (v_3w_1 - v_1w_3) ^ 2 + (v_1w_2 - w_1v_2) ^ 2 \\
            &= (v_1 ^ 2 + v_2 ^ 2 + v_3 ^ 2)(w_1 ^ 2 + w_2 ^ 2 + w_3 ^ 2) - (v_1w_1 + v_2w_2 + v_3w_3) ^ 2 \\
            &= |\mbf{v}| ^ 2 |\mbf{w}| ^ 2 - (\mbf{v \cdot w}) ^ 2 \\
            &= |\mbf{v}| ^ 2 |\mbf{w}| ^ 2 - (|\mbf{v}||\mbf{w}|\cos(\theta)) ^ 2 \\
            &= r^2s^2(1 - \cos(\theta)) ^ 2 \\
            &= (rs\sin(\theta)) ^ 2.
        \end{align*}
        Since $\sin\theta \geq 0$ for $\theta \in [0, \pi]$, we are done.
    \end{proof}
\end{lemma}

\subsection{Planes and Lines in $\R ^ 3$}
\subsubsection{Planes}

\textbf{Parametric Form}

A plane $\Pi$ in $\R ^ 3$ can be described in parametric form as the collection of all points corresponding to vectors of the form
\[
\mbf{a} + \lambda\mbf{d_1} + \mu\mbf{d_2}
\]
where $\lambda$ and $\mu$ each run through $\R$. $\mbf{a}$ is any point on the plane and $\mbf{d_1}$ and $\mbf{d_2}$ are any two direction vectors for $\Pi$, which lie on the plane and are not collinear.

For example, if $\mbf{a, b}$ and $\mbf{c}$ are all points on $\Pi$ which are not collinear then we could take $\mbf{d_1 = b - a}$ and $\mbf{d_2 = c - a}$.

We write
\[
\Pi = \{\mbf{a} + \lambda\mbf{d_1} + \mu\mbf{d_2} : \lambda, \mu \in \R\}.
\]

\textbf{Using the normal vector}

We can describe a plane via its normal vector.
\[
\mbf{d}_1 \cdot (\mbf{d}_1 \times \mbf{d}_2) = 0 = \mbf{d}_2 \cdot (\mbf{d}_1 \times \mbf{d}_2)
\]
that is, $\mbf{d}_1 \times \mbf{d}_2$ is orthogonal to both $\mbf{d}_1$ and $\mbf{d}_2$.

Let us write $\mbf{n}$ for $\mbf{d}_1 \times \mbf{d}_2$. Additionally we can write the plane in terms of the dot product
\[
\mbf{n \cdot(x - a)} = 0.
\]
We can rearrange this to have the normal form of the plane $\Pi$: the plane $\Pi$ is the set of vectors satisfying
\[
\mbf{n \cdot x} = \ell
\]
where $\ell = \mbf{n \cdot a} \in \R$ since $\mbf{n \cdot a}$ doesn't depend on $\mbf{x}$ it is just a real number.


\textbf{Cartesian form}

A variation of the normal form, really just the same description but now written in Cartesian coordinates immediately follows. Set $\mbf{n} = \begin{pmatrix} a \\ b \\ c \end{pmatrix}$ and $\mbf{x} = \begin{pmatrix} x \\ y \\ z \end{pmatrix}$. Then the normal form above becomes the Cartesian (or the Cartesian form) of the plane:
\[
ax + by + cz = \ell
\]
Thus we have
\[
\Pi = \{\mbf{x} \in \R ^ 3 : \mbf{n \cdot x} = \ell\} = \left\{\begin{pmatrix} a \\ b \\ c \end{pmatrix}\,:\,ax + by + cz = \ell\,\right\}
\]

\subsubsection{Lines}
\textbf{Parametric form}

A line $L$ in $\R ^ 3$ can be described in parametric form as the collection of all points corresponding to vectors of the form
\[
\mbf{a} + t\mbf{d}
\]
where $t$ runs through $\R$.
\[
L = \{\mbf{a} + t\mbf{d} : t \in \R\}.
\]

If $\mbf{b \neq a}$ is another point on $L$, then we could take $\mbf{d = b - a}$.

\textbf{Cartesian form}

Suppose the line is given in parametric form by
\[
\begin{pmatrix}
    x \\ y \\ z
\end{pmatrix}
=
\begin{pmatrix}
    a_1 \\ a_2 \\ a_3
\end{pmatrix}
+
t\begin{pmatrix}
    d_1 \\ d_2 \\ d_3
\end{pmatrix}.
\]

If all $d_i \neq 0$ then we may solve for $t$ in each component and obtain
\[
\frac{x - a_1}{d_1} = \frac{y - a_2}{d_2} = \frac{z - a_3}{d_3}.
\]
If one of the $d_i = 0$, such as $d_3 = 0$ then we would get two equations
\[
\frac{x - a_1}{d_1} = \frac{y - a_2}{d_2}\qquad\text{and}\qquad z = a_3
\]

\textbf{Solution set for a pair of linear equations}
A pair of planes in $\R ^ 3$ is either parallel, in which case they have the same normal vector, or else they intersect in a line.
\begin{align*}
    ax + by + cz &= \ell \\
    dx + ey + fz &= m
\end{align*}
the either the two normal vectors
\[
\mbf{n}_1 = \begin{pmatrix} a \\ b \\ c \end{pmatrix}\quad\text{and}\quad\mbf{n}_2 = \begin{pmatrix} d \\ e \\ f \end{pmatrix}
\]
are multiples of each other or else the solution is a line.

\begin{definition}[Scalar triple product]
    The scalar triple product is a function
    \[
    \R ^ 3 \times \R ^ 3 \times \R ^ 3 \rightarrow \R.
    \]
    Given three vectors $\mbf{a, b, c}$ in $\R ^ 3$ we define their scalar triple product $[\mbf{a, b, c}]$ by
    \[
    [\mbf{a, b, c}] = \mbf{a} \cdot(\mbf{b} \times \mbf{c}).
    \]
\end{definition}

\begin{lemma}[Invariance under cyclic permutation]
    For any three vectors $\mbf{a, b, c}$ in $\R ^ 3$ we have
    \[
    [\mbf{a, b, c}] = [\mbf{b, c, a}] = [\mbf{c, a, b}] = -[\mbf{a, b, c}] = -[\mbf{b, c, a}] = -[\mbf{c, a, b}].
    \]
\end{lemma}

\section{Matrix algebra}

\subsection{What is a matrix?}

\begin{definition}[Matrix]
    A matrix is a rectangular array of numbers (or symbols, or functions) arranged into rows and columns.
\end{definition}

\textbf{Notation and Terminology:}
\begin{itemize}
    \item A matrix which has $m$ rows and $n$ columns is referred to as an $m \times n$ matrix. The individual entries in the matrix are sometimes referred to as its elements. The entry in the $i$th row and $j$th column of a matrix is referred to as the $(i,\,j)$th element of a matrix $M$, usually denoted $M_{ij}$.
    \item Matrices which consist of a single column are (column) vectors. Matrices which consist of a single row are row vectors.
\end{itemize}

\begin{definition}[Transpose]
    Given an $m \times n$ matrix $W$ we define $W ^ t$, the transpose of $W$, to be the $n \times m$ matrix whose $(i,\,j)$th entry is the $(j,\,i)$th entry of $W$; that is the matrix fow which
    \[
    (W ^ t)_{ij} = W_{ji}.
    \]
\end{definition}

A matrix which is equal to its transpose is said to be symmetric.

One way of describing or otherwise specifying the elements of a matrix is as follows. If $f(i,\,j)$ is some expression depending on $i$ and $j$ then $(f(i,\,j))_{m \times n}$ stands for the $m \times n$ matrix whose $(i,\,j)$th entry is $f(i,\,j)$. We often write an $m \times n$ matrix as $X = (x_{ij})_{m \times n}$, or $X = (x_{ij})$ if the size is obvious.

We write $O_{m \times n}$ for $(0)_{m \times n}$ that is the $m \times n$ matrix all of whose entries are zero, this is called the $m \times n$ zero matrix.

For $r$ and $s$ integers, we define $\delta_{r\,s}$ to be $1$ if $r = s$ and to be $0$ if $r \neq s$. (This is called the Kronecker delta.) The matrix $(\delta_{ij})_{m \times n}$ is denoted $I_n$ and is referred to as the $n \times n$ identity matrix.

In general, we write $M_{m \times n}(\R)$ or $M_{m, n}(\R)$ for the set of all real $m \times n$ matrices. We also write $M_n(\R)$ for the set of square matrices of size $n$, so $M_n(\R) = M_{n, n}(\R)$. $M_{m, 1}(\R)$ is the same set as $\R ^ m$.

\subsection{Matrix operations I: matrix addition and scalar multiplication}

\begin{enumerate}[label = (\roman*)]
    \item Matrix addition. This is a function
    \[
    M_{m \times n}(\R) \times M_{m \times n}(\R) \rightarrow M_{m \times n}(\R).
    \]
    For $X = (x_{ij})_{m \times n}$ and $Y = (y_{ij})_{m \times n}$ both $m \times n$ matrices, we define
    \[
    X + Y = (x_{ij})_{m \times n} + (y_{ij})_{m \times n} = (x_{ij} + y_{ij})_{m \times n},
    \]
    that is, matrix addition is defined by component wise addition. Matrix addition isn't defined when the matrices are of different sizes.
    \item Scalar multiplication. This is a function
    \[
    \R \times M_{m \times n}(\R) \rightarrow M_{m \times n}(\R).
    \]
    For $X = (x_{ij})_{m \times n}$ in $M_{m \times n}(\R)$ and $\lambda \in \R$ we set
    \[
    \lambda X = (\lambda x_{ij})_{m \times n}
    \]
    that is, we multiply every entry of the matrix by $\lambda$.
\end{enumerate}

We write $-X$ for $(-1)X$. If $X ^ t = -X$ we say that $X$ is skew-symmetric (or anti-symmetric).

\begin{proposition}[Properties of matrix addition and scalar multiplication]
    Matrix addition and scalar multiplication for $M_{m, n}(\R)$ satisfy exactly the same properties as vector addition and scalar multiplication for $\R ^ n$. That is, the operations satisfy the axioms of a (real) vector space.

    \textbf{Axioms for addition}
    \begin{enumerate}[label = (\roman*)]
        \item existence of additive identity: there is a matrix $O_{m \times n}$ in $M_{m, n}(\R)$ so that for all $X$ in $M_{m, n}(\R)$ we have $X + O_{m \times n} = X = O_{m \times n} + X$ 
        \item commutativity: $X + Y = Y + x$ for all $X$ and $Y$ in $M_{m, n}(\R)$
        \item existence of additive inverses: for each $X$ in $M_{m, n}(\R)$ there is an element $-X$ in $M_{m, n}(\R)$ such that $X + (-X) = O_{m \times n} = (-X) + X$
        \item associativity: $(X + Y) + Z = X + (Y + Z)$ for all $X, Y, Z \in M_{m, n}(\R)$
    \end{enumerate}
    (As before this makes $M_{m, n}(\R)$ an abelian group under addition)

    \textbf{Axioms for Scalar Multiplication}
    \begin{enumerate}[label = (\roman*)]
        \item $0X = O_{m \times n}$ for all $X$ in $M_{m, n}(\R)$
        \item $1X = X$ for all $X$ in $M_{m, n}(\R)$
        \item associativity: $(\lambda\mu)X = \lambda(\mu X)$ for all $\lambda, \mu \in \R$ and for all $X \in M_{m, n}(\R)$
        \item distributivity: $(\lambda + \mu)X = \lambda X + \mu X$ and $\lambda (X + Y) = \lambda X + \lambda Y$ for all real $\lambda, \mu$ and for all $X$ and $Y$ in $M_{m, n}(\R)$.
    \end{enumerate}
    \begin{proof}
        Do
    \end{proof}
\end{proposition}

\subsection{Matrix Operations II: Matrix multiplication}
Matrix multiplication: This is a function
\[
M_{m \times n}(\R) \times M_{n \times p}(\R) \rightarrow M_{m \times p}(\R).
\]
If $X = (x_{ij})_{m \times n}$ is $m \times n$ and $Y = (y_{ij})_{n \times p}$ is $n \times p$ (if the number of columns of $X$ equals the number of rows of $Y$) then we may form the product matrix $XY$. This is defined to be the $m \times p$ matrix whose $(i,\,j)$th element $(XY)_{ij}$ is
\[
(XY)_{ij} = x_{i\,1}y_{1\,j} + x_{i\,2}y_{2\,j} + \dotsi + x_{i\,n}x_{n\,j} = \sum_{k = 1}^{n}{x_{ik}y_{kj}}.
\]
If $\mbf{x ^ t}_{i}$ denotes the $i$-th row of $X$ and $\mbf{y}_j$ denotes the $j$-th column of $Y$ then $(XY)_{ij} = \mbf{x}_i \cdot \mbf{y}_j$ (scalar product of column vectors).

\begin{proposition}[Multiplication rules]
    Let $X$ and $X'$ be $m \times n$, $Y$ and $Y'$ be $n \times p$ and $Z$ be $p \times q$ matrices. Let $\lambda$ and $\lambda'$ be scalars. Then
    \begin{enumerate}[label = (\roman*)]
        \item $O_{p \times m}X = O_{p \times n}$ and $XO_{n \times q} = O_{m \times q}$.
        \item $I_{m}X = X = XI_n$.
        \item $\lambda(XY) = (\lambda X)Y = X(\lambda Y)$.
        \item associativity: $(XY)Z = X(YZ)$
        \item distributive rules:
        \begin{enumerate}[label = (\alph*)]
            \item $(X + X')Y = XY + X'Y$ and $X(Y + Y') = XY + XY'$.
            \item $\lambda(X + X') = \lambda X + \lambda X'$ and $(\lambda + \lambda')X = \lambda X + \lambda' X$.
        \end{enumerate}
    \end{enumerate}
    \begin{proof}
        For (iv), the $k$th entry in the $i$th row of $XY$ is given by $\sum_{r = 1}^{n}x_{ir}y_{rk}$, with $k = 1,\dotsc, p$. Hence the $(i,\,j)$th entry of $(XY)Z$ is given by
        \[
        ((XY)Z)_{ij} = (i\text{th row of }XY) \cdot (j\text{th column of } Z) = \sum_{k = 1}^{p}\left(\sum_{r = 1}^{n}x_{ir}y_{rk}\right)z_{kj}.
        \]
        On the other hand, the $r$th entry in the $j$th column of $YZ$ is given by $\sum_{k = 1}^{p}y_{rk}z_{kj}$, with $r = 1, \dotsc, n$. Hence the $(i,\,j)$th entry of $X(YZ)$ is given by
        \[
        (X(YZ))_{ij} = (i\text{th row of } X) \cdot (j\text{th column of } YZ) = \sum_{r = 1}^{n}x_{ir}\left(\sum_{k = 1}^{p}y_{rk}z_{kj}\right).
        \]
        Switching the order of summation using associativity of the usual multiplication given $(XY)Z = X(YZ)$.
        
        \textbf{Prove rest!}
    \end{proof}
\end{proposition}

\textbf{Warning}: We do not necessarily have $XY = YX$ in general.

\textbf{Warning}: It is possible to have $XY = O_{m \times p}$ even though both $X$ and $Y$ are non-zero.

\textbf{A useful trick for sometimes multiplying big matrices more easily}

Let $\begin{pmatrix}
    A & B \\ C & D
\end{pmatrix} \in M_{m, n}(\R)$ and $\begin{pmatrix}
    A' & B' \\ C' & D'
\end{pmatrix} \in M_{n, p}(\R)$ be two matrices given by blocks $A \in M_{m_1, n_1}(\R),\, B \in M_{m_1, n_2}(\R),\, C \in M_{m_2, n_1}(\R),\, D \in M_{m_2, n_2}(\R),\, A' \in M_{n_1, p_1}(\R),\, B' \in M_{n_1, p_2}(\R),\, C' \in M_{n_2, p_1}(\R),\, D' \in M_{n_2, p_2}(\R)$ (so-called block matrices) with $m_1 + m_2 = m$ and $n_1 + n_2 = n$ and $p_1 + p_2 = p$. It can easily shown that
\[
\begin{pmatrix}
    A & B \\ C & D
\end{pmatrix}
\begin{pmatrix}
    A' & B' \\ C' & D'
\end{pmatrix}
=
\begin{pmatrix}
    AA' + BC' & AB' + BD' \\ CA' + DC' & CB' DD'
\end{pmatrix} \in M_{m, p}(\R)
\]
that is, if matrix sizes match correctly the formula for the multiplication of block matrices is the same as one of "usual two by two matrices"

For example, if $A = \begin{pmatrix}
    0 & 1 \\ 0 & 0
\end{pmatrix}$(so that $A ^ 2 = O_{2 \times 2}$) then
\[
\begin{pmatrix}
    1 & 0 & 1 & 0 \\
    0 & 1 & 0 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 0 & 0 & 0
\end{pmatrix} ^ 2
=
\begin{pmatrix}
    I_2 & I_2 \\ A & A
\end{pmatrix} ^ 2
=
\begin{pmatrix}
    I & I \\ A & A
\end{pmatrix}
\begin{pmatrix}
    I & I \\ A & A
\end{pmatrix}
=
\begin{pmatrix}
    I + A & I + A \\ A + A ^ 2 & A + A ^ 2
\end{pmatrix}
=
\begin{pmatrix}
    1 & 1 & 1 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 1 & 0 & 1 \\
    0 & 0 & 0 & 0
\end{pmatrix}
\]

\subsection{Inverse matrix}

\begin{definition}
    Let $A$ be a square $(n\times n)$ matrix. We say $A$ is invertible (or non-singular) if there exists an $n \times n$ matrix $B$ such that
    \[
    AB = BA = I_n.
    \]
    If such a matrix $B$ exists we call $B$ the inverse of $A$ and we write $A ^ {-1}$ for this inverse. (there can only exist at most one inverse).
    If no such inverse $B$ exists for a matrix $A$, we say $A$ is non-invertible (or singular).
\end{definition}

\begin{proposition}[Properties of the inverse and transpose, label = prop:propofinvandtransp]\label{linalg_prop_propofinvandtransp}
    \begin{enumerate}[label = (\roman*)]
        \item A matrix has at most one inverse.
        \item Assume $A$ and $B$ are invertible $n \times n$ matrices. Then the product $AB$ is invertible and
        \[
        (AB) ^ {-1} = B ^ {-1}A ^ {-1}.
        \]
        \item We have $(AB) ^ t = B ^ t A ^ t$ and $(\lambda A) ^ t = \lambda A ^ t$ for any matrix $A$ and any real constant $\lambda$.
        \item Assume $A$ is invertible. then the transpose $A ^ t$ is invertible and
        \[
        (A ^ t) ^ {-1} = (A ^ {-1}) ^ t.
        \]
        \item Let $A \in M_{n, n}(\R)$ and assume $BA = I_n$ for some $B \in M_{n, n}(\R)$. Then $AB = I_n$ as well as vice versa. So the "left inverse" is automatically the "right inverse".
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
            \item If $B'$ were a second inverse for $A$ then we would have
            \[
            B' = B'I_n = B'(AB) = (B'A)B = I_nB = B.
            \]
        \end{enumerate}
    \end{proof}
\end{proposition}

\begin{remark}
    We also have
    \[
    (X + Y) ^ t = X ^ t + Y ^ t,
    \]
    but it is not the case in general that
    \[
    (X + Y) ^ {-1} = X ^ {-1} + Y ^ {-1}.
    \]
\end{remark}

\newpage

\section{Gauss-Jordan elimination}

\subsection{Solving equations and Gauss-Jordan elimination}
We will turn to solving the general problem of solving $m$ linear equations in $n$ variables. To handle this, we need a systematic approach.

We begin by setting up our notation. We shall denote our variables by $x_1,\,x_2,\dotsc,x_n$, and consider the solution of the system of $m$ linear equations
\begin{align*}
    a_{1 1}x_{1} &+ a_{1 2}x_2 &+ \dotsi &+ a_{1 n}x_n &= b_1; \\ 
    a_{2 1}x_{1} &+ a_{2 2}x_2 &+ \dotsi &+ a_{2 n}x_n &= b_2; \\ 
    \vdots & \vdots & \ddots & \vdots &\vdots \vdots; \\ 
    a_{m 1}x_{1} &+ a_{m 2}x_2 &+ \dotsi &+ a_{m n}x_n &= b_m.
\end{align*}
For short we can write
\[
\sum_{j = 1}^{n}a_{ij}x_{j} = b_{i}\quad\text{for } i = 1,\dotsc,m.
\]
In vector notation we can write this using the dot product
\[
\mbf{a}_i \cdot \mbf{x} = \begin{pmatrix}
    a_{i 1} \\ \vdots \\ a_{i n}
\end{pmatrix}
\cdot
\begin{pmatrix}
    x_1 \\ \vdots \\ x_n
\end{pmatrix}
=
b_i\quad\text{for } i = 1, \dotsc, m.
\]
Each equation gives a so-called hyperplane with normal vector $\mbf{a}_i$.

To solve the equations we shall go through the following process.
\begin{itemize}
    \item First formalise the system of equations as a single equation of matrices, this is for notational convenience and systematises our process.
    \item Perform series of matrix operations (elementary row operations (ERO's)) that in effect change the original set of equations into a different set of linear equations, but ones that have the same solution set and are much easier to solve. This process is called Gauss-Jordan elimination, and the sort of matrix that represents the equations that are easier to solve is said to be in row reduced echelon form (RREF).
\end{itemize}

The first step we have essentially already done.
Using the notation from before,
the system of equations can be written as the matrix equation
\[
A\mbf{x} = \mbf{b}
\]
where $A$ is the matrix
\[
\begin{pmatrix}
    a_{1 1} & a_{1 2} & \dotsi & a_{1 n} \\
    a_{2 1} & a_{2 2} & \dotsi & a_{2 n} \\
    \vdots & \vdots & \phantom{} & \vdots \\
    a_{m 1} & a_{m 2} \dotsi a_{m n}
\end{pmatrix}
\]
and $\mbf{b}$ is the column vector
\[
\begin{pmatrix}
    b_1 \\ b_2 \\ \vdots \\ b_m
\end{pmatrix}.
\]

The augmented matrix associated with a linear system of equations is a rectangular array of numbers,
the coefficients of each equation are placed in order in each row,
followed by a vertical bar where the equal signs are with the $a_{i j}$ to the left,
and then the $b_i$ recorded to the right.
In other words
\[
\left(
\begin{array}{cccc|c}
    a_{1 1} & a_{1 2} & \dotsi & a_{1 n} & b_1 \\
    a_{2 1} & a_{2 2} & \dotsi & a_{2 n} & b_2 \\
    \vdots & \vdots & \phantom{} & \vdots & \vdots \\
    a_{m 1} & a_{m 2} & \dotsi & a_{m n} & b_m
\end{array}
\right).
\]
For example, the system
\begin{align*}
    3x_1 - 4x_2 + 3x_3 &= 1 \\
    2x_1 - x_2 + 2x_3 &= 2
\end{align*}
has augmented matrix $\left(
\begin{array}{ccc|c}
    3 & -4 & 3 & 1 \\
    2 & -1 & 2 & 2
\end{array}
\right).$

If as before we write the matrix
\[
\left(
\begin{array}{cccc|c}
    a_{1 1} & a_{1 2} & \dotsi & a_{1 n} & b_1 \\
    a_{2 1} & a_{2 2} & \dotsi & a_{2 n} & b_2 \\
    \vdots & \vdots & \phantom{} & \vdots & \vdots \\
    a_{m 1} & a_{m 2} & \dotsi & a_{m n} & b_m
\end{array}
\right),
\]
which we shall sometimes call the coefficient matrix of the linear system,
and as above write
\[
\mbf{b} = \begin{pmatrix}
    b_1 \\ \vdots \\ b_m
\end{pmatrix}
\quad\text{and}\quad\mbf{x} = \begin{pmatrix}
    x_1 \\ \vdots \\ x_n
\end{pmatrix},
\]
then the augmented matrix cab we written as $(A | \mbf{b})$.

We call a linear system homogenous if $\mbf{b} = \mbf{0}$,
and inhomogeneous if $\mbf{b} \neq \mbf{0}$.
Note that a homogeneous system always has at least one solution: $\mbf{x} =\mbf{0}$.

\begin{lemma}\label{linalg_lem_invprop}\phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item Let $A \in M_{n,\,n}(\R)$ be a square matrix and let $\mbf{v} \in \R ^ n$ be a non-zero vector such that $A\mbf{v} = \mbf{0}$. Then $A$ is singular.
        \item Let $A \in M_{n,\,n}(\R)$ be an invertible square matrix. Then the inhomogeneous system $A\mbf{x} = \mbf{b}$ has a single unique solution. In particular, the only solution to the homogeneous system $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$.
    \end{enumerate}
    \begin{proof}
        For (i), see example 2.4.2(iii) for an easily generalisable argument. For (ii), if $A$ is invertible then
        \[
        \mbf{x} = I_n\mbf{x} = (A ^ {-1} A)\mbf{x} = A ^ {-1}(A\mbf{x}) = A ^ {-1}\mbf{b}.
        \]
    \end{proof}
\end{lemma}

\subsection{Echelon form and its application to linear systems}

\begin{definition}
    A matrix of numbers is said to be in row reduced echelon form (RREF) if
    \begin{enumerate}[label = (\roman*)]
        \item The first (leftmost) non-zero entry in any non-zero row is a $1$ (called its leading $1$).
        \item If a row has its leading $1$ in the $j$th column then
        \begin{enumerate}[label = (\alph*)]
            \item all the other entries in the $j$th column are $0$; and
            \item the leading $1$s of subsequent rows are in columns to the right of the $j$th column.
        \end{enumerate}
    \item Any row(s) of zeros come after all the rows with non-zero entries.
    \end{enumerate}
\end{definition}

For example, the matrices
\[
\begin{pmatrix}
    1 & 4 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0
\end{pmatrix},
\qquad
\begin{pmatrix}
    0 & 1 & 0 & 0 & 2 \\ 0 & 0 & 1 & 0 & 3 \\ 0 & 0 & 0 & 1 & 4
\end{pmatrix}
\qquad\text{and}
\qquad
\begin{pmatrix}
    1 & 0 & 2 & 0 \\
    0 & 1 & 3 & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & 0 & 0
\end{pmatrix}
\]
are all in RREF.

In general, an RREF matrix looks as follows:
\[
\left(
\begin{array}{cccccccccccccccc}
    1 & * & \dotsi & * & 0 & * & \dotsi & * & 0 & * & \dotsi & * & 0 & * & \dotsi & * \\
    0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * & 0 & * & \dotsi & * & 0 & * & \dotsi & * \\
    0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * & 0 & * & \dotsi & * \\
    \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots \\
    0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & 1 & * & \dotsi & *
\end{array}
\right)
\]

\begin{example}
    Consider these three RREF's for a non-zero $2 \times 2$ matrix.
    \[
    (a): \begin{pmatrix}
        1 & \alpha \\ 0 & 0
    \end{pmatrix},\qquad
    (b): \begin{pmatrix}
        1 & 0 \\ 0 & 1
    \end{pmatrix},
    \qquad
    (c): \begin{pmatrix}
        0 & 1 \\ 0 & 0
    \end{pmatrix}.
    \]
    Here $\alpha$ is some scalar. Viewing these as the coefficient matrix for a linear system we have the following augmented matrices:
    \[
    (a): \begin{amatrix}{2}
        1 & \alpha & b_1 \\ 0 & 0 & b_2
    \end{amatrix},
    \qquad
    (b): \begin{amatrix}{2}
        1 & 0 & b_1 \\ 0 & 1 & b_2
    \end{amatrix},
    \qquad
    (c): \begin{amatrix}{2}
        0 & 1 & b_1 \\ 0 & 0 & b_2
    \end{amatrix}.
    \]
\end{example}

For $(a)$ and $(c)$ the correspond system has clearly only a solution if $b_2 = 0$ (because the second equation now reads $0x_1 + 0x_2 = 1$).
In these cases we obtain for the solution sets
\[
(a): x_1 = b_1 - \alpha x_2,\quad (b): x_1 = b_1,\,x_2 = b_2,\quad (c): x_2 = b_1
\]
note for $(b)$ we have only have a unique solution, while for $(a)$ and $(c)$ the solution set is a line.
More precisely, it is a non-horizontal line for $(a)$, and a horizontal one for $(c)$.

\subsection{The Elementary Row Operations and the big theorem}
The ERO's are simple operations you can do to a matrix to turn it into another matrix.
There are three fundamental operations, which are as follows,
\begin{enumerate}[label = (\alph*)]
    \item $P_{rs}$: switch row $r$ with row $s$;
    \item $M_{r}(\mbf{\lambda})$: multiple (all the entries in) row $r$ by the number $\lambda \neq 0$;
    \item $A_{rs}$: add (each entry in) row $r$ to (the corresponding entry in) row $s$.
    \item[(c')] $A_{rs}(\mbf{\lambda})$: add $\lambda$ times (each entry in) row $r$ to (the corresponding entry in) row $s$.
\end{enumerate}

Note that $A_{rs}(\lambda) = M_r\left(\frac{1}{\lambda}\right) \circ A_{rs} \circ M_r(\lambda)$,
read from right to left (in the same way as how $f \circ g$ means do $g$ then $f$).
\begin{example}
    \begin{align*}
    \begin{pmatrix}
        1 & 3 & 2 \\
        2 & 1 & -1 \\
        -1 & 1 & 2
    \end{pmatrix}
    &\xrightarrow{
    \begin{subarray}{c}
        A_{12}(-2) \\ A_{13}(1)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 3 & 2 \\
        0 & -5 & -5 \\
        0 & 4 & 4
    \end{pmatrix}
    &\xrightarrow{
    \begin{subarray}{c}
        M_2(-1 / 5) \\ M_3(1 / 4)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 3 & 2 \\
        0 & 1 & 1 \\
        0 & 1 & 1
    \end{pmatrix} 
    \\
    &\xrightarrow{
    \begin{subarray}{c}
        A_{23}(-1)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 3 & 2 \\
        0 & 1 & 1 \\
        0 & 0 & 0
    \end{pmatrix}
    &\xrightarrow{
    \begin{subarray}{c}
        A_{21}(-3)
    \end{subarray}
    }
    \begin{pmatrix}
        1 & 0 & -1 \\
        0 & 1 & 1 \\
        0 & 0 & 0
    \end{pmatrix}
    \end{align*}
\end{example}

\subsubsection{The basic routine}
\begin{enumerate}[label = (\alph*)]
    \item Find the first non-zero column (from the left).
    \item Go down the first non-zero column and find the first non-zero entry.
    \item If this is in the $r$-th row with $r \neq 1$ apply the elementary row operation $P_{1 r}$ to get a non-zero entry, $\lambda$ say, at the top of the first non-zero column.
    \item Apply the elementary row operation $M_1(1 / \lambda)$ to make the first non-zero entry of the first non-zero column $a$ $1$.
    \item Using elementary row operation of the form $A_{1 i}(\mu)$ clear all other non-zero entries in the first non-zero column.
\end{enumerate}

The resulting matrix is of the form
\[
\begin{pmatrix}
    1 & * & \dotsi & * \\
    0 & * & \dotsi & * \\
    \vdots & \vdots & \phantom{} & \vdots \\
    0 & * & \dotsi & *
\end{pmatrix}
\]
or, if the first non-zero column is not the first one,
\[
\begin{pmatrix}
    0 & \dotsi & 0 & 1 & * & \dotsi & * \\
    0 & \dotsi & 0 & 0 & * & \dotsi & * \\
    \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots \\
    0 & \dotsi & 0 & 0 & * & \dotsi & * \\
\end{pmatrix}
\]

\subsubsection{The algorithm}
Let $A$ be given $m \times n$ matrix.
The process of Gauss-Jordan elimination applied to $A$ is described by repeatedly applying the Basic Routine in stages.
(For simplicity of description we will assume that the first column of $A$ is non-zero).

\textbf{Stage 1}

Apply the basic routine to the given $m \times n$ matrix $A$.
This gives a new $m \times n$ matrix $A_1$ of the form:
\[
A_1 =
\begin{pmatrix}
    1 & * & \dotsi & * \\
    0 & * & \dotsi & * \\
    \vdots & \vdots & \phantom{} & \vdots \\
    0 & * & \dotsi & *
\end{pmatrix}
\]

\textbf{Stage 2}

If all of the remaining rows (below row $1$) are zero, it is easy to see that the matrix is now in RREF.
Otherwise:
\begin{enumerate}[label = (\roman*).]
    \item Apply the basic routine to the submatrix of $A_1$ obtained by ignoring the first row.
    This gives a new $m \times n$ matrix $\tilde{A}_2$ of the form:
    \[
    \begin{pmatrix}
        0 & \dotsi & 0 & 1 & * & \dotsi & * \\
        0 & \dotsi & 0 & 0 & * & \dotsi & * \\
        \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots \\
        0 & \dotsi & 0 & 0 & * & \dotsi & *
    \end{pmatrix}
    \]
    \item Apply an elementary row operation of the form $A_{2 1}(\mu)$,
    if necessary, to $\tilde{A}_2$ to make the entry above the first $1$ in the second row a zero.
    This gives a new $m \times n$ matrix $A_2$ of the form:
    \[
    A_2 = 
    \begin{pmatrix}
        1 & * & \dotsi & * & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & * & \dotsi & *
    \end{pmatrix}.
    \]
\end{enumerate}
Now continue inductively.
Assume stages $1$ to $k - 1$ have been completed (for $k - 1 < m$),
resulting in a matrix $A_{k - 1}$.

\textbf{Stage k (with $k \leq m$)}

If rows $k, k + 1, \dotsc, m$ of $A_{k - 1}$ are all zero then this matrix is in RREF and the algorithm terminates.
Similarly, if the leading $1$ in the $(k - 1)$th row of $A_{k - 1}$ lies in the $n$th column then it is easy to check that $A_{k - 1}$ is in RREF.
Otherwise:
\begin{enumerate}[label = (\roman*).]
    \item Apply the basic routine to the submatrix of $A_{k - 1}$ obtained by ignoring the first $k - 1$ rows.
    This gives a new $m \times n$ matrix $\tilde{A}_k$ of the form:
    \[
    \tilde{A}_k = \begin{pmatrix}
        1 & 0 & \dotsi & * & 0 & * & \dotsi & * & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 1 & * & \dotsi & * & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & * & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 1 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & *
    \end{pmatrix}.
    \]
    \item Apply elementary row operations of the form $A_{k r}(\mu)\ (\text{for } r = 1, 2, \dotsc, k - 1)$,
    if necessary, to $\tilde{A}_k$; this ensures all remaining non-zero entries in the column containing the first $1$ in the $k$-th row equal to zero and gives a new $m \times n$ matrix $A_k$ of the form:
    \[
    \tilde{A}_k = \begin{pmatrix}
        1 & 0 & \dotsi & * & 0 & * & \dotsi & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 1 & * & \dotsi & 0 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 1 & * & \dotsi & * \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & * \\
        \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \phantom{} & \vdots & \vdots & \phantom{} & \vdots \\
        0 & 0 & \dotsi & 0 & 0 & 0 & \dotsi & 0 & * & \dotsi & *
    \end{pmatrix}.
    \]
    By induction, this process must terminate either before Stage $m$ or after Stage $m$.
    In the former case,
    the reason for early termination must have been that the matrix had reached RREF.
    In the latter case we must have reached a matrix $A_m$ with a leading $1$ on every row.
    It is then easy to check that $A_m$ is in RREF.
\end{enumerate}

\subsection{Using G-J to solve a system of linear equations}
Recall, we are considering the solution set of a general system of $m$ linear equations in $n$ variables.

Two such systems are said to be equivalent if they have the same solution set.
We say an augmented matrix is in row reduced echelon form if the associated coefficient matrix is in row reduced echelon form.
Applying the elementary row operations to the augmented matrix $(A | \mbf{b})$ allows us to pass to equivalent systems and eventually to a system for which it is easy to spot the solutions.
This is justified by
\begin{theorem}[label = thm:lintoech]\label{linalg_thm_lintoech}
    Applying ERO's to the augmented matrix of a linear system simply changes the system to an equivalent one; that is, the solution set remains unchanged.
\end{theorem}

If a row is of the form $(0\ \dotsi\ \,|\, k)$ for $k \neq 0$ this would mean that there are no solutions,
we call a system like this inconsistent.

A quick way to solve a linear system,
there is no need to run G-J fully.
One can do the following:
\begin{enumerate}[label = \arabic*) :]
    \item Perform ERO's to bring coefficient matrix $A$ into upper triangular form,
    that is, all the entries below the diagonal vanish.
    (Also not necessary to scale the "leading ones" to be actually equal to $1$.)
    \item Then go back to the linear system and use back substitution from bottom to top to solve the system.
\end{enumerate}

\begin{example}
    \[
    \sysdelim..\systeme{
        x_1 + 2x_2 + 3x_3 = 1,
        2x_1 + 3x_2 + 2x_3 = 1,
        3x_1 + 2x_2 + x_3 = 1.
        }
    \]
    We put this into matrix form then EROs to get it into upper triangular form
    \begin{align*}
        &\begin{amatrix}{3}
            1 & 2 & 3 & 1 \\
            2 & 3 & 2 & 1 \\
            3 & 2 & 1 & 1
        \end{amatrix}
        &\xrightarrow{\begin{subarray}{c}
        A_{12}(-2) \\ A_{13}(-3)
        \end{subarray}}
        &\begin{amatrix}{3}
            1 & 2 & 3 & 1 \\
            0 & -1 & -4 & -1 \\
            0 & -4 & -8 & -2
        \end{amatrix}
        \\
        \xrightarrow{A_{23}(-4)}
        &\begin{amatrix}{3}
            1 & 2 & 3 & 1 \\
            0 & -1 & -4 & -1 \\
            0 & 0 & 8 & 2
        \end{amatrix}.
    \end{align*}
    Putting this back into our equations gives us:
    
    $3$rd equation: $x_3 = \frac{1}{4}$;
    
    $2$nd equation: $x_2 = -4x_3 + 1 = -4\frac{1}{4} + 1 = 0$;
    
    $1$st equation: $x_1 = 1 - 2x_2 - 3x_3 = 1 - \frac{3}{4} = \frac{1}{4}$, i.e.,
    \[
    \begin{pmatrix}
        \frac{1}{4} \\
        0 \\
        \frac{1}{4}
    \end{pmatrix}.
    \]
\end{example}

\subsection{Inverse matrices and linear systems}
\begin{lemma}\label{linalg_lem_solsetsam}
    Consider an arbitrary $m \times n$ linear system $A\mbf{x} = \mbf{b}$.
    Let $B \in M_{m \times m}(\R)$ be an invertible matrix.
    Then the new system $(BA)\mbf{x} = B\mbf{b}$ has the same solution set as the original system.
    \begin{proof}
        We need to show that the two systems have the same solution set.
        First assume that $\mbf{v}$ is a solution to the system $A\mbf{x} = \mbf{b}$, assume $A\mbf{v} = \mbf{b}$.
        Then, $B\mbf{b} = B(A\mbf{v}) = (BA)\mbf{v}$ as required.
        Conversely, assume $(BA)\mbf{v} = B\mbf{b}$. Then
        \[
        \mbf{b} = I_m\mbf{b} = (B ^ {-1}B)\mbf{b} = B ^ {-1}(B\mbf{b}) = B ^ {-1}(BA\mbf{v}) = (B ^ {-1} B) A\mbf{v} = A\mbf{v},
        \]
        as required.
    \end{proof}
\end{lemma}

\begin{theorem}[continues = thm:lintoech]
    \begin{proof}
        Suppose we want to perform elementary row operations on a matrix with $m$ rows.
        Then we can do this using the following $m \times m$ matrices.
        \begin{enumerate}[label = (\alph*)]
            \item The row operation $P_{rs}$ can be performed by left multiplication by the $m \times m$ matrix,
            which we also call $P_{rs}$,
            obtained from the identity by swapping the $r$th and $s$th columns.
            That is $(p_{ij})_{m \times m}$ with $p_{ii} = 1$ when $i \neq r, s, p_{rr} = p_{ss} = 0; p_{ij} = 0$ when $i \neq j$ and $\{i, j\} \neq \{r, s\}, p_{rs} = p_{sr} = 1$. That is,
            \[
            P_{rs} = \begin{pmatrix}
                1 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                0 & 1 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 1 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 1 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 1
            \end{pmatrix}.
            \]
            One can check that we have $P_{rs} ^ {-1} = P_{rs}$ (swapping twice leaves you back where you started).
            \item
            The row operation $M_r(\lambda)$ can be performed by left multiplication by the $m \times m$ matrix,
            which we also call $M_r(\lambda)$,
            obtained from the identity by replacing the $r$th diagonal entry with $\lambda$.
            That is $(m_{ij})_{m \times m}$ with $m_{ii} = 1$ when $i \neq r$ and $m_{rr} = \lambda$; $m_{ij} = 0$ when $i \neq j$.
            That is,
            \[
            M_r(\lambda) = \begin{pmatrix}
                1 & 0 & \dotsi & 0 & \dotsi & 0 \\
                0 & 1 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & \lambda & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 1
            \end{pmatrix}.
            \]
            We have $M_r(\lambda) ^ {-1} = M_r\left(\frac{1}{\lambda}\right)$.
            \item The row operation $A_{rs}(\lambda)$ has be performed by left multiplication by the $m \times m$ matrix,
            which we also call $A_{rs}(\lambda)$,
            obtained from the identity matrix by replacing the entry in the $r$th column and $s$th row with $\lambda$.
            That is $(a_{ij})_{m \times m}$ where $a_{ii} = 1; a_{ij} = 0$ when $i \neq j$ and $(i, j) \neq (s, r);\; a_{sr} = \lambda$.
            That is,
            \[
            \begin{pmatrix}
                1 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                0 & 1 & \dotsi & 0 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 1 & \dotsi & 0 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & \lambda & \dotsi & 1 & \dotsi & 0 \\
                \vdots & \vdots & \phantom{} & \vdots & \phantom{} & \vdots & \phantom{} & \vdots \\
                0 & 0 & \dotsi & 0 & \dotsi & 0 & \dotsi & 1
            \end{pmatrix}.
            \]
            We have $A_{rs}(\lambda) ^ {-1} = A_{rs}(-\lambda)$.
        \end{enumerate}
        We call any of these matrices an elementary matrix.
        Then if $E$ is an elementary $m \times m$ matrix the corresponding row operation is given by $EA\mbf{x} = E\mbf{b}$.
    \end{proof}
\end{theorem}

\begin{remark}
    Note that every elementary matrix is not only invertible,
    but the inverse of any elementary matrix is also an elementary matrix.
\end{remark}

\subsection{The fundamental theorem of invertible matrices}
\begin{lemma}\label{linalg_lem_sqmatrrefcond}
    Let $A \in M_{n \times n}(\R)$ be a square matrix.
    Then the following are (pairwise) equivalent,
    that is they are either all true or all false.
    \begin{enumerate}[label = (\roman*)]
        \item In each column of RREF of $A$ there exists a leading $1$.
        \item The RREF of $A$ is the identity matrix $I_n = \begin{pmatrix}
            1 & \phantom{} & \phantom{} \\
            \phantom{} & \ddots & \phantom{} \\
            \phantom{} & \phantom{} & 1
        \end{pmatrix}$.
        \item The only solution to the system $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$.
    \end{enumerate}
    \begin{proof}
        The equivalence of (i) and (ii) is clear:
        if the RREF is the identity then each column has a leading $1$.
        Conversely, if each column of the RREF has a leading $1$,
        then the first leading $1$ is in the $1, 1$-entry,
        the second must be in the $2, 2$-entry and so on.
        Then all the off-diagonal entries must be zero,
        that is, the RREF is indeed $I_n$.

        Next we show that (ii) implies (iii).
        So let's assume (ii).
        But by \autoref{linalg_lem_solsetsam} we can replace the equation $A\mbf{x} = \mbf{0}$ with the RREF of $A$,
        that is, we can consider $I_n\mbf{x} = \mbf{0}$,
        which means $\mbf{x} = \mbf{0}$.
        So (iii) holds.

        What remains is to show that (iii) implies (i).
        So let's assume (iii), that is,
        the only solution to the system $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$.
        Let's assume for a contradiction that the number of leading $1$'s in the RREF of $A$ is equal to $r < n$.
        This means that there are $n - r > 0$ free variables (one for each column without a leading $1$).
        To be explicit, after permutation of columns we can assume the RREF of $A$ looks as follows
        \[
        \begin{pmatrix}
            I_r & A'_{r, n - r} \\
            \mbf{0}_{n - r, r} & \mbf{0}_{n - r, n - r}
        \end{pmatrix}
        \]
        for some $r \times (n - r)$ matrix $A'$.
        But then any choice of real value for each of the free variables will yield a solution t the system $A\mbf{x} = \mbf{0}$.
        In particular there exists a non-trivial solution to the system.
        This is a contradiction, therefore there must have been $r = n$ leading $1$'s after all.
    \end{proof}
\end{lemma}

\begin{theorem}[Fundamental Theorem of Invertible Matrices]\label{linalg_thm_fundthmofinvertmatr}
    A square matrix $A \in M_{n \times n}(\R)$ is invertible if and only if any one of the conditions (i), (ii), (iii) of \autoref{linalg_lem_sqmatrrefcond} holds.
    \begin{proof}
        Suppose $A$ is invertible.
        Then by \autoref{linalg_lem_invprop}(ii) the only solution of $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$,
        which is \autoref{linalg_lem_sqmatrrefcond}(iii).

        Now assume that any one of (i), (ii), and (iii) hold.
        By (ii), there exist elementary matrices $E_1, E_2, \dotsc, E_\ell$ such that
        \[
        E_\ell E_{\ell - 1} \dotsi E_1 A = I_n.
        \]
        Thus $(E_\ell E_{\ell - 1} \dotsi E_1)$ is a left sided inverse of $A$.
        We must show it is a right sided inverse too.
        To do this, note that
        \begin{align*}
            AE_\ell E_{\ell - 1} \dotsi E_1 &= I_n A E_\ell E_{\ell - 1} \dotsi E_1 \\
            &= (E_1 ^ {-1} E_2 ^ {-1} \dotsi E_{\ell} ^ {-1}E_\ell E_{\ell - 1} \dotsi E_1)AE_\ell E_{\ell - 1} \dotsi E_1 \\
            &= E_1 ^ {-1} E_2 ^ {-1} \dotsi E_{\ell} ^ {-1}(E_\ell E_{\ell - 1} \dotsi E_1A)E_\ell E_{\ell - 1} \dotsi E_1 \\
            &= E_1 ^ {-1} E_2 ^ {-1} \dotsi E_{\ell} ^ {-1}E_\ell E_{\ell - 1} \dotsi E_1 = I_n.
        \end{align*}
        So $A$ is invertible with $A ^ {-1} = E_\ell E_{\ell - 1} \dotsi E_1$.
    \end{proof}
\end{theorem}
This proof immediately gives us
\begin{corollary}
    Any invertible square matrix is the product of elementary matrices (and vice versa).
    \begin{proof}
        Simply notice that $E_1 ^ {-1}E_2 ^ {-1} \dotsi E_i ^ {-1}$ is an inverse of $A ^ {-1} = E_1E_2 \dotsi E_i$.
        But, so is the matrix $A$.
        So, by uniqueness of the inverses,
        we must have $A = E_1 ^ {-1} E_2 ^ {-1} \dotsi E_\ell ^ {-1}$.
    \end{proof}
\end{corollary}

\begin{remark}
    Note that the product above is not necessarily unique.
\end{remark}

\begin{remark}
    We can also now show that the "left inverse" of a square matrix $A$ is also the "right inverse" of $A$,
    that is if $BA = I_n$, then also $AB = I_n$
\end{remark}

\begin{proposition}[continues = prop:propofinvandtransp]
    \begin{proof}[Proof of (v)]
        Suppose that $BA = I_n$ and consider $A\mbf{x} = \mbf{0}$.
        Left multiplication by $B$ shows that the only solution is $\mbf{x} = \mbf{0}$.
        Hence $A$ is invertible by \autoref{linalg_thm_fundthmofinvertmatr}.
        In particular, $A$ has an inverse $A ^ {-1}$ and we have
        \[
        AB = ABAA ^ {-1} = A(BA)A ^ {-1} = AA ^ {-1} = I_n.
        \]
        By the uniqueness of inverses, this shows that $A ^ {-1} = B$.
    \end{proof}
\end{proposition}

\subsection{An algorithm to compute the inverse of a matrix}
We run G-J for the matrix $A$ and keep track of the elementary operations/matrices we used.
Concretely, we run G-J on the augmented or double matrix
\[
(A \,|\, I_n) =
\left(\hspace{-5pt}
\begin{array}{cccc|cccc}
     a_{1 1} & a_{1 2} & \dotsi & a_{1 n} & 1 & 0 & \dotsi & 0 \\
     a_{2 1} & a_{2 2} & \dotsi & a_{2 n} & 0 & 1 & \dotsi & 0 \\
     \vdots & \vdots & \phantom{} & \vdots & \vdots & \vdots & \ddots & \vdots \\
     a_{n 1} & a_{n 2} & \dotsi & a_{n n} & 0 & 0 & \dotsi & 1
\end{array}
\hspace{-5pt}\right).
\]
Now perform G-J as always and get (if $A$ is invertible)
\[
(I_n\,|\,E_\ell \dotsi E_1),
\]
and $A ^ {-1} = E_\ell \dotsi E_1$.
If $A$ is singular,
you will not obtain the identity matrix on the left hand side.

\newpage

\section{Determinants}
The determinant of a square matrix is a scalar.
The determinant of a matrix $A$ with real entries is a real number.
So we can think of this as a function
\[
\det : M_{n \times n}(\R) \rightarrow \R.
\]

\subsection{Small values of $n$}
We look at particular cases of the function
\[
\det : M_{n\times n}(\R) \rightarrow \R.
\]
$n = 1$:
The equation $ax = b$ has a unique solution if and only if $a \neq 0$.
We set
\[
\det(a) = a.
\]

$n = 2$:
We know $\begin{pmatrix}
    a & b \\ c & d
\end{pmatrix}$
is invertible if and only if $ad - bc \neq 0$.
So, set
\[
\det\begin{pmatrix}
    a & b \\ c & d
\end{pmatrix} = ad - bc.
\]
Then the matrix equation (system of two linear equations)

has a unique 
\end{document}