\documentclass[10pt, a4paper]{article}
\usepackage{preamble}


\title{Linear Algebra I \\
    \large Important Results}
\author{Luke Phillips}
\date{January 2025}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Vectors in \texorpdfstring{$\R ^ n$}{}}

\begin{proposition}[Properties of the scalar product]
    \phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item Symmetry
        \[
        \mbf{u \cdot v = v \cdot u}\qquad\text{for all }\mbf{u, v}\text{ in } \R ^ n
        \]
        \item Linearity in first factor
        \begin{enumerate}[label = (\alph*)]
            \item $(\mbf{u} + \mbf{v}) \cdot \mbf{w} = \mbf{u} \cdot \mbf{w} + \mbf{v} \cdot \mbf{w}$\qquad for all $\mbf{u,\ v,\ w}$ in $\R ^ n$
            \item $(\lambda\mbf{u}) \cdot \mbf{v} = \lambda(\mbf{u} \cdot \mbf{v})$\qquad for all $\lambda \in \R$ and all $\mbf{u,\ v}$ in $\R ^ n$
        \end{enumerate}
        \item Linearity in the second factor
        \begin{enumerate}[label = (\alph*)]
            \item $\mbf{u} \cdot (\mbf{v} + \mbf{w}) = \mbf{u} \cdot \mbf{w} + \mbf{u} \cdot \mbf{v}$\qquad for all $\mbf{u,\ v,\ w}$ in $\R ^ n$
            \item $\mbf{u} \cdot (\lambda\mbf{v}) = \lambda(\mbf{u} \cdot \mbf{v})$\qquad for all $\lambda \in \R$ and all $\mbf{u,\ v}$ in $\R ^ n$
        \end{enumerate}
        \item Positivity
        
        $\mbf{v \cdot v} \geq 0$ for all $v$, and $\mbf{v \cdot v} = 0$ if and only if $\mbf{v = 0}$.
    \end{enumerate}
\end{proposition}

\begin{definition}[Magnitude]
    We define the magnitude o $\mbf{v}$,
    denoted $|\mbf{v}|$,
    by
    \[
    |\mbf{v}| = \sqrt{\mbf{v} \cdot \mbf{v}}.
    \]

    A vector is \textbf{non-trivial} if it has positive length,
    the only non-trivial vector in $\R ^ n$ is the zero vector $\mbf{0}$.
\end{definition}

\begin{lemma}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ 2$ of length $r > 0 $ and $s > 0$ respectively. Let $\theta$ be the angle between them. Then
    \[
    \mbf{v \cdot w} = rs\cos(\theta).
    \]
    In particular, we have
    \[
    |\mbf{v \cdot w}| \leq |\mbf{v}||\mbf{w}|
    \]
    with equality if and only if $\mbf{v}$ and $\mbf{w}$ are collinear, i.e. are scalar multiples of each other.
\end{lemma}

\begin{remark}
    Note that using this lemma we can obtain an expression for the angle between two vectors
    \[
    \cos(\theta) = \frac{\mbf{v \cdot w}}{|\mbf{v}||\mbf{w}|}.
    \]
\end{remark}

\begin{theorem}[Cauchy-Schwarz inequality]
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors in $\R ^ n$. Then
    \[
    |\mbf{v \cdot w}| \leq |\mbf{v}||\mbf{w}|
    \]
    with equality if and only if $\mbf{v}$ and $\mbf{w}$ are collinear.
\end{theorem}

\begin{corollary}
    The non-trivial vectors $\mbf{v}$ and $\mbf{w}$ in $\R ^ n$ are orthogonal if an only if we have $\mbf{v} \cdot \mbf{w} = 0$.
\end{corollary}

\begin{definition}[Vector product in $\R ^ 3$]
    The vector (or cross) product is a function
    \[
    \R ^ 3 \times \R ^ 3 \rightarrow \R ^ 3
    \]
    defined as follows. If $\mbf{v, w} \in \R ^ 3$, their cross product is given by
    \[
    \mbf{v \times w} =
    \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix}
    \times
    \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix}
    =
    \begin{pmatrix} v_2 w_3 - v_3 w_2 \\ v_3 w_1 - v_1 w_3 \\ v_1 w_2 - w_1 v_2 \end{pmatrix}
    \]
\end{definition}

\begin{proposition}[Properties of the cross product]\phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item Anti-Symmetry:
        \[
        \mbf{u \times v = -v \times u}\quad\text{for all }\mbf{u, v} \in \R ^ 3
        \]
        \item Linearity in first factor
        \begin{enumerate}[label = (\arabic*)]
            \item $(\mbf{u + v}) \times \mbf{w} = \mbf{u \times w} + \mbf{v \times w}\quad\text{for all }\mbf{u, v, w} \in \R ^ 3$
            \item $(\lambda\mbf{u}) \times \mbf{v} = \lambda(\mbf{u \times v})\quad\text{for all } \lambda \in \R \text{ and all }\mbf{u, v} \in \R ^ 3$
        \end{enumerate}
        \item Linearity in the second factor
        \begin{enumerate}[label = (\arabic*)]
            \item $\mbf{u} \times (\mbf{v + w}) = \mbf{u \times w} + \mbf{u \times v}\quad\text{for all }\mbf{u, v, w} \in \R ^ 3$
            \item $\mbf{u} \times (\lambda\mbf{v}) = \lambda(\mbf{u \times v})\quad\text{for all } \lambda \in \R \text{ and all }\mbf{u, v} \in \R ^ 3$
        \end{enumerate}
        \item Orthogonality to the input
        \[
        \mbf{v} \cdot (\mbf{v \times w}) = \mbf{w} \cdot (\mbf{v \times w}) = 0\quad\text{for all vectors } \mbf{v} \text{ and } \mbf{w} \text{ in } \R ^ 3.
        \]
    \end{enumerate}
\end{proposition}

\begin{lemma}
    Suppose that $\mbf{v}$ and $\mbf{w}$ are vectors of length $r > 0$ and $s > 0$ respectively. Let $\theta \in [0, \pi]$ be the angle between them. Then
    \[
    |\mbf{v \times w}| = rs\sin(\theta).
    \]
    In particular, we have
    \[
    \mbf{v \times w = 0}\quad\iff\quad\mbf{v}\text{ and }\mbf{w}\text{ are collinear}.
    \]
\end{lemma}

\begin{definition}[Parametric form of a plane]
    A plane $\Pi$ in $\R ^ 3$ can be described in parametric form as the collection of all points corresponding to vectors of the form
    \[
    \mbf{a} + \lambda\mbf{d}_1 + \mu\mbf{d}_2
    \]
    where $\lambda, \mu \in \R$,
    $\mbf{a}$ is a point in $\Pi$ and $\mbf{d}_1$ and $\mbf{d}_2$ are two direction vectors for $\Pi$.
\end{definition}

\begin{definition}[Normal vector form of a plane]
    A plane $\Pi$ in $\R ^ 3$ can be described as follows
    \[
    \mbf{x} \cdot (\mbf{d}_1 \times \mbf{d}_2) = 0
    \]
    where $\mbf{x}$ a point in $\Pi$,
    and $\mbf{d}_1, \mbf{d}_2$ direction vectors for $\Pi$.

    We can write this as
    \[
    \mbf{n} \cdot \mbf{x} = \ell
    \]
    where $\mbf{n}$ is the normal vector to $\Pi$ and $\mbf{x}$ is a point on $\Pi$.
\end{definition}

\begin{definition}[Cartesian form of a plane]
    We have the Cartesian form of a plane from the normal vector form of a plane
    \[
    \mbf{n} \cdot \mbf{x} = \begin{pmatrix}
        a \\ b \\ c
    \end{pmatrix} \cdot \begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix} = ax + by + cz = \ell.
    \]
    Thus we have
    \[
    \Pi = \left\{\{\mbf{x} \in \R ^ 3\,:\,\mbf{n} \cdot \mbf{x} = \ell\right\} = \left\{\begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix}\,:\, ax + by + cz = \ell\right\}.
    \]
\end{definition}

\begin{definition}[Scalar triple product]
    The scalar triple product is a function
    \[
    \R ^ 3 \times \R ^ 3 \times \R ^ 3 \rightarrow \R.
    \]
    Given three vectors $\mbf{a, b, c}$ in $\R ^ 3$ we define their scalar triple product $[\mbf{a, b, c}]$ by
    \[
    [\mbf{a, b, c}] = \mbf{a} \cdot(\mbf{b} \times \mbf{c}).
    \]
\end{definition}

\begin{lemma}[Invariance under cyclic permutation]
    For any three vectors $\mbf{a, b, c}$ in $\R ^ 3$ we have
    \[
    [\mbf{a, b, c}] = [\mbf{b, c, a}] = [\mbf{c, a, b}] = -[\mbf{a, b, c}] = -[\mbf{b, c, a}] = -[\mbf{c, a, b}].
    \]
\end{lemma}

\newpage

\section{Matrix algebra}

\begin{definition}[Transpose]
    Given an $m \times n$ matrix $W$ we define $W ^ t$, the transpose of $W$, to be the $n \times m$ matrix whose $(i,\,j)$th entry is the $(j,\,i)$th entry of $W$; that is the matrix for which
    \[
    (W ^ t)_{ij} = W_{ji}.
    \]
\end{definition}

\begin{proposition}[Properties of matrix addition and scalar multiplication]
    Matrix addition and scalar multiplication for $M_{m, n}(\R)$ satisfy exactly the same properties as vector addition and scalar multiplication for $\R ^ n$. That is, the operations satisfy the axioms of a (real) vector space.

    \textbf{Axioms for addition}
    \begin{enumerate}[label = (\roman*)]
        \item existence of additive identity: there is a matrix $O_{m \times n}$ in $M_{m, n}(\R)$ so that for all $X$ in $M_{m, n}(\R)$ we have $X + O_{m \times n} = X = O_{m \times n} + X$ 
        \item commutativity: $X + Y = Y + x$ for all $X$ and $Y$ in $M_{m, n}(\R)$
        \item existence of additive inverses: for each $X$ in $M_{m, n}(\R)$ there is an element $-X$ in $M_{m, n}(\R)$ such that $X + (-X) = O_{m \times n} = (-X) + X$
        \item associativity: $(X + Y) + Z = X + (Y + Z)$ for all $X, Y, Z \in M_{m, n}(\R)$
    \end{enumerate}

    \textbf{Axioms for Scalar Multiplication}
    \begin{enumerate}[label = (\roman*)]
        \item $0X = O_{m \times n}$ for all $X$ in $M_{m, n}(\R)$
        \item $1X = X$ for all $X$ in $M_{m, n}(\R)$
        \item associativity: $(\lambda\mu)X = \lambda(\mu X)$ for all $\lambda, \mu \in \R$ and for all $X \in M_{m, n}(\R)$
        \item distributivity: $(\lambda + \mu)X = \lambda X + \mu X$ and $\lambda (X + Y) = \lambda X + \lambda Y$ for all real $\lambda, \mu$ and for all $X$ and $Y$ in $M_{m, n}(\R)$.
    \end{enumerate}
\end{proposition}

\begin{definition}[Matrix Multiplication]
    For any $X \in M_{m \times n}(\R)$ and $Y \in M_{n \times p}(\R)$ we define matrix multiplication as follows
    \[
    (XY)_{ij} = x_{i\,1}y_{1\,j} + x_{i\,2}y_{2\,j} + \dotsi + x_{i\,n}x_{n\,j} = \sum_{k = 1}^{n}{x_{ik}y_{kj}},
    \]
    where we get a matrix of the form $M_{m \times p}(\R)$.
\end{definition}

\begin{proposition}[Multiplication rules]
    Let $X$ and $X'$ be $m \times n$, $Y$ and $Y'$ be $n \times p$ and $Z$ be $p \times q$ matrices. Let $\lambda$ and $\lambda'$ be scalars. Then
    \begin{enumerate}[label = (\roman*)]
        \item $O_{p \times m}X = O_{p \times n}$ and $XO_{n \times q} = O_{m \times q}$.
        \item $I_{m}X = X = XI_n$.
        \item $\lambda(XY) = (\lambda X)Y = X(\lambda Y)$.
        \item associativity: $(XY)Z = X(YZ)$
        \item distributive rules:
        \begin{enumerate}[label = (\alph*)]
            \item $(X + X')Y = XY + X'Y$ and $X(Y + Y') = XY + XY'$.
            \item $\lambda(X + X') = \lambda X + \lambda X'$ and $(\lambda + \lambda')X = \lambda X + \lambda' X$.
        \end{enumerate}
    \end{enumerate}
\end{proposition}

\begin{definition}[Invertible Matrix]
    Let $A$ be a square $(n\times n)$ matrix. We say $A$ is invertible (or non-singular) if there exists an $n \times n$ matrix $B$ such that
    \[
    AB = BA = I_n.
    \]
    If such a matrix $B$ exists we call $B$ the inverse of $A$ and we write $A ^ {-1}$ for this inverse. (there can only exist at most one inverse).
    If no such inverse $B$ exists for a matrix $A$, we say $A$ is non-invertible (or singular).
\end{definition}

\begin{proposition}[Properties of the inverse and transpose]\phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item A matrix has at most one inverse.
        \item Assume $A$ and $B$ are invertible $n \times n$ matrices. Then the product $AB$ is invertible and
        \[
        (AB) ^ {-1} = B ^ {-1}A ^ {-1}.
        \]
        \item We have $(AB) ^ t = B ^ t A ^ t$ and $(\lambda A) ^ t = \lambda A ^ t$ for any matrix $A$ and any real constant $\lambda$.
        \item Assume $A$ is invertible. then the transpose $A ^ t$ is invertible and
        \[
        (A ^ t) ^ {-1} = (A ^ {-1}) ^ t.
        \]
        \item Let $A \in M_{n, n}(\R)$ and assume $BA = I_n$ for some $B \in M_{n, n}(\R)$. Then $AB = I_n$ as well as vice versa. So the "left inverse" is automatically the "right inverse".
    \end{enumerate}
\end{proposition}

\begin{remark}
    We also have
    \[
    (X + Y) ^ t = X ^ t + Y ^ t.
    \]
\end{remark}

\newpage

\section{Gauss-Jordan elimination}

\begin{lemma}\phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item Let $A \in M_{n,\,n}(\R)$ be a square matrix and let $\mbf{v} \in \R ^ n$ be a non-zero vector such that $A\mbf{v} = \mbf{0}$. Then $A$ is singular.
        \item Let $A \in M_{n,\,n}(\R)$ be an invertible square matrix. Then the inhomogeneous system $A\mbf{x} = \mbf{b}$ has a single unique solution. In particular, the only solution to the homogeneous system $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$.
    \end{enumerate}
\end{lemma}

\begin{definition}
    A matrix of numbers is said to be in row reduced echelon form (RREF) if
    \begin{enumerate}[label = (\roman*)]
        \item The first (leftmost) non-zero entry in any non-zero row is a $1$ (called its leading $1$).
        \item If a row has its leading $1$ in the $j$th column then
        \begin{enumerate}[label = (\alph*)]
            \item all the other entries in the $j$th column are $0$; and
            \item the leading $1$s of subsequent rows are in columns to the right of the $j$th column.
        \end{enumerate}
    \item Any row(s) of zeros come after all the rows with non-zero entries.
    \end{enumerate}
\end{definition}

\begin{remark}
    If we can express variables in terms of the other ones,
    we call these free variables.
\end{remark}

\begin{remark}
    The ERO's are simple operations you can do to a matrix to turn it into another matrix.
    There are three fundamental operations, which are as follows,
    \begin{enumerate}[label = (\alph*)]
        \item $P_{rs}$: switch row $r$ with row $s$;
        \item $M_{r}(\pmb{\lambda})$: multiple (all the entries in) row $r$ by the number $\lambda \neq 0$;
        \item $A_{rs}$: add (each entry in) row $r$ to (the corresponding entry in) row $s$.
        \item[(c')] $A_{rs}(\pmb{\lambda})$: add $\lambda$ times (each entry in) row $r$ to (the corresponding entry in) row $s$.
    \end{enumerate}
\end{remark}

\begin{remark}
    When an augmented matrix is in RREF,
    a variable $x_j$ being a free variable corresponds to column $j$ of the coefficient matrix note containing a leading $1$.
\end{remark}

\begin{lemma}
    Let $A \in M_{n \times n}(\R)$ be a square matrix.
    Then the following are (pairwise) equivalent.
    \begin{enumerate}[label = (\roman*)]
        \item In each column of RREF of $A$ there exists a leading $1$.
        \item The RREF of $A$ is the identity matrix $I_n = \begin{pmatrix}
            1 & \phantom{} & \phantom{} \\
            \phantom{} & \ddots & \phantom{} \\
            \phantom{} & \phantom{} & 1
        \end{pmatrix}$.
        \item The only solution to the system $A\mbf{x} = \mbf{0}$ is $\mbf{x} = \mbf{0}$.
    \end{enumerate}
\end{lemma}

\begin{theorem}[Fundamental Theorem of Invertible Matrices]
    A square matrix $A \in M_{n \times n}(\R)$ is invertible if and only if any one of the conditions (i), (ii), (iii) of the previous lemma holds.
\end{theorem}

\newpage

\section{Determinants}

\begin{remark}
    For integers $r$ and $s$ between $1$ and $n$ write $A_{r, s}$ for the $(n - 1) \times (n - 1)$ matrix obtained from $A$ by deleting its $r$th row and its $s$th column.
    $A_{r, s}$ is called the $(r, s)$th minor of $A$.
\end{remark}

\begin{remark}
    The value $\det(A_{r, s})$ is called the $(r, s)$th unsigned cofactor of $A$ and the value $(-1) ^ {r + s}\det(A_{r, s})$ is called the $(r, s)$th (signed) cofactor of $A$.
\end{remark}

\begin{definition}[The determinant]
    For any matrix $A \in M_{m \times n}(\R)$,
    the determinant of $A$ is defined by the following
    \[
    \det{A} = \sum_{k = 1}^{n}(-1) ^ {1 + k}a_{1k}\det(A_{1, k}).
    \]
\end{definition}

\begin{lemma}
    If $A$ is an $n \times n$ matrix then for any $1 \leq i \leq n$ we have
    \[
    \det(A) = \sum_{k = 1}^{n}(-1) ^ {i + k}a_{ik}\det(A_i, k).
    \]
    That is,
    the determinant formula remains valid via expansion along the $i$th row
    (note however the signs).
\end{lemma}

\begin{proposition}[Fundamental properties of the determinant]
    For notational convenience,
    we write in terms of the row vector $\mbf{a}_r$ of an $n \times n$ matrix $A = (a_{ij})$ that is $\mbf{a}_r = (a_{r1}\; a_{r2}\; \dotsi \; a_{rn})$
    \begin{enumerate}[label = (\roman*)]
        \item $\det I_n = 1$.
        \item If one multiplies
        (all the elements in)
        the $r$th row by some scalar $\lambda$
        (that is, if we apply the ERO $M_r(\lambda)$),
        then the determinant changes by precisely this factor:
        \[
        \det(M_r(\lambda)A) = \det\begin{pmatrix}
            \mbf{a} \\ \vdots \\ \lambda\mbf{a}_r \\ \vdots \\ \mbf{a}_n
        \end{pmatrix} = \lambda\det A.
        \]
        \item For any row vector $\mbf{b}_r$ we have
        \[
        \det\begin{pmatrix}
            \mbf{a}_1 \\ \vdots \\ \mbf{a}_r + \mbf{b}_r \\ \vdots \\ \mbf{a}_n
        \end{pmatrix} =
        \det \begin{pmatrix}
            \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_n
        \end{pmatrix}
        +
        \det\begin{pmatrix}
            \mbf{a}_1 \\ \vdots \\ \mbf{b}_r \\ \vdots \\ \mbf{a}_n
        \end{pmatrix}.
        \]
        Note that (ii) and (iii) can be summarised as linearity in each row.
        \item Anti-symmetry in rows.
        The determinant switches signs if one swaps a pair of rows (i.e. if one applies the ERO $P_{rs}$);
        that is,
        \[
        \det(A) = \det\begin{pmatrix}
            \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_n
        \end{pmatrix}
        = -\det\begin{pmatrix}
            \mbf{a}_1 \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_n
        \end{pmatrix}
        =
        -\det(P_{rs}A).
        \]
        \item $\det A = 0$ if $A$ has two rows that are equal
        (or even two rows that are collinear - follows simply by property (ii)).
        \item $\det A = 0$ if $A$ has a row of zeros.
        \item The determinant does not change under the ERO $A_{rs}(\lambda)$; that is,
        \[
        \det(A_{rs}(\lambda)A) = \det\begin{pmatrix}
            \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s + \lambda\mbf{a}_r \\ \vdots \\ \mbf{a}_n
        \end{pmatrix}
        =
        \det\begin{pmatrix}
            \mbf{a}_1 \\ \vdots \\ \mbf{a}_r \\ \vdots \\ \mbf{a}_s \\ \vdots \\ \mbf{a}_n
        \end{pmatrix}
        = \det A.
        \]
        \end{enumerate}
\end{proposition}

\begin{remark}
    Given any matrix $A \in M_n(\R)$,
    the operation $A_{r, s}$ doesn't change the value of the determinant,
    $M_{r}(\lambda)$ changes the determinant by $\lambda\det{A}$,
    finally $P_{rs}$ changes the determinant to $-\det{A}$.
\end{remark}

\begin{theorem}
    The $n \times n$ matrix $A$ has an inverse if and only if $\det(A) \neq 0$.
\end{theorem}

\begin{theorem}
    Let $A$ and $B$ be $n \times n$ matrices.
    Then
    \[
    \det(AB) = \det(A)\det(B).
    \]
\end{theorem}

\begin{theorem}
    We have
    \[
    \det(A ^ t) = \det(A).
    \]
\end{theorem}











\end{document}