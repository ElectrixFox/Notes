\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\title{Induction Notes}
\author{Luke Phillips}
\date{September 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}
Here I have made notes for all of the induction questions.

\section{Set Theory}

\subsection{Basic definitions}
A set is defined by the elements that belong to the set. If an element $x$ belongs to a set $S$, we write $x \in S$. \\
\\
Sets can be finite or countably infinite, this means they can be written as a finite or infinite list. For example, a finite set with finitely many integers, 
\[
S = \{e_1, e_2,\dotsc,e_n\}.
\]
An example of a countably infinite set, with elements labelled by all positive integers,
\[
S = \{e_1, e_2, e_3, \dotsc\}.
\]
Elements of a set are listed between the curly braces. The order of elements in a set is unimportant, for example
\[
\{0, 1, 2, 3\} = \{3, 1, 2, 0\}.
\]
When we work with sets, we ignore repeated elements,
\[
\{0, 1, 1, 2, 1, 3\} = \{0, 1, 2, 3\}.
\]
Sets can also be defined using the following notation,
\[
S = \{n : n \textnormal{ is an integer},\,0 \leq n \leq 9\}
\]
this means all $n$ such that the properties right of the ':'\footnote{Some books use '$\mid$' instead of ':'.} hold. \\
\\
\subsection{Standard sets}
$\Z = \{\dotsc, -2, -1, 0, 1, 2, \dotsc\} = \{0, \pm 1, \pm 2, \dotsc\}$, the set of integers (whole numbers: positive, negative, and zero). \\
$\N = \{1, 2, \dotsc\} = \{n: n \in \Z, n > 0\}$, the set of natural number (positive integers). \\
$\R$ is the set of real numbers. \\
$\emptyset = \{\}$ is the empty set (the set with no elements). \\
\\
Some useful sets are intervals in the real line, for all real numbers between $a$ and $b$, we write
\[
[a,\,b] = \{x \in \R : a \leq x \leq b\}.
\]
Square brackets are used when we want to include the endpoints: $a \in [a,\,b]$ and $b \in [a,\,b]$. When endpoints shouldn't be included, we use round brackets and write
\[
(a,\,b) = \{x \in \R : a < x < b\}.
\]
$[a,\,b]$ is a closed interval, and $(a,\,b)$ is an open interval. These can be mixed, for example $[a,\,b) = \{x \in \R : a \leq x < b\}$.

\subsection{More notation}
$x \in S$ means the element $x$ is an element of the set $S$. The negation is $x \notin S$. For example, $3 \in \Z$ but $-4 \notin \N$. \\
If $R$ and $S$ are sets, then $R \subseteq S$ means that $R$ is a subset of $S$, every element of $R$ is also an element of $S$. \\
For any set $S$, we have $\emptyset \subseteq S$ and $S \subseteq S$. We also have $\N \subseteq \Z \subseteq \R$. \\
If $S$ is a finite set, its size or cardinality, denoted by $|S|$, is the number of elements it contains. For example, $|\{1, 3, 5, 7, 9\}| = 5$ and $|\emptyset| = 0$.

\subsection{Set operations}
Given several sets, there are a few important ways of combining them to make further sets. \\
Given sets $A$ and $B$, $A \cup B$ is the union of $A$ and $B$ (the set consisting of elements in $A$, $B$, or $A$ and $B$). For example,
\[
\{-2, -1, 0\} \cup \{0, 1, 2\} = \{-2, -1, 0, 1, 2\}.
\]
$A \cap B$ is the intersection of sets $A$ and $B$ (the set consisting of elements in both $A$ and $B$). For example,
\[
\{-2, -1, 0\} \cap \{0, 1, 2\} = \{0\}.
\]
From the definitions,
\[
A \cup B = B \cup A
\]
and
\[
A \cap B = B \cap A.
\]
Additionally, $A \cup \emptyset = A$ and $A \cap \emptyset = \emptyset$. \\
The set difference between $A$ and $B$, sometimes called $A$ minus $B$, is the set of elements of $A$ not in $B$. Written as
\[
A \setminus B = \{x \in A : x \notin B\}.
\]
For example,
\[
\Z \setminus \N = \{0, -1, -2, \dotsc\}.
\]

\subsection{Many sets}
If we want to take the union or intersection of several sets, we can use the following notation,
\[
\bigcap_{i = 1}^{n}{A_i} = A_1 \cap A_2 \cap A_3 \cap \cdots \cap A_n = \{x : x \in A_i \textnormal{ for every } i\}.
\]
Similarly we can write
\[
\bigcup_{i = 1}^{n}{A_i} = A_1 \cup A_2 \cup A_3 \cup \cdots \cup A_n = \{x : x \in A_i \textnormal{ for at least one } i\}.
\]

\newpage

\section{Functions of a real variable}

\subsection{Functions of a real variable}
A real function is a map, $f$, that associates a real number to every element of a set $S$. $S$ is called the domain (or domain of definition) of the function, it can be any set: the real numbers, the unit interval $[0,\,1]$, and so on. \\
For each number $x$ in $S$, we write $f(x)$ for the number assigned to $x$ by $f$. \\
\\
For example, we can take $S$ to be the set of reals, and $f(x) = 2x ^ 2 + 3$ for each $x \in \R$. Another example, take $S = [0,\,4]$, and define $g$ as follows
\[
g(x) =
\begin{cases}
    x ^ 2 + 3 &\textnormal{if } 0 \leq x < 2 \\
    -x - 2 &\textnormal{if } 2 < x \leq 4 \\
    0 &\textnormal{if } x = 2.
\end{cases}
\]
This is a valid function as it defines a value for each $x \in [0,\,4]$. Note, cannot say what $g(5)$ is, as it is undefined, since $5$ isn't in the domain of $g$.  Drawing a graph of $g$ would look as follows,
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}
        [
        xmin = -2, xmax = 6,
        ymin = -8, ymax = 8,
        axis x line = center,
        axis y line = center,
        xtick = {-2, 2, 4, 6},
        ytick = {-6, -4, -2, 2, 4, 6}
        ]
        \addplot [domain=0:1.999, smooth, thick] { x ^ 2 + 3 };
        \addplot [domain=1.999:4, smooth, thick] { -x - 2 };
        \addplot [only marks, mark=*, mark options = {fill=black}] coordinates {(2, 0)};
        \end{axis}
    \end{tikzpicture}
    \caption{The function $g$.}
    \label{fig:func1}
\end{figure}
If the domain of a function isn't given, take it to be the largest possible subset of $\R$. \\
We call the set of values taken by the function its image. We say the image as the set $I$ defined by
\[
I = \{y : y = f(x) \textnormal{ for some } x \in S\}.
\]
The image of our first function ($f$) is the set of all $y \geq 3$, so we could write $I = \{y : y \geq 3\}$, or $I = [3,\,\infty)$. \\
The image of the second function is more complicated. The image of $g$ is
\[
I = [-6,\,-4) \cup [3,\,7) \cup \{0\}.
\]
We often want to describe the domain and the image of a function in its definition, as well as what it does. We often use notation such as:
\begin{align*}
f : S &\rightarrow I \\
x &\mapsto 2x ^ 2 + 3.
\end{align*}
This is read as, $f$ is defined as a function from $S$ to $I$ which assigns the value $2x ^ 2 + 3$ to each $x$. \\
This notation is helpful when defining complicated functions, e.g.
\begin{align*}
f : \R ^ 2 &\rightarrow \R \\
(x, y) &\mapsto x\sin{(y)}.
\end{align*}
Defines a function taking two reals as inputs $x$ and $y$, producing $f(x, y) = x\sin{(y)}$. \\
\\
A function $f$ is even if for all $x$, $f(x) = f(-x)$\footnote{This means that the graph has reflectional symmetry around $x = 0$.}. A function $f$ is odd if for all $x$, $f(x) = -f(-x)$\footnote{Its graph has rotational symmetry around the origin (and that we must have $f(0) = 0$).}.


\subsubsection{Inverses}
Given a function $f$ with domain $S$ and image $I$, an inverse function is a function $g$ with domain $I$ and image $S$ such that, for every $y \in I$,
\[
f(g(y)) = y.
\]
Usually, the inverse is denoted $f ^ {-1}$. \\
Often, an inverse function can be found by writing
\[
y = f(x)
\]
and then rearranging so that $x$ is the subject. \\
We can't always find the inverse of a function over a given domain. For example, $f(x) = x ^ 2$ doesn't have an inverse over whole real line, because there are two $x$-values for each $y$-value. If only the positive part of the real line is considered, an inverse can be found: $f ^ {-1}(x) = \sqrt{x}$.


\subsubsection{Graphs and transformations}
The graph of a function $f : \R \rightarrow \R$ can be shifted, reflected and rescaled. \\
\textbf{Translation:} \\
$f(x) + a$ shifts the graph of $f(x)$ up by an amount $a$. \\
$f(x + b)$ shifts the graph of $f(x)$ to the left by an amount $b$. \\
\textbf{Reflection:} \\
$-f(x)$ reflects the graph of $f(x)$ about the $x$-axis. \\
$f(-x)$ reflects the graph of $f(x)$ about the $y$-axis. \\
$|f(x)|$ reflects the portions of the graph of $f(x)$ that are below the $x$-axis about the $x$-axis. \\
\textbf{Scaling:} \\
$\lambda f(x)$ is a vertical stretch of the graph of $f(x)$ if $\lambda > 1$ and a vertical contraction if $0 < \lambda < 1$. \\
$f(\mu x)$ is a horizontal contraction of the graph of $f(x)$ if $\mu > 1$ and a horizontal stretch if $0 < \mu < 1$. \\
\\
Tips for graphing reciprocal $\frac{1}{f(x)}$ from the graph of $f(x)$:
\begin{itemize}
    \item If $f(x)$ is increasing (decreasing) then $\frac{1}{f(x)}$ is decreasing (increasing).
    \item The graphs of $f(x)$ and $\frac{1}{f(x)}$ intersect exactly when $f(x) = \pm 1$.
\end{itemize}
The graph of the inverse of a function, $f ^ {-1} (x)$, is found by reflecting the graph of $f(x)$ in the line $y = x$.

\subsection{Exponentials and logarithms}
An exponential function is any function of the form $f(x) = b ^ x$, where $b$ (called the base) is a positive real number not equal to $1$. \\
For all values of $b$, the graph passes through $(0, 1)$. There are two types of behaviour:
\begin{itemize}
    \item If $b > 1$, the graph is increasing and has a horizontal asymptote $y = 0$ along the negative $x$-axis.
    \item If $0 < b < 1$, the graph is decreasing and has a horizontal asymptote $y = 0$ along the positive $x$-axis.
\end{itemize}
Two important properties of exponential functions are
\[
b ^ x b ^ y = b ^ {x + y}\quad\textnormal{and}\quad\left(b ^ x\right) ^ y = b ^ {xy}.
\]
A special base is $e$, called the natural base.
\[
\dfrac{d}{dx}e ^ x = e ^ x\footnote{It's gradient is always $e ^ x$}.
\]
We can define $e$ as such,
\[
e = 1 + \dfrac{1}{1!} + \dfrac{1}{2!} + \dfrac{1}{3!} + \dfrac{1}{4!} + \dotsc = \sum_{i = 0}^{\infty}{\dfrac{1}{i!}}.
\]
$e$ is transcendental: it is not the root of any polynomial equation with rational coefficients.


\subsubsection{Logarithms}
A logarithm is the inverse of an exponential function: if $y = b ^ x$, then $x = \log_{b}{(y)}$. \\
When the base of the logarithm is $e$, we call the function the natural logarithm, usually denoted by $\ln{(x)}$. \\
The logarithm has a vertical asymptote at $x = 0$. \\
Some properties of logarithms are:
\begin{itemize}
    \item $\log_{b}{1} = 0$ (this follows from $b ^ 0 = 1$.)
    \item $\log_{b}{b ^ x} = x$ (from the definition as the inverse of an exponential)
    \item $\log_{b}{(\alpha \beta)} = \log_{b}{\alpha} + \log_{b}{\beta}$ (compare this to $b ^ \alpha b ^ \beta = b ^ {\alpha + \beta}$)
    \item $\log_{b}{(\frac{\alpha}{\beta})} = \log_{b}{\alpha} - \log_{b}{\beta}$ (compare to $\frac{b ^ \alpha}{b ^ \beta} = b ^ {\alpha - \beta}$)
    \item $\log_{b}{x ^ r} = r\log_{b}{x}$ (from $\left(a ^ x\right) ^ r = a ^ {xr}$)
\end{itemize}

\subsection{Trigonometry}
$\sin$ and $\cos$ have the domain $\R$ and the image $[-1,\,1]$. $\sin{(x)}$ is an odd function, $\cos{(x)}$ is an even function. $\sin{(x)} = \cos{(x - \frac{\pi}{2})}$, and $\cos{(x)} = \sin{(\frac{\pi}{2} - x)}$. Both $\sin{(x)}$ and $\cos{(x)}$ are periodic, with period $2\pi$. \\
\\
\textbf{Periodic functions} A function $f(x)$ is periodic if we can find $p > 0$ such that $f(x + p) = f(x)$ is true for every $x$. The smallest value of $p$ is called the period of $f$. \\
\\
$\tan{x} = \frac{\sin{x}}{\cos{x}}$ has period $\pi$. It has vertical asymptotes at $x = \left(\frac{1}{2} + n\right)\pi$, corresponding with the roots of $\cos$. Its domain is $\R \setminus \{x = \left(\frac{1}{2} + n\right)\pi\textnormal{ for } n \in \Z\}$. Its image is $\R$. \\
\\
\textbf{Reciprocals of trigonometric functions} \\
\[
    \mathrm{cosec}{(x)} = \dfrac{1}{\sin{(x)}} \qquad \qquad \sec{(x)} = \dfrac{1}{\cos{(x)}} \qquad \qquad \cot{(x)} = \dfrac{1}{\tan{(x)}}.
\]
The domain of $\mathrm{cosec}(x)$ is the set of $x$ for which $\sin{(x)}$ is not $0$, so it is
\[
\mathrm{dom}(\mathrm{cosec}) = \R \setminus \{x = 2\pi n \textnormal{ for an integer } n\}.
\]
Its image is $\R \setminus (-1, 1)$. \\
\\
\textbf{Inverses of trigonometric functions} \\
The inverse of $\sin$ is usually denoted $\sin^{-1}(x)$ or $\arcsin(x)$. It is defined as the value of $\theta \in (-\frac{\pi}{2}, \frac{\pi}{2}]$ such that $\sin(\theta) = x$. We similarly define $\arccos(x)$ and $\arctan(x)$. \\
\\
\textbf{Trigonometric identities} \\
Here are some important identities:
\[
\sin^2(x) + \cos^2(x) = 1
\]
\[
\tan^2(x) + 1 = \sec^2(x)
\]

\textbf{Addition Formulae} \\
\begin{align*}
    \sin(x \pm y) &= \sin(x)\cos(y) \pm \cos(x)\sin(y) \\
    \cos(x \pm y) &= \cos(x)\cos(y) \mp \sin(x)\sin(y) \\
    \tan(x \pm y) &= \dfrac{\tan(x) \pm \tan(y)}{1 \mp \tan(x)\tan(y)}.
\end{align*}
From these we can obtain the double-angle formulae
\begin{align*}
    \sin(2x) &= 2\sin(x)\cos(x) \\
    \cos(2x) &= \cos^2(x) - \sin^2(x) \\
    \tan(2x) &= \dfrac{2\tan(x)}{1 - \tan^2(x)}.
\end{align*}
The half-angle formulae:
\begin{align*}
    \sin^2(x) &= \frac{1}{2} - \frac{1}{2}\cos(2x) \\
    \cos^2(x) &= \frac{1}{2} + \frac{1}{2}\cos(2x).
\end{align*}
The sum of two trigonometric functions can be written as follows \\
\[
a\cos(x) + b\sin(x) = R\sin\left(x + \arctan\left(\frac{a}{b}\right)\right) = R\cos\left(x - \arctan\left(\frac{a}{b}\right)\right),
\]
where $R = \sqrt{a ^ 2 + b ^ 2}$.

\newpage

\section{Algebra}

\subsection{Polynomials and rational functions}

\subsubsection{Polynomials}
A polynomial is a function of the form
\[
p(x) = a_n x ^ n + \dotsc + a_1 x + a_0.
\]
When $a_n \neq 0$, we say that $n$ is the degree of $p(x)$. A polynomial of degree $n$ has at most $n$ distinct roots, values $\alpha$ such that $p(\alpha) = 0$. If $\alpha$ is a root of $p(x)$, then $(x - \alpha)$ is a factor of $p(x)$. If there are $n$ distinct roots, then $p(x)$ can be completely factorised into linear (degree $1$) terms of the form $x - \alpha$ to give
\[
p(x) = a_n (x - \alpha_1)(x - \alpha_2)\dotsc(x - a_n). 
\]
When the coefficients $a_i$ are real, the roots of $p(x)$ are either real or occur in complex conjugate pairs. This means $p(x)$ will have linear factors of the form $x - \alpha$ as well as quadratic factors of the form $x ^ 2 + \beta x + \gamma$ which will have no real roots. \\
With quadratics of the form $p(x) = ax ^ 2 + bx + c$, it is helpful to complete the square, by writing them in the following form,
\[
p(x) = \pm (Ax - B) ^ 2 + C.
\]
Completing the square yields a formula for the roots of the quadratic. \\
Assume that $a > 0$. Comparing coefficients between
\[
p(x) = ax ^ 2 + bx + c
\]
and
\[
p(x) = (Ax - B) ^ 2 + C = A ^ 2 x ^ 2 - 2ABx + B ^ 2 + C,
\]
we see that $a = A ^ 2$, $b = -2AB$, and $c = B ^ 2 + C$. We can rearrange to find $A, B$ and $C$: $A = \sqrt{a}, B = \frac{-b}{2\sqrt{a}}$, and $C = c - \frac{b ^ 2}{4a}$. As long as $C < 0$, we can write our quadratic as
\[
p(x) = \left(\sqrt{a}x + \dfrac{b}{2\sqrt{a}}\right) ^ 2 + c - \dfrac{b ^ 2}{4a} = a(x - \alpha_1)(x - \alpha_2),
\]
with roots given by
\[
\alpha_1 = \dfrac{-b - \sqrt{b ^ 2 - 4ac}}{2a},\qquad\alpha_2 = \dfrac{-b + \sqrt{b ^ 2 - 4ac}}{2a}.
\]

\subsubsection{Rational functions}
A rational function is a quotient of two polynomials $f(x) = \frac{p(x)}{q(x)}$. \\
\\
\textbf{Partial fractions} \\
Partial fractions means simplifying a quotient function and writing it as a sum of fractions with lower-degree polynomials in their denominators.
\[
\dfrac{5}{(x + 5)(x - 5)} = \dfrac{1}{2}\dfrac{1}{x - 5} - \dfrac{1}{2}\dfrac{1}{x + 5}.
\]
To do this we do as follows,
\begin{align*}
\dfrac{5}{(x + 5)(x - 5)} &= \dfrac{A}{x - 5} + \dfrac{B}{x + 5} \\
&= \dfrac{A(x - 5) + B(x + 5)}{(x + 5)(x - 5)}.
\end{align*}
Since we need
\[
5 = (A + B)x + (5A - 5B)
\]
to be true for all $x$, so we solve
\begin{align*}
    A + B &= 0 \\
    5A - 5B &= 5
\end{align*}
to find $A$ and $B$. \\
\\
When there is a repeated factor in the denominator, a term is needed for each exponent. For example,
\[
\dfrac{2x + 3}{(x - 1) ^ 3 (x + 1)},
\]
we would need to write
\[
\dfrac{2x + 3}{(x - 1) ^ 3 (x + 1)} = \dfrac{A}{x - 1} + \dfrac{B}{(x - 1) ^ 2} + \dfrac{C}{(x - 1) ^ 3} + \dfrac{D}{x +1}
\]
and then find $A, B, C$, and $D$. \\
When there is an irreducible quadratic factor in the denominator, we coefficients are needed.
\[
\dfrac{x + 5}{(x + 2)(x ^ 2 + 3)} = \dfrac{A}{x + 2} + \dfrac{Bx + C}{x ^ 2 + 3}.
\]
\textbf{Proper fractions} \\
When the degree of the numerator is greater than (or equal to) that of the denominator, rational functions can be rewritten as a sum of a polynomial and a proper fraction. For example:
\[
\dfrac{x ^ 3 + 2x - 6}{x ^ 2 - x - 2} = x + 1 + \dfrac{2}{x - 2} + \dfrac{3}{x + 1}.
\]


\subsection{Series and summations}

\subsubsection{Series notation}
When writing the sum of a sequence of terms in a predictable way we would use series notation. For example, for the sum of integers starting at $1$ and ending at $100$, we would write
\[
M = \sum_{j = 1}^{100}{j}.
\]
We use the $\Sigma$ symbol to tell us that we are adding. $j = 1$ indicates what changes per term (in this case $j$) and what the first value of $j$ is. $100$ indicates where the addition stops. \\
\textbf{Example}: \\
The formula for the sum of the first $n$ integers, we'll call $M(n)$. Instead of adding $1$, then $2$, then $3$, we add $n$, then $n - 1$, then $n - 2$. This means that
\[
M(n) = \sum_{j = 1}^{n}{j} = \sum_{j = 1}^{n}{(n + 1 - j)}.
\]
If we add both 'versions' of $M(n)$ we get
\[
2M(n) = \sum_{j = 1}^{n}
\]
The right hand side shows adding $n$ copies of $n + 1$ (usually called multiplication) which leads us to
\[
M(n) = \frac{1}{2}n(n + 1).
\]


\subsubsection{Some properties of series}
If all the terms in the series are the same
\[
\sum_{j = 1}^{n}{c} = c \times n
\]

If all of the terms are multiplied by a common factor, the sum can be multiplied by this too.
\[
\sum_{j = 1}^{n}{a \times f(j)} = a \times \sum_{j = 1}^{n}{f(j)}.
\]
Two series can be added together (providing the number of terms is the same)
\[
\sum_{j = 1}^{n}{f(j)} + \sum_{k = 1}^{n}{g(k)} = \sum_{j = 1}^{n}{f(j) + g(j)}.
\]


\subsubsection{Useful series to know}
\textbf{Sums of powers}
\begin{align*}
    \sum_{j = 1}^{n}{j} &= \frac{n}{2}(n + 1) \\
    \sum_{j = 1}^{n}{j ^ 2} &= \frac{n}{6}(n + 1)(2n + 1) \\
    \sum_{j = 1}^{n}{j ^ 3} &= \left(\frac{n}{2}(n + 1)\right) ^ 2.
\end{align*}

\textbf{Geometric series}
\[
S_n = a + ar + ar ^ 2 + \dotsc + ar ^ {n - 1},
\]
then multiplying both sides by $r$ we can see that
\begin{align*}
    S_n &= a + ar + ar ^ 2 + \dotsc + ar ^ {n - 1} \\
    rS_n &= \phantom{ } + ar + ar ^ 2 + \dotsc + ar ^ {n - 1} + ar ^ n.
\end{align*}
Subtracting the second line from the first
\[
(1 - r)S_n = a - ar ^ n,
\]
so (as long as $r \neq 1$) the sum of the first $n$ terms of a geometric series is
\[
S_n = \dfrac{a(1 - r ^ n)}{1 - r}.
\]
If $|r| < 1$, then the series approaches a finite limit. The value of the limit is
\[
S = \dfrac{a}{1 - r}.
\]

\textbf{Binomial expansions}
\[
(a + b) ^ n = \sum_{k = 0}^{n}{\binom{n}{k}a ^ k b ^ {n - k}}.
\]
Here $\binom{n}{k}$ represents the choose function, which counts how many different ways there are of choosing $k$ distinct options from a list of $n$ items. When $n \geq k$ we define
\[
\binom{n}{k} = \dfrac{n!}{k!(n - k)!}.
\]
Some useful properties of the choose function include:
\begin{itemize}
    \item $\binom{n}{0}$ is always $1$: there is only one way to choose nothing.
    \item $\binom{n}{1}$ is always $n$: there are $n$ different ways to choose, for each of the $n$ options.
    \item $\binom{n}{k} = \binom{n}{n - k}$ counting the ways of selecting $k$ things is equivalent to counting the different ways of leaving $n - k$ things behind.
\end{itemize}

\subsection{Power series}
When a series is of the form
\[
P(x) = \sum_{n = 0}^{\infty}b_n x ^ n,
\]
we say that it is a power series. Examples of power series include the geometric series, when $b_n$ is always equal to $a$, and a Maclaurin series, which allow us to represent a function as a sum of polynomial terms. \\
\\
\textbf{Maclaurin series} \\
The Maclaurin series expansion of a function $f(x)$ is a way of expressing it as a power series. Suppose we can write
\[
f(x) = b_0 + b_1 x + b_2 x ^ 2 + \dots = \sum_{n = 0}^{\infty}b_n x ^ n.
\]
This relationship has to be true for every $x$, so when $x = 0$ we get $f(0) = b_0$. \\
As long as we can differentiate both sides, we get
\[
f'(x) = b_1 + 2b_2 x + 3b_3 x ^ 2 + \dots = \sum_{n = 0}^{\infty}(n + 1)b_{n + 1} x ^ n.
\]
This also has to be true for every $x$; setting $x = 0$ again, we get $f'(0) = b_1$.

We can keep repeating this trick of differentiating and setting $x = 0$; if we do it one more time, we get
\[
f''(x) = 2b_2 + 2 \times 3b_3 x + 3 \times 4b_4 x ^ 2 + \dots = \sum_{n = 0}^{\infty}(n + 1)(n + 2)b_{n + 2} x ^ n.
\]
We can see that $f''(0) = 2 \times b_2$, and if we continue the pattern, we always get $f^{(k)}(0) = k! \times b_k$.

Now we know each of our $b$s we have found our Maclaurin series:
\[
f(x) = f(0) + f'(0)x + \frac{f''(0)}{2}x ^ 2 + \dots + \frac{f^{(k)}(0)}{k!}x ^ k + \dotsc.
\]
To make this work, we need to be able to differentiate $f$ as many times as we wanted to: it had to be continuous. We say that the Maclaurin series converges whenever the value of the function is equal to the value of the series. For some functions, this is for all $x$, but for others, it only works in a certain interval. If it converges whenever $|x| < R$, we say that $R$ is the radius of convergence of the series.
\\

\textbf{Examples of Maclaurin series}

\begin{itemize}
    \item $e ^ x = 1 + x + \frac{x ^ 2}{2!} + \frac{x ^ 3}{3!} + \dots + \frac{x ^ k}{k!} + \dotsc$
    \item $\cos(x) = 1 - \frac{x ^ 2}{2!} + \frac{x ^ 4}{4!} - \frac{x ^ 6}{6!} + \dotsc$
    \item $\sin(x) = x - \frac{x ^ 3}{3!} + \frac{x ^ 5}{5!} - \frac{x ^ 7}{7!} + \dotsc$
\end{itemize}

\textbf{Building new Maclaurin series}

We can build new series with the ones we already know. For example,
\[
\sin(2x) = 2x - \frac{(2x) ^ 3}{3!} + \frac{(2x) ^ 5}{5!} + \dotsc
\]

We can add and multiply Maclaurin series. For example,
\begin{align*}
    \sin(x) + \cos(x) &= \left(1 - \frac{x ^ 2}{2!} + \frac{x ^ 4}{4!} - \frac{x ^ 6}{6!} + \dotsc\right) + \left(x - \frac{x ^ 3}{3!} + \frac{x ^ 5}{5!} - \frac{x ^ 7}{7!} + \dotsc\right) \\
    &= 1 + x - \frac{x ^ 2}{2!} - \frac{x ^ 3}{3!} + \frac{x ^ 4}{4!} + \frac{x ^ 5}{5!} - \dotsc.
\end{align*}

Similarly, we can multiply the series for products of series. For example,
\begin{align*}
    e ^ x \cos(x) &= \left(1 + x + \frac{x ^ 2}{2!} + \frac{x ^ 3}{3!} + \dotsc\right) \left(1 - \frac{x ^ 2}{2!} + \frac{x ^ 4}{4!} - \frac{x ^ 6}{6!} + \dotsc\right) \\
    &= 1 + x - \frac{2x ^ 3}{3} - \frac{7x ^ 4}{4!} + \dotsc.
\end{align*}

\textbf{Taylor series} \\
A Taylor series around a point $x = a$ looks like:
\[
f(x) = f(a) + (x - a)f'(a) + \frac{(x - a) ^ 2}{2}f''(a) + \dots + \frac{(x - a) ^ k}{k!}f^{(k)}(a) + \dotsc.
\]

\subsection{Proof techniques}
\subsubsection{Direct proof}
In direct proof, we start from the facts we are given and combine them into logically-equivalent statements until we reach the conclusion we want.

\textbf{Example: sums of even numbers} \\
To prove that the sum of any two even numbers is an even number, using the definition of an even number. $x$ is even if and only if $x = 2a$ where $a$ is an integer. If we have two even numbers $x$ and $y$ we must be able to find integers $a$ and $b$ such that $x = 2a$ and $y = 2b$. Then we know their sum is $x + y = 2a + 2b = 2(a + b)$. Since $a$ and $b$ were integers, $a + b$ must be an integer too. So $x + y$ is two times an integer which is also an even number.

\subsubsection{Proof by induction}
Suppose we want to prove a statement for all integers $n$. To use proof by induction prove that the statement is true for an early case, usually $n = 0$ or $n = 1$. Next, prove that "the statement is true when $n = N$" implies that "the statement is true for $n = N + 1$". By combining these two facts, we know that the statement must be true for every possible value of $n$.

\textbf{Example: sum of squares} \\
\[
\sum_{j = 1}^{n}{j ^ 2} = \frac{n}{6}(n + 1)(2n + 1)
\]
Starting with the base case, $n = 1$. On the left $1 ^ 2 = 1$ which holds. On the right, $\frac{1}{6} \times (1 + 1) \times (2 \times 1 + 1) = 1$. So the formula holds when $n = 1$.

Next, assume the sum of the first $N$ squares is
\[
\sum_{j = 1}^{N}{j ^ 2} = \frac{N}{6}(N + 1)(2N + 1).
\]
Now using the assumption we can write
\[
\sum_{j = 1}^{N + 1}{j ^ 2} = \sum_{j = 1}^{N}{j ^ 2} + (N + 1) ^ 2.
\]

\begin{align*}
    \sum_{j = 1}^{N + 1}{j ^ 2} &= \sum_{j = 1}^{N}{j ^ 2} + (N + 1) ^ 2 \\
    &= \frac{N}{6}(N + 1)(2N + 1) + (N + 1) ^ 2 \\
    &= \frac{N + 1}{6}(N + 1)\left(N(2N + 1) + 6(N + 1)\right) \\
    &= \frac{N + 1}{6}(N + 1)(N + 2)(2N + 3).
\end{align*}
Finally, it is true for $n = 1$, then by the inductive step it is true for $n = N$ then we have shown that it is true for $n = N + 1$ for all of the integers by induction.

\subsubsection{Proof by contradiction}
To prove that a statement is true by contradiction, we start off by assuming that it is false, and then look for a series of logical steps that ends with something impossible\footnote{This is also called "reductio ad impossibile"}.

\textbf{Example: infinitely many primes} \\
Assume that there are only finitely many primes.

Now, if there are a finite number of primes, we can make a complete list out of them:
\[
p_1, p_2, \dotsc, p_n.
\]
Next, we will construct a number which causes a problem. If we take a multiple of $k$ and then add on $1$, we no longer have a multiple of $k$ (as long as $k \neq 1$). So if we multiply all the primes in our list and add on $1$ we will get a number which is either a prime itself or a composite number whose prime factor(s) are missing from our list. Either way, there is a prime missing from our list. This contradicts our assumption that we could make a finite list of every prime number. Hence there must be infinitely many prime numbers.

\newpage

\section{Differentiation and Integration}

\subsection{Differentiation}
Given a suitable function $f(x)$, the derivative of $f(x)$ is another function denoted by $f'(x)$ or $\dfrac{df}{dx}$. It is the rate of change of $f(x)$ with respect to $x$, and $f'(a)$ is the gradient of the tangent to the graph of $y = f(x)$ at $x = a$. This allows us, to locate stationary points such as maximum and minima since the tangent will be horizontal here and $f'(x) = 0$.

Here are some derivatives

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        $f(x)$ & $f'(x)$ \\
        \hline
        $x ^ n$ & $nx ^ {n - 1}$ for constant $n$ \\
        \hline
        $e ^ x$ & $e ^ x$ \\
        \hline
        $\ln{x}$ & $x ^ {-1}$ \\
        \hline
        $\sin{x}$ & $\cos{x}$ \\
        \hline
        $\cos{x}$ & $-\sin{x}$ \\
        \hline
    \end{tabular}
\end{table}

We then differentiate more general functions by applying various rules:

\textbf{Sum Rule}: $(f(x) + g(x))' = f'(x) + g'(x)$.

\textbf{Product Rule}: $(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)$.

\textbf{Chain Rule}: $f(g(x))' = f'(g(x))g'(x)$. \\

The chain rule can also be remembered as 
\[
\frac{d}{dx}f(g(x)) = \frac{df}{dy}\frac{dy}{dx}.
\]
The derivative of a reciprocal
\[
\frac{d}{dx}\left(\frac{1}{g(x)}\right) = -\frac{g'(x)}{g(x) ^ 2}.
\]
Applying the product rule,
\[
\left(\frac{f(x)}{g(x)}\right)' = \left(f(x) \cdot \frac{1}{g(x)}\right)' = f'(x) \cdot \frac{1}{g(x)} - f(x) \cdot \frac{g'(x)}{g(x) ^ 2} = \frac{f'(x)g(x) - f(x)g'(x)}{g(x) ^ 2}.
\]

\subsection{Integration}
Given a suitable function $f(x)$ and real numbers $a \leq b$, the definite integral $\displaystyle\int_{a}^{b}f(x)\,dx$ is the area under the graph of $y = f(x)$ between $x = a$ and $x = b$. \\
\[
F(x) = \int_{a}^{x}f(t)\,dt\qquad\implies\qquad F'(x) = f(x).
\]
A function $F(x)$ satisfying $F'(x) = f(x)$ is called an indefinite integral of $f(x)$ and is denoted by $\displaystyle\int f(x)\,dx$. since constants have zero derivative, if $F(x)$ is an indefinite integral of $f(x)$, then so is $F(x) + c$ for any constant $c$ so indefinite integrals are only defined up to adding this constant\footnote{Don't forget to add the constant for indefinite integrals.}.

The indefinite integral is denoted as such,

Let $\displaystyle F(x) = \int f(x)\,dx$
\[
\int_{a}^{b}f(x)\,dx = F(b) - F(a).
\]
Here are some standard indefinite integrals

\begin{minipage}{0.55\textwidth}
\centering
\bgroup
\def\setlength{25pt}
\def\arraystretch{2}
\begin{tabular}{|c|c|}
    \hline
    $f(x)$ & $\displaystyle\int f(x)\,dx$ \\
    \hline
    $x ^ n$ & $\dfrac{x ^ {n + 1}}{n + 1} + c$ for constant $n \neq 1$ \\
    \hline
    $\dfrac{1}{x}$ & $\ln{x}$ \\
    \hline
    $e ^ x$ & $e ^ x + c$ \\
    \hline
    $\sin{x}$ & $-\cos{x} + c$ \\
    \hline
    $\cos{x}$ & $\sin{x} + c$ \\
    \hline
\end{tabular}
\egroup
\end{minipage}
\begin{minipage}{0.55\textwidth}
\centering
\bgroup
\def\setlength{25pt}
\def\arraystretch{2}
\begin{tabular}{|c|c|}
    \hline
    $f(x)$ & $\displaystyle\int f(x)\,dx$ \\
    \hline
    $\sec ^ 2 x$ & $\tan x + c$ \\
    \hline
    $\csc ^ 2 x$ & $-\cot x + c$ \\
    \hline
    $\dfrac{1}{1 + x ^ 2}$ & $\tan ^ {-1} x + c$ \\
    \hline
    $\dfrac{1}{\sqrt{1 - x ^ 2}}$ & $\sin ^ {-1} x + c$ \\
    \hline
    $\dfrac{1}{\sqrt{1 + x ^ 2}}$ & $\sinh ^ {-1} x + c$ \\
    \hline
\end{tabular}
\egroup
\end{minipage}

\textbf{Integration by substitution}: This arises by integrating the chain rule:
\[
F(x) = \int f(x)\,dx\qquad\implies\qquad F(u(x)) = \int f(u(x))u'(x)\,dx.
\]
In practice, we are substituting from $x$ to $u(x)$ and obtaining
\[
\int f(x)\,dx = \int f(u)\,du\qquad\text{ where }\qquad du = u'(x)\,dx.
\]
For example, to calculate $\int \cot x \, dx$, make the substitution $u = \sin x,\ du = \cos x\ dx$ and rewrite the integral in terms of $u$:

\begin{align*}
\int\cot x\ dx = \int \frac{1}{\sin x}\cos x\ dx &= \int \frac{1}{u}\ du \\
&= \ln |u| + c = \ln |\sin x| + c.
\end{align*}
When we are integrating a fraction where the numerator is the derivative of the denominator $\frac{g'(x)}{g(x)}$ for some function $g(x)$, then substitute $u = g(x)$. We have $du - g'(x)\ dx$ and so
\[
\int \frac{g'(x)}{g(x)}\ dx = \int \frac{1}{u}\ du = \ln |u| + c = \ln |g(x)| + c.
\]

\textbf{Integration by parts}: This arises by integrating the product rule and rearranging
\[
\int f(x)g'(x)\ dx = f(x)g(x) - \int f'(x)g(x)\ dx.
\]
This expresses the integral of one product $f(x)g'(x)$ in terms of the integral of another product $f'(x)g(x)$ which we hope is easier to find.

For example, with $f(x) = x$ and $g'(x) = \cos x$, we take $f'(x) = 1$ and $g(x) = \sin x$ so
\[
\int x\cos x\ dx = x\sin x - \int \sin x\ dx = x\sin x + \cos x + c.
\]

\textbf{Integration of rational functions}: To integrate a rational function, we can use partial fractions. Factorise the denominator into linear terms $x - \alpha$ and quadratic terms which have no real roots $x ^ 2 \beta x + \gamma$. Then the partial fractions will take the forms
\[
\frac{A}{(x - \alpha) ^ k}\qquad\text{and}\qquad\frac{Bx + C}{(x ^ 2 + \beta x + \gamma) ^ l}
\]
Then the partial fractions can be integrated using the usual techniques such as substitution. For example,
\begin{align*}
\int \frac{x ^ 3 + 2x - 6}{x ^ 2 - x - 2}\ dx &= \int x + 1 + \frac{2}{x - 2} + \frac{3}{x + 1}\ dx \\
&= \frac{x ^ 2}{2} + x + 2\ln |x - 2| + 3\ln |x + 1| + c.
\end{align*}

\newpage

\section{Hyperbolic functions}
The three fundamental hyperbolic functions are
\[
\sinh x = \frac{e ^ x - e ^ {-x}}{2},\quad\cosh x = \frac{e ^ x + e ^ {-x}}{2},\quad\tanh x = \frac{\sinh x}{\cosh x}.
\]

$\cosh x$ is an even function. On the other hand, $\sinh x$ and $\tanh x$ is an odd function. \\
We can see that 
\begin{align*}
    \cosh ^ 2 \theta - \sinh ^ 2 \theta &= \frac{(e ^ \theta + e ^ {-\theta}) ^ 2}{4} - \frac{(e ^ \theta - e ^ {-\theta}) ^ 2}{4} \\
    &= \frac{(e ^ {2\theta} + 2 + e ^ {-2\theta}) - (e ^ {2\theta} - 2 + e ^ {-2\theta})}{4} \\
    &= \frac{4}{4} \\
    &= 1.
\end{align*}

\subsection{The relationship between cosh and cos (and sinh and sin)}
Euler's formula states $e ^ {i\theta} = \cos \theta + i\sin \theta$. By considering $e ^ {-i\theta} = \cos \theta - i\sin \theta$ we can define the trigonometric functions $\sin$ and $\cos$ using exponentials and complex numbers:
\[
\sin \theta = \frac{e ^ {i\theta} - e ^ {-i\theta}}{2i},\qquad\cos \theta = \frac{e ^ {i\theta} + e ^ {-i\theta}}{2}.
\]
Comparing with the definitions, we can see that:
\[
\sin (\theta) = -i\sinh (i\theta),\qquad\cos (\theta) = \cosh (i\theta).
\]

\subsubsection{Reciprocals}
We also define
\[
\csch (\theta) = \frac{1}{\sinh (\theta)},\qquad\sech (\theta) = \frac{1}{\cosh (\theta)},\qquad\text{and}\qquad\coth (\theta) = \frac{1}{\tanh (\theta)}.
\]
We can also combine our definitions to derive new ones.
\[
1 - \tanh ^ 2 (\theta) = \sech ^ 2 (\theta).
\]

\textbf{Osborn's rule} helps us to remember how hyperbolic identities correspond to standard trigonometric identities: it tells us that we can take a standard trigonometric identity, swap $\sin$ for $\sinh$ and $\cos$ for $\cosh$, and then change the sign anywhere there is a product of two $\sinh$ terms. This includes "implied" products of $\sinh$ terms, such as $\tanh ^ 2$.

\subsection{Addition Formulae}
This formulae for $\sinh (\theta \pm \phi)$ and $\cosh (\theta \pm \phi)$ are similar to those for $\sin (\theta \pm \phi)$ and $\cos (\theta \pm \phi)$. We have
\begin{align*}
    \sinh (\theta \pm \phi) &= \sinh \theta \cosh \phi \pm \cosh \theta \sinh \phi \\
    \cosh (\theta \pm \phi) &= \cosh \theta \cosh \phi \pm \sinh \theta \sinh \phi.
\end{align*}

\begin{align*}
    \sinh \theta \cosh \phi + \cosh \theta \sinh \phi &= \left(\frac{e ^ \theta - e ^ {-\theta}}{2}\right)\left(\frac{e ^ \phi + e ^ {-\phi}}{2}\right) + \left(\frac{e ^ \theta + e ^ {-\theta}}{2}\right)\left(\frac{e ^ \phi - e ^ {-\phi}}{2}\right) \\
    &= \frac{e ^ {\theta + \phi} + e ^ {\theta - \phi} - e ^ {-\theta + \phi} - e ^ {-\theta -\phi}}{4} + \frac{e ^ {\theta + \phi} - e ^ {\theta - \phi} + e ^ {-\theta - \phi} - e ^ {-\theta -\phi}}{4} \\
    &= \frac{e ^ {\theta + \phi} - e ^ {-(\theta + \phi)}}{2} \\
    &= \sinh (\theta + \phi)
\end{align*}

Similarly, for the case where $\theta = \phi$.
\[
\sinh (2\theta) = 2\sinh\theta\cosh\theta\qquad\text{and}\qquad\cosh (2\theta) = \cosh ^ 2 \theta + \sinh ^ 2 \theta. 
\]

\subsection{Differentiation and Integration}
Here we can use the definitions of $\sinh$ and $\cosh$ to make hyperbolic calculus easy.
\[
\frac{d}{dx}\sinh x = \frac{d}{dx}\frac{e ^ x - e ^ {-x}}{2} = \frac{e ^ x + e ^ {-x}}{2} = \cosh x.
\]
Similarly,
\[
\frac{d}{dx}\cosh x = \frac{d}{dx}\frac{e ^ x + e ^ {-x}}{2} = \frac{e ^ x  e ^ {-x}}{2} = \sinh x.
\]
The integration of these functions is just the reverse.

\newpage

\section{Complex numbers}
A complex number is written as follows $z = x + yi$, where $x, y$ are real numbers and $i$ is the imaginary unit (which means $i = \sqrt{-1}$). $x$ is the real part of $z$ notated as $\operatorname{Re}(z) = x$. Similarly, $y$ is the imaginary part of $z$ notated as $\operatorname{Im}(z) = y$.

\textbf{Modulus} \\
The modulus of a complex number $z$ written as $|z|$ defines the distance of a complex number from the origin.
\[
|z| = \sqrt{x ^ 2 + y ^ 2}.
\]

\textbf{Argument} \\
The argument of a complex number $z$ written as $\arg (z)$ defines the anticlockwise angle from the real axis. 
\[
\arg (z) = \tan ^ {-1} \left(\frac{y}{x}\right).
\]
We usually choose to represent $\arg (z)$ using the principal argument which is the value of $\arg (z)$ in the interval $(-\pi, \pi]$. So the principle argument of $i$ is $\frac{\pi}{2}$.

\subsection{Polar form and De Moivre}
The location of a complex number could be represented in polar form where $r = |z|$ and $\theta = \arg (z)$ which allows us to write the coordinate as $[r, \theta]$.

Via basic trigonometry we can see that
\[
z = x + iy = r\cos\theta + ir\sin\theta = r(\cos\theta + i\sin\theta).
\]
Often this is written as $z = re ^ {i\theta}$ where $e ^ {i\theta}$ is defined by Euler's formula
\[
e ^ {i\theta} = \cos\theta + i\sin\theta.
\]

\textbf{De Moivre's identity} \\
For any integer $n$ we have
\[
\left(e ^ {i\theta}\right) ^ n = (\cos\theta + i\sin\theta) ^ n = \cos (n\theta) + i\sin (n\theta) = e ^ {in\theta}.
\]
This is usually proving by induction.

\newpage

\section{Vectors and Matrices}

\subsection{Vectors}
A vector is "something" that has direction and magnitude.
\[
\mathbf{v} = 
\begin{bmatrix}
   a \\
   b
\end{bmatrix}
\]

The length (or magnitude) of a vector $\mathbf{v}$ is given by $|\mathbf{v}|$ which is written as
\[
|\mathbf{v}| = \sqrt{a ^ 2 + b ^ 2}.
\]
There is a special vector ${\mathbf{0}}=\begin{bmatrix} 0 \\ 0 \end{bmatrix}$ which corresponds to the origin.

Similar can be done in $3$-dimensional space,
\[
\mathbf{v} = 
\begin{bmatrix}
a \\
b \\
c
\end{bmatrix}
\qquad\text{and}\qquad|\mathbf{v}| = \sqrt{a ^ 2 + b ^ 2 + c ^ 2}.
\]
This can be scaled up to $n$-dimensional vectors for any $n$. \\

Vector addition is done by adding vectors by their respective components. For example, $2$-dimensional vectors are added as follows,
\[
\begin{bmatrix} v_1 \\ v_2 \end{bmatrix} + \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} = \begin{bmatrix} v_1 + w_1 \\ v_2 + w_2 \end{bmatrix}.
\]
Vector multiplication is done by simply multiplying each component of a given vector $\mathbf{v}$ by a scalar $\lambda \in \R$ to form a new vector $\lambda\mathbf{v}$. For example, a $2$-dimensional vector multiplied by $\lambda$ results in
\[
\lambda\begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} \lambda v_1 \\ \lambda v_2 \end{bmatrix}.
\]

\subsection{Matrices}
A matrix is a rectangular array of entries arranged in rows and columns. A matrix has $m$ rows and $n$ columns, matrices are defined by their dimensions, $m\times n$ matrices. For example,
\[
\begin{bmatrix}
    1 & 2 \\ 
    3 & 4
\end{bmatrix}
\]
is a $2\times 2$ matrix.

Matrix addition is very similar to vector addition, it is done by adding corresponding entries. For example,
\[
\begin{bmatrix}
    1 & 2 \\ 
    3 & 4
\end{bmatrix} + 
\begin{bmatrix}
    5 & 6 \\ 
    7 & 8
\end{bmatrix} = 
\begin{bmatrix}
    6 & 8 \\ 
    10 & 12
\end{bmatrix}.
\]
However, the matrices need to be of the same size.

Similar to vectors, matrix multiplication by a scalar $\lambda \in \R$ can be performed by multiplying each entry by the scalar $\lambda$. For example,
\[
\lambda\begin{bmatrix}
    1 & 2 \\ 
    3 & 4
\end{bmatrix} = \begin{bmatrix}
    \lambda & 2\lambda \\ 
    3\lambda & 4\lambda
\end{bmatrix}
\]

\subsubsection{Matrix multiplication}
Two matrices $A$ and $B$ can be multiplied together to produce a matrix $AB$ providing that if $A$ is an $m\times n$ matrix and $B$ is an $n\times r$. In other words, $A$ must have the same number of columns as $B$ has rows. Here is an example of matrix multiplication,
\[
\begin{bmatrix}
    10 & 20 \\ 
    30 & 40
\end{bmatrix}
\begin{bmatrix}
    1 & 2 & 3 \\ 
    4 & 5 & 6
\end{bmatrix}
=
\begin{bmatrix}
    10 \times 1 + 20 \times 4 & 10 \times 2 + 20 \times 5 & 10 \times 3 + 20 \times 6 \\ 
    30 \times 1 + 40 \times 4 & 30 \times 2 + 40 \times 5 & 30 \times 3 + 40 \times 6
\end{bmatrix}
=
\begin{bmatrix}
    90 & 120 & 150 \\ 
    190 & 260 & 330
\end{bmatrix}.
\]
Matrix multiplication isn't commutative $AB \neq BA$ for some $A$, $B$. There is a special matrix $I$ (the identity matrix) for which for all square matrices $A$ when multiplied by $I$ have this property $AI = IA = A$. This leads to the idea of an inverse matrix which is a matrix $B$ for which when multiplied by the square matrix $A$, $AB = BA = I$. \\

Given a square matrix $A$, there exists the determinant of this matrix $\det (A)$. The determinant for a $2\times 2$ matrix $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$, is defined by the formula
\[
\det (A) = \det \begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc.
\]
The determinant tells us when the inverse of a square matrix exists. $A$ is invertible when $\det (A) \neq 0$.

The formula for the inverse of $A$ (a $2\times 2$ matrix) uses the determinant
\[
A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}\qquad\implies\qquad A ^ {-1} = \frac{1}{ad - bc}\begin{bmatrix} d & -b \\ -c & a \end{bmatrix}.
\]

\end{document}
