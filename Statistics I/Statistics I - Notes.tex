\documentclass[10pt, a4paper]{article}
\usepackage{preamble}


\title{Statistics I}
\author{Luke Phillips}
\date{January 2025}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{}

\subsection{High Profile Applications}
\textbf{Useless films}.

\subsection{What is Statistics?}
Statistics involves the mathematical representation of key real world quantities of interest and their associated uncertainties
(often,
but not always using probability),
the coherent incorporation of any
(uncertain)
knowledge,
information or observed data into this framework and the subsequent learning,
prediction,
future experimental design and decision making in the presence of uncertainty that this structure facilitates.

\subsection{Frequentists and Bayesian Statistics}
Bayesian statistics requires more input.

The relative frequency interpretation of probability is linked to frequentist statistics.

The subjective interpretation of probability is linked to frequentist Bayesian statistics.

\textit{Genuine waffle for about half an hour}.

\begin{example}[Motivating example 1 - Covid-19 Disease Test]
    A new test for Covid-19 has been developed.
    It is fast and cheap,
    but has moderate accuracy.
    It has been tested on a limited set of people with known Covid status.
    
    You are selected at random from the UK population in August $2020$ and you test positive.
    What is the probability you have Covid?
    How about if you were selected from the London population in January $2021$.
\end{example}

\begin{example}[Motivating example 2 - US Presidential Election polling]
    $22$nd of October $2024$,
    two weeks before the $2024$ US presidential election.
    You run a polling company and you have conducted a poll of $1000$ people form Pennsylvania state.
    Out of $1000$ people:
    $485$ said they'd vote for Harris,
    $515$ said they'd vote for Trump.
    Everyone wants to know your prediction.
\end{example}

\subsection{Foundations of Statistics and Interpretation of Probability}

\subsubsection{Probability: Revision}

\begin{definition}[Axioms of Probability]
    For a sample space $\Omega$,
    with collection $\mathcal{F}$ of events,
    the probability $\P(A)$ satisfies the axioms:
    \begin{enumerate}[label = A\arabic*]
        \item $\P(A) \geq 0$,
        for every $A \in \mathcal{F}$.

        \item $\P(\Omega) = 1$.

        \item For $A$ and $B$ disjoint then:
        \[
        \P(A \cup B) = \P(A) + \P(B)
        \]
    \end{enumerate}
\end{definition}

The axioms lead to some of the following consequences
\begin{proposition}
    \begin{enumerate}[label = (\roman*)]
        \item $0 \leq \P(A) \leq 1$.
        
        \item $\P(A ^ c) = 1 - \P(A)$.
        
        \item $\P(\emptyset) = 0$.
    \end{enumerate}
\end{proposition}

\subsubsection{Interpretations of Probability}
\textbf{Classical}

This is based on an assumption underlying equally likely events.

\textbf{Frequentist}

An event $A$ has probability $\P(A)$ given by:
\[
\P(A) = \liminfty\frac{n_A}{n},
\]
where $n_A$ is the number of times event $A$ occurred in $n$ repetitions of the experiment.

\textbf{Subjective}

Probabilities are viewed as subjective about the likelihood of an event $A$ occurring.
This can be defined in a precise way and Probability Axioms are actually derived as requirements of coherence.

\subsubsection{A Summary of Useful Probability Results}
\underline{Combining Events}

The event $A, B$ or both occur is $A \cup B$.
The event $A$ and $B$ occur is $A \cap B$.
These are related by the following rule
\[
\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B).
\]
Events are disjoint if they cannot occur at the same time.

Addition law for disjoint events:
If $A$ and $B$ are disjoint events
\[
\P(A \cup B) = \P(A) + \P(B).
\]

\underline{Conditional Probability and Independence}

For any two events $A, B$,
the notation $\P(A | B)$ means the conditional probability that $A$ occurs,
assuming that the event $B$ has already occurred.

Conditional probability is obtained directly or by using the conditional probability rule:
\[
\P(A | B) = \frac{\P(A | B)}{\P(B)},
\]
for $\P(B) > 0$.

Rearranging we get the general multiplication rule
\[
\P(A | B) = \P(A | B)\P(B).
\]

Two events are independent when the occurrence of one has no bearing on the occurrence of the other.
If $A, B$ are independent then
\[
\P(A | B) = \P(A).
\]

\underline{Partitions}

Suppose $E_1, \dotsc, E_n$ are mutually disjoint events,
and suppose exactly one must happen.
Such a collection of events is called a
(sure)
partition.

We can write any other event $A$ in combination with this event:
in general,
\[
\P(A) = \P(A \cap E_1) + \P(A \cap E_2) + \dotsc + \P(A \cap E_n),
\]
which simplifies to
\[
\P(A) = \P(A | E_1)\P(E_1) + \dotsc + \P(A | E_n)\P(E_n)
\]
using the multiplication rule.

\underline{Bayes Theorem}

For any two events $A, B$,
the multiplication rule gives the formula
\[
\P(A \cap B) = \P(A | B)\P(B).
\]
Another equivalent formula is obviously
\[
\P(A \cap B) = \P(B | A)\P(A).
\]

Equating these we get the formula known as Bayes theorem:
\[
\P(A | B) = \frac{\P(B | A)\P(A)}{\P(B)}.
\]
Often the probability in the denominator must be calculated using the simplifying method shown in the last section;
i.e. via a partition.

\newpage

\section{Disease Testing using Bayesian Methods}

\subsection{Testing for Covid-19}

Remember motivating example $1$.
\begin{enumerate}[label = (\roman*)]
    \item 
    We have data from a limited sample of $620$ patients given the new test is the 'gold standard'
    (i.e. perfect).

    Events
    \begin{align*}
        D ^ {+} &= \text{person has disease}. \\
        D ^ {-} &= \text{person doesn't have disease}. \\
        T ^ {+} &= \text{person tests positive}. \\
        T ^ {-} &= \text{person tests negative}.
    \end{align*}
    \begin{table}[H]
        \begin{tabular}{c|c|c|c}
             & Yes $D ^ {+}$ & No $D ^ {-}$ & Total \\
             \hline
             Test result positive $T ^ {+}$ & $209$ & $6$ & $215$ \\
             Test result negative $T ^ {-}$ & $11$ & $394$ & $405$ \\
             \hline
             Total & $220$ & $400$ & $620$
        \end{tabular}
    \end{table}

    Probability distribution
    \begin{align*}
        \P_t &= \text{probability distribution for test group of $620$ people}. \\
        \P &= \text{probability distribution of general UK population}.
    \end{align*}

    Probabilities of interest
    \begin{align*}
        \P_t\left(T ^ {+} \mmid D ^ {+}\right) &= \text{sensitivity of the test} \\
        \P_t\left(T ^ {-} \mmid D ^ {-}\right) &= \text{specificity of the test} \\
        \P_t\left(T ^ {+} \mmid D ^ {-}\right) &= \text{probability of a false positive} \\
        \P_t\left(T ^ {-} \mmid D ^ {+}\right) &= \text{probability of a false negative}
    \end{align*}

    Sensitivity:
    \begin{align*}
        \P_t\left(T ^ {+} \mmid D ^ {+}\right) &= \frac{\P_t(T ^ {+} \cap D ^ {+})}{\P_t(D ^ {+})} \\
        &= \frac{\frac{209}{620}}{\frac{270}{620}} \\
        &= \frac{209}{270} \\
        &= 0.95.
    \end{align*}

    Specificity:
    \begin{align*}
        \P_t\left(T ^ {-} \mmid D ^ {-}\right) &= \frac{\P_t(T ^ {-} \cap D ^ {-})}{\P_t(D ^ {-}} \\
        &= \frac{\frac{396}{620}}{\frac{400}{620}} \\
        &= \frac{396}{400} \\
        &= 0.8985.
    \end{align*}

    False positive:
    \begin{align*}
        \P_t\left(T ^ {+} \mmid D ^ {-}\right) &= 1 - \P_t\left(T ^ {-} \mmid D ^ {-}\right) \\
        &= 1 - 0.8985 \\
        &= 0.015.
    \end{align*}

    False negative:
    \begin{align*}
        \P_t\left(T ^ {-} \mmid D ^ {+}\right) &= 1 - \P_t\left(T ^ {+} \mmid D ^ {+}\right) \\
        &= 1 - 0.95 \\
        &= 0.05.
    \end{align*}

    Question:

    We care about the whole U.K. population.
    How does it relate to our test set?

    Assumption

    The sensitivity and specificity are a property of the test only.

    Hence
    \begin{align*}
        \P\left(T ^ {+} \mmid D ^ {+}\right) &= \P_t\left(T ^ {+} \mmid D ^ {+}\right). \\
        \P\left(T ^ {-} \mmid D ^ {-}\right) &= \P_t\left(T ^ {-} \mmid D ^ {-}\right). \\
        \P\left(T ^ {+} \mmid D ^ {-}\right) &= \P_t\left(T ^ {+} \mmid D ^ {-}\right). \\
        \P\left(T ^ {-} \mmid D ^ {+}\right) &= \P_t\left(T ^ {-} \mmid D ^ {+}\right).
    \end{align*}
    This is not true for other probabilities such as $\P(D ^ {+}) \neq \P_t(D ^ {+})$.
\end{enumerate}

\subsection{Bayesian Inference: from prior to posterior}
\begin{definition}
    Prior,
    likelihood,
    posterior.
    \[
    \P(D ^ {+}) = \text{prior probability of having Covid-19.\footnotemark}
    \]
    \footnotetext{Prevalence in the relevant population.}
    \[
    \P\left(T ^ {+} \mmid D ^ {+}\right), \P\left(T ^ {+} \mmid D ^ {-}\right) = \text{likelihood of positive test given disease states.}
    \]
    \[
    \P\left(D ^ {+} \mmid T ^ {+}\right) = \text{posterior probability of having Covid-19 given a positive test.}
    \]
    The posterior is critical and found using Bayes Theorem:
    \begin{align*}
        \P\left(D ^ {+} \mmid T ^ {+}\right) &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right)}{\P\left(T ^ {+}\right)} \quad\text{(by Bayes theorem)} \\
        &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right)}{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right) + \P\left(T ^ {+} \mmid D ^ {-}\right)\P\left(D ^ {-}\right)} \quad\text{(by partition)}\\
        &= \frac{\P_t\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right)}{\P_t\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right) + \P_t\left(T ^ {+} \mmid D ^ {-}\right)\P\left(D ^ {-}\right)} \quad\text{(by assumptions)} \\
    \end{align*}
    Covid-19 Example:
    August $2020$ U.K.

    In August $2020$ U.K. it was thought that $0.00025$ of the U.K. population had Covid-19.
    \[
    \P(D ^ {+}) = 0.00025
    \]
    and
    \[
    \P(D ^ {-}) = 1 - \P(D ^ {+})
    \]
    \begin{align*}
        \P\left(D ^ {+} \mmid T ^ {+}\right) &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right)}{\P\left(T ^ {+}\right)} \\
        &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right)}{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right) + \P\left(T ^ {+} \mmid D ^ {-}\right)\P\left(D ^ {-}\right)} \\
        &= \frac{0.95 \times 0.00025}{0.95 \times 0.00025 + 0.015 \times (1 - 0.00025)} \\
        &= \frac{0.0002375}{0.0002375 + 0.01499625} \\
        &= 0.0156.
    \end{align*}
\end{definition}

Why is the posterior probability $\P\left(D ^ {+} \mmid T ^ {+}\right)$ still so low?

Because our prior probability $\P(D ^ {+}) = 0.00025$ was so low,
that although the evidence from $T ^ {+}$ increases our probability,
it still ends up as relatively small.

Another way to see this is if we examine the denominator of Bayes Theorem which gives us the probability of a positive test result $T ^ {+}$.
We can see that a positive test comes from either you had the disease or you did not,
hence
\begin{align*}
    \P(T ^ {+}) &= \P\left(T ^ {+} \mmid D ^ {+}\right)\P(D ^ {+}) + \P\left(T ^ {+} \mmid D ^ {-}\right)\P(D ^ {-}) \\
    &= 0.0002375 + 0.01499625.
\end{align*}
We can see that the contribution from having the disease is much smaller than the contribution of having a false positive.

\textbf{Covid-19 Test Example:}
Applying Bayes Theorem
(London January $2021$ case)

If the above discussion is correct,
we should see a different result for our second question:
\begin{center}
    \textit{If instead you were selected at random from the London population in January $2021$
    (and you test positive),
    what is the probability
    (that you have Covid-19)
    then?}
\end{center}

It was thought,
in early January $2021$ that $0.03$ of the London population had Covid-19.

We can similarly apply Bayes theorem but now using the much larger prior probability of $\P(D ^ {+}) = 0.03$,
this yields:
\begin{align*}
    \P\left(D ^ {+} \mmid T ^ {+}\right) &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right)\P(D ^ {+})}{\P(T ^ {+})} \\
    &= \frac{0.95 \cdot 0.03}{\P(T ^ {+})} \\
    &= \frac{0.95 \cdot 0.03}{\P\left(T ^ {+} \mmid D ^ {+}\right)\P(D ^ {+}) + \P\left(T ^ {+} \mmid D ^ {-}\right)\P(D ^ {-})} \\
    &= \frac{0.95 \cdot 0.03}{0.95 \cdot 0.03 + 0.015 \cdot (1 - 0.03)} \\
    &= \frac{0.0285}{0.0285 + 0.01455} \\
    &= 0.662.
\end{align*}
So given a positive result,
you now have a $66.2\%$ chance of having Covid-19,
this is much higher.

Note that the route of having the disease contributes a lot more to the denominator of Bayes theorem than the route of not having the disease.

\subsection{More Advanced Bayesian Calculations}

\subsubsection{Performing Further Tests}
Consider the August $2020$ UK example above.
If you had tested positive you still only had a $1.56\%$ chance of having Covid-19.

Naturally,
you would take the test again.
You do this and receive a second positive test,
what is your probability now?

We denote two positive tests as $T ^ {++}$,
but now we have to consider the nature of the test further.

A possible assumption is that the test results are conditionally independent given disease status,
implying that
\[
\P\left(T ^ {++} \mmid D ^ {+}\right) = \P\left(T ^ {+} \mmid D ^ {+}\right) ^ 2\qquad\text{and}\qquad\P\left(T ^ {++} \mmid D ^ {-}\right) = \P\left(T ^ {+} \mmid D ^ {-}\right) ^ 2.
\]
This might be reasonable if the test fails independently each time,
i.e. due to reasons entirely unrelated to the specific attributes of the patient.

The other extreme is that the test fails precisely due to the specifics of the patient,
and will fail the same way again and again,
leading to $\P\left(T ^ {++} \mmid D ^ {+}\right) = \P\left(T ^ {+}\mmid D ^ {+}\right)$ etc.

\begin{example}[Covid-19 Test Example: Two Positive Test Results]
    We are again in August $2020$ UK and assume the same prior as before:
    $\P(D ^ {+}) = 0.00025$.

    But now we want the probability of Covid-19 given two positive test results $T ^ {++}$:
    \begin{align*}
        \P\left(D ^ {+} \mmid T ^ {++}\right) &= \frac{\P\left(T ^ {++} \mmid D ^ {+}\right)\P(D ^ {+})}{\P(T ^ {++})} \\
        &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right) ^ 2 \cdot 0.0025}{\P\left(T ^ {++} \mmid D ^ {+}\right)\P(D ^ {+}) + \P\left(T ^ {++} \mmid D ^ {-}\right)\P(D ^ {-})} \\
        &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right) ^ 2 \cdot 0.0025}{\P\left(T ^ {+} \mmid D ^ {+}\right) ^ 2\P(D ^ {+}) + \P\left(T ^ {+} \mmid D ^ {-}\right) ^ 2\P(D ^ {-})} \\
        &= \frac{0.95 ^ 2 \cdot 0.0025}{0.95 ^ 2 \cdot 0.00025 + (0.015) ^ 2 \cdot (1 - 0.00025)} \\
        &= 0.501.
    \end{align*}
    Under the conditional independence assumption,
    your posterior probability after two tests is now $50.1\%$:
    much higher than the $1.56\%$ from a single positive test.
\end{example}

\subsubsection{A Model for Sequential Learning}

Sequential learning occurs when we do two tests in a row.
Receiving one positive result,
then a second we had:
\begin{align*}
    \P(D ^ {+}) &\rightarrow \P\left(D ^ {+} \mmid T ^ {+}\right) &\rightarrow \P\left(D ^ {+} \mmid T ^ {++}\right) \\
    0.00025 &\rightarrow 0.0156 &\rightarrow 0.501
\end{align*}
that is,
as more information comes in,
we update our beliefs about the probability of $D ^ {+}$ again and again.

When we calculated the two test result we actually went straight from
\begin{align*}
    \P(D ^ {+}) &\rightarrow \P\left(D ^ {+} \mmid T ^ {++}\right) \\
    0.00025 &\rightarrow 0.501.
\end{align*}
Bayes theorem is consistent in the sense that we can update sequentially by one $T ^ {+}$ then by another $T ^ {+}$,
or all at once
(i.e. by $T ^ {++}$)
and we are guaranteed to obtain the same result.

\begin{example}
    Label the first positive test as $T_1 ^ {+}$
    (occurring on day $1$)
    and the second test as $T_2 ^ {+}$
    (occurring on day $2$).
    If,
    after seeing a single positive result $T_1 ^ {+}$ we updated to get posterior $\P\left(D ^ {+} \mmid T_1 ^ {+}\right)$ and trivially also $\P\left(D ^ {-} \mmid T_1 ^ {+}\right)$,
    then we could treat these posteriors as our priors for day $2$.
    Hence on day $2$ we could update using the second positive test $T_2 ^ {+}$ using Bayes theorem
    (with $T ^ {++} = \{T_1 ^ {+}, T_2 ^ {+}\}$):
    \begin{align*}
        \P\left(D ^ {+} \mmid T ^ {++}\right) = \P\left(D ^ {+} \mmid T_1 ^ {+}, T_2 ^ {+}\right) &= \frac{\P\left(T_2 ^ {+} \mmid D ^ {+}, T_1 ^ {+}\right)\P\left(D ^ {+} \mmid T_1 ^ {+}\right)}{\P\left(D ^ {+} \mmid T_2 ^ {+}\right)} \\
        &= \frac{\P\left(T_2 ^ {+} \mmid D ^ {+}, T_1 ^ {+}\right)\P\left(D ^ {+} \mmid T_1 ^ {+}\right)}{\P\left(T_2 ^ {+} \mmid D ^ {+}, T_1 ^ {+}\right)\P\left(D ^ {+} \mmid T_1 ^ {+}\right) + \P\left(T_2 ^ {+} \mmid D ^ {-}, T_1 ^ {+}\right)\P\left(D ^ {-} \mmid T_1 ^ {+}\right)} \\
        &= \frac{\P\left(T_2 ^ {+} \mmid D ^ {+}, T_1 ^ {+}\right)\P\left(D ^ {+} \mmid T_1 ^ {+}\right)}{\P\left(T_2 ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+} \mmid T_1 ^ {+}\right) + \P\left(T_2 ^ {+} \mmid D ^ {-}\right)\P\left(D ^ {-} \mmid T_1 ^ {+}\right)}
    \end{align*}
    where we have used the conditional independence assumption to simplify terms such as $\P\left(T_2 ^ {+} \mmid D ^ {+}, T_1 ^ {+}\right) = \P\left(T_2 ^ {+} \mmid D ^ {+}\right)$.
\end{example}

\subsubsection{Sensitivity of the Posterior \texorpdfstring{$\P\left(D ^ {+} \mmid T ^ {+}\right)$}{} to the Sensitivity and Specificity}

We can perform a simple sensitivity analysis by examining the derivative of the posterior with respect to the sensitivity and the specificity,
keeping all other things constant when performing the derivative.
\[
\text{Sensitivity:}\quad\frac{d\P\left(D ^ {+} \mmid T ^ {+}\right)}{d\P\left(T ^ {+} \mmid D ^ {+}\right)}\qquad\text{Specificity:}\quad\frac{d\P\left(D ^ {+} \mmid T ^ {+}\right)}{d\P\left(T ^ {-} \mmid D ^ {-}\right)}
\]
For example,
denoting the specificity as $x = \P\left(T ^ {-} \mmid D ^ {-}\right)$,
then we have
\begin{align*}
    \P\left(D ^ {+} \mmid T ^ {+}\right) &= \frac{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+})}{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+}) + (1 - x)\P(D ^ {-})} \\
    \frac{d\P\left(D ^ {+} \mmid T ^ {+}\right)}{d\P\left(T ^ {-} \mmid D ^ {-}\right)} = \frac{d}{dx}\cP{D ^ {+}}{T ^ {+}} &= \frac{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+})\P(D ^ {-})}{[\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+}) + (1 - x)\P(D ^ {-})] ^ 2}.
\end{align*}

\subsubsection{Odds and the Likelihood Ratio}
\begin{definition}[Prior and posterior odds and the likelihood ratio]
    Prior odds in favour of $A = \frac{\P(A)}{\P(A ^ c)}$.

    Posterior odds in favour of $A$ given $B = \frac{\cP{A}{B}}{\cP{A ^ c}{B}}$.

    Likelihood ratio for $B$ given $A = \frac{\cP{B}{A}}{\cP{B}{A ^ c}}$.

    It is simple to show that
    \[
    \text{posterior odds} = \text{prior odds} \times \text{likelihood ratio}.
    \]
\end{definition}

The likelihood ratio can be interpreted as a factor which determines whether the posterior odds are smaller
(likelihood ratio $< 1$)
or larger
(likelihood ratio $> 1$)
than the prior odds,
and by how much.

\newpage

\section{US Presidential Election Polling via Frequentist Methods}

\subsection{Random Variables,
Expectation and Variance:
Revision and Notation}

\subsubsection{Discrete Random Variables}
A discrete random variable $X$ is a variable which can take a finite set of different numerical values $x_1, x_2, \dotsc, \in \mathcal{X}$.

The probability mass function is
\[
f(x) = \P(X = x),\qquad\forall x \in \mathcal{X}.
\]

Given
\[
f\left(x \mmid \lambda\right) = \P(X = x) = \begin{cases}
    \frac{e ^ {-\lambda}\lambda ^ x}{x!} & \text{for } x = 0, 1, 2, \dotsc \\
    0 & \text{otherwise}.
\end{cases}
\]
We write $f\left(x \mmid \lambda\right)$ for the $f$ of $x$ given $\lambda$.

\subsubsection{Expectation and Variance of a Discrete Random Variable}
The expected value of a discrete random variable $X$ is defined as
\[
\E(X) = \sum_{x \in \mathcal{X}}x\P(X = x) = \sum_{x \in \mathcal{X}}xf(x)
\]
that is,
we take a weighted average of $X$.

The expected value of any function $h(X)$ of $X$ is defined to be
\[
\E(h(X)) = \sum_{x \in \mathcal{X}}h(x)\P(X = x) = \sum_{x \in X}h(x)f(x).
\]

The variance of $X$ is defined to be
\begin{align*}
    \Var(X) &= \E((X - \E(X)) ^ 2) \\
    &= \E(X ^ 2) - \E(X) ^ 2.
\end{align*}
This measures the theoretical average of the distance of the values of the random variable $X$ from its central value as measured by the expected value.
$\Var(X)$ measures the spread of values of a random variable.

The standard deviation of a random variable is just the square root of the variance
\[
\mathrm{SD}(X) = \sqrt{\Var(X)}.
\]

\subsubsection{Continuous Random Variables}
The probability density function $f$ of a continuous random variable $X$ describes the probability density of each value,
defined as follows
\begin{align*}
    \P(x \leq X \leq x + dx) &= f(x)\,dx \\
    \P(a \leq X \leq b) &= \int_{a}^{b}f(x)\,dx.
\end{align*}

\subsubsection{Expectation and Variance of a Continuous Random Variable}
The expectation,
variance and standard deviation of a continuous random variable $X$ with probability density function $f(x)$ are:
\begin{align*}
    \E(X) &= \int xf(x)\,dx \\
    \Var(X) &= \E((X - \E(X)) ^ 2) \\
    &= \E(X ^ 2) - (\E(X)) ^ 2 &\left(\text{with } \E(X ^ 2) = \int x ^ 2f(x)\,dx\right) \\
    \mathrm{SD}(X) = \sqrt{\Var(X)}
\end{align*}
where integration is over the full range of values of $X$.

\subsubsection{Properties of Expectations and Variances}
\textbf{Change in scale and location}

For any random variable $X$ and any two real numbers $a, b$.
\begin{align*}
    \E(aX + b) &= a\E(X) + b \\
    \Var(aX + b) &= a ^ 2\Var(X) \\
    \mathrm{SD}(aX + b) &= a\mathrm{SD}(X).
\end{align*}

\textbf{Sums and differences}

For any two random variables $X, Y$ and any two real numbers $a, b$.
\begin{align*}
    \E(aX + bY) &= a\E(X) + b\E(Y). \\
    \intertext{If $X$ and $Y$ are independent} \\
    \Var(aX + bY) &= a ^ 2\Var(X) + b ^ 2\Var(Y). \\
    \intertext{If $X$ and $Y$ are not independent} \\
    \Var(aX + bY) &= a ^ 2\Var(X) + b ^ 2\Var(Y) + 2ab\Cov(X, Y).
\end{align*}
These rules can be extended to sums of several random variables.

Consider any $n$ random variables $X_1, X_2, \dotsc, X_n$ then
\[
\E(X_1 + X_2 + \dotsi + X_n) = \E(X_1) + \E(X_2) + \dotsi + \E(X_n)
\]
additionally if all $X_i$ are independent of each other then
\[
\Var(X_1 + X_2 + \dotsi + X_n) = \Var(X_1) + \Var(X_2) + \dotsi + \Var(X_n).
\]

\subsection{Random Sampling,
Proportions and the Bernoulli Distribution}

\subsubsection{The Bernoulli Distribution}

For an experiment where there are only two possible outcomes,
and we can designate the two possible outcomes as $0$ or $1$,
we can apply the following definition.
\begin{definition}[Bernoulli Distribution]
    We say that a random variable $X$ has a Bernoulli distribution with parameter $p$ with
    ($0 \leq p \leq 1$)
    if $X$ can take only the values $0$ and $1$ with respective probabilities
    \begin{align*}
        \P(X = 1) = p&&\text{and}&&\P(X = 0) = 1 - p
    \end{align*}
    setting $q = 1 - p$ we can write the pmf as
    \[
    f\left(x \mmid p\right) = \begin{cases}
        p ^ xq ^ {1 - x} &\text{for } x = 0, 1, \\
        0 &\text{otherwise}.
    \end{cases}
    \]
    To see that $f\left(x \mmid p\right)$ represents the Bernoulli distribution as given by the probabilities we can just note that
    \begin{align*}
        f\left(1 \mmid p\right) = p &&\text{and}&&f\left(0 \mmid p\right) = q.
    \end{align*}
\end{definition}

If $X$ has a Bernoulli distribution with parameter $p$ then we can directly calculate its expectation and variance:
\begin{align*}
    \E(X) &= \sum_{x \in X}xf\left(x \mmid p\right) = p \\
    \E(X ^ 2) &= \sum_{x \in X}x ^ 2f\left(x \mmid p\right) = p \\
    \intertext{Hence the variance is given by} \\
    \Var(X) &= \E(X ^ 2) - \E(X) ^ 2 = pq.
\end{align*}

Furthermore,
we can calculate the moment generating function as
\[
\psi(t) = \E(e ^ {tX}) = \sum_{x \in X}e ^ {tx}f\left(x \mmid p\right) = q + pe ^ t\qquad\text{for } -\infty < t < \infty.
\]

\subsubsection{Bernoulli Trials}
If we have a sequence of $n$ random variables $X_1, \dotsc, X_n$ which are independent and identically distributed and if each of the $X_i$ has a Bernoulli distribution with parameter $p$ then we say that the variables $X_1, \dotsc, X_n$ form $n$ Bernoulli trials with parameter $p$.

\subsubsection{Statistical Inference}
If we view $p$ as real,
fixed with a "true" value that is just unknown,
we may seek to construct from the data $X_1, \dotsc, X_n$ some way of giving an estimate of $p$,
and then subsequently examining the attributes of this estimator.

We can also view $p$ as an unknown quantity for which we should represent our subjective uncertainty about via a probability distribution.
Then we can do somewhat similar as what we did with treating it as a fixed "true" value.

\subsubsection{Total and sample proportions as estimators}
Let us redefine the random quantity $X$ to be the sum of all successes over the $n$ trials,
\begin{align*}
    X = \sum_{i = 1}^{n}X_i&&\text{with } X_i = \begin{dcases*}
        1 & when $i$ is a success \\
        0 & when $i$ is a failure.
    \end{dcases*}
\end{align*}

To guide us in estimating $p$,
it is worth noting what the pmf is for a set of Bernoulli trials $X_1, \dotsc, X_n$:
as the $X_i$ are independent we can then just multiply the individual pmfs.
Therefore
\begin{align*}
    f\left(x_1, x_2, \dotsc, x_n \mmid p\right) &= \prod_{i = 1}^{n}f\left(x_i \mmid p\right) \\
    &= \prod_{i = 1}^{n}p ^ {x_i}q ^ {1 - x_i} \\
    &= p ^ {\sum_{i = 1}^{n}x_i}q ^ {\sum_{i = 1}^{n}(1 - x_i)} \\
    &= p ^ xq ^ {n - x}
\end{align*}
where we have denoted $x = \sum_{i = 1}^{n}x_i$ the sum of the successes.

We can see that the pmf only relies on the probability $p$,
the total number of Bernoulli trials $n$ and the number of successes $x$.

To get a good estimator we could ask what value of $p$ will maximise the pmf $f\left(x_1, \dotsc, x_n \mmid p\right)$?
To maximise $f\left(x_1, \dotsc, x_n \mmid p\right)$ we can maximise the $\log$ of $f\left(x_1, \dotsc, x_n \mmid p\right)$ as this is easier to differentiate
\begin{align*}
    f\left(x_1, \dotsc, x_n \mmid p\right) &= p ^ xq ^ {n - x} \\
    \implies \log{f\left(x_1, \dotsc, x_n \mmid p\right)} &= x\log{p} + (n - x)\log{q} \\
    \implies \pd{p}\log{f\left(x_1, \dotsc, x_n \mmid p\right)} &= \frac{x}{p} - \frac{n - x}{1 - p}
\end{align*}
therefore an estimate for $p$,
which we will denote $\hat{p}$,
which maximises $f\left(x_1, \dotsc, x_n \mmid p\right)$ is given by
\begin{align*}
    \frac{x}{\hat{p}} - \frac{n - x}{1 - \hat{p}} &= 0 \\
    \implies x(1 - \hat{p}) &= (n - x)\hat{p} \\
    \implies x - x\hat{p} &= n\hat{p} - x\hat{p} \\
    \implies x &= n\hat{p} \\
    \implies \hat{p} &= \frac{x}{n}
\end{align*}

This suggests we use the proportion of successes out of our $n$ trials as our estimate for the true probability $p$.

This estimate $\hat{p}$ is also called the sample proportion $Y$ and in terms of random variables we would write
\[
\hat{p} = Y = \frac{X}{n} = \frac{1}{n}\sum_{i = 1}^{n}X_i
\]
so $Y$ is simply the proportion of successes in our set of Bernoulli trials of size $n$.

\textit{Note:
$\hat{p} = Y$ can also be considered as simply the mean of the $X_i$.}

\subsubsection{The Binomial Distribution}
\begin{definition}[The Binomial Distribution]
    A random variable $X$ has a binomial distribution with parameters $n$ and $p$ if $X$ has a discrete distribution for which the pmf is as follows:
    \[
    f\left(x \mmid n, p\right) = \begin{cases}
        \binom{n}{x}p ^ xq ^ {n - x} & \text{for } x = 0, 1, 2, \dotsc, n, \\
        0 & \text{otherwise}.
    \end{cases}
    \]
    Here $n$ must be a positive integer and $p$ must lie in the interval $0 \leq p \leq 1$.
    We write $X \sim \Bin(n, p)$.
    The Binomial distribution is of fundamental importance to us because of the following result:

    \textit{If the collection of random variables $X_1, \dotsc, X_n$ form $n$ Bernoulli trials with parameter $p$,
    and if $X = X_1 + \dotsi + X_n$ represents the sum of the successes,
    then $X$ has a binomial distribution with parameters $n$ and $p$.}
\end{definition}

When $X$ is viewed as the sum of $n$ Bernoulli trials,
we can find the expectation,
variance and moment generating function of the binomial distribution easily.
\begin{align*}
    \E(X) &= \sum_{i = 1}^{n}\E(X_i) = np, \\
    \Var(X) &= \sum_{i = 1}^{n}\Var(X_i) = npq = np(1 - p), \\
    \psi(t) &= \E(e ^ {tX}) = \prod_{i = 1}^{n}\E(e ^ {tX_i}) = (pe ^ t + q) ^ n.
\end{align*}
The above expectation and variance of $X$ are as usual calculated from a view point before $X$ is actually measured.

We can similarly understand our estimator $\hat{p} = Y = \frac{X}{n}$ of the probability $p$.



\textbf{Aside:
Normal approximation to Binomial and Continuity Correction}

We know $X \sim \Bin(n, p)$ but this is sometimes cumbersome to calculate probabilities with.
We can approximate the binomial distribution by a Normal distribution with the same mean and variance:
\[
\Bin(n, p)\text{ is approximately } N(np, np(1 - p))
\]
this approximation is acceptable when $np \geq 10$ and $n(1 - p) \geq 10$ and the larger these value the better.

For small $n$,
a continuity correction might be appropriate:

if $X \sim \Bin(n, p)$ and $X' \sim N(np, np(1 - p))$,
then
\begin{align*}
    \P(X \leq k) &\simeq \P(X' \leq k + 1 / 2) \\
    \P(k_1 \leq X \leq k_2) &\simeq \P(k_1 - 1 / 2 \leq X' \leq k_2 + 1 / 2)
\end{align*}

\subsection{The Sample Proportion as an Estimator for \texorpdfstring{$p$}{}}
As $X \sim \Bin(n, p)$ we have that
\begin{align*}
    \E(\hat{p}) = \E(Y) &= \E\left(\frac{X}{n}\right) = \frac{1}{n}\E(X) \\
    &= \frac{1}{n}np = p \\
    \Var(\hat{p}) = \Var(Y) &= \Var\left(\frac{X}{n}\right) = \frac{1}{n ^ 2}(X) \\
    &= \frac{1}{n ^ 2}npq = \frac{pq}{n} = \frac{p(1 - p)}{n}.
\end{align*}

We have that $\E(\hat{p}) = p$,
that is the expectation of our estimator $\hat{p}$ is equal to the thing we are trying to estimate.
We say the estimator is unbiased.

As we have $\Var(\hat{p}) = \frac{p(1 - p)}{n}$ we now have some understanding and some control over how accurate our estimate is:
if we double the size of the sample $n$ then the variance will decrease by $2$.
The corresponding standard deviation would only decrease by $\sqrt{2}$.

\textbf{Normal Approximation}

As $X \sim \Bin(n, p)$ and $Y = \frac{X}{n}$,
we can calculate probabilities for $Y = \hat{p}$
(if we are given $n$ and $p$ say),
but if $n$ is large this is cumbersome.

Instead we can employ the Normal approximation for $X$ and then $Y$:
\begin{align*}
    X &\sim \Bin(n, p) \\
    \implies X &\sim N(np, np(1 - p)) \\
\implies \hat{p} = Y &\sim N\left(p, \frac{p(1 - p)}{n}\right).
\end{align*}

\textbf{Margin of Error}

How accurate is our estimator $Y$?
The margin of error of an estimate can be roughly defined
(for now)
as two standard deviations $\mathrm{SD}(Y)$ on either side of the estimate,
suggesting the true value $p$ may lie in an interval of the form:
\[
Y \pm 2\sqrt{\frac{p(1 - p)}{n}}
\]
this is pretty useless since we don't know $p$,
if we did then there would be no point in estimating.

We can find a very useful upper bound on the margin of error:
\[
\max_{p \in [0, 1]}p(1 - p) = \frac{1}{4}\text{ at } p = \frac{1}{2}.
\]
Therefore,
\[
\text{Margin of Error } = 2\sqrt{\frac{p(1 - p)}{n}} \leq 2\sqrt{\frac{1}{4n}} = \frac{1}{\sqrt{n}}
\]
that is,
the margin of error is bounded by $\frac{1}{\sqrt{n}}$ no matter what the value of $p$.
So our suggested standard deviation interval becomes the famous result:
\[
Y \pm 2\sqrt{\frac{p(1 - p)}{n}} = Y \pm \frac{1}{\sqrt{n}}.
\]

\begin{example}
    $22$nd October $2024$,
    we are about to ask $1000$ people in our poll.
    
    Question $1$:
    Find the expected value and the margin of error for the proportion of Trump voters in our sample if we were told that
    \begin{enumerate}[label = (\alph*)]
        \item $p = 0.5$
        \item $p = 0.3$.
    \end{enumerate}
    
    \begin{solution}
        \begin{enumerate}[label = (\alph*)]
            \item
            $\E(X) = np = 1000 \times 0.5 = 500$.
    
            $\E(\hat{p}) = p = 0.5 = 50\%$.
    
            $\Var(X) = np(1 - p) = 1000 \times 0.5 \times 0.5 = 250$.
    
            $\mathrm{SD}(X) = 15.8$ people.

            \[
            \Var(\hat{p}) = \frac{p(1 - p)}{n} = \frac{0.5(0.5)}{1000} = 0.00025
            \]

            Margin of error $2\sqrt{\frac{p(1 - p)}{n}} = 2\sqrt{\frac{0.5 \times 0.5}{1000}} \approx 0.0316 = 3.16\%$.
            
            \item
            If $p = 0.3 \implies \E(\hat{p}) = 0.3$ and margin of error is $0.0290 = 2.90\%$.
        \end{enumerate}
    \end{solution}
\end{example}

\begin{example}
    Poll was performed $n = 1000$,
    $515$ for Trump $485$ for Harris.

    What is your estimate for $p$ and its margin or error?

    \begin{solution}
        We use $\hat{p}$ to estimate $p$:
        \[
        \hat{p} = \frac{x}{n} = \frac{515}{1000} = 0.515 = 51.5\%.
        \]
        We do not know $p$,
        so we use the maximum bound for the margin of error
        \[
        \frac{1}{\sqrt{n}} = \frac{1}{\sqrt{1000}} \approx 0.0316 = 3.16\%.
        \]
        So we thing $p$ will lie within:
        \[
        51.5\% \pm 3.16\%.
        \]

        \textit{Note that $50\%$ lies in this interval.}
    \end{solution}
\end{example}

\begin{example}
    Your company has time to perform one more poll before the election.
    How large a poll is needed to ensure a margin of error less than $1\%$?

    \begin{solution}
        Must have:
        \[
        \frac{1}{\sqrt{n}} < 0.01 \iff n > \frac{1}{0.01 ^ 2} \iff n > 10000.
        \]
    \end{solution}
\end{example}


We wanted $p$ and had data
\[
X_i = \frac{0 \text{ or } 1}{i = 1, \dotsc, n}
\]

Used estimator:
$\hat{p}$ as a guess.

$\hat{p}$ properties:
\begin{align*}
    \E(\hat{p}) &= p \\
    \Var(\hat{p}) &= \frac{p(1 - p)}{n} < \frac{1}{4n}.
\end{align*}

\textbf{Big question:}
Does this structure generalise e.g. $X_i \in \R$
(not just $0, 1$)
or where $X_i$ are generated from more complex distributions?

Answer:
Yes.

\section{The Frequentist Statistics Paradigm}

\subsection{Sampling,
Inference and the Sample Mean}

\textbf{Terminology:}

A population is a group of $N$ people or objects of interest.
Call the objects units or subjects.

A sample is the subset of the population that is examined.

Many ways to choose this sample.

A simple random sample
(SRS)
of size $n$.

A stratified random sample take a simple random sample from strata e.g. men,
women,
old,
young.

A quota sample.

A population parameter:
e.g. the mean of population $\mu$,
the variance of the population $\sigma ^ 2$
could be from a Normal distribution or more complex.
The probability $p$ of success in a binomial distribution.

We want to make inferences i.e. to learn about these population parameters.
A statistic is a number that is calculated from the sample data,
usually a summary:
e.g. the sample mean $\Bar{X}$ and the sample variance $s ^ 2$,
e.g. the sample proportion $Y = \hat{p}$.

The sampling distribution is the distribution of the statistic that describes how it can vary from sample to sample.

Statistics are often used as estimators of population parameters.
For example we saw $\hat{p} = Y$ which is the estimator for $p$.

We will be concerned with certain aspects of estimators:
whether or not they are biased e.g. if $\E(\text{estimator}) \overset{\neq}{=} \text{ population parameter}$,
degree of variability
(i.e. their variance).

\textbf{Randomisation:}
is the key weapon to ensure zero bias.

\textbf{Variability:}
is usually proportional to $(\text{sample size}) ^ {-1}$.

We can look at an estimator with:
high bias and high variance,
high bias and low variance,
low bias and high variance,
low bias and low variance.

\subsubsection{Inference}
Inference is the act of drawing conclusions from certain basic facts and premises.

The inferences we draw are rarely certain:
uncertainty is everywhere.













\end{document}