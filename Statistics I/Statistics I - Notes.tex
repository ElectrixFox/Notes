\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\newcommand{\cP}[2]{\P\left(#1 \mmid #2\right)}

\title{Statistics I}
\author{Luke Phillips}
\date{January 2025}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{}

\subsection{High Profile Applications}
\textbf{Useless films}.

\subsection{What is Statistics?}
Statistics involves the mathematical representation of key real world quantities of interest and their associated uncertainties
(often,
but not always using probability),
the coherent incorporation of any
(uncertain)
knowledge,
information or observed data into this framework and the subsequent learning,
prediction,
future experimental design and decision making in the presence of uncertainty that this structure facilitates.

\subsection{Frequentists and Bayesian Statistics}
Bayesian statistics requires more input.

The relative frequency interpretation of probability is linked to frequentist statistics.

The subjective interpretation of probability is linked to frequentist Bayesian statistics.

\textit{Genuine waffle for about half an hour}.

\begin{example}[Motivating example 1 - Covid-19 Disease Test]
    A new test for Covid-19 has been developed.
    It is fast and cheap,
    but has moderate accuracy.
    It has been tested on a limited set of people with known Covid status.
    
    You are selected at random from the UK population in August $2020$ and you test positive.
    What is the probability you have Covid?
    How about if you were selected from the London population in January $2021$.
\end{example}

\begin{example}[Motivating example 2 - US Presidential Election polling]
    $22$nd of October $2024$,
    two weeks before the $2024$ US presidential election.
    You run a polling company and you have conducted a poll of $1000$ people form Pennsylvania state.
    Out of $1000$ people:
    $485$ said they'd vote for Harris,
    $515$ said they'd vote for Trump.
    Everyone wants to know your prediction.
\end{example}

\subsection{Foundations of Statistics and Interpretation of Probability}

\subsubsection{Probability: Revision}

\begin{definition}[Axioms of Probability]
    For a sample space $\Omega$,
    with collection $\mathcal{F}$ of events,
    the probability $\P(A)$ satisfies the axioms:
    \begin{enumerate}[label = A\arabic*]
        \item $\P(A) \geq 0$,
        for every $A \in \mathcal{F}$.

        \item $\P(\Omega) = 1$.

        \item For $A$ and $B$ disjoint then:
        \[
        \P(A \cup B) = \P(A) + \P(B)
        \]
    \end{enumerate}
\end{definition}

The axioms lead to some of the following consequences
\begin{proposition}
    \begin{enumerate}[label = (\roman*)]
        \item $0 \leq \P(A) \leq 1$.
        
        \item $\P(A ^ c) = 1 - \P(A)$.
        
        \item $\P(\emptyset) = 0$.
    \end{enumerate}
\end{proposition}

\subsubsection{Interpretations of Probability}
\textbf{Classical}

This is based on an assumption underlying equally likely events.

\textbf{Frequentist}

An event $A$ has probability $\P(A)$ given by:
\[
\P(A) = \liminfty\frac{n_A}{n},
\]
where $n_A$ is the number of times event $A$ occurred in $n$ repetitions of the experiment.

\textbf{Subjective}

Probabilities are viewed as subjective about the likelihood of an event $A$ occurring.
This can be defined in a precise way and Probability Axioms are actually derived as requirements of coherence.

\subsubsection{A Summary of Useful Probability Results}
\underline{Combining Events}

The event $A, B$ or both occur is $A \cup B$.
The event $A$ and $B$ occur is $A \cap B$.
These are related by the following rule
\[
\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B).
\]
Events are disjoint if they cannot occur at the same time.

Addition law for disjoint events:
If $A$ and $B$ are disjoint events
\[
\P(A \cup B) = \P(A) + \P(B).
\]

\underline{Conditional Probability and Independence}

For any two events $A, B$,
the notation $\P(A | B)$ means the conditional probability that $A$ occurs,
assuming that the event $B$ has already occurred.

Conditional probability is obtained directly or by using the conditional probability rule:
\[
\P(A | B) = \frac{\P(A | B)}{\P(B)},
\]
for $\P(B) > 0$.

Rearranging we get the general multiplication rule
\[
\P(A | B) = \P(A | B)\P(B).
\]

Two events are independent when the occurrence of one has no bearing on the occurrence of the other.
If $A, B$ are independent then
\[
\P(A | B) = \P(A).
\]

\underline{Partitions}

Suppose $E_1, \dotsc, E_n$ are mutually disjoint events,
and suppose exactly one must happen.
Such a collection of events is called a
(sure)
partition.

We can write any other event $A$ in combination with this event:
in general,
\[
\P(A) = \P(A \cap E_1) + \P(A \cap E_2) + \dotsc + \P(A \cap E_n),
\]
which simplifies to
\[
\P(A) = \P(A | E_1)\P(E_1) + \dotsc + \P(A | E_n)\P(E_n)
\]
using the multiplication rule.

\underline{Bayes Theorem}

For any two events $A, B$,
the multiplication rule gives the formula
\[
\P(A \cap B) = \P(A | B)\P(B).
\]
Another equivalent formula is obviously
\[
\P(A \cap B) = \P(B | A)\P(A).
\]

Equating these we get the formula known as Bayes theorem:
\[
\P(A | B) = \frac{\P(B | A)\P(A)}{\P(B)}.
\]
Often the probability in the denominator must be calculated using the simplifying method shown in the last section;
i.e. via a partition.

\newpage

\section{Disease Testing using Bayesian Methods}

\subsection{Testing for Covid-19}

Remember motivating example $1$.
\begin{enumerate}[label = (\roman*)]
    \item 
    We have data from a limited sample of $620$ patients given the new test is the 'gold standard'
    (i.e. perfect).

    Events
    \begin{align*}
        D ^ {+} &= \text{person has disease}. \\
        D ^ {-} &= \text{person doesn't have disease}. \\
        T ^ {+} &= \text{person tests positive}. \\
        T ^ {-} &= \text{person tests negative}.
    \end{align*}
    \begin{table}[H]
        \begin{tabular}{c|c|c|c}
             & Yes $D ^ {+}$ & No $D ^ {-}$ & Total \\
             \hline
             Test result positive $T ^ {+}$ & $209$ & $6$ & $215$ \\
             Test result negative $T ^ {-}$ & $11$ & $394$ & $405$ \\
             \hline
             Total & $220$ & $400$ & $620$
        \end{tabular}
    \end{table}

    Probability distribution
    \begin{align*}
        \P_t &= \text{probability distribution for test group of $620$ people}. \\
        \P &= \text{probability distribution of general UK population}.
    \end{align*}

    Probabilities of interest
    \begin{align*}
        \P_t\left(T ^ {+} \mmid D ^ {+}\right) &= \text{sensitivity of the test} \\
        \P_t\left(T ^ {-} \mmid D ^ {-}\right) &= \text{specificity of the test} \\
        \P_t\left(T ^ {+} \mmid D ^ {-}\right) &= \text{probability of a false positive} \\
        \P_t\left(T ^ {-} \mmid D ^ {+}\right) &= \text{probability of a false negative}
    \end{align*}

    Sensitivity:
    \begin{align*}
        \P_t\left(T ^ {+} \mmid D ^ {+}\right) &= \frac{\P_t(T ^ {+} \cap D ^ {+})}{\P_t(D ^ {+})} \\
        &= \frac{\frac{209}{620}}{\frac{270}{620}} \\
        &= \frac{209}{270} \\
        &= 0.95.
    \end{align*}

    Specificity:
    \begin{align*}
        \P_t\left(T ^ {-} \mmid D ^ {-}\right) &= \frac{\P_t(T ^ {-} \cap D ^ {-})}{\P_t(D ^ {-}} \\
        &= \frac{\frac{396}{620}}{\frac{400}{620}} \\
        &= \frac{396}{400} \\
        &= 0.8985.
    \end{align*}

    False positive:
    \begin{align*}
        \P_t\left(T ^ {+} \mmid D ^ {-}\right) &= 1 - \P_t\left(T ^ {-} \mmid D ^ {-}\right) \\
        &= 1 - 0.8985 \\
        &= 0.015.
    \end{align*}

    False negative:
    \begin{align*}
        \P_t\left(T ^ {-} \mmid D ^ {+}\right) &= 1 - \P_t\left(T ^ {+} \mmid D ^ {+}\right) \\
        &= 1 - 0.95 \\
        &= 0.05.
    \end{align*}

    Question:

    We care about the whole U.K. population.
    How does it relate to our test set?

    Assumption

    The sensitivity and specificity are a property of the test only.

    Hence
    \begin{align*}
        \P\left(T ^ {+} \mmid D ^ {+}\right) &= \P_t\left(T ^ {+} \mmid D ^ {+}\right). \\
        \P\left(T ^ {-} \mmid D ^ {-}\right) &= \P_t\left(T ^ {-} \mmid D ^ {-}\right). \\
        \P\left(T ^ {+} \mmid D ^ {-}\right) &= \P_t\left(T ^ {+} \mmid D ^ {-}\right). \\
        \P\left(T ^ {-} \mmid D ^ {+}\right) &= \P_t\left(T ^ {-} \mmid D ^ {+}\right).
    \end{align*}
    This is not true for other probabilities such as $\P(D ^ {+}) \neq \P_t(D ^ {+})$.
\end{enumerate}

\subsection{Bayesian Inference: from prior to posterior}
\begin{definition}
    Prior,
    likelihood,
    posterior.
    \[
    \P(D ^ {+}) = \text{prior probability of having Covid-19.\footnotemark}
    \]
    \footnotetext{Prevalence in the relevant population.}
    \[
    \P\left(T ^ {+} \mmid D ^ {+}\right), \P\left(T ^ {+} \mmid D ^ {-}\right) = \text{likelihood of positive test given disease states.}
    \]
    \[
    \P\left(D ^ {+} \mmid T ^ {+}\right) = \text{posterior probability of having Covid-19 given a positive test.}
    \]
    The posterior is critical and found using Bayes Theorem:
    \begin{align*}
        \P\left(D ^ {+} \mmid T ^ {+}\right) &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right)}{\P\left(T ^ {+}\right)} \quad\text{(by Bayes theorem)} \\
        &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right)}{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right) + \P\left(T ^ {+} \mmid D ^ {-}\right)\P\left(D ^ {-}\right)} \quad\text{(by partition)}\\
        &= \frac{\P_t\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right)}{\P_t\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right) + \P_t\left(T ^ {+} \mmid D ^ {-}\right)\P\left(D ^ {-}\right)} \quad\text{(by assumptions)} \\
    \end{align*}
    Covid-19 Example:
    August $2020$ U.K.

    In August $2020$ U.K. it was thought that $0.00025$ of the U.K. population had Covid-19.
    \[
    \P(D ^ {+}) = 0.00025
    \]
    and
    \[
    \P(D ^ {-}) = 1 - \P(D ^ {+})
    \]
    \begin{align*}
        \P\left(D ^ {+} \mmid T ^ {+}\right) &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right)}{\P\left(T ^ {+}\right)} \\
        &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right)}{\P\left(T ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+}\right) + \P\left(T ^ {+} \mmid D ^ {-}\right)\P\left(D ^ {-}\right)} \\
        &= \frac{0.95 \times 0.00025}{0.95 \times 0.00025 + 0.015 \times (1 - 0.00025)} \\
        &= \frac{0.0002375}{0.0002375 + 0.01499625} \\
        &= 0.0156.
    \end{align*}
\end{definition}

Why is the posterior probability $\P\left(D ^ {+} \mmid T ^ {+}\right)$ still so low?

Because our prior probability $\P(D ^ {+}) = 0.00025$ was so low,
that although the evidence from $T ^ {+}$ increases our probability,
it still ends up as relatively small.

Another way to see this is if we examine the denominator of Bayes Theorem which gives us the probability of a positive test result $T ^ {+}$.
We can see that a positive test comes from either you had the disease or you did not,
hence
\begin{align*}
    \P(T ^ {+}) &= \P\left(T ^ {+} \mmid D ^ {+}\right)\P(D ^ {+}) + \P\left(T ^ {+} \mmid D ^ {-}\right)\P(D ^ {-}) \\
    &= 0.0002375 + 0.01499625.
\end{align*}
We can see that the contribution from having the disease is much smaller than the contribution of having a false positive.

\textbf{Covid-19 Test Example:}
Applying Bayes Theorem
(London January $2021$ case)

If the above discussion is correct,
we should see a different result for our second question:
\begin{center}
    \textit{If instead you were selected at random from the London population in January $2021$
    (and you test positive),
    what is the probability
    (that you have Covid-19)
    then?}
\end{center}

It was thought,
in early January $2021$ that $0.03$ of the London population had Covid-19.

We can similarly apply Bayes theorem but now using the much larger prior probability of $\P(D ^ {+}) = 0.03$,
this yields:
\begin{align*}
    \P\left(D ^ {+} \mmid T ^ {+}\right) &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right)\P(D ^ {+})}{\P(T ^ {+})} \\
    &= \frac{0.95 \cdot 0.03}{\P(T ^ {+})} \\
    &= \frac{0.95 \cdot 0.03}{\P\left(T ^ {+} \mmid D ^ {+}\right)\P(D ^ {+}) + \P\left(T ^ {+} \mmid D ^ {-}\right)\P(D ^ {-})} \\
    &= \frac{0.95 \cdot 0.03}{0.95 \cdot 0.03 + 0.015 \cdot (1 - 0.03)} \\
    &= \frac{0.0285}{0.0285 + 0.01455} \\
    &= 0.662.
\end{align*}
So given a positive result,
you now have a $66.2\%$ chance of having Covid-19,
this is much higher.

Note that the route of having the disease contributes a lot more to the denominator of Bayes theorem than the route of not having the disease.

\subsection{More Advanced Bayesian Calculations}

\subsubsection{Performing Further Tests}
Consider the August $2020$ UK example above.
If you had tested positive you still only had a $1.56\%$ chance of having Covid-19.

Naturally,
you would take the test again.
You do this and receive a second positive test,
what is your probability now?

We denote two positive tests as $T ^ {++}$,
but now we have to consider the nature of the test further.

A possible assumption is that the test results are conditionally independent given disease status,
implying that
\[
\P\left(T ^ {++} \mmid D ^ {+}\right) = \P\left(T ^ {+} \mmid D ^ {+}\right) ^ 2\qquad\text{and}\qquad\P\left(T ^ {++} \mmid D ^ {-}\right) = \P\left(T ^ {+} \mmid D ^ {-}\right) ^ 2.
\]
This might be reasonable if the test fails independently each time,
i.e. due to reasons entirely unrelated to the specific attributes of the patient.

The other extreme is that the test fails precisely due to the specifics of the patient,
and will fail the same way again and again,
leading to $\P\left(T ^ {++} \mmid D ^ {+}\right) = \P\left(T ^ {+}\mmid D ^ {+}\right)$ etc.

\begin{example}[Covid-19 Test Example: Two Positive Test Results]
    We are again in August $2020$ UK and assume the same prior as before:
    $\P(D ^ {+}) = 0.00025$.

    But now we want the probability of Covid-19 given two positive test results $T ^ {++}$:
    \begin{align*}
        \P\left(D ^ {+} \mmid T ^ {++}\right) &= \frac{\P\left(T ^ {++} \mmid D ^ {+}\right)\P(D ^ {+})}{\P(T ^ {++})} \\
        &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right) ^ 2 \cdot 0.0025}{\P\left(T ^ {++} \mmid D ^ {+}\right)\P(D ^ {+}) + \P\left(T ^ {++} \mmid D ^ {-}\right)\P(D ^ {-})} \\
        &= \frac{\P\left(T ^ {+} \mmid D ^ {+}\right) ^ 2 \cdot 0.0025}{\P\left(T ^ {+} \mmid D ^ {+}\right) ^ 2\P(D ^ {+}) + \P\left(T ^ {+} \mmid D ^ {-}\right) ^ 2\P(D ^ {-})} \\
        &= \frac{0.95 ^ 2 \cdot 0.0025}{0.95 ^ 2 \cdot 0.00025 + (0.015) ^ 2 \cdot (1 - 0.00025)} \\
        &= 0.501.
    \end{align*}
    Under the conditional independence assumption,
    your posterior probability after two tests is now $50.1\%$:
    much higher than the $1.56\%$ from a single positive test.
\end{example}

\subsubsection{A Model for Sequential Learning}

Sequential learning occurs when we do two tests in a row.
Receiving one positive result,
then a second we had:
\begin{align*}
    \P(D ^ {+}) &\rightarrow \P\left(D ^ {+} \mmid T ^ {+}\right) &\rightarrow \P\left(D ^ {+} \mmid T ^ {++}\right) \\
    0.00025 &\rightarrow 0.0156 &\rightarrow 0.501
\end{align*}
that is,
as more information comes in,
we update our beliefs about the probability of $D ^ {+}$ again and again.

When we calculated the two test result we actually went straight from
\begin{align*}
    \P(D ^ {+}) &\rightarrow \P\left(D ^ {+} \mmid T ^ {++}\right) \\
    0.00025 &\rightarrow 0.501.
\end{align*}
Bayes theorem is consistent in the sense that we can update sequentially by one $T ^ {+}$ then by another $T ^ {+}$,
or all at once
(i.e. by $T ^ {++}$)
and we are guaranteed to obtain the same result.

\begin{example}
    Label the first positive test as $T_1 ^ {+}$
    (occurring on day $1$)
    and the second test as $T_2 ^ {+}$
    (occurring on day $2$).
    If,
    after seeing a single positive result $T_1 ^ {+}$ we updated to get posterior $\P\left(D ^ {+} \mmid T_1 ^ {+}\right)$ and trivially also $\P\left(D ^ {-} \mmid T_1 ^ {+}\right)$,
    then we could treat these posteriors as our priors for day $2$.
    Hence on day $2$ we could update using the second positive test $T_2 ^ {+}$ using Bayes theorem
    (with $T ^ {++} = \{T_1 ^ {+}, T_2 ^ {+}\}$):
    \begin{align*}
        \P\left(D ^ {+} \mmid T ^ {++}\right) = \P\left(D ^ {+} \mmid T_1 ^ {+}, T_2 ^ {+}\right) &= \frac{\P\left(T_2 ^ {+} \mmid D ^ {+}, T_1 ^ {+}\right)\P\left(D ^ {+} \mmid T_1 ^ {+}\right)}{\P\left(D ^ {+} \mmid T_2 ^ {+}\right)} \\
        &= \frac{\P\left(T_2 ^ {+} \mmid D ^ {+}, T_1 ^ {+}\right)\P\left(D ^ {+} \mmid T_1 ^ {+}\right)}{\P\left(T_2 ^ {+} \mmid D ^ {+}, T_1 ^ {+}\right)\P\left(D ^ {+} \mmid T_1 ^ {+}\right) + \P\left(T_2 ^ {+} \mmid D ^ {-}, T_1 ^ {+}\right)\P\left(D ^ {-} \mmid T_1 ^ {+}\right)} \\
        &= \frac{\P\left(T_2 ^ {+} \mmid D ^ {+}, T_1 ^ {+}\right)\P\left(D ^ {+} \mmid T_1 ^ {+}\right)}{\P\left(T_2 ^ {+} \mmid D ^ {+}\right)\P\left(D ^ {+} \mmid T_1 ^ {+}\right) + \P\left(T_2 ^ {+} \mmid D ^ {-}\right)\P\left(D ^ {-} \mmid T_1 ^ {+}\right)}
    \end{align*}
    where we have used the conditional independence assumption to simplify terms such as $\P\left(T_2 ^ {+} \mmid D ^ {+}, T_1 ^ {+}\right) = \P\left(T_2 ^ {+} \mmid D ^ {+}\right)$.
\end{example}

\subsubsection{Sensitivity of the Posterior \texorpdfstring{$\P\left(D ^ {+} \mmid T ^ {+}\right)$}{} to the Sensitivity and Specificity}

We can perform a simple sensitivity analysis by examining the derivative of the posterior with respect to the sensitivity and the specificity,
keeping all other things constant when performing the derivative.
\[
\text{Sensitivity:}\quad\frac{d\P\left(D ^ {+} \mmid T ^ {+}\right)}{d\P\left(T ^ {+} \mmid D ^ {+}\right)}\qquad\text{Specificity:}\quad\frac{d\P\left(D ^ {+} \mmid T ^ {+}\right)}{d\P\left(T ^ {-} \mmid D ^ {-}\right)}
\]
For example,
denoting the specificity as $x = \P\left(T ^ {-} \mmid D ^ {-}\right)$,
then we have
\begin{align*}
    \P\left(D ^ {+} \mmid T ^ {+}\right) &= \frac{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+})}{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+}) + (1 - x)\P(D ^ {-})} \\
    \frac{d\P\left(D ^ {+} \mmid T ^ {+}\right)}{d\P\left(T ^ {-} \mmid D ^ {-}\right)} = \frac{d}{dx}\cP{D ^ {+}}{T ^ {+}} &= \frac{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+})\P(D ^ {-})}{[\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+}) + (1 - x)\P(D ^ {-})] ^ 2}.
\end{align*}

\subsubsection{Odds and the Likelihood Ratio}
\begin{definition}[Prior and posterior odds and the likelihood ratio]
    Prior odds in favour of $A = \frac{\P(A)}{\P(A ^ c)}$.

    Posterior odds in favour of $A$ given $B = \frac{\cP{A}{B}}{\cP{A ^ c}{B}}$.

    Likelihood ratio for $B$ given $A = \frac{\cP{B}{A}}{\cP{B}{A ^ c}}$.

    It is simple to show that
    \[
    \text{posterior odds} = \text{prior odds} \times \text{likelihood ratio}.
    \]
\end{definition}

The likelihood ratio can be interpreted as a factor which determines whether the posterior odds are smaller
(likelihood ratio $< 1$)
or larger
(likelihood ratio $> 1$)
than the prior odds,
and by how much.

\newpage

\section{US Presidential Election Polling via Frequentist Methods}

\subsection{Random Variables,
Expectation and Variance:
Revision and Notation}

\subsubsection{Discrete Random Variables}
A discrete random variable $X$ is a variable which can take a finite set of different numerical values $x_1, x_2, \dotsc, \in \mathcal{X}$.

The probability mass function is
\[
f(x) = \P(X = x),\qquad\forall x \in \mathcal{X}.
\]

Given
\[
f\left(x \mmid \lambda\right) = \P(X = x) = \begin{cases}
    \frac{e ^ {-\lambda}\lambda ^ x}{x!} & \text{for } x = 0, 1, 2, \dotsc \\
    0 & \text{otherwise}.
\end{cases}
\]
We write $f\left(x \mmid \lambda\right)$ for the $f$ of $x$ given $\lambda$.

\subsubsection{Expectation and Variance of a Discrete Random Variable}
The expected value of a discrete random variable $X$ is defined as
\[
\E(X) = \sum_{x \in \mathcal{X}}x\P(X = x) = \sum_{x \in \mathcal{X}}xf(x)
\]
that is,
we take a weighted average of $X$.

The expected value of any function $h(X)$ of $X$ is defined to be
\[
\E(h(X)) = \sum_{x \in \mathcal{X}}h(x)\P(X = x) = \sum_{x \in X}h(x)f(x).
\]

The variance of $X$ is defined to be
\begin{align*}
    \Var(X) &= \E((X - \E(X)) ^ 2) \\
    &= \E(X ^ 2) - \E(X) ^ 2.
\end{align*}
This measures the theoretical average of the distance of the values of the random variable $X$ from its central value as measured by the expected value.
$\Var(X)$ measures the spread of values of a random variable.

The standard deviation of a random variable is just the square root of the variance
\[
\mathrm{SD}(X) = \sqrt{\Var(X)}.
\]

\subsubsection{Continuous Random Variables}
The probability density function $f$ of a continuous random variable $X$ describes the probability density of each value,
defined as follows
\begin{align*}
    \P(x \leq X \leq x + dx) &= f(x)\,dx \\
    \P(a \leq X \leq b) &= \int_{a}^{b}f(x)\,dx.
\end{align*}

\subsubsection{Expectation and Variance of a Continuous Random Variable}
The expectation,
variance and standard deviation of a continuous random variable $X$ with probability density function $f(x)$ are:
\begin{align*}
    \E(X) &= \int xf(x)\,dx \\
    \Var(X) &= \E((X - \E(X)) ^ 2) \\
    &= \E(X ^ 2) - (\E(X)) ^ 2 &\left(\text{with } \E(X ^ 2) = \int x ^ 2f(x)\,dx\right) \\
    \mathrm{SD}(X) = \sqrt{\Var(X)}
\end{align*}
where integration is over the full range of values of $X$.

\subsubsection{Properties of Expectations and Variances}
\textbf{Change in scale and location}

For any random variable $X$ and any two real numbers $a, b$.
\begin{align*}
    \E(aX + b) &= a\E(X) + b \\
    \Var(aX + b) &= a ^ 2\Var(X) \\
    \mathrm{SD}(aX + b) &= a\mathrm{SD}(X).
\end{align*}

\textbf{Sums and differences}

For any two random variables $X, Y$ and any two real numbers $a, b$.
\begin{align*}
    \E(aX + bY) &= a\E(X) + b\E(Y). \\
    \intertext{If $X$ and $Y$ are independent} \\
    \Var(aX + bY) &= a ^ 2\Var(X) + b ^ 2\Var(Y). \\
    \intertext{If $X$ and $Y$ are not independent} \\
    \Var(aX + bY) &= a ^ 2\Var(X) + b ^ 2\Var(Y) + 2ab\Cov(X, Y).
\end{align*}
These rules can be extended to sums of several random variables.

Consider any $n$ random variables $X_1, X_2, \dotsc, X_n$ then
\[
\E(X_1 + X_2 + \dotsi + X_n) = \E(X_1) + \E(X_2) + \dotsi + \E(X_n)
\]
additionally if all $X_i$ are independent of each other then
\[
\Var(X_1 + X_2 + \dotsi + X_n) = \Var(X_1) + \Var(X_2) + \dotsi + \Var(X_n).
\]

















\end{document}