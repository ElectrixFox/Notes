\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\title{Statistics I \\
    \large Important Results}
\author{Luke Phillips}
\date{April 2025}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Basic results}

\begin{theorem}[Bayes theorem]
    \[
    \cP{A}{B} = \frac{\cP{B}{A}\P(A)}{\P(B)}.
    \]
\end{theorem}

\begin{theorem}[Partition theorem]
    \[
    \P(A) = \cP{A}{E_1}\P(E_1) + \cP{A}{E_2}\P(E_2) + \cP{A}{E_3}\P(E_3) + \dotsc + \cP{A}{E_n}\P(E_n).
    \]
\end{theorem}

\section{Disease testing}

\textbf{Important definitions}
\begin{enumerate}[label = (\roman*)]
    \item Sensitivity of a test:
    \[
    \cP{T ^ {+}}{D ^ {+}}.
    \]
    
    \item Specificity of a test:
    \[
    \cP{T ^ {-}}{D ^ {-}}.
    \]
    
    \item Probability of a false positive:
    \[
    \cP{T ^ {+}}{D ^ {-}}.
    \]
    
    \item Probability of a false negative:
    \[
    \cP{T ^ {-}}{D ^ {+}}.
    \]
\end{enumerate}

\begin{enumerate}[label = (\roman*)]
    \item $\P(D ^ {+})$ is the prior probability of disease.
    
    \item $\cP{T ^ {+}}{D ^ {+}}$ and $\cP{T ^ {+}}{D ^ {-}}$ is the likelihood of a positive test given disease status.
    
    \item $\cP{D ^ {+}}{T ^ {+}}$ is the posterior probability of disease given positive test.
\end{enumerate}

If the test results are conditionally independent then we have
\[
\cP{T ^ {++}}{D ^ {+}} = \cP{T ^ {+}}{D ^ {+}} ^ 2\quad\text{and}\quad\cP{T ^ {++}}{D ^ {-}} = \cP{T ^ {+}}{D ^ {-}} ^ 2.
\]

\newpage

\section{Random variables}
The normal distribution $X \sim N(\mu, \sigma ^ 2)$ is given by its pdf
\[
f(x\mid \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma ^ 2}}e ^ {-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right) ^ 2}.
\]

\begin{definition}[Bernoulli distribution]
    $X \sim \mathrm{Bernoulli}(p)$ takes only $X = 0, 1$ with pdf
    \[
    f(x\mid p) = \begin{cases}
        p ^ x(1 - p) ^ {1 - x} &\text{for } x = 0, 1, \\
        0 & \text{otherwise}.
    \end{cases}
    \]
\end{definition}

Looking at an estimator for $p$.
We can set $X = \sum_{i = 1}^{n}X_i$ with $X_i$ being $1, 0$ if $i$ is success or failure.
We have
\[
f(x_1, x_2, \dotsc, x_n\mid p) = \prod_{i = 1}^{n}f(x_i\mid p) = p ^ x(1 - p) ^ {n - x}
\]
with $x = \sum_{i = 1}^{n}x_i$.

We can then take this as our likelihood
\[
\ell(\theta) = f(x_1, x_2, \dotsc, x_n\mid p) = p ^ x(1 - p) ^ {n - x}
\]
with $\theta = p$.

Logging and differentiating gives us a maximum for $p$
($\theta$)
\[
\pd{p}\log{f(x_1, \dotsc, x_n\mid p)} = \frac{x}{p} - \frac{n - x}{1 - p}.
\]
Now setting this to zero and solving for our maximal $p$,
which we will denote $\hat{p}$,
this gets
\[
\hat{p} = \frac{x}{n}.
\]

$\hat{p}$ is also called the sample proportion $Y$,
in terms of random variables this is
\[
\hat{p} = Y = \frac{X}{n} = \frac{1}{n}\sum_{i = 1}^{n}X_i.
\]

\begin{definition}[Binomial distribution]
    $X \sim \Bin(n, p)$ has probability mass function
    \[
    f(x\mid n, p) = \begin{cases}
        \binom{n}{x}p ^ x(1 - p) ^ {n - x} &\text{for } x = 0, 1, \dotsc, n, \\
        0 &\text{otherwise}.
    \end{cases}
    \]
\end{definition}

\subsection{Binomial-Normal approximation}
\[
\Bin(n, p) \approx N(np, np(1 - p))
\]
when $np \geq 10$ and $n(1 - p) \geq 10$ this is acceptable.

For $n$ small a correction is needed,
$X \sim \Bin(n, p)$,
$X' \sim N(np, np(1 - p))$,
then
\[
\P(X \leq k) \simeq \P(X' \leq k + 1 / 2)\quad\P(k_1 \leq X \leq k_2) \simeq \P(k_1 - 1 / 2 \leq X' \leq k_2 + 1 / 2).
\]

\subsection{Estimation \texorpdfstring{$\hat{p}$}{} for binomial}
We can find the expectation and variance of $\hat{p}$ for Binomial
\begin{align*}
    \E(\hat{p}) &= \E(Y) = \frac{1}{n}\E(X) = \frac{1}{n}np = p \\
    \Var(\hat{p}) &= \Var(Y) = \frac{1}{n ^ 2}\Var(X) = \frac{1}{n ^ 2}np(1 - p) = \frac{p(1 - p)}{n}.
\end{align*}

Since $\E(\hat{p}) = p$ we have an \textbf{unbiased estimator} for $p$.

\subsection{Margin of error}

The margin of error is defined as $Y \pm 2\mathrm{SD}(Y)$ which may look like
\[
Y \pm 2\sqrt{\frac{p(1 - p)}{n}}
\]
that can be shown to be
\[
Y \pm 2\sqrt{\frac{p(1 - p)}{n}} \iff Y \pm 2\sqrt{\frac{\frac{1}{2}\left(1 - \frac{1}{2}\right)}{n}} \iff Y \pm \frac{1}{\sqrt{n}}.
\]

\newpage

\section{Estimator for population mean}
Have $\bar{X}$ the sample mean of $X_1, \dotsc, X_n$,
i.e.
\[
\bar{X} = \frac{1}{n}\sum_{i = 1}^{n}X_i.
\]
Taking $\mu = \E(X_i)$ and $\sigma = \mathrm{SD}(X_i)$.

So
\begin{align*}
    \E(\bar{X}) &= \E\left(\frac{1}{n}\sum_{i = 1}^{n}X_i\right) = \frac{1}{n}\sum_{i = 1}^{n}\E(X_i) = \frac{1}{n}\sum_{i = 1}^{n}\mu = \frac{1}{n}n\mu = \mu. \\
    \Var(\bar{X}) &= \Var\left(\frac{1}{n}\sum_{i = 1}^{n}X_i\right) = \frac{1}{n ^ 2}\Var\left(\sum_{i = 1}^{n}X_i\right) = \frac{1}{n ^ 2}\sum_{i = 1}^{n}\Var(X_i) = \frac{1}{n ^ 2}n\sigma ^ 2 = \frac{\sigma ^ 2}{n}.
\end{align*}
Since $\E(\bar{X}) = \mu$ $\bar{X}$ is unbiased.

\subsection{Central limit theorem}
\begin{theorem}[Central limit theorem]
    Suppose $X_1, \dotsc, X_n$ are independent and identically distributed random variables,
    each with mean $\mu$ and variance $\sigma ^ 2$.

    The distribution of the sample mean $\bar{X} = \frac{1}{n}\sum_{i}X_i$ is such that
    \[
    \bar{X} \to N\left(\mu, \frac{\sigma ^ 2}{n}\right)\quad\text{as } n \to \infty.
    \]
\end{theorem}

\newpage

\section{Confidence intervals}

To find the confidence interval for $\mu$ we have these cases:
\begin{enumerate}[label = (\roman*)]
    \item 
    $\sigma$ is known:
    $n$ large or Normal population
    (and $n$ small)
    \[
    \bar{x} \pm z ^ {*}\frac{\sigma}{\sqrt{n}}.
    \]

    \item
    $\sigma$ is unknown:
    $n$ small and population Normal
    \[
    \bar{x} \pm t_{n - 1} ^ {*}\frac{s}{\sqrt{n}}.
    \]

    $n$ large
    \[
    \bar{x} \pm z ^ {*}\frac{s}{\sqrt{n}}.
    \]
\end{enumerate}

\subsection{Hypothesis testing using confidence intervals}

\textbf{Two-sided case}

Given $H_0 : \theta = r$ and $H_a : \theta \neq r$ and a significance level $\alpha$,
we construct a $1 - \alpha$ confidence interval for $\theta$,
then we reject $H_0$ if $r$ lies outside the confidence interval,
then the test is statistically significant at the $\alpha\%$ level of significance.

Otherwise we fail to reject $H_0$ and the test is not significant at the $\alpha\%$ significance level.

\textbf{One-sided case}

Given $H_0 : \theta = r$,
$H_a : \theta < r$
(or $H_a : \theta > r$)
and a level of significance $\alpha$;
we can construct a $1 - 2\alpha$ confidence interval for $\theta$,
and we can reject $H_0$ if $r$ falls to the right
(left)
of the confidence interval,
otherwise we fail to reject $H_0$.

\subsection{Hypothesis testing using \texorpdfstring{$p$}{}-values}

For a one-sided test the $p$-value is the probability of being in the tail.
In a two-sided test it is the probability of being in both tails,
so we can just double the probability of being in one of the tails
(since it is symmetric).

\subsection{Types of test}
Type I error:
\[
\cP{\text{reject $H_0$}}{\text{$H_0$ true}}.
\]

Type II error:
\[
\cP{\text{not reject $H_0$}}{\text{$H_0$ false}}.
\]

The significance level $\alpha$ of a test is $\cP{\text{type I error}}{H_0}$.

The power of a test is $1 - \cP{\text{type II error}}{H_a}$.

\newpage

\section{Bayesian}

We have
\[
f(\theta\mid x) = \frac{f(x\mid \theta)f(\theta)}{f(x)}
\]
which is equivalent to
\[
\text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{Data probability}}
\]

\newpage

\section{Methods}

\subsection{Disease testing}

Given a test group and data in the following form
\begin{table}[H]
    \centering
    \begin{tabular}{c|cc|c}
        & $D ^ {+}$ & $D ^ {-}$ & \text{Total}  \\
        \hline
        $T ^ {+}$ & $t_{11}$ & $t_{12}$ & $t_{11} + t_{12}$ \\
        $T ^ {-}$ & $t_{21}$ & $t_{22}$ & $t_{21} + t_{22}$ \\
        \hline
        \text{Total} & $t_{11} + t_{21}$ & $t_{12} + t_{22}$
    \end{tabular}
\end{table}

We can define the sensitivity of a test as:
the \textbf{probability} of testing \textbf{positive} given \textbf{has disease}:
\[
\cP{T ^ {+}}{D ^ {+}}.
\]
Similarly,
the specificity can be defined as:
the probability of testing \textbf{negative} given \textbf{doesn't have disease}:
\[
\cP{T ^ {-}}{D ^ {-}}.
\]

The false positive probability can be defined as
\[
\cP{T ^ {+}}{D ^ {-}}
\]
similarly for the false negative:
\[
\cP{T ^ {-}}{D ^ {+}}.
\]

We can calculate the probability of false positive then we would need
\[
\cP{D ^ {+}}{T ^ {+}} = \frac{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+})}{\P(T ^ {+})} = \frac{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+})}{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+}) + \cP{T ^ {+}}{D ^ {-}}\P(D ^ {-})}.
\]
When we have conditionally independent test results given disease status,
we can say $\cP{T ^ {n+}}{D ^ {+}} = \cP{T ^ {+}}{D ^ {+}} ^ n$ similarly for $\cP{T ^ {n+}}{D ^ {-}}$.
Then we would simply repeat the previous calculation to find the probability of $\cP{D ^ {+}}{T ^ {++}}$.

\subsection{Sets of Bernoulli trials}
Say $n$ Bernoulli trials $X_1, \dotsc, X_n$ are performed with probability $p$ of success.
Let $X = \sum_{i = 1}^{n}X_i$.

The \textbf{distribution} of $X$ is $X \sim \Bin(n, p)$.

We can find the \textbf{likelihood} as a function of $p$ by
\[
\ell(p) = f(x\mid p) = \binom{n}{x}p ^ x(1 - p) ^ {n - x}.
\]

To find the \textbf{maximum likelihood estimate} of $p$ given $n = 20$ and $x = 5$ then we would use the log likelihood,
i.e.
\[
\mathcal{L}(p) = \log{\ell(p)} = \log{\binom{n}{x}} + x\log{p} + (n - x)\log(1 - p)
\]
which for $p_{MLE}$ we would need
\[
0 = \pd[\mathcal{L}]{p} = \pd{p}\left(\log{\binom{n}{x}} + x\log{p} + (n - x)\log(1 - p)\right) = \frac{x}{\hat{p}} - \frac{n - x}{1 - \hat{p}}
\]
which is equivalent to
\[
\frac{x}{\hat{p}} = \frac{n - x}{1 - \hat{p}} \iff \hat{p} = \frac{x}{n}
\]
in our case this would be $\hat{p} = \frac{1}{4}$.


Given a prior with distribution $p \sim \mathrm{Beta}(a, b)$,
in order to derive a posterior pdf we would need
\[
f(p\mid x) \propto f(x\mid p)f(p)
\]
(using Bayes theorem)
then
\[
f(p \mid x) \propto \binom{n}{x}p ^ x(1 - p) ^ {n - x}\times\frac{1}{B(a, b)}p ^ {a - 1}(1 - p) ^ {b - 1}\propto p ^ {a + x - 1}(1 - p) ^ {b + n - x - 1}
\]
giving us a posterior distribution:
$\mathrm{Beta}(a + x, b + n - x)$.


\subsection{Making quantile plots}

To make a quantile plot find the data $x_k$ then find $\Phi(z_k) = \frac{k}{k + 1}$ and what $z_k$ would approximately be,
this gives $(z_k, x_k)$ as the coordinate for the plot.


\subsection{Finding the likelihood from a pdf}
Given a continuous random quantity $X$ with pdf $f(x\mid \alpha)$.

We will use the case $f(x\mid \alpha) = 4\alpha ^ 4x ^ 3e ^ {-(\alpha x) ^ 4}$ with $\alpha > 0$.

If we assume $n$ i.i.d. observations $x_1, \dotsc, x_n$ are sampled from this distribution.
Derive the likelihood function for $\alpha$ based on these data.

We simply use
\[
\ell(\alpha\mid x_1, \dotsc, x_n) = \prod_{i = 1}^{n}f(x_i\mid \alpha).
\]

To find a sufficient statistic for estimating $\alpha$ we can use the factorisation theorem and rewrite the $\ell(\alpha\mid x_1, \dotsc, x_n)$ in terms
\[
g(T, \alpha)h(x_1, \dotsc, x_n)
\]
with $T = \sum_{i = 1}^{n}x_i ^ 4$ in our example.
Thus $T = \sum_{i = 1}^{n}x_i ^ 4$ and $n$ are sufficient statistics for $\alpha$.

To find the maximum likelihood estimate of $\alpha$ we can log the likelihood function and solve for when $\pd[\mathcal{L}]{\alpha} = 0$.
\textbf{Don't forget to show it is a maximum by differentiating again and showing it is $< 0$}.

To show that for general data $x_1, \dotsc, x_n$ conjugate priors for $\alpha$ can be specified using the probability density function
\[
f(a) \propto g(a)
\]
all we need to show is that we can write $g(a)$ in the form of $f(a)$,
in our example this would be
\begin{align*}
    f(\alpha\mid x_1, \dotsc, x_n) &\propto f(x_1, \dotsc, x_n\mid \alpha)f(\alpha) \\
    &\propto \ell(\alpha\mid x_1, \dotsc, x_n)f(\alpha) \\
    &\propto \alpha ^ {4n + b\nu}e ^ {-(T\alpha ^ 4 + \tau\alpha ^ b)}
\end{align*}
so for our example $b = 4$.

The parameter $\nu$ would be $y_1, \dotsc, y_{\nu}$ of prior sample and $\tau = \sum_{i = 1}^{\nu}y_i ^ 4$.

\subsection{Finding credible intervals with i.i.d's}

Let $X_1, \dotsc, X_n$ be an i.i.d. sample of size $n$ from a $N(\mu, 1 / \tau)$ distribution,
where the precision $\tau = 1 / \sigma ^ 2$ is assumed known.

If the prior for $\mu$ is given a normal distribution such that $\mu \sim N(m, 1 / t)$,
the posterior for $\mu$ will also be normal with
\begin{align*}
    \mu&\mid x_1, \dotsc, x_n \sim N\left(m_1, \frac{1}{t_1}\right), \\
    \text{where}\quad t_1 &= t + n\tau,\quad\text{and}\qquad m_1 = \frac{tm + n\tau\bar{x}}{t_1}.
\end{align*}

If we want to find the posterior distribution for $\mu$ we would do the following:

Obtain the prior precision using $t = \frac{1}{v ^ 2}$ and $\mu \sim N(m, v ^ 2)$.
Then we can use the given information to work out the precision $\tau$ to get our conjugate prior.
The posterior for $\mu\mid x_1, \dotsc, x_n$ will be given by
\[
\mu\mid x_1, \dotsc, x_n \sim N\left(m_1, v_1 ^ 2 = \frac{1}{t_1}\right)
\]
where $t_1 = t + n\tau$,
and $m_1 = \frac{tm + n\tau\bar{x}}{t_1}$.

Since the posterior is normal it is symmetric and therefore the EQT and HPD will be equal.

We can fine the $95\%$ EQT credible interval by
\[
m \pm z_{\alpha / 2}v.
\]









\end{document}