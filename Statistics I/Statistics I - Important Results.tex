\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\title{Statistics I \\
    \large Important Results}
\author{Luke Phillips}
\date{April 2025}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Basic results}

\begin{theorem}[Bayes theorem]
    \[
    \cP{A}{B} = \frac{\cP{B}{A}\P(A)}{\P(B)}.
    \]
\end{theorem}

\begin{theorem}[Partition theorem]
    \[
    \P(A) = \cP{A}{E_1}\P(E_1) + \cP{A}{E_2}\P(E_2) + \cP{A}{E_3}\P(E_3) + \dotsc + \cP{A}{E_n}\P(E_n).
    \]
\end{theorem}

\section{Disease testing}

\textbf{Important definitions}
\begin{enumerate}[label = (\roman*)]
    \item Sensitivity of a test:
    \[
    \cP{T ^ {+}}{D ^ {+}}.
    \]
    
    \item Specificity of a test:
    \[
    \cP{T ^ {-}}{D ^ {-}}.
    \]
    
    \item Probability of a false positive:
    \[
    \cP{T ^ {+}}{D ^ {-}}.
    \]
    
    \item Probability of a false negative:
    \[
    \cP{T ^ {-}}{D ^ {+}}.
    \]
\end{enumerate}

\begin{enumerate}[label = (\roman*)]
    \item $\P(D ^ {+})$ is the prior probability of disease.
    
    \item $\cP{T ^ {+}}{D ^ {+}}$ and $\cP{T ^ {+}}{D ^ {-}}$ is the likelihood of a positive test given disease status.
    
    \item $\cP{D ^ {+}}{T ^ {+}}$ is the posterior probability of disease given positive test.
\end{enumerate}

If the test results are conditionally independent then we have
\[
\cP{T ^ {++}}{D ^ {+}} = \cP{T ^ {+}}{D ^ {+}} ^ 2\quad\text{and}\quad\cP{T ^ {++}}{D ^ {-}} = \cP{T ^ {+}}{D ^ {-}} ^ 2.
\]

We have the important formula
\[
\text{posterior odds} \times \text{prior odds} \times \text{likelihood ratio}
\]
which corresponds to
\[
\frac{\cP{A}{B}}{\cP{A ^ c}{B}} = \frac{\P(A)}{\P(A ^ c)}\times\frac{\cP{B}{A}}{\cP{B}{A ^ c}}.
\]

\newpage

\section{Random variables}
The normal distribution $X \sim N(\mu, \sigma ^ 2)$ is given by its pdf
\[
f(x\mid \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma ^ 2}}e ^ {-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right) ^ 2}.
\]

\begin{definition}[Bernoulli distribution]
    $X \sim \mathrm{Bernoulli}(p)$ takes only $X = 0, 1$ with pdf
    \[
    f(x\mid p) = \begin{cases}
        p ^ x(1 - p) ^ {1 - x} &\text{for } x = 0, 1, \\
        0 & \text{otherwise}.
    \end{cases}
    \]
\end{definition}

Looking at an estimator for $p$.
We can set $X = \sum_{i = 1}^{n}X_i$ with $X_i$ being $1, 0$ if $i$ is success or failure.
We have
\[
f(x_1, x_2, \dotsc, x_n\mid p) = \prod_{i = 1}^{n}f(x_i\mid p) = p ^ x(1 - p) ^ {n - x}
\]
with $x = \sum_{i = 1}^{n}x_i$.

We can then take this as our likelihood
\[
\ell(\theta) = f(x_1, x_2, \dotsc, x_n\mid p) = p ^ x(1 - p) ^ {n - x}
\]
with $\theta = p$.

Logging and differentiating gives us a maximum for $p$
($\theta$)
\[
\pd{p}\log{f(x_1, \dotsc, x_n\mid p)} = \frac{x}{p} - \frac{n - x}{1 - p}.
\]
Now setting this to zero and solving for our maximal $p$,
which we will denote $\hat{p}$,
this gets
\[
\hat{p} = \frac{x}{n}.
\]

$\hat{p}$ is also called the sample proportion $Y$,
in terms of random variables this is
\[
\hat{p} = Y = \frac{X}{n} = \frac{1}{n}\sum_{i = 1}^{n}X_i.
\]

\begin{definition}[Binomial distribution]
    $X \sim \Bin(n, p)$ has probability mass function
    \[
    f(x\mid n, p) = \begin{cases}
        \binom{n}{x}p ^ x(1 - p) ^ {n - x} &\text{for } x = 0, 1, \dotsc, n, \\
        0 &\text{otherwise}.
    \end{cases}
    \]
\end{definition}

\subsection{Binomial-Normal approximation}
\[
\Bin(n, p) \approx N(np, np(1 - p))
\]
when $np \geq 10$ and $n(1 - p) \geq 10$ this is acceptable.

For $n$ small a correction is needed,
$X \sim \Bin(n, p)$,
$X' \sim N(np, np(1 - p))$,
then
\[
\P(X \leq k) \simeq \P(X' \leq k + 1 / 2)\quad\P(k_1 \leq X \leq k_2) \simeq \P(k_1 - 1 / 2 \leq X' \leq k_2 + 1 / 2).
\]

\subsection{Estimation \texorpdfstring{$\hat{p}$}{} for binomial}
We can find the expectation and variance of $\hat{p}$ for Binomial
\begin{align*}
    \E(\hat{p}) &= \E(Y) = \frac{1}{n}\E(X) = \frac{1}{n}np = p \\
    \Var(\hat{p}) &= \Var(Y) = \frac{1}{n ^ 2}\Var(X) = \frac{1}{n ^ 2}np(1 - p) = \frac{p(1 - p)}{n}.
\end{align*}

Since $\E(\hat{p}) = p$ we have an \textbf{unbiased estimator} for $p$.

\subsection{Margin of error}

The margin of error is defined as $Y \pm 2\mathrm{SD}(Y)$ which may look like
\[
Y \pm 2\sqrt{\frac{p(1 - p)}{n}}
\]
that can be shown to be
\[
Y \pm 2\sqrt{\frac{p(1 - p)}{n}} \iff Y \pm 2\sqrt{\frac{\frac{1}{2}\left(1 - \frac{1}{2}\right)}{n}} \iff Y \pm \frac{1}{\sqrt{n}}.
\]

\newpage

\section{Estimator for population mean}
Have $\bar{X}$ the sample mean of $X_1, \dotsc, X_n$,
i.e.
\[
\bar{X} = \frac{1}{n}\sum_{i = 1}^{n}X_i.
\]
Taking $\mu = \E(X_i)$ and $\sigma = \mathrm{SD}(X_i)$.

So
\begin{align*}
    \E(\bar{X}) &= \E\left(\frac{1}{n}\sum_{i = 1}^{n}X_i\right) = \frac{1}{n}\sum_{i = 1}^{n}\E(X_i) = \frac{1}{n}\sum_{i = 1}^{n}\mu = \frac{1}{n}n\mu = \mu. \\
    \Var(\bar{X}) &= \Var\left(\frac{1}{n}\sum_{i = 1}^{n}X_i\right) = \frac{1}{n ^ 2}\Var\left(\sum_{i = 1}^{n}X_i\right) = \frac{1}{n ^ 2}\sum_{i = 1}^{n}\Var(X_i) = \frac{1}{n ^ 2}n\sigma ^ 2 = \frac{\sigma ^ 2}{n}.
\end{align*}
Since $\E(\bar{X}) = \mu$ $\bar{X}$ is unbiased.

\subsection{Central limit theorem}
\begin{theorem}[Central limit theorem]
    Suppose $X_1, \dotsc, X_n$ are independent and identically distributed random variables,
    each with mean $\mu$ and variance $\sigma ^ 2$.

    The distribution of the sample mean $\bar{X} = \frac{1}{n}\sum_{i}X_i$ is such that
    \[
    \bar{X} \to N\left(\mu, \frac{\sigma ^ 2}{n}\right)\quad\text{as } n \to \infty.
    \]
\end{theorem}

\newpage

\section{Confidence intervals}

To find the confidence interval for $\mu$ we have these cases:
\begin{enumerate}[label = (\roman*)]
    \item 
    $\sigma$ is known:
    $n$ large or Normal population
    (and $n$ small)
    \[
    \bar{x} \pm z ^ {*}\frac{\sigma}{\sqrt{n}}.
    \]

    \item
    $\sigma$ is unknown:
    $n$ small and population Normal
    \[
    \bar{x} \pm t_{n - 1} ^ {*}\frac{s}{\sqrt{n}}.
    \]

    $n$ large
    \[
    \bar{x} \pm z ^ {*}\frac{s}{\sqrt{n}}.
    \]
\end{enumerate}

\subsection{Hypothesis testing using confidence intervals}

\textbf{Two-sided case}

Given $H_0 : \theta = r$ and $H_a : \theta \neq r$ and a significance level $\alpha$,
we construct a $1 - \alpha$ confidence interval for $\theta$,
then we reject $H_0$ if $r$ lies outside the confidence interval,
then the test is statistically significant at the $\alpha\%$ level of significance.

Otherwise we fail to reject $H_0$ and the test is not significant at the $\alpha\%$ significance level.

\textbf{One-sided case}

Given $H_0 : \theta = r$,
$H_a : \theta < r$
(or $H_a : \theta > r$)
and a level of significance $\alpha$;
we can construct a $1 - 2\alpha$ confidence interval for $\theta$,
and we can reject $H_0$ if $r$ falls to the right
(left)
of the confidence interval,
otherwise we fail to reject $H_0$.

\subsection{Hypothesis testing using \texorpdfstring{$p$}{}-values}

For a one-sided test the $p$-value is the probability of being in the tail.
In a two-sided test it is the probability of being in both tails,
so we can just double the probability of being in one of the tails
(since it is symmetric).

\subsection{Types of test}
Type I error:
\[
\cP{\text{reject $H_0$}}{\text{$H_0$ true}}.
\]

Type II error:
\[
\cP{\text{not reject $H_0$}}{\text{$H_0$ false}}.
\]

The significance level $\alpha$ of a test is $\cP{\text{type I error}}{H_0}$.

The power of a test is $1 - \cP{\text{type II error}}{H_a}$.

\newpage

\section{Bayesian}

We have
\[
f(\theta\mid x) = \frac{f(x\mid \theta)f(\theta)}{f(x)}
\]
which is equivalent to
\[
\text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{Data probability}}
\]

\subsection{Credible intervals}
For the equal tailed credible interval
(EQT)
we can find this $(1 - \alpha)100\%$ interval by simply finding
\[
\P(\ell \leq u) = \frac{\alpha}{2} \text{ and } \P(\ell \geq u) = \frac{\alpha}{2}
\]
with equal probability in each tail.

For the highest posterior density interval
(HPD)
we find the region with the highest density,
i.e.
the "tallest" parts of the graph.
The HPD has probability $(1 - \alpha)100\%$,
to show that an interval is HPD we can simply show the probability in it is $(1 - \alpha)100\%$.

\subsection{Point estimates}
The posterior mode/maximum posterior density
(MAP)
is the mode of the posterior distribution.


To find if a statistic is sufficient we can use the factorisation theorem
\begin{definition}[Sufficiency]
    If we have a sample of data $x_1, \dotsc, x_n$,
    a statistic $T(x_1, \dotsc, x_n)$ is said to be sufficient for $\theta$ if any only if we can factorise the likelihood $\ell(\theta)$ in the following form:
    \[
    \ell(\theta) = f(x_1, \dotsc, x_n\mid\theta) = g(T, \theta)h(x_1, \dotsc, x_n)
    \]
    where we see that all the $\theta$ behaviour of the likelihood is tied together with $T$ through the function $g(T, \theta)$,
    but the extra behaviour of $h(x_1, \dotsc, x_n)$ that contains additional details of the data,
    is essentially irrelevant.

    Therefore,
    a sufficient statistic $T$ contains all of the information about the parameter $\theta$ that we can extract from the sample $x_1, \dotsc, x_n$.
\end{definition}

\newpage

\section{Methods}

\subsection{Disease testing}

Given a test group and data in the following form
\begin{table}[H]
    \centering
    \begin{tabular}{c|cc|c}
        & $D ^ {+}$ & $D ^ {-}$ & \text{Total}  \\
        \hline
        $T ^ {+}$ & $t_{11}$ & $t_{12}$ & $t_{11} + t_{12}$ \\
        $T ^ {-}$ & $t_{21}$ & $t_{22}$ & $t_{21} + t_{22}$ \\
        \hline
        \text{Total} & $t_{11} + t_{21}$ & $t_{12} + t_{22}$
    \end{tabular}
\end{table}

We can define the sensitivity of a test as:
the \textbf{probability} of testing \textbf{positive} given \textbf{has disease}:
\[
\cP{T ^ {+}}{D ^ {+}}.
\]
Similarly,
the specificity can be defined as:
the probability of testing \textbf{negative} given \textbf{doesn't have disease}:
\[
\cP{T ^ {-}}{D ^ {-}}.
\]

The false positive probability can be defined as
\[
\cP{T ^ {+}}{D ^ {-}}
\]
similarly for the false negative:
\[
\cP{T ^ {-}}{D ^ {+}}.
\]

We can calculate the probability of false positive then we would need
\[
\cP{D ^ {+}}{T ^ {+}} = \frac{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+})}{\P(T ^ {+})} = \frac{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+})}{\cP{T ^ {+}}{D ^ {+}}\P(D ^ {+}) + \cP{T ^ {+}}{D ^ {-}}\P(D ^ {-})}.
\]
When we have conditionally independent test results given disease status,
we can say $\cP{T ^ {n+}}{D ^ {+}} = \cP{T ^ {+}}{D ^ {+}} ^ n$ similarly for $\cP{T ^ {n+}}{D ^ {-}}$.
Then we would simply repeat the previous calculation to find the probability of $\cP{D ^ {+}}{T ^ {++}}$.

\subsection{Sets of Bernoulli trials}
Say $n$ Bernoulli trials $X_1, \dotsc, X_n$ are performed with probability $p$ of success.
Let $X = \sum_{i = 1}^{n}X_i$.

The \textbf{distribution} of $X$ is $X \sim \Bin(n, p)$.

We can find the \textbf{likelihood} as a function of $p$ by
\[
\ell(p) = f(x\mid p) = \binom{n}{x}p ^ x(1 - p) ^ {n - x}.
\]

To find the \textbf{maximum likelihood estimate} of $p$ given $n = 20$ and $x = 5$ then we would use the log likelihood,
i.e.
\[
\mathcal{L}(p) = \log{\ell(p)} = \log{\binom{n}{x}} + x\log{p} + (n - x)\log(1 - p)
\]
which for $p_{MLE}$ we would need
\[
0 = \pd[\mathcal{L}]{p} = \pd{p}\left(\log{\binom{n}{x}} + x\log{p} + (n - x)\log(1 - p)\right) = \frac{x}{\hat{p}} - \frac{n - x}{1 - \hat{p}}
\]
which is equivalent to
\[
\frac{x}{\hat{p}} = \frac{n - x}{1 - \hat{p}} \iff \hat{p} = \frac{x}{n}
\]
in our case this would be $\hat{p} = \frac{1}{4}$.


Given a prior with distribution $p \sim \mathrm{Beta}(a, b)$,
in order to derive a posterior pdf we would need
\[
f(p\mid x) \propto f(x\mid p)f(p)
\]
(using Bayes theorem)
then
\[
f(p \mid x) \propto \binom{n}{x}p ^ x(1 - p) ^ {n - x}\times\frac{1}{B(a, b)}p ^ {a - 1}(1 - p) ^ {b - 1}\propto p ^ {a + x - 1}(1 - p) ^ {b + n - x - 1}
\]
giving us a posterior distribution:
$\mathrm{Beta}(a + x, b + n - x)$.



\textbf{Using $\propto$ for finding this prior is very important}


\subsection{Making quantile plots}

To make a quantile plot find the data $x_k$ then find $\Phi(z_k) = \frac{k}{k + 1}$ and what $z_k$ would approximately be,
this gives $(z_k, x_k)$ as the coordinate for the plot.


\subsection{Finding the likelihood from a pdf}
Given a continuous random quantity $X$ with pdf $f(x\mid \alpha)$.

We will use the case $f(x\mid \alpha) = 4\alpha ^ 4x ^ 3e ^ {-(\alpha x) ^ 4}$ with $\alpha > 0$.

If we assume $n$ i.i.d. observations $x_1, \dotsc, x_n$ are sampled from this distribution.
Derive the likelihood function for $\alpha$ based on these data.

We simply use
\[
\ell(\alpha\mid x_1, \dotsc, x_n) = \prod_{i = 1}^{n}f(x_i\mid \alpha).
\]

To find a sufficient statistic for estimating $\alpha$ we can use the factorisation theorem and rewrite the $\ell(\alpha\mid x_1, \dotsc, x_n)$ in terms
\[
g(T, \alpha)h(x_1, \dotsc, x_n)
\]
with $T = \sum_{i = 1}^{n}x_i ^ 4$ in our example.
Thus $T = \sum_{i = 1}^{n}x_i ^ 4$ and $n$ are sufficient statistics for $\alpha$.

To find the maximum likelihood estimate of $\alpha$ we can log the likelihood function and solve for when $\pd[\mathcal{L}]{\alpha} = 0$.
\textbf{Don't forget to show it is a maximum by differentiating again and showing it is $< 0$}.

To show that for general data $x_1, \dotsc, x_n$ conjugate priors for $\alpha$ can be specified using the probability density function
\[
f(a) \propto g(a)
\]
all we need to show is that we can write $g(a)$ in the form of $f(a)$,
in our example this would be
\begin{align*}
    f(\alpha\mid x_1, \dotsc, x_n) &\propto f(x_1, \dotsc, x_n\mid \alpha)f(\alpha) \\
    &\propto \ell(\alpha\mid x_1, \dotsc, x_n)f(\alpha) \\
    &\propto \alpha ^ {4n + b\nu}e ^ {-(T\alpha ^ 4 + \tau\alpha ^ b)}
\end{align*}
so for our example $b = 4$.

The parameter $\nu$ would be $y_1, \dotsc, y_{\nu}$ of prior sample and $\tau = \sum_{i = 1}^{\nu}y_i ^ 4$.

\subsection{Finding credible intervals with i.i.d's}

Let $X_1, \dotsc, X_n$ be an i.i.d. sample of size $n$ from a $N(\mu, 1 / \tau)$ distribution,
where the precision $\tau = 1 / \sigma ^ 2$ is assumed known.

If the prior for $\mu$ is given a normal distribution such that $\mu \sim N(m, 1 / t)$,
the posterior for $\mu$ will also be normal with
\begin{align*}
    \mu&\mid x_1, \dotsc, x_n \sim N\left(m_1, \frac{1}{t_1}\right), \\
    \text{where}\quad t_1 &= t + n\tau,\quad\text{and}\qquad m_1 = \frac{tm + n\tau\bar{x}}{t_1}.
\end{align*}

If we want to find the posterior distribution for $\mu$ we would do the following:

Obtain the prior precision using $t = \frac{1}{v ^ 2}$ and $\mu \sim N(m, v ^ 2)$.
Then we can use the given information to work out the precision $\tau$ to get our conjugate prior.
The posterior for $\mu\mid x_1, \dotsc, x_n$ will be given by
\[
\mu\mid x_1, \dotsc, x_n \sim N\left(m_1, v_1 ^ 2 = \frac{1}{t_1}\right)
\]
where $t_1 = t + n\tau$,
and $m_1 = \frac{tm + n\tau\bar{x}}{t_1}$.

Since the posterior is normal it is symmetric and therefore the EQT and HPD will be equal.

We can fine the $95\%$ EQT credible interval by
\[
m \pm z_{\alpha / 2}v.
\]









\end{document}