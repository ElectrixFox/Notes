\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\newcommand{\seq}[1][x]{(#1_n)_{n \in \N}}
\newcommand{\dseq}[2][n]{(#2_#1)_{#1 \in \N}}
\newcommand{\limas}[3][n]{#2 \rightarrow #3 \text{ as } #1 \rightarrow \infty}

\title{Analysis I \\
    \large Prereading}
\author{Luke Phillips}
\date{December 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Functions, Limits, and Continuity}

\subsection{Open and closed sets}
Let $a \leq b$.
Recall that we call
\[
(a, b) := \{t \in \R;\,a < t < b\}
\]
an open interval inside $\R$.
We do allow $a = -\infty$ or $b = \infty$.
Similarly,
we have the closed
(or compact)
interval inside $\R$
\[
[a, b] := \{t \in \R;\, a \leq t \leq b\}.
\]
We also have half-open intervals:
$[a, b)$ or $(a, b]$.

We call a subset $X \subseteq \R$ open if for each $c \in X$ there exists an open interval $(c - \delta, c + \delta)$ with $\delta > 0$ which is contained in $X : (c - \delta, c + \delta) \subseteq X$.
\textit{Note:
that $\delta$ typically depends on the point $c$ and that $\delta$ isn't certainly unique.}

If you have found one $\delta > 0$ then all positive reals less than $\delta$ will also work.
\textit{Note:
This definition of an open interval $(a, b)$ is indeed open:
Given $c \in (a, b)$ one can take $\delta = \min\{b - c, c - a\}$.}

Finally,
$c \in X$ is an interior point if there exists an open subset $U$ or an open interval $(a, b)$ containing $c$ which lies completely in $X$.

\hfill

\begin{lemma}\label{pre_analy_lem_limofconvseqinclosedintinint}
    Let $\seq$ be a convergent sequence in the closed interval $[a, b]$.
    Then its limit $L$ lies also in $[a, b]$.
    \begin{proof}
        Assume $L \notin [a, b]$.
        Let $\varepsilon = \min\{L - b, a - L\} > 0$.
        Then by the definition of the limit there exists an $N$ such that $|x_n - L| < \varepsilon$ for all $n \geq N$.
        But this means in particular $x_n \notin [a, b]$ for $n \geq N$.
        Contradiction.
    \end{proof}
\end{lemma}
\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
The proof is sort of saying
(by my understanding)
that if $\varepsilon$ is in the interval of the limit and $L$ is outside of the limit,
then $x_n$ must not be in $[a, b]$,
but this contradicts our assumption.
\end{minipage}
}
\end{center}
\hfill

Of course this does not hold for open intervals $(a, b)$.
The sequence $(1/n)_{n \in \N}$ lies on $(0, 1]$ but has limit $0$ outside the set.
However,
considering $(a, b) \subset [a, b]$ we immediately obtain.

\begin{corollary}
    Let $\seq$ be a converging sequence in the open interval $(a, b)$.
    Then its limit $L$ lies in the closed interval $[a, b]$.
\end{corollary}

$[a, b]$ are the only potential limit points for a sequence in $(a, b)$.
We generalise now that a given set $X \subseteq \R$ we let $\bar{X}$ be the set of all limits of all convergent sequences in $X$.
We call $\bar{X}$ the closure of $X$.

In general,
this can be quite complicated to describe but we will be mainly concerned with situations where $X$ is an interval or finite union of such.

\begin{corollary}[Bolzano-Weierstrass]
    Let $\seq$ be a sequence in the compact interval $[a, b]$.
    Then the sequence has a subsequence which converges in $[a, b]$.

    \begin{proof}
        The sequence $\seq$ is bounded,
        hence by Bolzano-Weierstrass a converging subsequence.
        By \autoref{pre_analy_lem_limofconvseqinclosedintinint} its limit must lie in $[a, b]$.
    \end{proof}
\end{corollary}

\subsection{Limits of functions}
Throughout we let $f : X \rightarrow \R$ be a function on a subset $X \subseteq \R$ of the reals.
Sometimes we might also write $D = D(f)$ for the domain of $f$.

\begin{definition}
    Let $f : X \rightarrow \R$ be a function and assume $X$ contains the open interval $(a, b)$ with the possible exception of a point $c \in (a, b)$.
    We say
    \[
    \lim_{x \rightarrow c}f(x) = L
    \]
    for some $L \in \R$,
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - L| < \varepsilon
    \]
    for all $x \in (a, b) \setminus \{c\}$ with
    \[
    |x - c| < \delta.
    \]
\end{definition}

\textit{Note:
if $c \in X$,
then $L$ can be $f(c)$,
but need not be.
(If it is we will call $f$ continuous at $c$,
see below).}
One often also writes $\lim_{x \rightarrow c}f(x) = L$ in the form
\[
f(x) \rightarrow L\quad\text{as}\quad x \rightarrow c.
\]

\begin{proposition}\label{pre_analy_prop_funclimcriterion}
    Let $f : X \rightarrow \R$ be a function and assume $X$ contains the open interval $(a, b)$ with the possible exception of a point $c \in (a, b)$.
    
    Then $\lim_{x \rightarrow c}f(x) = L$ if and only if for all sequences $\seq$ in $X \setminus \{c\}$ with $x_n \rightarrow c$ as $n \rightarrow \infty$ we have $\lim_{n \rightarrow \infty}f(x_n) = L$.
    \begin{proof}
        First assume $\lim_{x \rightarrow c}f(x) = L$ and let $(x_n)$ be a sequence in $X \setminus \{c\}$ converging to $c$.
        Pick $\varepsilon > 0$.
        Then by assumption there exists a $\delta > 0$ such that
        \[
        |f(x) - L| < \varepsilon\quad\text{provided that}\quad|x - c| < \delta
        \]
        and $x \in (a, b) \setminus \{c\}$.
        Since $x_n \rightarrow c$ as $n \rightarrow \infty$ there exists an $N \in \N$ with
        \[
        |x_n - c| < \delta
        \]
        for all $n \geq N$.
        But then $|f(x_n) - L| < \varepsilon$ for all $n \geq N$.
        This means that $f(x_n) \rightarrow L$ as $n \rightarrow \infty$.

        Now let us assume that $\lim_{x \rightarrow c}f(x) \neq L$
        (or does not exist).
        We need to find a sequence $(x_n)$ in $X \setminus \{c\}$ converging to $c$ such that the sequence $(f(x_n))$ does not converge to $L$.
        By assumption there exists an $\varepsilon > 0$ such that for all $\delta > 0$ there exists a $x \neq c$ with $|x - c| < \delta$ and $|f(x) - L| \geq \varepsilon$.
        So given this $\varepsilon$ we can choose $\delta = \frac{1}{n}$ and then there exists an $x_n \neq c$ with $|x_n - c| < \frac{1}{n}$,
        but $|f(x_n) - L| \geq \varepsilon$.
        But then $x_n \rightarrow c$ as $n \rightarrow \infty$,
        while $f(x_n) \nrightarrow L$.
    \end{proof}
\end{proposition}

We now will extend the definition of a limit of a function to one-sided limits and also to $c = \pm\infty$.
\begin{definition}[Right-sided Limit]
    Let $f : (a, b) \rightarrow \R$ be a function.
    We define the right-sided limit as
    \[
    \lim_{x \rightarrow a ^ {+}}f(x) = L
    \]
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - L| < \varepsilon\qquad\text{for all } x \in (a, b)\text{ with }|x - a| = x - a < \delta.
    \]
\end{definition}

\begin{definition}[Left-sided Limit]
    Let $f : (a, b) \rightarrow \R$ be a function.
    We define the left-sided limit as
    \[
    \lim_{x \rightarrow a ^ {-}}f(x) = L
    \]
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - L| < \varepsilon\qquad\text{for all } x \in (a, b)\text{ with }|x - b| < \delta.
    \]
\end{definition}

We can also define the left/right-sided limit for an interior point $c \in (a, b)$ in same way
(formally,
pick $a = c$ respectively.
$b = c$ in the definition.
Naturally,
the one-sided limits if they exist don't have to be the same!)

If the domain $X$ of $f$ is not bounded above or below
(containing intervals of the form $(b, \infty)$ or $(-\infty, a)$),
we say
\begin{definition}
    \[
    \lim_{x \rightarrow \infty}f(x) = L
    \]
    if for every $\varepsilon > 0$ there exists a $K > 0$ such that
    \[
    |f(x) - L| < \varepsilon\text{ for every } x \in X\text{ with } x \geq K.
    \]
    If $X$ is not bounded below,
    we can define
    \[
    \lim_{x \rightarrow -\infty}f(x) = L
    \]
    analogously.
\end{definition}

We can use the limit criterion \autoref{pre_analy_prop_funclimcriterion} to easily carry over COLT to the limit of functions
\begin{theorem}
    Let $f, g : X \rightarrow \R$ be two functions with $\lim_{x \rightarrow c}f(x) = L_1$ and $\lim_{x \rightarrow c}g(x) = L_2$.
    \begin{enumerate}[label = (\roman*)]
        \item For any $\alpha, \beta \in \R$ the limit $\lim_{x \rightarrow c}\alpha\cdot f(x) + \beta\cdot g(x)$ exists and is equal to $\alpha L_1 + \beta L_2$.
        \item The limit $\lim_{x \rightarrow c}f(x)g(x)$ exists and is equal to $L_1L_2$.
        \item Assume $L_2 \neq 0$.
        The limit $\lim_{x \rightarrow c}f(x) / g(x)$ exists and is equal to $L_1 / L_2$.
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
        \item
            We will use the criterion \autoref{pre_analy_prop_funclimcriterion}.
            For this let $(x_n)$ be a sequence in $X$ with $\limas{x_n}{c}$.
    
            By COLT for sequences and the hypothesis we then have that $\alpha \cdot f(x_n) + \beta \cdot g(x_n)$ converges to $\alpha \cdot L_1 + \beta \cdot L_2$ which then implies the first claim by \autoref{pre_analy_prop_funclimcriterion}.
        
        \textit{The other claims follow in the same way.}
        \item \textbf{Prove!}
        \item \textbf{Prove!}
        \end{enumerate}
    \end{proof}
\end{theorem}

Similarly we also have,
\begin{theorem}
    Let $X \subset \R$,
    $c \in X$ and $f, g, h : X \rightarrow \R$ be three functions $f(x) \leq g(x) \leq h(x)$ for all $x \in X$ and also $\lim_{x \rightarrow c}f(x) = \lim_{x \rightarrow c}h(x) = L$.
    Then also
    \[
    \lim_{x \rightarrow c}g(x) \text{ exists with } \lim_{x \rightarrow c}g(x) = L.
    \]
    \begin{proof}
        This follows from \autoref{pre_analy_prop_funclimcriterion} in exactly the same way.
        So let $(x_n)$ be a sequence in $X$ with $\limas{x_n}{c}$.
        Hence by hypothesis $f(x_n) \leq g(x_n) \leq h(x_n)$ for all $x \in X$ and also $\liminfty f(x_n) = \liminfty h(x_n) = L$.
        Now apply squeezing for sequences to conclude that $\liminfty g(x_n)$ exists and is equal to $L$.
        This implies the claim by \autoref{pre_analy_prop_funclimcriterion}.
    \end{proof}
\end{theorem}

\subsection{Continuity}

\begin{definition}
    Let $f : X \rightarrow \R$ be a function and let $c$ be an interior point of $X$.
    Then $f$ is called continuous at $c \in X$ if
    \[
    \lim_{x \rightarrow c}f(x) = f(c).
    \]
    That is,
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - f(c)| < \varepsilon\quad\text{for all $x \in X$}\quad\text{with } |x - c| < \delta.
    \]
    The function $f : X \rightarrow \R$ is called continuous,
    if it is continuous for all $c \in X$.
\end{definition}

\begin{remark}\phantom{}
    \begin{itemize}
        \item 
        $f$ must be defined at the point $c$ for it to be continuous at $c$.

        \item
        Additionally,
        continuity at a point $c$ is a local property,
        it only depends on the behaviour of $f$ in a
        (small)
        (open)
        interval around $c$.
        So the function defined on $X$ is continuous at $c$ if and only if the function $f$ is restricted to an interval around $c$ is continuous at $c$.
        
        This can make things simpler if $f$ is restricted to a closed and bounded interval around $c$.
    
        \item 
        We can define left-continuity and right-continuity analogously as for the limit of functions.
        It is easy to see that $f$ is continuous at $c$ if and only if it is left and right continuous at $c$.

        In this way one can also define continuity for a function $f$ on the closed interval $[a, b]$ at the boundary points $a$ and $b$.
    \end{itemize}
\end{remark}

\begin{example}
    The identity function $f : \R \rightarrow \R$ given by $f(x) = x$ is continuous.
    \begin{proof}
        Given $c \in \R$ and $\varepsilon > 0$,
        we can choose $\delta = \varepsilon$.
        This completes the proof.
    \end{proof}
\end{example}

\begin{example}
    The function $f : \R \setminus \{0\} \rightarrow \R$ given by $f(x) = 1 / x$ is also continuous.

    This function is not defined at $x = 0$,
    so it is not a question whether $f$ is continuous at $c = 0$.

    At $c \neq 0$ we have
    \[
    \left|\frac{1}{x} - \frac{1}{c}\right| = \left|\frac{c - x}{cx}\right|.
    \]
    This needs to be smaller than the given $\varepsilon > 0$.
    The $c - x$ term is harmless,
    the $c$ in the denominator is just a constant.
    Our problem is the $x$ in the denominator which if we are close to zero can become large.
    We need to restrict ourselves away from $x = 0$.
    Consider the function in the restricted region $|x| > |c| / 2$.
    Note $c$ is certainly still an interior point.
    So now for $|x| > |c| / 2$ have
    \[
    \left|\frac{1}{x} - \frac{1}{c}\right| = \left|\frac{c - x}{cx}\right| \leq \frac{2}{c ^ 2}|c - x|\footnotemark.
    \]
    \footnotetext{This step is a handful at first,
    think about the $|x| > |c| / 2$ inequality hard enough then you should be able to understand this ridiculous transition.}
    This needs to be less than $\varepsilon$.
    We can pick $\delta = \frac{c ^ 2 \varepsilon}{2}$.
    Then if $|x - c| < \delta = \frac{c ^ 2 \varepsilon}{2}$
    (and $|x| > |c| / 2$)
    we get
    \[
    \left|\frac{1}{x} - \frac{1}{c}\right| = \left|\frac{c - x}{cx}\right| \leq \frac{2}{c ^ 2}|c - x| < \frac{2}{c ^ 2}\frac{c ^ 2\varepsilon}{2} = \varepsilon,
    \]
    as required.
    We have shown this works for all $c$ so we have shown that $\frac{1}{x}$ is continuous on all of $\R \setminus \{0\}$\footnote{It may seem weird but since $c$ can get really close to $0$ it technically covers all of our domain,
    I hate it too.}.

    Alternatively,
    if you don't like the trick of restricting the function away from the bad point $x = 0$,
    you could take $\delta = \min\{|c| / 2, c ^ 2\varepsilon / 2\}$,
    and the same calculation will work.
    However,
    I find this quite over the top,
    if you think about the first method hard enough it seems more intuitive.
\end{example}

\begin{example}
    The function $f : [0, \infty) \rightarrow \R$ given by $f(x) = \sqrt{x}$ is continuous.
    First consider $c = 0$.
    Given $\varepsilon > 0$,
    choose $\delta = \varepsilon ^ 2$.
    Then for $|x| = |x - 0| < \delta = \varepsilon$ we get
    \[
    |\sqrt{x} - \sqrt{0}| = |\sqrt{x}| < \sqrt{\delta} = \varepsilon,
    \]
    so $f$ is
    (right)
    continuous at $c = 0$.
    At $c \neq 0$,
    we can write
    \[
    |\sqrt{x} - \sqrt{c}| = \frac{|x - c|}{\sqrt{c} + \sqrt{x}} \leq \frac{|x - c|}{\sqrt{c}}.
    \]
    So given $\varepsilon > 0$,
    choose $\delta = \sqrt{c}\varepsilon$.
\end{example}
\textit{Note:
in the above example we have to consider $c = 0$ as a separate case since we cannot use the same argument we used for $c \neq 0$ when $c = 0$ as we would be dividing by $0$!}

\begin{example}
    Consider the sign function $\mathrm{sgn} : \R \rightarrow \R$ given by
    \[
    \mathrm{sgn}(x) = \begin{cases}
        -1 & x < 0 \\
        0 & x = 0 \\
        1 & x > 0.
    \end{cases}
    \]
    Then $\mathrm{sgn}$ is continuous for every $c \neq 0$,
    but not at $c = 0$.
\end{example}

\begin{example}
    Consider $d : \R \rightarrow \R$ given by
    \[
    d(x) = \begin{cases}
        0 & x \in \Q \\
        1 & x \notin \Q.
    \end{cases}
    \]
    Then $d$ is not continuous at any $c \in \R$.
\end{example}

From \autoref{pre_analy_prop_funclimcriterion} we obtain the very useful criterion.
\begin{proposition}\label{pre_analy_prop_contfuncoflimeqlimofcontfunc}
    Let $X \subset \R$,
    $c \in X$,
    and $f : X \rightarrow \R$ a function.
    Then $f$ is continuous at $c$ if and only if for all sequences $\seq$ in $X$ with $\limas{x_n}{c}$ we have $\limas{f(x_n)}{f(c)}$.
    That is,
    \[
    \liminfty f(x_n) = f\left(\liminfty x_n\right).
    \]
\end{proposition}

We can use this to carry over COLT to continuous functions.
\begin{theorem}\label{pre_analy_thm_coltforcontfuncs}
    Let $X \subset \R$,
    $c \in X$ and $f, g : X \rightarrow \R$ be continuous at $c$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item $h(x) := \alpha \cdot f(x) + \beta \cdot g(x)$ is continuous at $c$ for any $\alpha, \beta \in \R$.
        \item $h(x) := f(x) \cdot g(x)$ is continuous at $c$.
        \item $h(x) := f(x) / g(x)$ is continuous at $c$,
        provided that $g(c) \neq 0$.
    \end{enumerate}
    \begin{proof}
        To show that $h(x)$ is continuous at $c$,
        let $(x_n)$ be a sequence in $X$ with $\limas{x_n}{c}$.
        By COLT and \autoref{pre_analy_prop_contfuncoflimeqlimofcontfunc}
        (applied to the continuous functions $f$ and $g$),
        $\limas{h(x_n)}{h(c)}$.
        By \autoref{pre_analy_prop_contfuncoflimeqlimofcontfunc} $h$ is continuous at $c$.
    \end{proof}
\end{theorem}

\begin{example}
    Given two polynomials $p, q : \R \rightarrow \R$,
    the rational function $r : X \rightarrow \R$ given by
    \[
    r(x) = \frac{p(x)}{q(x)}
    \]
    is continuous,
    where $X = \{x \in \R\,\mid\,q(x) \neq 0\}$ by COLT for continuous functions.
\end{example}

\begin{theorem}\label{pre_analy_thm_contfunccompcontfunciscont}
    Let $X, Y \subset \R$,
    $c \in X$,
    $f : X \rightarrow \R$,
    $g : Y \rightarrow \R$ with $f(X) \subset Y$.
    If $f$ is continuous at $c \in X$ and $g$ is continuous at $f(c) \in Y$,
    then $g \circ f$ is continuous at $c \in X$.

    \begin{proof}
        Let $(x_n)$ be a sequence in $X$ converging to $c$.
        Then $y_n = f(x_n) \rightarrow f(c)$ by \autoref{pre_analy_prop_contfuncoflimeqlimofcontfunc},
        since $f$ is continuous at $c$.
        Then $g(y_n) \rightarrow g(f(c))$ by \autoref{pre_analy_prop_contfuncoflimeqlimofcontfunc},
        since $g$ is continuous at $f(c)$.
        This means $\limas{g \circ f(x_n)}{g \circ f(c)}$,
        and $g \circ f$ is continuous at $c$ by \autoref{pre_analy_prop_contfuncoflimeqlimofcontfunc}.
    \end{proof}
\end{theorem}

\begin{example}
    The exponential function $f : \R \rightarrow \R$ given by $f(x) = e ^ x$ is continuous.
    \begin{proof}
        First we look at $c = 0$,
        so $f(0) = e ^ 0 = 1$.
        Recall the following inequality,
        for $x < 1$
        \[
        1 + x \leq e ^ x \leq \frac{1}{1 - x}.
        \]
        Hence by the squeezing theorem $\lim_{x \rightarrow 0}e ^ x = 1 = e ^ 0$.
        So $e ^ x$ is continuous at $c = 0$.

        Now let $c \neq 0$.
        Then $e ^ {x - c}$ is continuous at $x = c$ as an application of \autoref{pre_analy_thm_contfunccompcontfunciscont}
        (with $f(x) = e ^ x$ and $g(x) = x - c$)\footnote{This can be thought of very slightly easier as $\exp{(x - c)}$.}.
        Using the addition law for the exponential we conclude
        \[
        \lim_{x \rightarrow c}e ^ x = e ^ c \lim_{x \rightarrow c}e ^ {x - c} = e ^ ce ^ 0 = e ^ c,
        \]
        as claimed.
    \end{proof}
\end{example}

\subsection{The Intermediate Value Theorem and the Extreme Value Theorem}

If a function $f : [a, b] \rightarrow \R$ is continuous on a closed interval,
the Intermediate Value Theorem states that $f$ takes on any value between $f(a)$ and $f(b)$.
For example,
if $f(a)$ is negative and $f(b)$ is positive,
there has to be a point $c \in (a, b)$ with $f(c) = 0$.
More precisely,
we have

\begin{theorem}[Intermediate Value Theorem]
    Let $a, b \in \R$ with $a < b$ and $f : [a, b] \rightarrow \R$ be continuous.
    If $f(a) < f(b)$ and $d \in [f(a), f(b)]$,
    then there exists $c \in [a, b]$ with $f(c) = d$.
    Likewise,
    if $f(b) < f(a)$ and $d \in [f(b), f(a)]$,
    then there exists $c \in [a, b]$ with $f(c) = d$.

    \begin{proof}
        We assume that $f(a) < f(b)$.
        We can also assume $d < f(b)$
        (otherwise pick $c = b$)\footnote{As $f(c) = d = f(b)$ but $d < f(b)$.}.
        Let
        \[
        X = \{x \in [a, b]\,|\,f(x) \leq d\}.
        \]
        Then $X$ is bounded above by $b$,
        and $a \in X$.
        So $c = \sup{X}$ exists.
        Hence by
        (some result I cannot remember the name of,
        plus the completeness axiom)
        there is a sequence $(x_n)$ in $X$ with $x_n \rightarrow c$.
        By continuity and \autoref{pre_analy_lem_limofconvseqinclosedintinint} we have $f(x_n) \rightarrow f(c) \leq d < f(b)$;
        in particular $c < b$.
        If we had $f(c) < d$,
        we could find by continuity $\delta > 0$ with $c + \delta < b$ and
        \[
        |f(x) - f(c)| < \frac{d - f(c)}{2}(:= \varepsilon)
        \]
        for all $x \in [a, b]$ with $|x - c| < \delta$.
        But then $f(c + \delta / 2) < d$,
        contradicting that $c$ is the supremum of $X$.
        Therefore $f(c) = d$.
    \end{proof}
\end{theorem}

If $f : X \rightarrow \R$ is a function,
the range of $f$ is the set
\[
f(X) = \{y \in \R\mid y = f(x)\text{ for some } x \in X\}.
\]
If $X$ is an interval and $a < b$ both in $X$,
we can restrict $f$ to $[a, b]$ and get that all values between $f(a)$ and $f(b)$ are taken.
This means that $f(X)$ is an interval as well.

\textit{Note that if $f(X)$ is not bounded above,
the upper endpoint of the interval is $\infty$.}

If $f(X)$ is bounded above,
the upper endpoint of the interval is given by the supremum of $f(X)$.
This supremum may or may not be in the range,
but this is okay for an interval.
The lower endpoint is treated similarly,
so we get

\begin{corollary}\label{pre_analy_corl_funcofintisint}
    Let $I$ be an interval,
    and $f : I \rightarrow \R$ be continuous.
    Then the image $f(I)$ of $I$ under $f$ is an interval.
\end{corollary}

\begin{example}
    Let $f : (0, 2) \rightarrow \R$ be given by
    \[
    f(x) = \frac{x - 1}{x(2 - x)}.
    \]
    Then the range is the interval $(-\infty, \infty) = \R$.
\end{example}

If the domain is a closed interval $[a, b]$,
this cannot happen,
and the range is also a closed interval.

\begin{theorem}
    Let $a, b \in \R$ with $a < b$ and $f : [a, b] \rightarrow \R$ be continuous.
    Then
    
    Every continuous function on a compact interval attains its maximum and minimum.

    Formulated differently,
    the image of $f$ is given by $[c, d]$ for some $c, d \in \R$.
    In particular,
    there exist $u, v \in [a, b]$ with $f(u) = c$ and $f(v) = d$.
    So in conclusion.

    The continuous image of a compact interval is again a compact interval.

    \begin{proof}
        We first show that $f([a, b])$ cannot be unbounded above.
        For otherwise there exists a sequence $\seq$ in $[a, b]$ such that $f(x_n) \geq n$.
        By Bolzano-Weierstrass there is a subsequence $(x_{n_j})$ which is convergent to a $c \in [a, b]$.
        By continuity we have $f(x_{n_j}) \rightarrow f(c) \in \R$.
        But this is a contradiction to $f(x_{n_j}) \geq n_j$ for all $j$.

        Therefore $f([a, b])$ is bounded above,
        and $d = \sup{f([a, b])}$ exists.
        By
        (the same result that I cannot remember)
        there exists a sequence $(y_n)$ in $f([a, b])$ with $\limas{y_n}{d}$.
        But there also exists a sequence $(x_n)$ in $[a, b]$ with $f(x_n) = y_n$.
        By Bolzano-Weierstrass there is a convergent subsequence $(x_{n_j})$ with limit $c \in [a, b]$.
        Then
        \[
        f(c) = f(\liminfty[j]x_{n_j}) = \liminfty[j]f(x_{n_j}) = \liminfty[j]y_{n_j} = d.
        \]
        So $d \in f([a, b])$.
        Similarly we get that $c = \inf{f([a, b])} \in f([a, b])$,
        and by \autoref{pre_analy_corl_funcofintisint} we get $f([a, b]) = [c, d]$.
    \end{proof}
\end{theorem}

\subsection{Uniform continuity}
Recall the definition of continuity for a function $f$ on an interval $I$
\[
\forall c \in I\, \forall \varepsilon > 0\, \exists\delta > 0\, \forall x \in I\quad (|x - c| < \delta \implies |f(x) - f(c)| < \varepsilon)
\]

\begin{remark}\phantom{}
    \begin{itemize}
        \item Note that $\delta$ not only depends on the initial choice of $\varepsilon > 0$,
        but also
        (potentially)
        on the actual point $c$.
        \item Recall that if one $\delta > 0$ does the job,
        then so will any other positive number less than $\delta$.
        \item Recall that continuity at a point $c$ is a local property,
        that is,
        it only depends on the behaviour of $f$ in a
        (small)
        neighbourhood of $c$ and not on the entire domain of definition $I$.
        So if we restrict $f$ to a small(er) interval $J$ containing $c$ as an inner point then the question of continuity at $c$ is unaffected.
    \end{itemize}
\end{remark}

\begin{definition}[Uniform Continuity]
    Let $f$ be a function on an interval $I$.
    We call $f$ uniformly continuous on $I$  if
    \[
    \forall \varepsilon > 0\,\exists \delta > 0\,\forall c, x \in I \quad (|x - c| < \delta \implies |f(x) - f(c)| < \varepsilon).
    \]
    The point is that now $\delta$ has to be the same for all $c \in I$,
    that is,
    $\delta$ no longer depends on the individual point $c$
    (but of course still on $\varepsilon$ and naturally the function $f$ itself).
    In particular,
    uniform continuity is no longer a local concept but a 'global' one as it depends on the entire interval $I$.
\end{definition}

\begin{theorem}
    Let $f$ be a continuous function on a compact interval $[a, b]$.
    Then $f$ is uniformly continuous on $[a, b]$.
    \begin{proof}
        Assume $f$ is not uniformly continuous on $[a, b]$.
        This means
        \[
        \exists\varepsilon > 0\, \forall\delta > 0\, \exists x, y \in [a, b]\text{ with } |x - y| < \delta \text{ but } |f(x) - f(y)| \geq \varepsilon.
        \]
        Fix such an $\varepsilon > 0$ and take $\delta = \delta_n = \frac{1}{n}$ for each $n \in \N$.
        We find $x_n, y_n \in [a, b]$ such that
        \[
        |x_n - y_n| < \frac{1}{n}\quad\text{but}\quad|f(x_n) - f(y_n)| \geq \varepsilon.
        \]
        The key is,
        by Bolzano-Weierstrass we can find a subsequence $(x_{n_j})$ of $(x_n)$ which converges to a point $x*$ in $[a, b]$.
        (Here we use the fact that we are dealing with a compact interval).
        We claim that also $y_{n_j} \rightarrow x*$.
        Indeed let $\varepsilon' > 0$.
        Then there exists an $N$ such that for all $n_j > N$ we have $|x_{n_j} - x*| < \varepsilon'$ and hence for all $n_j > \max\{1 / \varepsilon', N\}$ we have
        \[
        |y_{n_j} - x*| = |y_{n_j} - x_{n_j} + x_{n_j} - x*| \leq |y_{n_j} - x_{n_j}| + |x_{n_j} - x*| < \varepsilon' + \frac{1}{n_j} < 2\varepsilon'.
        \]
        Thus the $(y_{n_j})$ also converge to $x*$.
        But now we
        (finally)
        use the continuity of $f$ at $x*$.
        There exists a $\delta > 0$ such that $|f(x*) - f(x)| < \varepsilon / 2$ for all $x \in [a, b]$ with $|x - x*| < \delta$.
        Now for $n_j$ sufficiently large we have $|x_{n_j} - x*| < \delta$ and also $|y_{n_j} - x*| < \delta$ and then
        \[
        |f(x_{n_j} - f(y_{n_j})| \leq |f(x_{n_j}) - f(x*)| + |f(x*) - f(y_{n_j})| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
        \]
        in contradiction to $1$.
    \end{proof}
\end{theorem}

\begin{remark}\phantom{}
    \begin{itemize}
        \item We have seen\footnote{In a problem sheet that I cannot be bothered to find.} that a Lipschitz continuous function
        ($|f(x) - f(y)| \leq M|x - y|$ for all $x, y \in I$)
        is uniformly continuous.

        Later we will see that differentiable functions with bounded derivative are Lipschitz and hence are uniformly continuous.

        \item The function $f(x) = \sqrt{x}$ is continuous on $[0, 1]$ but not Lipschitz.
        However,
        the theorem states that it is uniformly continuous.
        So Lipschitz continuity is really a stronger concept than uniform continuity.

        \item The function $f(x) = \frac{1}{x}$ is not uniformly continuous on $(0, 1]$.
        So one really needs a compact interval in the theorem to conclude that continuity implies uniform continuity.

        \item The function $\exp(x) = e ^ x$ is not uniformly continuous on $\R$.
        However,
        $\log(x)$ is uniformly continuous on $[1, \infty)$
        (even though both functions diverge as $x \rightarrow \infty$).
    \end{itemize}
\end{remark}

\subsection{Injective continuous functions and their inverses}

A function $f : X \rightarrow \R$ is said to be strictly increasing if $f(x) < f(y)$ for all $x, y \in X$ with $x < y$.
Similarly,
it is said to be strictly decreasing if $f(x) > f(y)$ for all $x, y \in X$ with $x < y$.
Also recall that $f$ is called injective
(or one-to-one)
if $f(x_1) = f(x_2)$ implies $x_1 = x_2$.
Of course strictly increasing/decreasing functions on $\R$ are injective.
The converse does not always hold.
However,
\begin{proposition}
    Let $f : I \rightarrow \R$ be continuous and injective on an interval $I$.
    Then $f$ is either strictly increasing or strictly decreasing.
    \begin{proof}
        If $f$ was neither we would find elements $a, b, c \in I$ with $a < c < b$ but with $f(c)$ bigger
        (smaller goes the same)
        than both $f(a)$ and $f(b)$,
        say for example $f(a) < f(b) < f(c)$.
        But then $f$ must take the value $f(b)$ in the interval $[a, c]$
        (which does not contain $b$)
        again.
        Thus $f$ would not be one-to-one.
    \end{proof}
\end{proposition}

Assume $f : X \rightarrow \R$ is injective.
Recall we can form its inverse $f ^ {-1} : f(X) \rightarrow \R$ by $f ^ {-1}(x) = y$,
where $y \in X$ is the unique element with $f(y) = x$.
In particular,
the domain of $f ^ {-1}$ is the range of $f$ and the range of $f ^ {-1}$ is the domain $X$ of $f$.
Note $f(f ^ {-1}(c)) = c$ for all $c \in f(X)$ and $f ^ {-1}(f(d)) = d$ for all $d \in X$.

\begin{example}
    The logarithm was defined as the inverse function of the exponential function.
    So with $f(x) = e ^ x$,
    we get $f ^ {-1}(x) = \log{x}$.
    Notice that $f ^ {-1}$ is only defined for $x > 0$,
    since $e ^ x > 0$ for all $x \in \R$.
\end{example}

\begin{theorem}\label{pre_analy_thm_imgofinvcontinjfunciscont}
    Let $f : I \rightarrow \R$ be continuous and injective on a non-trivial interval $I$ with image $J := f(I)$.
    Then $f ^ {-1} : J \rightarrow \R$ is also continuous.

    \begin{proof}
        Let $d \in J$,
        say $d = f(c)$ for some $c \in I$.
        We need to show that given $\varepsilon > 0$ there exists $\delta > 0$ so that $|f ^ {-1}(d) - f ^ {-1}(y)| < \varepsilon$ for all $y \in J$ with $|y - d| < \delta$.

        Assume for simplicity that $f$ is strictly increasing
        (and hence also $f ^ {-1}$).
        Assume for the moment that $c$ is an interior point of $I$.
        Then we can assume $[c - \varepsilon / 2, c + \varepsilon / 2] \subset I$.
        Then under $f$ this becomes $[f(c - \varepsilon / 2), f(c + \varepsilon / 2)]$ with $f(c - \varepsilon / 2) < d < f(c + \varepsilon / 2)$.
        Now find $\delta > 0$ such that $(d - \delta, d + \delta) \subset [f(c - \varepsilon / 2), f(c + \varepsilon / 2)]$.
        Then for every $y \in J$ with $|y - d| < \delta$ we have $f ^ {-1}(y) \in [c - \varepsilon / 2, c + \varepsilon / 2]$.
        Hence $|f ^ {-1}(y) - f ^ {-1}(d)| = |f ^ {-1}(y) - f ^ {-1}(f(c))| = |g(y) - c| \leq \varepsilon / 2 < \varepsilon$ as desired.
        If $c$ is an endpoint of $I$
        (if it exists),
        say the upper one,
        then so is $d \in J$,
        and the argument carries through with $c + \varepsilon / 2$ and $d + \delta$ replaced by $c$ and $d$ respectively.
        The lower endpoint goes the same.
    \end{proof}
\end{theorem}

\begin{example}\phantom{}
    \begin{itemize}
        \item The logarithm $\log(x) : \R_{+} \rightarrow \R$ is continuous.
        \item It is important to note that $f$ is defined on an interval in \autoref{pre_analy_thm_imgofinvcontinjfunciscont} to get the continuity of the inverse function.

        Consider $f : [0, 1] \cup (2, 3] \rightarrow \R$ given by
        \[
        f(x) = \begin{cases}
            2x & \text{for } x \in [0, 1] \\
            x & \text{for } x \in (2, 3].
        \end{cases}
        \]
        This is easily seen to be continuous and injective
        (note that $2$ is not in the domain of $f$).
        The inverse function $f ^ {-1} : [0, 2] \rightarrow \R$ is given by
        \[
        f ^ {-1}(x) = \begin{cases}
            \frac{1}{2}x & \text{for } x \in [0, 2] \\
            \frac{1}{2}x & \text{for } x \in (2, 3]
        \end{cases}
        \]
        and it is not continuous at $2$,
        as $f ^ {-1}(2 + 1 / n) \rightarrow 2 \neq f(2)$.

        It is instructive to revisit the proof of the continuous inverse proposition above and to investigate what goes wrong for this function.
        It is fairly subtle at $d = 2 = f(1)$.
        Namely,
        while $c = 1$ is the endpoint of one interval of the domain of $f$,
        the value $d = 2$ is not for the range.
    \end{itemize}
\end{example}

\begin{remark}
    In general,
    one want would like to be true is that if the function $f$ is "nice"
    (has a certain property),
    then the inverse function $f$ should be equally "nice"
    (has the same property).
    Unfortunately,
    this does not always hold;
    one usually has to require an extra condition.
    We have already seen this for continuous functions on the real line to itself
    (in a mild way).
\end{remark}

\newpage

\section{Differentiable Functions}

\subsection{Basics on differentiable functions}

Let us first define differentiability
\begin{definition}
    Let $X \subseteq \R$ be open and $f : X \rightarrow \R$ be a function.
    $f$ is differentiable at $c \in X$ if $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}$ exists.
    We denote this limit by $f'(c)$ and call $f'(c)$ the derivative of $f$ at $c$.
    $f$ is called a differentiable function if $f$ is differentiable at all points $c \in X$.
\end{definition}

One often encounters the formulation
\[
f'(c) = \lim_{h \rightarrow 0}\frac{f(c - h) - f(c)}{h},
\]
which is the same
(set $h = x - c$).
Sometimes this formulation is more convenient.

\begin{remark}\phantom{}
    \begin{itemize}
        \item The expression $\frac{f(x) - f(c)}{x - c}$ has an important geometric interpretation:
        It is the slope of the straight line passing through the two points $(c, f(c)) \in \R ^ 2$ and $(x, f(x)) \in \R ^ 2$ of the graph of $f$.
        As $x \rightarrow c$,
        the second point $(x, f(x))$ approaches the first point $(c, f(c))$,
        and the limit describes the slope of the tangent of the graph of $f$ at the point $(c, f(c)) \in \R ^ 2$.

        \item Differentiability at $c \in X$ can also be described in the $(\varepsilon, \delta)$-formalism.
        $f$ is differentiable at $c \in X$ if there exists a number $L \in \R$
        (the derivative $f'(c)$)
        such that there exists for every $\varepsilon > 0$ a positive number $\delta > 0$ such that
        \[
        \left|\frac{f(c) - f(x)}{c - x} - L\right| < \varepsilon\qquad\forall x \in X \text{ with } |x - c| < \delta.
        \]

        \item We can also define
        (one-sided)
        differentiability for a function defined on a closed interval $[a, b]$ at the endpoints $a$ and $b$ by considering the one-sided limit of the difference quotient.

        We now give another equivalent formulation for differentiability which is not only handy but it also generalises to higher dimensions.
    \end{itemize}
\end{remark}

\begin{lemma}[First order Taylor]\label{pre_analy_lem_firstordertaylorindiffdef}
    Let $f : X \rightarrow \R$ be a function as above.
    Then $f$ is differentiable at $c \in X$ if and only if the following holds:

    There exists a constant $m \in \R$ an a function $r(x)$ on $X$ such that
    \[
    f(x) = f(c) + m(x - c) + r(x)(x - c)
    \]
    such that
    \[
    r(x)\text{ is continuous at } c \qquad\text{and}\qquad\lim_{x \rightarrow c}r(x) = r(c) = 0.
    \]
    In this case $m = f'(c)$.
\end{lemma}
Before we get into the juicy proof of this,
let us have a few remarks.
\begin{remark}\phantom{}
    \begin{itemize}
        \item We can view differentiability as the attempt to approximate $f(x)$ in the neighbourhood of $c$ by a linear function $L(x) = f(c) + m(x - c)$.
        Then differentiability stipulates that the error $r(x)(x - c)$ is of 'higher order'
        (not linear)
        as $\lim_{x \rightarrow c}r(x) = 0$.

        \item From the perspective of the previous,
        we can view continuity as the 'zero order Taylor'.
        The function $f$ is continuous at $c$ if $f(x) = f(c) + \tilde{r}(x)$ with $\lim_{x \rightarrow c}\tilde{r}(x) = 0$.
        We approximate $f$ by the constant function $f(c)$ and the error vanishes in the limit.
    \end{itemize}
\end{remark}

\begin{remark}
    In practice one often considers $f_1(x) = m + r(x)$ which is continuous at $c$ with value $f_1(c) = m$.
    Thus differentiability at $c$ is equivalent to:

    There exists a function $f_1 : X \rightarrow \R$ such that
    \[
    f(x) = f(c) + (x - c)f_1(x)
    \]
    and $f_1$ is continuous at $c$.
    Then $\lim_{x \rightarrow c}f_1(x) = f_1(c) = f'(c)$.
    (In this formulation one doesn't need to know the value of the derivative in advance).
\end{remark}

\begin{lemma}[continues = pre_analy_lem_firstordertaylorindiffdef]
    \begin{proof}
        Assume $f$ is differentiable at $c$,
        so the limit $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}$ exists.
        Let's denote this limit by $m$ and set
        \[
        r(x) = \begin{cases}
            \frac{f(x) - (f(c) + m(x - c))}{x - c} & \text{if } x \neq c; \\
            0 & \text{if } x = c.
        \end{cases}
        \]
        Then if $r(x)$ is continuous at $c$ holds,
        and we only need to check continuity at $c$:
        But
        \[
        \lim_{x \rightarrow c}r(x) = \lim_{x \rightarrow c}\frac{f(x) - (f(c) + m(x - c))}{x - c} = \lim_{x \rightarrow c}\left(\frac{f(x) - f(c)}{x - c} - m\right) = m - m = 0,
        \]
        as required.

        Conversely,
        if $\lim_{x \rightarrow c}r(x) = r(c) = 0$ holds with $r$ continuous at $c$ and $r(c) = 0$,
        then
        \[
        0 = r(c) = \lim_{x \rightarrow c}r(x) = \lim_{x \rightarrow c}\frac{f(x) - (f(c) + m(x - c))}{x - c} = \lim_{x \rightarrow c}\left(\frac{f(x) - f(c)}{x - c} - m\right).
        \]
        But this is only possible if $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}$ exists and is equal to $m$.
    \end{proof}
\end{lemma}

\begin{example}\phantom{}
    \begin{itemize}
        \item Let us prove differentiability of $f(x) = \frac{1}{x}$ on $(0, \infty)$.
        Let $c \in (0, \infty)$.
        Then we have
        \[
        \frac{f(x) - f(c)}{x - c} = \frac{c - x}{xc(x - c)} = -\frac{1}{xc},
        \]
        which implies that
        \[
        \lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c} = -\lim_{x \rightarrow c}\frac{1}{xc} = -\frac{1}{c ^ 2}.
        \]
        This shows that $f'(c) = -\frac{1}{c ^ 2}$.
        Now for the 'remainder' $r(x)$ we have
        \[
        r(x) = \frac{f(x) - f(c)}{x - c} - f'(c) = -\frac{1}{xc} + \frac{1}{c ^ 2} = \frac{x - c}{xc ^ 2},
        \]
        which indeed is continuous at $c$ with limit $0$.

        \item Let $f(x)$ be a polynomial.
        Then $f(x) - f(c)$ is also a polynomial which has a zero at $x = c$.
        Thus by polynomial division we can write
        \[
        f(x) - f(c) = (x - c)f_1(x)
        \]
        for some polynomial $f_1(x)$.
        In particular,
        $f_1(x)$ is continuous.
        We therefore have shown that all polynomials are differentiable!
        (Of course,
        we haven't shown what the derivative actually is;
        for $x ^ n$ it is fairly direct to determine $f_1(x)$ and its limit $nc ^ {n - 1}$ as $x \rightarrow c$.).
    \end{itemize}
\end{example}

Differentiability is a stronger property than continuity:
\begin{theorem}
    Let $X \subset \R$ and $f : X \rightarrow \R$.
    If $f$ is differentiable at $c \in X$ then $f$ is also continuous at $c$.
    \begin{proof}
        We have
        \[
        f(x) - f(c) = (x - c) \cdot \frac{f(x) - f(c)}{x - c} \rightarrow 0 \cdot f'(c) = 0\text{ as } x \rightarrow c.
        \]
        This shows that $\lim_{x \rightarrow c}f(x) = f(c)$,
        in other words,
        $f$ is continuous at $c$.
    \end{proof}
\end{theorem}

























\subsection{Inverse functions}

\subsection{Mean value theorems}

\subsection{L' H\^opital's rule}

\subsection{Taylor's theorem}

\newpage

\section{Power Series}

\subsection{A quick review of infinite series}

\subsection{Radius of convergence}

\subsection{Power series as a function}

\subsection{The exponentials, logarithm, and trigonometric functions}

\subsubsection{The exponential function}

\subsubsection{The logarithm}

\subsubsection{The trigonometric functions}

\subsection{Taylor series}

\subsection{Complex power series}

\newpage

\section{Sequences of functions, uniform convergence, and limit theorems}

\subsection{Notions of convergence of functions: pointwise and uniform}

\subsection{Uniform convergence preserves continuity}

\subsection{Uniform convergence for series of functions: Weierstrass M-Test}

\subsection{Complex case}

\newpage

\section{Fonctions réglées (Regulated functions)}

\subsection{Definition of regulated functions}

\subsection{Examples}

\subsection{Further properties of regulated functions}

\newpage

\section{Integration}

\subsection{Regulated integral: definition}

\subsection{Comparison with the Riemann integral}

\subsection{Regulated integral: basic properties}

\subsection{Fundamental theorem of calculus}

\subsection{Functions via integrals}

\subsection{Uniform convergence and integration}

\subsection{Pointwise convergence and integration}

\subsection{Improper integrals}















\end{document}