\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\newcommand{\seq}[1][x]{(#1_n)_{n \in \N}}
\newcommand{\dseq}[2][n]{(#2_#1)_{#1 \in \N}}
\newcommand{\limas}[3][n]{#2 \rightarrow #3 \text{ as } #1 \rightarrow \infty}
\newcommand{\lhopital}[0]{L' H\^opital}
\newcommand{\infsumo}{\infsum[k = 0]}

\title{Analysis I \\
    \large Prereading}
\author{Luke Phillips}
\date{December 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Functions, Limits, and Continuity}

\subsection{Open and closed sets}
Let $a \leq b$.
Recall that we call
\[
(a, b) := \{t \in \R;\,a < t < b\}
\]
an open interval inside $\R$.
We do allow $a = -\infty$ or $b = \infty$.
Similarly,
we have the closed
(or compact)
interval inside $\R$
\[
[a, b] := \{t \in \R;\, a \leq t \leq b\}.
\]
We also have half-open intervals:
$[a, b)$ or $(a, b]$.

We call a subset $X \subseteq \R$ open if for each $c \in X$ there exists an open interval $(c - \delta, c + \delta)$ with $\delta > 0$ which is contained in $X : (c - \delta, c + \delta) \subseteq X$.
\textit{Note:
that $\delta$ typically depends on the point $c$ and that $\delta$ isn't certainly unique.}

If you have found one $\delta > 0$ then all positive reals less than $\delta$ will also work.
\textit{Note:
This definition of an open interval $(a, b)$ is indeed open:
Given $c \in (a, b)$ one can take $\delta = \min\{b - c, c - a\}$.}

Finally,
$c \in X$ is an interior point if there exists an open subset $U$ or an open interval $(a, b)$ containing $c$ which lies completely in $X$.

\hfill

\begin{lemma}\label{pre:analy:lem:limofconvseqinclosedintinint}
    Let $\seq$ be a convergent sequence in the closed interval $[a, b]$.
    Then its limit $L$ lies also in $[a, b]$.
    \begin{proof}
        Assume $L \notin [a, b]$.
        Let $\varepsilon = \min\{L - b, a - L\} > 0$.
        Then by the definition of the limit there exists an $N$ such that $|x_n - L| < \varepsilon$ for all $n \geq N$.
        But this means in particular $x_n \notin [a, b]$ for $n \geq N$.
        Contradiction.
    \end{proof}
\end{lemma}
\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
The proof is sort of saying
(by my understanding)
that if $\varepsilon$ is in the interval of the limit and $L$ is outside of the limit,
then $x_n$ must not be in $[a, b]$,
but this contradicts our assumption.
\end{minipage}
}
\end{center}
\hfill

Of course this does not hold for open intervals $(a, b)$.
The sequence $(1/n)_{n \in \N}$ lies on $(0, 1]$ but has limit $0$ outside the set.
However,
considering $(a, b) \subset [a, b]$ we immediately obtain.

\begin{corollary}
    Let $\seq$ be a converging sequence in the open interval $(a, b)$.
    Then its limit $L$ lies in the closed interval $[a, b]$.
\end{corollary}

$[a, b]$ are the only potential limit points for a sequence in $(a, b)$.
We generalise now that a given set $X \subseteq \R$ we let $\bar{X}$ be the set of all limits of all convergent sequences in $X$.
We call $\bar{X}$ the closure of $X$.

In general,
this can be quite complicated to describe but we will be mainly concerned with situations where $X$ is an interval or finite union of such.

\begin{corollary}[Bolzano-Weierstrass]
    Let $\seq$ be a sequence in the compact interval $[a, b]$.
    Then the sequence has a subsequence which converges in $[a, b]$.

    \begin{proof}
        The sequence $\seq$ is bounded,
        hence by Bolzano-Weierstrass a converging subsequence.
        By \autoref{pre:analy:lem:limofconvseqinclosedintinint} its limit must lie in $[a, b]$.
    \end{proof}
\end{corollary}

\subsection{Limits of functions}
Throughout we let $f : X \rightarrow \R$ be a function on a subset $X \subseteq \R$ of the reals.
Sometimes we might also write $D = D(f)$ for the domain of $f$.

\begin{definition}
    Let $f : X \rightarrow \R$ be a function and assume $X$ contains the open interval $(a, b)$ with the possible exception of a point $c \in (a, b)$.
    We say
    \[
    \lim_{x \rightarrow c}f(x) = L
    \]
    for some $L \in \R$,
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - L| < \varepsilon
    \]
    for all $x \in (a, b) \setminus \{c\}$ with
    \[
    |x - c| < \delta.
    \]
\end{definition}

\textit{Note:
if $c \in X$,
then $L$ can be $f(c)$,
but need not be.
(If it is we will call $f$ continuous at $c$,
see below).}
One often also writes $\lim_{x \rightarrow c}f(x) = L$ in the form
\[
f(x) \rightarrow L\quad\text{as}\quad x \rightarrow c.
\]

\begin{proposition}\label{pre:analy:prop:funclimcriterion}
    Let $f : X \rightarrow \R$ be a function and assume $X$ contains the open interval $(a, b)$ with the possible exception of a point $c \in (a, b)$.
    
    Then $\lim_{x \rightarrow c}f(x) = L$ if and only if for all sequences $\seq$ in $X \setminus \{c\}$ with $x_n \rightarrow c$ as $n \rightarrow \infty$ we have $\lim_{n \rightarrow \infty}f(x_n) = L$.
    \begin{proof}
        First assume $\lim_{x \rightarrow c}f(x) = L$ and let $(x_n)$ be a sequence in $X \setminus \{c\}$ converging to $c$.
        Pick $\varepsilon > 0$.
        Then by assumption there exists a $\delta > 0$ such that
        \[
        |f(x) - L| < \varepsilon\quad\text{provided that}\quad|x - c| < \delta
        \]
        and $x \in (a, b) \setminus \{c\}$.
        Since $x_n \rightarrow c$ as $n \rightarrow \infty$ there exists an $N \in \N$ with
        \[
        |x_n - c| < \delta
        \]
        for all $n \geq N$.
        But then $|f(x_n) - L| < \varepsilon$ for all $n \geq N$.
        This means that $f(x_n) \rightarrow L$ as $n \rightarrow \infty$.

        Now let us assume that $\lim_{x \rightarrow c}f(x) \neq L$
        (or does not exist).
        We need to find a sequence $(x_n)$ in $X \setminus \{c\}$ converging to $c$ such that the sequence $(f(x_n))$ does not converge to $L$.
        By assumption there exists an $\varepsilon > 0$ such that for all $\delta > 0$ there exists a $x \neq c$ with $|x - c| < \delta$ and $|f(x) - L| \geq \varepsilon$.
        So given this $\varepsilon$ we can choose $\delta = \frac{1}{n}$ and then there exists an $x_n \neq c$ with $|x_n - c| < \frac{1}{n}$,
        but $|f(x_n) - L| \geq \varepsilon$.
        But then $x_n \rightarrow c$ as $n \rightarrow \infty$,
        while $f(x_n) \nrightarrow L$.
    \end{proof}
\end{proposition}

We now will extend the definition of a limit of a function to one-sided limits and also to $c = \pm\infty$.
\begin{definition}[Right-sided Limit]
    Let $f : (a, b) \rightarrow \R$ be a function.
    We define the right-sided limit as
    \[
    \lim_{x \rightarrow a ^ {+}}f(x) = L
    \]
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - L| < \varepsilon\qquad\text{for all } x \in (a, b)\text{ with }|x - a| = x - a < \delta.
    \]
\end{definition}

\begin{definition}[Left-sided Limit]
    Let $f : (a, b) \rightarrow \R$ be a function.
    We define the left-sided limit as
    \[
    \lim_{x \rightarrow a ^ {-}}f(x) = L
    \]
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - L| < \varepsilon\qquad\text{for all } x \in (a, b)\text{ with }|x - b| < \delta.
    \]
\end{definition}

We can also define the left/right-sided limit for an interior point $c \in (a, b)$ in same way
(formally,
pick $a = c$ respectively.
$b = c$ in the definition.
Naturally,
the one-sided limits if they exist don't have to be the same!)

If the domain $X$ of $f$ is not bounded above or below
(containing intervals of the form $(b, \infty)$ or $(-\infty, a)$),
we say
\begin{definition}
    \[
    \lim_{x \rightarrow \infty}f(x) = L
    \]
    if for every $\varepsilon > 0$ there exists a $K > 0$ such that
    \[
    |f(x) - L| < \varepsilon\text{ for every } x \in X\text{ with } x \geq K.
    \]
    If $X$ is not bounded below,
    we can define
    \[
    \lim_{x \rightarrow -\infty}f(x) = L
    \]
    analogously.
\end{definition}

We can use the limit criterion \autoref{pre:analy:prop:funclimcriterion} to easily carry over COLT to the limit of functions
\begin{theorem}
    Let $f, g : X \rightarrow \R$ be two functions with $\lim_{x \rightarrow c}f(x) = L_1$ and $\lim_{x \rightarrow c}g(x) = L_2$.
    \begin{enumerate}[label = (\roman*)]
        \item For any $\alpha, \beta \in \R$ the limit $\lim_{x \rightarrow c}\alpha\cdot f(x) + \beta\cdot g(x)$ exists and is equal to $\alpha L_1 + \beta L_2$.
        \item The limit $\lim_{x \rightarrow c}f(x)g(x)$ exists and is equal to $L_1L_2$.
        \item Assume $L_2 \neq 0$.
        The limit $\lim_{x \rightarrow c}f(x) / g(x)$ exists and is equal to $L_1 / L_2$.
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
        \item
            We will use the criterion \autoref{pre:analy:prop:funclimcriterion}.
            For this let $(x_n)$ be a sequence in $X$ with $\limas{x_n}{c}$.
    
            By COLT for sequences and the hypothesis we then have that $\alpha \cdot f(x_n) + \beta \cdot g(x_n)$ converges to $\alpha \cdot L_1 + \beta \cdot L_2$ which then implies the first claim by \autoref{pre:analy:prop:funclimcriterion}.
        
        \textit{The other claims follow in the same way.}
        \item \textbf{Prove!}
        \item \textbf{Prove!}
        \end{enumerate}
    \end{proof}
\end{theorem}

Similarly we also have,
\begin{theorem}
    Let $X \subset \R$,
    $c \in X$ and $f, g, h : X \rightarrow \R$ be three functions $f(x) \leq g(x) \leq h(x)$ for all $x \in X$ and also $\lim_{x \rightarrow c}f(x) = \lim_{x \rightarrow c}h(x) = L$.
    Then also
    \[
    \lim_{x \rightarrow c}g(x) \text{ exists with } \lim_{x \rightarrow c}g(x) = L.
    \]
    \begin{proof}
        This follows from \autoref{pre:analy:prop:funclimcriterion} in exactly the same way.
        So let $(x_n)$ be a sequence in $X$ with $\limas{x_n}{c}$.
        Hence by hypothesis $f(x_n) \leq g(x_n) \leq h(x_n)$ for all $x \in X$ and also $\liminfty f(x_n) = \liminfty h(x_n) = L$.
        Now apply squeezing for sequences to conclude that $\liminfty g(x_n)$ exists and is equal to $L$.
        This implies the claim by \autoref{pre:analy:prop:funclimcriterion}.
    \end{proof}
\end{theorem}

\subsection{Continuity}

\begin{definition}
    Let $f : X \rightarrow \R$ be a function and let $c$ be an interior point of $X$.
    Then $f$ is called continuous at $c \in X$ if
    \[
    \lim_{x \rightarrow c}f(x) = f(c).
    \]
    That is,
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - f(c)| < \varepsilon\quad\text{for all $x \in X$}\quad\text{with } |x - c| < \delta.
    \]
    The function $f : X \rightarrow \R$ is called continuous,
    if it is continuous for all $c \in X$.
\end{definition}

\begin{remark}\phantom{}
    \begin{itemize}
        \item 
        $f$ must be defined at the point $c$ for it to be continuous at $c$.

        \item
        Additionally,
        continuity at a point $c$ is a local property,
        it only depends on the behaviour of $f$ in a
        (small)
        (open)
        interval around $c$.
        So the function defined on $X$ is continuous at $c$ if and only if the function $f$ is restricted to an interval around $c$ is continuous at $c$.
        
        This can make things simpler if $f$ is restricted to a closed and bounded interval around $c$.
    
        \item 
        We can define left-continuity and right-continuity analogously as for the limit of functions.
        It is easy to see that $f$ is continuous at $c$ if and only if it is left and right continuous at $c$.

        In this way one can also define continuity for a function $f$ on the closed interval $[a, b]$ at the boundary points $a$ and $b$.
    \end{itemize}
\end{remark}

\begin{example}
    The identity function $f : \R \rightarrow \R$ given by $f(x) = x$ is continuous.
    \begin{proof}
        Given $c \in \R$ and $\varepsilon > 0$,
        we can choose $\delta = \varepsilon$.
        This completes the proof.
    \end{proof}
\end{example}

\begin{example}
    The function $f : \R \setminus \{0\} \rightarrow \R$ given by $f(x) = 1 / x$ is also continuous.

    This function is not defined at $x = 0$,
    so it is not a question whether $f$ is continuous at $c = 0$.

    At $c \neq 0$ we have
    \[
    \left|\frac{1}{x} - \frac{1}{c}\right| = \left|\frac{c - x}{cx}\right|.
    \]
    This needs to be smaller than the given $\varepsilon > 0$.
    The $c - x$ term is harmless,
    the $c$ in the denominator is just a constant.
    Our problem is the $x$ in the denominator which if we are close to zero can become large.
    We need to restrict ourselves away from $x = 0$.
    Consider the function in the restricted region $|x| > |c| / 2$.
    Note $c$ is certainly still an interior point.
    So now for $|x| > |c| / 2$ have
    \[
    \left|\frac{1}{x} - \frac{1}{c}\right| = \left|\frac{c - x}{cx}\right| \leq \frac{2}{c ^ 2}|c - x|\footnotemark.
    \]
    \footnotetext{This step is a handful at first,
    think about the $|x| > |c| / 2$ inequality hard enough then you should be able to understand this ridiculous transition.}
    This needs to be less than $\varepsilon$.
    We can pick $\delta = \frac{c ^ 2 \varepsilon}{2}$.
    Then if $|x - c| < \delta = \frac{c ^ 2 \varepsilon}{2}$
    (and $|x| > |c| / 2$)
    we get
    \[
    \left|\frac{1}{x} - \frac{1}{c}\right| = \left|\frac{c - x}{cx}\right| \leq \frac{2}{c ^ 2}|c - x| < \frac{2}{c ^ 2}\frac{c ^ 2\varepsilon}{2} = \varepsilon,
    \]
    as required.
    We have shown this works for all $c$ so we have shown that $\frac{1}{x}$ is continuous on all of $\R \setminus \{0\}$\footnote{It may seem weird but since $c$ can get really close to $0$ it technically covers all of our domain,
    I hate it too.}.

    Alternatively,
    if you don't like the trick of restricting the function away from the bad point $x = 0$,
    you could take $\delta = \min\{|c| / 2, c ^ 2\varepsilon / 2\}$,
    and the same calculation will work.
    However,
    I find this quite over the top,
    if you think about the first method hard enough it seems more intuitive.
\end{example}

\begin{example}
    The function $f : [0, \infty) \rightarrow \R$ given by $f(x) = \sqrt{x}$ is continuous.
    First consider $c = 0$.
    Given $\varepsilon > 0$,
    choose $\delta = \varepsilon ^ 2$.
    Then for $|x| = |x - 0| < \delta = \varepsilon$ we get
    \[
    |\sqrt{x} - \sqrt{0}| = |\sqrt{x}| < \sqrt{\delta} = \varepsilon,
    \]
    so $f$ is
    (right)
    continuous at $c = 0$.
    At $c \neq 0$,
    we can write
    \[
    |\sqrt{x} - \sqrt{c}| = \frac{|x - c|}{\sqrt{c} + \sqrt{x}} \leq \frac{|x - c|}{\sqrt{c}}.
    \]
    So given $\varepsilon > 0$,
    choose $\delta = \sqrt{c}\varepsilon$.
\end{example}
\textit{Note:
in the above example we have to consider $c = 0$ as a separate case since we cannot use the same argument we used for $c \neq 0$ when $c = 0$ as we would be dividing by $0$!}

\begin{example}
    Consider the sign function $\mathrm{sgn} : \R \rightarrow \R$ given by
    \[
    \mathrm{sgn}(x) = \begin{cases}
        -1 & x < 0 \\
        0 & x = 0 \\
        1 & x > 0.
    \end{cases}
    \]
    Then $\mathrm{sgn}$ is continuous for every $c \neq 0$,
    but not at $c = 0$.
\end{example}

\begin{example}
    Consider $d : \R \rightarrow \R$ given by
    \[
    d(x) = \begin{cases}
        0 & x \in \Q \\
        1 & x \notin \Q.
    \end{cases}
    \]
    Then $d$ is not continuous at any $c \in \R$.
\end{example}

From \autoref{pre:analy:prop:funclimcriterion} we obtain the very useful criterion.
\begin{proposition}\label{pre:analy:prop:contfuncoflimeqlimofcontfunc}
    Let $X \subset \R$,
    $c \in X$,
    and $f : X \rightarrow \R$ a function.
    Then $f$ is continuous at $c$ if and only if for all sequences $\seq$ in $X$ with $\limas{x_n}{c}$ we have $\limas{f(x_n)}{f(c)}$.
    That is,
    \[
    \liminfty f(x_n) = f\left(\liminfty x_n\right).
    \]
\end{proposition}

We can use this to carry over COLT to continuous functions.
\begin{theorem}\label{pre:analy:thm:coltforcontfuncs}
    Let $X \subset \R$,
    $c \in X$ and $f, g : X \rightarrow \R$ be continuous at $c$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item $h(x) := \alpha \cdot f(x) + \beta \cdot g(x)$ is continuous at $c$ for any $\alpha, \beta \in \R$.
        \item $h(x) := f(x) \cdot g(x)$ is continuous at $c$.
        \item $h(x) := f(x) / g(x)$ is continuous at $c$,
        provided that $g(c) \neq 0$.
    \end{enumerate}
    \begin{proof}
        To show that $h(x)$ is continuous at $c$,
        let $(x_n)$ be a sequence in $X$ with $\limas{x_n}{c}$.
        By COLT and \autoref{pre:analy:prop:contfuncoflimeqlimofcontfunc}
        (applied to the continuous functions $f$ and $g$),
        $\limas{h(x_n)}{h(c)}$.
        By \autoref{pre:analy:prop:contfuncoflimeqlimofcontfunc} $h$ is continuous at $c$.
    \end{proof}
\end{theorem}

\begin{example}
    Given two polynomials $p, q : \R \rightarrow \R$,
    the rational function $r : X \rightarrow \R$ given by
    \[
    r(x) = \frac{p(x)}{q(x)}
    \]
    is continuous,
    where $X = \{x \in \R\,\mid\,q(x) \neq 0\}$ by COLT for continuous functions.
\end{example}

\begin{theorem}\label{pre:analy:thm:contfunccompcontfunciscont}
    Let $X, Y \subset \R$,
    $c \in X$,
    $f : X \rightarrow \R$,
    $g : Y \rightarrow \R$ with $f(X) \subset Y$.
    If $f$ is continuous at $c \in X$ and $g$ is continuous at $f(c) \in Y$,
    then $g \circ f$ is continuous at $c \in X$.

    \begin{proof}
        Let $(x_n)$ be a sequence in $X$ converging to $c$.
        Then $y_n = f(x_n) \rightarrow f(c)$ by \autoref{pre:analy:prop:contfuncoflimeqlimofcontfunc},
        since $f$ is continuous at $c$.
        Then $g(y_n) \rightarrow g(f(c))$ by \autoref{pre:analy:prop:contfuncoflimeqlimofcontfunc},
        since $g$ is continuous at $f(c)$.
        This means $\limas{g \circ f(x_n)}{g \circ f(c)}$,
        and $g \circ f$ is continuous at $c$ by \autoref{pre:analy:prop:contfuncoflimeqlimofcontfunc}.
    \end{proof}
\end{theorem}

\begin{example}
    The exponential function $f : \R \rightarrow \R$ given by $f(x) = e ^ x$ is continuous.
    \begin{proof}
        First we look at $c = 0$,
        so $f(0) = e ^ 0 = 1$.
        Recall the following inequality,
        for $x < 1$
        \[
        1 + x \leq e ^ x \leq \frac{1}{1 - x}.
        \]
        Hence by the squeezing theorem $\lim_{x \rightarrow 0}e ^ x = 1 = e ^ 0$.
        So $e ^ x$ is continuous at $c = 0$.

        Now let $c \neq 0$.
        Then $e ^ {x - c}$ is continuous at $x = c$ as an application of \autoref{pre:analy:thm:contfunccompcontfunciscont}
        (with $f(x) = e ^ x$ and $g(x) = x - c$)\footnote{This can be thought of very slightly easier as $\exp{(x - c)}$.}.
        Using the addition law for the exponential we conclude
        \[
        \lim_{x \rightarrow c}e ^ x = e ^ c \lim_{x \rightarrow c}e ^ {x - c} = e ^ ce ^ 0 = e ^ c,
        \]
        as claimed.
    \end{proof}
\end{example}

\subsection{The Intermediate Value Theorem and the Extreme Value Theorem}

If a function $f : [a, b] \rightarrow \R$ is continuous on a closed interval,
the Intermediate Value Theorem states that $f$ takes on any value between $f(a)$ and $f(b)$.
For example,
if $f(a)$ is negative and $f(b)$ is positive,
there has to be a point $c \in (a, b)$ with $f(c) = 0$.
More precisely,
we have

\begin{theorem}[Intermediate Value Theorem]
    Let $a, b \in \R$ with $a < b$ and $f : [a, b] \rightarrow \R$ be continuous.
    If $f(a) < f(b)$ and $d \in [f(a), f(b)]$,
    then there exists $c \in [a, b]$ with $f(c) = d$.
    Likewise,
    if $f(b) < f(a)$ and $d \in [f(b), f(a)]$,
    then there exists $c \in [a, b]$ with $f(c) = d$.

    \begin{proof}
        We assume that $f(a) < f(b)$.
        We can also assume $d < f(b)$
        (otherwise pick $c = b$)\footnote{As $f(c) = d = f(b)$ but $d < f(b)$.}.
        Let
        \[
        X = \{x \in [a, b]\,|\,f(x) \leq d\}.
        \]
        Then $X$ is bounded above by $b$,
        and $a \in X$.
        So $c = \sup{X}$ exists.
        Hence by
        (some result I cannot remember the name of,
        plus the completeness axiom)
        there is a sequence $(x_n)$ in $X$ with $x_n \rightarrow c$.
        By continuity and \autoref{pre:analy:lem:limofconvseqinclosedintinint} we have $f(x_n) \rightarrow f(c) \leq d < f(b)$;
        in particular $c < b$.
        If we had $f(c) < d$,
        we could find by continuity $\delta > 0$ with $c + \delta < b$ and
        \[
        |f(x) - f(c)| < \frac{d - f(c)}{2}(:= \varepsilon)
        \]
        for all $x \in [a, b]$ with $|x - c| < \delta$.
        But then $f(c + \delta / 2) < d$,
        contradicting that $c$ is the supremum of $X$.
        Therefore $f(c) = d$.
    \end{proof}
\end{theorem}

If $f : X \rightarrow \R$ is a function,
the range of $f$ is the set
\[
f(X) = \{y \in \R\mid y = f(x)\text{ for some } x \in X\}.
\]
If $X$ is an interval and $a < b$ both in $X$,
we can restrict $f$ to $[a, b]$ and get that all values between $f(a)$ and $f(b)$ are taken.
This means that $f(X)$ is an interval as well.

\textit{Note that if $f(X)$ is not bounded above,
the upper endpoint of the interval is $\infty$.}

If $f(X)$ is bounded above,
the upper endpoint of the interval is given by the supremum of $f(X)$.
This supremum may or may not be in the range,
but this is okay for an interval.
The lower endpoint is treated similarly,
so we get

\begin{corollary}\label{pre:analy:corl:funcofintisint}
    Let $I$ be an interval,
    and $f : I \rightarrow \R$ be continuous.
    Then the image $f(I)$ of $I$ under $f$ is an interval.
\end{corollary}

\begin{example}
    Let $f : (0, 2) \rightarrow \R$ be given by
    \[
    f(x) = \frac{x - 1}{x(2 - x)}.
    \]
    Then the range is the interval $(-\infty, \infty) = \R$.
\end{example}

If the domain is a closed interval $[a, b]$,
this cannot happen,
and the range is also a closed interval.

\begin{theorem}
    Let $a, b \in \R$ with $a < b$ and $f : [a, b] \rightarrow \R$ be continuous.
    Then
    
    Every continuous function on a compact interval attains its maximum and minimum.

    Formulated differently,
    the image of $f$ is given by $[c, d]$ for some $c, d \in \R$.
    In particular,
    there exist $u, v \in [a, b]$ with $f(u) = c$ and $f(v) = d$.
    So in conclusion.

    The continuous image of a compact interval is again a compact interval.

    \begin{proof}
        We first show that $f([a, b])$ cannot be unbounded above.
        For otherwise there exists a sequence $\seq$ in $[a, b]$ such that $f(x_n) \geq n$.
        By Bolzano-Weierstrass there is a subsequence $(x_{n_j})$ which is convergent to a $c \in [a, b]$.
        By continuity we have $f(x_{n_j}) \rightarrow f(c) \in \R$.
        But this is a contradiction to $f(x_{n_j}) \geq n_j$ for all $j$.

        Therefore $f([a, b])$ is bounded above,
        and $d = \sup{f([a, b])}$ exists.
        By
        (the same result that I cannot remember)
        there exists a sequence $(y_n)$ in $f([a, b])$ with $\limas{y_n}{d}$.
        But there also exists a sequence $(x_n)$ in $[a, b]$ with $f(x_n) = y_n$.
        By Bolzano-Weierstrass there is a convergent subsequence $(x_{n_j})$ with limit $c \in [a, b]$.
        Then
        \[
        f(c) = f(\liminfty[j]x_{n_j}) = \liminfty[j]f(x_{n_j}) = \liminfty[j]y_{n_j} = d.
        \]
        So $d \in f([a, b])$.
        Similarly we get that $c = \inf{f([a, b])} \in f([a, b])$,
        and by \autoref{pre:analy:corl:funcofintisint} we get $f([a, b]) = [c, d]$.
    \end{proof}
\end{theorem}

\subsection{Uniform continuity}
Recall the definition of continuity for a function $f$ on an interval $I$
\[
\forall c \in I\, \forall \varepsilon > 0\, \exists\delta > 0\, \forall x \in I\quad (|x - c| < \delta \implies |f(x) - f(c)| < \varepsilon)
\]

\begin{remark}\phantom{}
    \begin{itemize}
        \item Note that $\delta$ not only depends on the initial choice of $\varepsilon > 0$,
        but also
        (potentially)
        on the actual point $c$.
        \item Recall that if one $\delta > 0$ does the job,
        then so will any other positive number less than $\delta$.
        \item Recall that continuity at a point $c$ is a local property,
        that is,
        it only depends on the behaviour of $f$ in a
        (small)
        neighbourhood of $c$ and not on the entire domain of definition $I$.
        So if we restrict $f$ to a small(er) interval $J$ containing $c$ as an inner point then the question of continuity at $c$ is unaffected.
    \end{itemize}
\end{remark}

\begin{definition}[Uniform Continuity]
    Let $f$ be a function on an interval $I$.
    We call $f$ uniformly continuous on $I$  if
    \[
    \forall \varepsilon > 0\,\exists \delta > 0\,\forall c, x \in I \quad (|x - c| < \delta \implies |f(x) - f(c)| < \varepsilon).
    \]
    The point is that now $\delta$ has to be the same for all $c \in I$,
    that is,
    $\delta$ no longer depends on the individual point $c$
    (but of course still on $\varepsilon$ and naturally the function $f$ itself).
    In particular,
    uniform continuity is no longer a local concept but a 'global' one as it depends on the entire interval $I$.
\end{definition}

\begin{theorem}\label{pre:analy:thm:contoncompactthenunicontonint}
    Let $f$ be a continuous function on a compact interval $[a, b]$.
    Then $f$ is uniformly continuous on $[a, b]$.
    \begin{proof}
        Assume $f$ is not uniformly continuous on $[a, b]$.
        This means
        \[
        \exists\varepsilon > 0\, \forall\delta > 0\, \exists x, y \in [a, b]\text{ with } |x - y| < \delta \text{ but } |f(x) - f(y)| \geq \varepsilon.
        \]
        Fix such an $\varepsilon > 0$ and take $\delta = \delta_n = \frac{1}{n}$ for each $n \in \N$.
        We find $x_n, y_n \in [a, b]$ such that
        \[
        |x_n - y_n| < \frac{1}{n}\quad\text{but}\quad|f(x_n) - f(y_n)| \geq \varepsilon.
        \]
        The key is,
        by Bolzano-Weierstrass we can find a subsequence $(x_{n_j})$ of $(x_n)$ which converges to a point $x*$ in $[a, b]$.
        (Here we use the fact that we are dealing with a compact interval).
        We claim that also $y_{n_j} \rightarrow x*$.
        Indeed let $\varepsilon' > 0$.
        Then there exists an $N$ such that for all $n_j > N$ we have $|x_{n_j} - x*| < \varepsilon'$ and hence for all $n_j > \max\{1 / \varepsilon', N\}$ we have
        \[
        |y_{n_j} - x*| = |y_{n_j} - x_{n_j} + x_{n_j} - x*| \leq |y_{n_j} - x_{n_j}| + |x_{n_j} - x*| < \varepsilon' + \frac{1}{n_j} < 2\varepsilon'.
        \]
        Thus the $(y_{n_j})$ also converge to $x*$.
        But now we
        (finally)
        use the continuity of $f$ at $x*$.
        There exists a $\delta > 0$ such that $|f(x*) - f(x)| < \varepsilon / 2$ for all $x \in [a, b]$ with $|x - x*| < \delta$.
        Now for $n_j$ sufficiently large we have $|x_{n_j} - x*| < \delta$ and also $|y_{n_j} - x*| < \delta$ and then
        \[
        |f(x_{n_j} - f(y_{n_j})| \leq |f(x_{n_j}) - f(x*)| + |f(x*) - f(y_{n_j})| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
        \]
        in contradiction to $1$.
    \end{proof}
\end{theorem}

\begin{remark}\phantom{}
    \begin{itemize}
        \item We have seen\footnote{In a problem sheet that I cannot be bothered to find.} that a Lipschitz continuous function
        ($|f(x) - f(y)| \leq M|x - y|$ for all $x, y \in I$)
        is uniformly continuous.

        Later we will see that differentiable functions with bounded derivative are Lipschitz and hence are uniformly continuous.

        \item The function $f(x) = \sqrt{x}$ is continuous on $[0, 1]$ but not Lipschitz.
        However,
        the theorem states that it is uniformly continuous.
        So Lipschitz continuity is really a stronger concept than uniform continuity.

        \item The function $f(x) = \frac{1}{x}$ is not uniformly continuous on $(0, 1]$.
        So one really needs a compact interval in the theorem to conclude that continuity implies uniform continuity.

        \item The function $\exp(x) = e ^ x$ is not uniformly continuous on $\R$.
        However,
        $\log(x)$ is uniformly continuous on $[1, \infty)$
        (even though both functions diverge as $x \rightarrow \infty$).
    \end{itemize}
\end{remark}

\subsection{Injective continuous functions and their inverses}

A function $f : X \rightarrow \R$ is said to be strictly increasing if $f(x) < f(y)$ for all $x, y \in X$ with $x < y$.
Similarly,
it is said to be strictly decreasing if $f(x) > f(y)$ for all $x, y \in X$ with $x < y$.
Also recall that $f$ is called injective
(or one-to-one)
if $f(x_1) = f(x_2)$ implies $x_1 = x_2$.
Of course strictly increasing/decreasing functions on $\R$ are injective.
The converse does not always hold.
However,
\begin{proposition}
    Let $f : I \rightarrow \R$ be continuous and injective on an interval $I$.
    Then $f$ is either strictly increasing or strictly decreasing.
    \begin{proof}
        If $f$ was neither we would find elements $a, b, c \in I$ with $a < c < b$ but with $f(c)$ bigger
        (smaller goes the same)
        than both $f(a)$ and $f(b)$,
        say for example $f(a) < f(b) < f(c)$.
        But then $f$ must take the value $f(b)$ in the interval $[a, c]$
        (which does not contain $b$)
        again.
        Thus $f$ would not be one-to-one.
    \end{proof}
\end{proposition}

Assume $f : X \rightarrow \R$ is injective.
Recall we can form its inverse $f ^ {-1} : f(X) \rightarrow \R$ by $f ^ {-1}(x) = y$,
where $y \in X$ is the unique element with $f(y) = x$.
In particular,
the domain of $f ^ {-1}$ is the range of $f$ and the range of $f ^ {-1}$ is the domain $X$ of $f$.
Note $f(f ^ {-1}(c)) = c$ for all $c \in f(X)$ and $f ^ {-1}(f(d)) = d$ for all $d \in X$.

\begin{example}
    The logarithm was defined as the inverse function of the exponential function.
    So with $f(x) = e ^ x$,
    we get $f ^ {-1}(x) = \log{x}$.
    Notice that $f ^ {-1}$ is only defined for $x > 0$,
    since $e ^ x > 0$ for all $x \in \R$.
\end{example}

\begin{theorem}\label{pre:analy:thm:imgofinvcontinjfunciscont}
    Let $f : I \rightarrow \R$ be continuous and injective on a non-trivial interval $I$ with image $J := f(I)$.
    Then $f ^ {-1} : J \rightarrow \R$ is also continuous.

    \begin{proof}
        Let $d \in J$,
        say $d = f(c)$ for some $c \in I$.
        We need to show that given $\varepsilon > 0$ there exists $\delta > 0$ so that $|f ^ {-1}(d) - f ^ {-1}(y)| < \varepsilon$ for all $y \in J$ with $|y - d| < \delta$.

        Assume for simplicity that $f$ is strictly increasing
        (and hence also $f ^ {-1}$).
        Assume for the moment that $c$ is an interior point of $I$.
        Then we can assume $[c - \varepsilon / 2, c + \varepsilon / 2] \subset I$.
        Then under $f$ this becomes $[f(c - \varepsilon / 2), f(c + \varepsilon / 2)]$ with $f(c - \varepsilon / 2) < d < f(c + \varepsilon / 2)$.
        Now find $\delta > 0$ such that $(d - \delta, d + \delta) \subset [f(c - \varepsilon / 2), f(c + \varepsilon / 2)]$.
        Then for every $y \in J$ with $|y - d| < \delta$ we have $f ^ {-1}(y) \in [c - \varepsilon / 2, c + \varepsilon / 2]$.
        Hence $|f ^ {-1}(y) - f ^ {-1}(d)| = |f ^ {-1}(y) - f ^ {-1}(f(c))| = |g(y) - c| \leq \varepsilon / 2 < \varepsilon$ as desired.
        If $c$ is an endpoint of $I$
        (if it exists),
        say the upper one,
        then so is $d \in J$,
        and the argument carries through with $c + \varepsilon / 2$ and $d + \delta$ replaced by $c$ and $d$ respectively.
        The lower endpoint goes the same.
    \end{proof}
\end{theorem}

\begin{example}\phantom{}
    \begin{itemize}
        \item The logarithm $\log(x) : \R_{+} \rightarrow \R$ is continuous.
        \item It is important to note that $f$ is defined on an interval in \autoref{pre:analy:thm:imgofinvcontinjfunciscont} to get the continuity of the inverse function.

        Consider $f : [0, 1] \cup (2, 3] \rightarrow \R$ given by
        \[
        f(x) = \begin{cases}
            2x & \text{for } x \in [0, 1] \\
            x & \text{for } x \in (2, 3].
        \end{cases}
        \]
        This is easily seen to be continuous and injective
        (note that $2$ is not in the domain of $f$).
        The inverse function $f ^ {-1} : [0, 2] \rightarrow \R$ is given by
        \[
        f ^ {-1}(x) = \begin{cases}
            \frac{1}{2}x & \text{for } x \in [0, 2] \\
            \frac{1}{2}x & \text{for } x \in (2, 3]
        \end{cases}
        \]
        and it is not continuous at $2$,
        as $f ^ {-1}(2 + 1 / n) \rightarrow 2 \neq f(2)$.

        It is instructive to revisit the proof of the continuous inverse proposition above and to investigate what goes wrong for this function.
        It is fairly subtle at $d = 2 = f(1)$.
        Namely,
        while $c = 1$ is the endpoint of one interval of the domain of $f$,
        the value $d = 2$ is not for the range.
    \end{itemize}
\end{example}

\begin{remark}
    In general,
    one want would like to be true is that if the function $f$ is "nice"
    (has a certain property),
    then the inverse function $f$ should be equally "nice"
    (has the same property).
    Unfortunately,
    this does not always hold;
    one usually has to require an extra condition.
    We have already seen this for continuous functions on the real line to itself
    (in a mild way).
\end{remark}

\newpage

\section{Differentiable Functions}

\subsection{Basics on differentiable functions}

Let us first define differentiability
\begin{definition}
    Let $X \subseteq \R$ be open and $f : X \rightarrow \R$ be a function.
    $f$ is differentiable at $c \in X$ if $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}$ exists.
    We denote this limit by $f'(c)$ and call $f'(c)$ the derivative of $f$ at $c$.
    $f$ is called a differentiable function if $f$ is differentiable at all points $c \in X$.
\end{definition}

One often encounters the formulation
\[
f'(c) = \lim_{h \rightarrow 0}\frac{f(c - h) - f(c)}{h},
\]
which is the same
(set $h = x - c$).
Sometimes this formulation is more convenient.

\begin{remark}\phantom{}
    \begin{itemize}
        \item The expression $\frac{f(x) - f(c)}{x - c}$ has an important geometric interpretation:
        It is the slope of the straight line passing through the two points $(c, f(c)) \in \R ^ 2$ and $(x, f(x)) \in \R ^ 2$ of the graph of $f$.
        As $x \rightarrow c$,
        the second point $(x, f(x))$ approaches the first point $(c, f(c))$,
        and the limit describes the slope of the tangent of the graph of $f$ at the point $(c, f(c)) \in \R ^ 2$.

        \item Differentiability at $c \in X$ can also be described in the $(\varepsilon, \delta)$-formalism.
        $f$ is differentiable at $c \in X$ if there exists a number $L \in \R$
        (the derivative $f'(c)$)
        such that there exists for every $\varepsilon > 0$ a positive number $\delta > 0$ such that
        \[
        \left|\frac{f(c) - f(x)}{c - x} - L\right| < \varepsilon\qquad\forall x \in X \text{ with } |x - c| < \delta.
        \]

        \item We can also define
        (one-sided)
        differentiability for a function defined on a closed interval $[a, b]$ at the endpoints $a$ and $b$ by considering the one-sided limit of the difference quotient.

        We now give another equivalent formulation for differentiability which is not only handy but it also generalises to higher dimensions.
    \end{itemize}
\end{remark}

\begin{lemma}[First order Taylor]\label{pre:analy:lem:firstordertaylorindiffdef}
    Let $f : X \rightarrow \R$ be a function as above.
    Then $f$ is differentiable at $c \in X$ if and only if the following holds:

    There exists a constant $m \in \R$ an a function $r(x)$ on $X$ such that
    \[
    f(x) = f(c) + m(x - c) + r(x)(x - c)
    \]
    such that
    \[
    r(x)\text{ is continuous at } c \qquad\text{and}\qquad\lim_{x \rightarrow c}r(x) = r(c) = 0.
    \]
    In this case $m = f'(c)$.
\end{lemma}
Before we get into the juicy proof of this,
let us have a few remarks.
\begin{remark}\phantom{}
    \begin{itemize}
        \item We can view differentiability as the attempt to approximate $f(x)$ in the neighbourhood of $c$ by a linear function $L(x) = f(c) + m(x - c)$.
        Then differentiability stipulates that the error $r(x)(x - c)$ is of 'higher order'
        (not linear)
        as $\lim_{x \rightarrow c}r(x) = 0$.

        \item From the perspective of the previous,
        we can view continuity as the 'zero order Taylor'.
        The function $f$ is continuous at $c$ if $f(x) = f(c) + \tilde{r}(x)$ with $\lim_{x \rightarrow c}\tilde{r}(x) = 0$.
        We approximate $f$ by the constant function $f(c)$ and the error vanishes in the limit.
    \end{itemize}
\end{remark}

\begin{remark}
    In practice one often considers $f_1(x) = m + r(x)$ which is continuous at $c$ with value $f_1(c) = m$.
    Thus differentiability at $c$ is equivalent to:

    There exists a function $f_1 : X \rightarrow \R$ such that
    \[
    f(x) = f(c) + (x - c)f_1(x)
    \]
    and $f_1$ is continuous at $c$.
    Then $\lim_{x \rightarrow c}f_1(x) = f_1(c) = f'(c)$.
    (In this formulation one doesn't need to know the value of the derivative in advance).
\end{remark}

\begin{lemma}[continues = pre:analy:lem:firstordertaylorindiffdef]
    \begin{proof}
        Assume $f$ is differentiable at $c$,
        so the limit $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}$ exists.
        Let's denote this limit by $m$ and set
        \[
        r(x) = \begin{cases}
            \frac{f(x) - (f(c) + m(x - c))}{x - c} & \text{if } x \neq c; \\
            0 & \text{if } x = c.
        \end{cases}
        \]
        Then if $r(x)$ is continuous at $c$ holds,
        and we only need to check continuity at $c$:
        But
        \begin{align*}
            \lim_{x \rightarrow c}r(x) &= \lim_{x \rightarrow c}\frac{f(x) - (f(c) + m(x - c))}{x - c} \\
            &= \lim_{x \rightarrow c}\left(\frac{f(x) - f(c)}{x - c} - m\right) \\
            &= m - m = 0,
        \end{align*}
        as required.

        Conversely,
        if $\lim_{x \rightarrow c}r(x) = r(c) = 0$ holds with $r$ continuous at $c$ and $r(c) = 0$,
        then
        \begin{align*}
            0 &= r(c) \\
            &= \lim_{x \rightarrow c}r(x) \\
            &= \lim_{x \rightarrow c}\frac{f(x) - (f(c) + m(x - c))}{x - c} \\
            &= \lim_{x \rightarrow c}\left(\frac{f(x) - f(c)}{x - c} - m\right).
        \end{align*}
        But this is only possible if $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}$ exists and is equal to $m$.
    \end{proof}
\end{lemma}

\begin{example}\phantom{}
    \begin{itemize}
        \item Let us prove differentiability of $f(x) = \frac{1}{x}$ on $(0, \infty)$.
        Let $c \in (0, \infty)$.
        Then we have
        \[
        \frac{f(x) - f(c)}{x - c} = \frac{c - x}{xc(x - c)} = -\frac{1}{xc},
        \]
        which implies that
        \[
        \lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c} = -\lim_{x \rightarrow c}\frac{1}{xc} = -\frac{1}{c ^ 2}.
        \]
        This shows that $f'(c) = -\frac{1}{c ^ 2}$.
        Now for the 'remainder' $r(x)$ we have
        \[
        r(x) = \frac{f(x) - f(c)}{x - c} - f'(c) = -\frac{1}{xc} + \frac{1}{c ^ 2} = \frac{x - c}{xc ^ 2},
        \]
        which indeed is continuous at $c$ with limit $0$.

        \item Let $f(x)$ be a polynomial.
        Then $f(x) - f(c)$ is also a polynomial which has a zero at $x = c$.
        Thus by polynomial division we can write
        \[
        f(x) - f(c) = (x - c)f_1(x)
        \]
        for some polynomial $f_1(x)$.
        In particular,
        $f_1(x)$ is continuous.
        We therefore have shown that all polynomials are differentiable!
        (Of course,
        we haven't shown what the derivative actually is;
        for $x ^ n$ it is fairly direct to determine $f_1(x)$ and its limit $nc ^ {n - 1}$ as $x \rightarrow c$.).
    \end{itemize}
\end{example}

Differentiability is a stronger property than continuity:
\begin{theorem}\label{pre:analy:thm:iffdiffatcthencontatc}
    Let $X \subset \R$ and $f : X \rightarrow \R$.
    If $f$ is differentiable at $c \in X$ then $f$ is also continuous at $c$.
    \begin{proof}
        We have
        \[
        f(x) - f(c) = (x - c) \cdot \frac{f(x) - f(c)}{x - c} \rightarrow 0 \cdot f'(c) = 0\text{ as } x \rightarrow c.
        \]
        This shows that $\lim_{x \rightarrow c}f(x) = f(c)$,
        in other words,
        $f$ is continuous at $c$.
    \end{proof}
\end{theorem}

\begin{remark}
    There are continuous functions which are not differentiable.
    The easiest example is the function $f(x) = |x|$,
    which is continuous on $\R$ but not differentiable at $c = 0$.
    However,
    the construction of a function $f : \R \rightarrow \R$ which is everywhere continuous but nowhere differentiable is a much more difficult task.
    
    Historically,
    there was a belief that every continuous function should be differentiable everywhere except a set of isolated points.
\end{remark}

\begin{theorem}\label{pre:analy:thm:sumscalarprodcomprulesfordiff}
    \begin{enumerate}[label = (\alph*)]
        \item 
        Let $f, g$ be two functions which are differentiable at $c$ and let $\alpha \in \R$ be a constant.
        Then $f + g$ and $\alpha f$ are also differentiable at $c$,
        and we have
        \begin{align*}
            (f + g)'(c) &= f'(c) + g'(c) \\
            (\alpha f)'(c) &= \alpha f'(c)
        \end{align*}
        Similarly,
        their product $fg$ is also differentiable at $c$ with
        \[
        (fg)'(c) = f(c)g'(c) + f'(c)g(c).
        \]

        \item
        Let $f, g$ be two functions such that $g$ is differentiable at $c$ and $f$ is differentiable at $g(c)$.
        Then the composition $f \circ g$ is also differentiable at $c$ and we have
        \[
        (f \circ g)'(c) = f'(g(c))g'(c).
        \]
        
        \item 
        Let $f$ be a function which is differentiable at $c$ and we have $f(c) \neq 0$.
        Then $1 / f$ is also differentiable at $c$ and we have
        \[
        \left(\frac{1}{f}\right)'(c) = -\frac{f'(c)}{f ^ 2(c)}.
        \]
    \end{enumerate}
    \begin{proof}
        We only prove the product of the functions and the composition of functions.
        This can be proven by the equivalent formulation given in \autoref{pre:analy:lem:firstordertaylorindiffdef} and its subsequence remark.
        \begin{enumerate}[label = (\alph*)]
            \item
            We have $f(x) = f(c) + (x - c)f_1(x)$ and $g(x) = g(c) + (x - c)g_1(x)$.
            This implies that
            \[
            (fg)(x) = (fg)(c) + (x - c)h(x).
            \]
            With $h(x) = f(c)g_1(x) + f_1(x)g(c) + (x - c)f_1(x)g_1(x)$.
            It is easy to see that $h$ is continuous at $c$ and,
            therefore,
            $fg$ is differentiable at $c$ with derivative
            \[
            (fg)'(c) = \lim_{x \rightarrow c}h(x) = f(c)g_1(c) + f_1(c)g(c) = f(c)g'(c) + f'(c)g(c).
            \]
            \item
            We can write $f(y) = f(g(c)) + (y - g(c))f_1(y)$ and $g(x) = g(c) + (x - c)g_1(x)$ with $f_1$ continuous at $g(c)$ and $g_1$ continuous at $c$.
            Then we have
            \begin{multline*}
                (f \circ g)(x) = f(g(x)) = f(g(c)) + (g(x) - g(c))f_1(g(x)) = \\
                f(g(c)) + (x - c)g_1(c)f_1(g(x)) = (f \circ g)(c) + (x - c)h(x)
            \end{multline*}
            with $h(x) = (f_1 \circ g)(c)g_1(c)$.
            Since $f_1$ is continuous at $g(c)$ and $g$ is continuous at $c$ by \autoref{pre:analy:thm:iffdiffatcthencontatc},
            the composition $f_1 \circ g$ is continuous at $c$.
            Using again the equivalent formulation for differentiability,
            we conclude that $f \circ g$ is differentiable at $c$ and we have
            \[
            (f \circ g)'(c) = \lim_{x \rightarrow c}h(x) = f_1(g(c))g_1(c) = f'(g(c))g'(c).
            \]
        \end{enumerate}

        The rest of the proof is an \textbf{exercise}.
    \end{proof}
\end{theorem}

\begin{remark}
    One may be tempted to prove the chain rule easily in the following way:
    \[
    \frac{f(g(x)) - f(g(c))}{x - c} = \frac{f(g(x)) - f(g(c))}{x - c} \times \frac{g(x) - g(c)}{x - c}
    \]
    and argue that in the limit the first fraction goes to $f'(g(c))$,
    while the second goes to $g'(c)$.

    In some sense this is how Leibniz' differential calculus would work.


    However,
    this is problematic since one cannot rule out $g(x) - g(c) = 0$,
    e.g.,
    if $g$ is constant in a neighbourhood of $c$ or if there is a sequence $x_n$ with $\liminfty x_n = c$ and $g(x_n) - g(c) = 0$.
    In general,
    the above approach can be made to work if $g'(c) \neq 0$,
    but $g'(c) = 0$ becomes a bit painful to consider.
    The above proof avoids these complications.
\end{remark}

\begin{example}
    \begin{itemize}
        \item The derivative of $f(x) = x$ is clearly $f'(x) = 1$.
        Then by \autoref{pre:analy:thm:sumscalarprodcomprulesfordiff}(a) we obtain the standard formula for the derivative of polynomials and hence using (c) the one for rational functions.

        \item We now show that the derivative of $e ^ x$ is $e ^ x$ itself.

        We start again at $c = 0$.

        Using the $e ^ x$ inequality lemma
        (which needs adding in here)
        again we have for $x < 1$ we have
        \[
        x \leq e ^ x - 1 \leq \frac{x}{1 - x}.
        \]
        Hence by squeezing $\lim_{x \rightarrow 0}\frac{e ^ x - 1}{x} = 1$.
        For general $c$ we have
        \[
        \frac{e ^ x - e ^ c}{x - c} = e ^ c\frac{e ^ {x - c} - 1}{x - c} \rightarrow e ^ c
        \]
        as $x \rightarrow c$ by the same argument
        ($x - c$ goes to $0$;
        if you don't like that take any sequence $x_n \rightarrow c$ so that the sequence $x_n - c$ goes to zero and apply the sequence criterion for limit of functions).

        In conclusion:
        $e ^ x$ is differentiable on all of $\R$ being its own derivative!

        \item Consider $c ^ x = e ^ {\log(c)x}$ for $c > 0$.
        Then by the chain rule $(c ^ x)' = \log(c)c ^ x$.
    \end{itemize}
\end{example}

\subsection{Inverse functions}

We want for $f$ being differentiable and the one-to-one inverse $f ^ {-1}$ also being differentiable.
However,
while this is a natural expectation it does not always have a positive answer.
We begin with the following warning:

When $f$ is differentiable and invertible with inverse $g$ then this does not necessarily imply that $g$ is differentiable!
The function $f(x) = x ^ 3$ on $\R$ is continuous and differentiable and strictly monotone increasing and hence has a continuous inverse $g(x) = \sqrt[3]{x}$,
the third root.
However $g$ is not differentiable at zero where the horizontal tangent line of $f$ turns into a vertical tangent line of $g$!

However,
we have
\begin{proposition}\label{pre:analy:prop:contfuncderivofinverse}
    Let $f : I \rightarrow \R$ be continuous on the interval $I$,
    differentiable at a point $d$.
    Assume that $f$ is invertible with inverse $f ^ {-1}$ and that $f'(d) \neq 0$.

    Then $f ^ {-1}$ is differentiable at $c := f(d)$ with
    \[
    (f ^ {-1})'(c) = \frac{1}{f'(f ^ {-1}(c))}.
    \]
    \begin{proof}
        Write $g := f ^ {-1}$.
        We need to show that for any sequence $(y_n)$ with limit $\liminfty y_n = c$ we have $\liminfty \frac{g(y_n) - g(c)}{y_n - c} = \frac{1}{f'(g(c))}$.
        Now set $x_n = g(y_n)$.
        Since $g$ is continuous by \autoref{pre:analy:thm:imgofinvcontinjfunciscont} we have $\liminfty x_n = \liminfty g(y_n) = g(\liminfty y_n) = g(c) = d$.
        Then
        \[
        \liminfty\frac{g(y_n) - g(c)}{y_n - c} = \liminfty \frac{x_n - d}{f(x_n) - f(d)} = \liminfty\frac{1}{\frac{f(x_n) - f(d)}{x_n - d}} = \frac{1}{f'(d)}.
        \]
    \end{proof}
\end{proposition}

\begin{remark}
    If for one reason one already knows or assumes that the inverse $g$ is differentiable one has a very quick argument to obtain the formula for $g'$.
    Namely,
    we use the chain rule to differentiate the equation $x = f \circ g(x)$ and obtain
    \[
    1 = g'(x)f'(g(x)),
    \]
    which then gives $g'(x) = 1 / f'(g(x))$.
    But of course,
    for this one needs to know in advance that $g$ is differentiable.
    There is however still another merit in this calculation:
    It identifies a necessary condition for the $g$ being differentiable at $x: f'(g(x))$ has to be non-zero.
\end{remark}

\begin{example}
    We know $\log(x)$ is the inverse function of $e ^ x$.
    Using $(e ^ x)' = e ^ x$ we immediately see
    \[
    \log'(x) = \frac{1}{e ^ {\log{x}}} = \frac{1}{x}.
    \]
    Using this we can now compute the derivative if $f(x) := x ^ \alpha = e ^ {\alpha\log(x)}$ for $\alpha \in \R$ and $x > 0$.
    Namely,
    by the chain rule we see
    \[
    f'(x) = \frac{\alpha}{x}x ^ \alpha = \alpha x ^ {\alpha - 1},
    \]
    as desired!
\end{example}

\subsection{Mean value theorems}

We have already seen the following fact and its proof in the Calculus course:
\begin{proposition}\label{pre:analy:prop:fdiffatcthenlocmaxorminatdifffc}
    If $f$ is differentiable at $c$ and has a local maximum or minimum at $c$ then $f'(c) = 0$.

    \begin{proof}
        The difference quotient $\frac{f(x) - f(c)}{x - c}$ must have opposite signs for $x > c$ and $x < c$.
        Hence for the one-sided limits we obtain $\geq 0$ and $\leq 0$ respectively.
        But the limits must be the same which is only possible with $0$.
    \end{proof}
\end{proposition}

We have also already seen Rolle's Theorem and the Mean Value Theorem.
However,
for completeness we will restate them.
\begin{theorem}[Rolle's Theorem]\label{pre:analy:thm:rollesthm}
    Let $f : [a, b] \rightarrow \R$ be continuous and differentiable on $(a, b)$ and suppose that $f(a) = f(b)$.
    Then there exists $c \in (a, b)$ such that $f'(c) = 0$.
    \begin{proof}
        As a continuous function on a closed interval $f$ attains its minimum and maximum.
        If one of them is in the interior we are done by the pervious proposition        
        (\autoref{pre:analy:prop:fdiffatcthenlocmaxorminatdifffc}).
        Otherwise,
        minimum and maximum occur at the endpoints.
        Since $f(a) = f(b)$ this implies $f$ must be constant in which case $f'(x) = 0$ for all $x \in (a, b)$.
    \end{proof}
\end{theorem}

\begin{theorem}[Mean Value Theorem]\label{pre:analy:thm:meanvaluethm}
    Let $f : [a, b] \rightarrow \R$ be continuous and differentiable on $(a, b)$.
    Then there exists $c \in (a, b)$ such that
    \[
    f'(c) = \frac{f(b) - f(a)}{b - a}.
    \]
    \begin{proof}
        Set
        \[
        g(x) := f(x) - \frac{f(b) - f(a)}{b - a}(x - a).
        \]
        Then $g$ is continuous on $[a, b]$ and differentiable on $(a, b)$ with
        \[
        g'(x) = f'(x) - \frac{f(b) - f(a)}{b - a}.
        \]
        Moreover,
        $g(a) = f(a)$ and $g(b) = f(a)$.
        Hence by Rolle there exists a $c \in (a, b)$ such that
        \[
        0 = f'(c) - \frac{f(b) - f(a)}{b - a},
        \]
        as claimed.
    \end{proof}
\end{theorem}

An immediate consequence of the Mean Value Theorem is the following
\begin{theorem}\label{pre:analy:thm:contdiffthenfunchasprops}
    Let $f : I \rightarrow \R$ be a continuous function on an interval $I$,
    differentiable in its interior points.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item If $f'(x) = 0$ for all $x$,
        then $f$ is constant.
        \item If $f'(x) \leq 0$ ($\leq 0$) for all $x$,
        then $f$ is monotone increasing
        (decreasing).
        \item If $f'(x) > 0$ ($< 0$) for all $x$,
        then $f$ is strictly monotone increasing
        (decreasing).
    \end{enumerate}
    \begin{proof}
        Let $c < d$ in $I$ be two arbitrary points.
        Then by the Mean Value Theorem there exists $\alpha$ in the interval $(c, d)$ such that
        \[
        f(d) - f(c) = (d - c)f'(\alpha) \begin{dcases*}
            = 0 & for case (i); \\
            \geq 0 & for case (ii); \\
            > 0 & for case (iii).
        \end{dcases*}
        \]
        Thus
        \[
        \begin{dcases*}
            f(c) = f(d) & for case (i); \\
            f(c) \leq f(d) & for case (ii); \\
            f(c) < f(d) & for case (iii).
        \end{dcases*}
        \]
        The claim follows since $c$ and $d$ were arbitrarily chosen.
    \end{proof}
\end{theorem}

The requirement on $I$ being an interval is absolutely essential!
For example $f(x) = 1 /x$ is differentiable on $\R* = (-\infty, 0) \cup (0, \infty)$,
the union of two intervals.
Now $f'(x) = -1 / x ^ 2 < 0$ for all $x$,
but $f(x)$ is certainly not overall decreasing,
but only on the two individual intervals $(-\infty, 0)$ and $(0, \infty)$.

Th issue is that an interval is connected\footnote{Don't worry about this concept at the moment,
it is next years problem.}.

Rolle's theorem implies a more general mean value theorem
(from which the regular Mean Value follows by setting $g(x) = x$).

\begin{theorem}[Cauchy's Generalised Mean Value Theorem]\label{pre:analy:thm:genmeanvalthm}
    Let $f, g : [a, b] \rightarrow \R$ be continuous and differentiable on $(a, b)$.
    Assume that $g'(x) \neq 0$ for all $x \in (a, b)$.
    Then there exists $c \in (a, b)$ such that
    \[
    \frac{f'(c)}{g'(c)} = \frac{f(b) - f(a)}{g(b) - g(a)}.
    \]
    \begin{center}
    \fbox{
    \begin{minipage}{0.9\textwidth}
    You may wonder why this is not a simple corollary of the Mean Value theorem.
    But applying \autoref{pre:analy:thm:meanvaluethm} to both functions $f$ and $g$ would lead to
    \[
    \frac{f'(c_1)}{g'(c_2)} = \frac{f(b) - f(a)}{g(b) - g(a)}.
    \]
    The important point of this theorem is that we can choose $c_1 = c_2$ on the left hand side.
    \end{minipage}
    }
    \end{center}
    \hfill
    \begin{proof}
        Consider the function
        \[
        h(x) = (g(b) - g(a))f(x) - (f(b) - f(a))g(x).
        \]
        Then we have
        \[
        h(a) = g(b)f(a) - f(b)g(a) = h(b).
        \]
        $h$ is obviously continuous on $[a, b]$ and differentiable on $(a, b)$,
        and we can apply Rolle's theorem to $h$.
        Therefore,
        there exists $c \in (a, b)$ such that
        \begin{equation}\label{pre:analy:eq:1}
            0 = h'(c) = (g(b) - g(a))f'(c) - (f(b) - f(a))g'(c).
        \end{equation}
        Since $g'(x) \neq 0$ for all $x \in (a, b)$,
        we can conclude from \autoref{pre:analy:thm:meanvaluethm} that $g(b) - g(a) \neq 0$,
        and we can rewrite \eqref{pre:analy:eq:1} as
        \[
        \frac{f'(c)}{g'(c)} = \frac{f(b) - f(a)}{g(b) - g(a)}.
        \]
    \end{proof}
\end{theorem}

We define higher derivatives in the usual way.
\textit{Note that for $f''(c)$ as the derivative of $f'(x)$ at $x = c$ we need to assume that $f$ is differentiable in an interval around $c$.}

However,
we will not be discussing the role of the second derivatives.

\subsection{L' H\^opital's rule}

An application of \autoref{pre:analy:thm:genmeanvalthm} is L' H\^opital's Rule.
We give the following version
(There exists some with weaker hypotheses,
e.g. the one in Spivak's Calculus book)

\begin{theorem}[L' H\^opital's Rule]\label{pre:analy:thm:lhopitalsrule}
    Let $f$ and $g$ be two differentiable functions in some interval $(a, b)$.

    Assume that
    (the one-sided limits)
    $\lim_{x \rightarrow a ^ {+}}f(x) = 0$ and $\lim_{x \rightarrow a ^ {+}}g(x) = 0$ and $g(x) \neq 0$,
    $g'(x) \neq 0$ for all $x$.

    Then if $\lim_{x \rightarrow a ^ {+}}f'(x) / g'(x)$ exists,
    then also $\lim_{x \rightarrow a ^ {+}}f(x) / g(x)$ exists,
    and we have
    \[
    \lim_{x \rightarrow a ^ {+}}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a ^ {+}}\frac{f'(x)}{g'(x)}.
    \]
    \begin{proof}
        We can extend $f$ and $g$ continuously to $x = a$ by setting $f(a) = g(a) = 0$.
        Let's write $L = \lim_{x \rightarrow a ^ {+}}f'(x) / g'(x)$
        ($L = \pm\infty$ is actually ok).
        Let $(x_n)$ be any sequence in $(a, b)$ such that $\liminfty x_n = a$.
        We need to show
        \[
        \liminfty\frac{f(x_n)}{g(x_n)} = L.
        \]
        By the generalised mean value theorem for each $n$ we can find $y_n$ with $a < y_n < x_n$ such that
        \[
        \frac{f'(y_n)}{g'(y_n)} = \frac{f(x_n) - f(a)}{g(x_n) - g(a)} = \frac{f(x_n)}{g(x_n)}.
        \]
        By squeezing we see $\liminfty y_n = a$,
        and hence
        \[
        L = \liminfty\frac{f'(y_n)}{g'(y_n)} = \liminfty\frac{f(x_n)}{g(x_n)},
        \]
        as claimed.
    \end{proof}
\end{theorem}

\begin{remark}
    There exists a simple proof for L' H\^opital's Rule in the special case $f(c) = g(c) = 0$ and differentiable at $c$ with $g'(c) \neq 0$.
    Then
    \[
    \lim_{x \rightarrow c}\frac{f(x)}{g(x)} = \lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}\frac{x - c}{g(x)} = \lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}\lim_{x \rightarrow c}\frac{x - c}{g(x)} = \frac{f'(c)}{g'(c)}.
    \]
    \textit{Note how we needed the extra assumptions in order to apply COLT}.
\end{remark}

\begin{remark}
    \begin{enumerate}[label = (\roman*)]
        \item There is of course a version for the other one-sided limit and for regular two-sided limits as well.
        \item The cases $a = -\infty$ or $b = \infty$ are also ok.
        This can be done by considering $f(1 / x)$ and $g(1 / x)$.
        Indeed,
        assuming $\lim_{x \rightarrow \pm\infty}f'(x) / g'(x)$ exists we compute
        \[
        \lim_{x \rightarrow \pm\infty}\frac{f(x)}{g(x)} = \lim_{x \rightarrow 0 ^ {\pm}}\frac{f(1 / x)}{g(1 / x)} = \lim_{x \rightarrow 0 ^ {\pm}}\frac{-\frac{1}{x ^ 2}f'(1 / x)}{-\frac{1}{x ^ 2}g'(1 / x)} = \lim_{x \rightarrow \pm\infty}\frac{f'(x)}{g'(x)}.
        \]
        \item Limits of the form "$\frac{\infty}{\infty}$" could in principle be handled by considering $\frac{f(x)}{g(x)} = \frac{1 / g(x)}{1 / f(x)}$,
        but this only works in special examples.
        Rather one can upgrade the given proof and obtain
        \[
        \lim_{x \rightarrow a ^ {\pm}}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a ^ {\pm}}\frac{f'(x)}{g'(x)}.
        \]
        \item Limits of the form "$0 \cdot \infty$" can often be handled by considering $f(x)g(x) = \frac{f(x)}{1 / g(x)}$.
        \item In applications one sometimes has to apply L' H\^opital's Rule more than once to get a limit of the ratio of higher derivatives.
    \end{enumerate}
\end{remark}
Be careful that all conditions of L' H\^opital's Rule are satisfied,
in particular the condition that $f, g$ are differentiable and that $f(c) = g(c) = 0$.
Without checking these conditions,
L' H\^opital's Rule may lead to absolutely wrong results,
like the following:
\[
\lim_{x \rightarrow 1}\frac{x ^ 2 + x + 2}{x - 2} "=" \lim_{x \rightarrow 1}\frac{2x + 1}{1} = 3.
\]
In fact,
the correct limit is $-4$ by continuity.
\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item Tautological use.
        Using L' H\^opital's Rule we see
        \[
        \lim_{x \rightarrow 0}\frac{e ^ {x} - 1}{x} \lim_{x \rightarrow 0}\frac{e ^ x}{1} = e ^ 0 = 1.
        \]
        This is a bit of tautology or circular reasoning since this is exactly the calculation to show that $e ^ x$ is differentiable
        (at $0$)
        in the first place!

        In general,
        for $f$ differentiable computing
        \[
        \lim_{x \rightarrow c}\frac{f(x) - f(c)}{x -c} = \lim_{x \rightarrow c}\frac{f'(x)}{1} = f'(c).
        \]
        appears to be a bit silly.
        However,
        if one only knows that $f$ is differentiable in $(c, d)$ but not yet at $c$ itself,
        then this calculation shows that $f$ is differentiable at $c$
        (if the limit exists!)
        
        \item A special case of Taylor's theorem.
        Using L' H\^opital's Rule twice we see
        \[
        \lim_{x \rightarrow 0}\frac{\log(1 + x) - x}{x ^ 2} = \lim_{x \rightarrow 0}\frac{1 / (1 + x) - 1}{2x} = \lim_{x \rightarrow 0}\frac{-1 / (1 + x) ^ 2}{2} = -\frac{1}{2}.
        \]
        We can rewrite this as
        \[
        \log(1 + x) = x - \frac{1}{2}x ^ 2 + r(x)x ^ 2
        \]
        with $\lim_{x \rightarrow 0}r(x) = 0$.
        This version of Taylor's theorem which we consider in the next subsection.

        \item Using \lhopital's Rule twice - in detail.

        The exact reasoning for using \lhopital's Rule twice as above goes as follows:
        Let $f(x) = \log(1 + x) - x$ and $g(x) = x ^ 2$.
        Then $f$ and $g$ are differentiable and we have $f(0) = g(0) = 0$,
        so the first assumption of \autoref{pre:analy:thm:lhopitalsrule} is satisfied.
        But we still do not know whether $\lim_{x \rightarrow 0}f'(x) / g'(x)$ exists,
        since $\lim_{x \rightarrow 0}f'(x) = \lim_{x \rightarrow 0}g'(x) = 0$.
        But the first assumption of \autoref{pre:analy:thm:lhopitalsrule} is still satisfied for the differentiable functions $f'(x) = 1 / (1 + x) - 1$ and $g'(x) = 2x$,
        since $f'(0) = 0$ and $g'(0) = 0$,
        and we see that
        \[
        \lim_{x \rightarrow 0}\frac{f''(x)}{g''(x)} = \lim_{x \rightarrow 0}\frac{-1 / (1 + x) ^ 2}{2} = -\frac{1}{2}
        \]
        exists.
        Arguing backwards,
        this shows that $\lim_{x \rightarrow 0}f'(x) / g'(x)$ exists and then also $\lim_{x \rightarrow 0}f(x) / g(x)$ exists etc.

        The point is that one really needs to read the above calculation backwards.
        
        \item Powers beat logs
        \begin{enumerate}[label = (\alph*)]
            \item 
            Let $\alpha > 0$.
            Consider
            \[
            \liminfty[x]\frac{\log(x)}{x ^ {\alpha}}.
            \]
            This is of the type "$\frac{\infty}{\infty}$".
            Taking derivatives gives
            \[
            \liminfty[x]\frac{1 / x}{\alpha x ^ {\alpha - 1}} = \liminfty[x]\frac{1}{\alpha x ^ \alpha} = 0.
            \]
            By \lhopital we conclude
            \[
            \liminfty[x]\frac{\log(x)}{x ^ {\alpha}} = 0.
            \]

            \item
            Let $\alpha > 0$.
            Consider
            \[
            \lim_{x \rightarrow 0 ^ {+}}\log(x)x ^ {\alpha}.
            \]
            This is of the type "$\infty \cdot 0$".
            We turn this into "$\frac{\infty}{\infty}$" by writing $x ^ {\alpha} = 1 / x ^ {-\alpha}$.
            \[
            \lim_{x \rightarrow 0 ^ {+}}\frac{\log(x)}{x ^ {-\alpha}} = \lim_{x \rightarrow 0 ^ {+}}\frac{1 / x}{-\alpha x ^ {-\alpha - 1}} = \lim_{x \rightarrow 0 ^ {+}}\frac{1}{-\alpha x ^ {-\alpha}} = \lim_{x \rightarrow 0 ^ {+}}x ^ {\alpha} = 0.
            \]
            We have shown
            \[
            \lim_{x \rightarrow 0 ^ {+}}\log(x)x ^ {\alpha} = 0.
            \]
            We could have deduced this directly by using ($a$):
            \[
            \liminfty[x] = \frac{\log(x)}{x ^ {\alpha}} = \lim_{x \rightarrow 0 ^ {+}}\frac{\log(1 / x)}{(1 / x) ^ {\alpha}} = \lim_{x \rightarrow 0 ^ {+}}\frac{-\log(x)}{x ^ {-\alpha}}.
            \]
        \end{enumerate}
        \item Exponentials beat powers.

        Let $\alpha > 0$.
        Consider
        \[
        \liminfty[x]x ^ {\alpha}e ^ {-x} = \liminfty\frac{x ^ {\alpha}}{e ^ x}.
        \]
        We apply \lhopital repeatedly
        ($n$ times)
        until $\alpha - n \leq 0$.
        Then
        \[
        \liminfty[x]\frac{x ^ {\alpha}}{e ^ x} = \liminfty[x]\frac{\alpha x ^ {\alpha - 1}}{e ^ x} = \dotsi = \alpha(\alpha - 1)\dotsi(\alpha - n + 1)\liminfty\frac{x ^ {\alpha - n}}{e ^ x} = 0.
        \]
        So
        \[
        \liminfty[x]x ^ {\alpha}e ^ {-x} = 0.
        \]
        Alternatively,
        we could have substituted $x = \log(y)$ and obtained
        \[
        \liminfty[x]x ^ {\alpha}e ^ {-x} = \alpha\liminfty[y]\frac{\log(y)}{y} = 0
        \]
        by (iv)(a).
    \end{enumerate}
\end{example}

\subsection{Taylor's theorem}

Above we saw that we can consider the definition of differentiability of a function $f$ at a point as closely related to approximating a function by a linear function.
It is hence a very natural question to approximate $f$ by higher degree polynomials.
This is addressed by the following
\begin{theorem}[Taylor's Theorem]\label{pre:analy:thm:taylorsthm}
    Let $f : I \rightarrow \R$ be a function which is differentiable $n$ times in the interval $I$.
    Then there exists a function $r_n(x)$
    (depending on $c$)
    such that
    \begin{equation}\label{pre:analy:eq:2}
        f(x) = f(c) + f'(c)(x - c) + \frac{f''(c)}{2}(x - c) ^ 2 + \dotsi + \frac{f ^ {(n)}(c)}{n!}(x - c) ^ n + r_n(x)(x - c) ^ n
    \end{equation}
    with $\lim_{x \rightarrow c}r_n(x) = 0$.
    This is called the Peano form of the remainder
    (and the direct generalisation of differentiability at $x = c$).

    If in addition $f$ is $n + 1$ times differentiable in $I$ then we write
    \begin{align*}
        f(x) = f(c) + f'(c)(x - c) + \frac{f''(c)}{2}(x - c) ^ 2 + \dotsi &+ \frac{f ^ {(n)}(c)}{n!}(x - c) ^ n \\
        &+ \frac{f ^ {(n + 1)}(\xi)}{(n + 1)!}(x - c) ^ {n + 1},
    \end{align*}
    for a number $\xi$ between $x$ and $c$
    (depending on $x$ and $c$).
    This is called the Lagrange form of the remainder.
    \begin{proof}
        The first form is a direct application of \lhopital's Rule.
        Namely,
        consider
        \[
        F(x) := f(x) - \left(f(c) + f'(c)(x - c) + \frac{f''(c)}{2}(x - c) ^ 2 + \dotsi + \frac{f ^ {(n)}(c)}{n!}(x - c) ^ n\right).
        \]
        Then we directly see $F(0) = F'(c) = F''(c) = \dotsi = F ^ {(n)}(c) = 0$.
        Then
        \[
        r_n(x) = \frac{F(x)}{(x - c) ^ n}.
        \]
        We can apply \lhopital's Rule $n$-times and obtain
        \[
        \lim_{x \rightarrow c}r_n(x) = \lim_{x \rightarrow c}\frac{F(x)}{(x - c) ^ n} = \lim_{x \rightarrow c}\frac{F'(x)}{n(x - c) ^ {n - 1}} = \dotsi = \lim_{x \rightarrow c}\frac{F ^ {(n)}(x)}{n!} = 0,
        \]
        as claimed.

        To obtain the Lagrange remainder we fix $x$ and introduce the auxiliary function for $t$ between $x$ and $c$ by
        \[
        F(t) := f(x) - \left(f(t) + f'(t)(x - t) + \frac{f''(t)}{2}(x - t) ^ 2 + \dotsi + \frac{f ^ {(n)}(t)}{n!}(x - t) ^ n\right)
        \]
        so that $F(c) = r_n(x)(x - c) ^ n$ for which we need to find a $\xi$ such that it equals $\frac{f ^ {(n + 1)}(\xi)}{(n + 1)!}(x - c) ^ {n + 1}$.
        By a little calculation we obtain
        \[
        F'(t) = -\frac{f ^ {(n + 1)}(t)}{n!}(x - t) ^ {n}.
        \]
        Now we apply Cauchy's Generalised Mean Value Theorem
        (\autoref{pre:analy:thm:genmeanvalthm})
        for $F$ and
        \[
        G(t) = (x - t) ^ {n + 1};\quad G'(t) = -(n + 1)(x - t) ^ n,
        \]
        and obtain
        \begin{align*}
            \frac{F(c)}{(x - c) ^ {n + 1}} &= \frac{F(c)}{G(c)} \\
            &= \frac{F(c) - F(x)}{G(c) - G(x)} \\
            &= \frac{F'(\xi)}{G'(\xi)} \\
            &= \frac{\frac{f ^ {(n + 1)}(\xi)}{n!}(x - \xi) ^ n}{(n + 1)(x - \xi) ^ n} \\
            &= \frac{1}{(n + 1)!}f ^ {(n + 1)}(\xi)
        \end{align*}
        for some $\xi$ between $x$ and $c$.
        But this is the claim.
    \end{proof}
\end{theorem}

\begin{corollary}\label{pre:analy:cor:polypartoftaylor}
    The polynomial part $T_{f, c}^{(n)}(x)$ on the right hand side of \eqref{pre:analy:eq:2}
    (the first definition of $f(x)$ in \autoref{pre:analy:thm:taylorsthm}) 
    is called the $n$-th Taylor polynomial associated to $f$ at $c$.
\end{corollary}

Assume we have $|f ^ {(n + 1)}(x)| \leq M_{n + 1}$ for some constant $M_{n + 1}$
(say if $f ^ {(n + 1)}$ is continuous on $I = [a, b]$).
Then
\[
|f(x) - T_{f, c}^{(n)}(x)| \leq \frac{M_{n + 1}}{(n + 1)!}|x - c| ^ {n + 1}.
\]
So from this perspective Taylor's theorem describes how well one can approximate a
(complicated)
function by an
(easy)
polynomial.

\begin{example}
    The $4$-th Taylor polynomial for $e ^ x$ at $x = 0$ is given by
    \[
    1 + x + \frac{1}{2}x ^ 2 + \frac{1}{6}x ^ 3 + \frac{1}{24}x ^ 4.
    \]
    This approximates $e ^ x$ between $[-1 / 2, 1 / 2]$ with an error
    (take $M = 2 > e ^ {1 / 2}$)
    \[
    \left|e ^ {x} - \left(1 + x + \frac{1}{2}x ^ 2 + \frac{1}{6}x ^ 3 + \frac{1}{24}x ^ 4\right)\right| \leq \frac{2}{5!}\left(\frac{1}{2}\right) ^ 5 = \frac{1}{1920} < 0.001.
    \]
\end{example}

\begin{remark}
    Taylor's theorem gives a different way of considering \lhopital's Rule in the case $f, g$ are
    (sufficiently)
    often differentiable at $x = c$.
    Let $n$ be the first index such that $f ^ {(n)}(c) \neq 0$ and let $m$ be the first index such that $g ^ {(m)}(c0 \neq 0$
    \[
    \lim_{x \rightarrow c}\frac{f(x)}{g(x)} = \begin{cases}
        0 & \text{if } n > m; \\
        \frac{f ^ {(n)}(c)}{g ^ {(n)}(c)} & \text{if } n = m; \\
        \pm\infty & \text{if } n < m.
    \end{cases}
    \]
\end{remark}

\newpage

\section{Power Series}
A
(real)
power series is an expansion of the form $\infsum[k = 0]a_kx ^ k$ with $a_k \in \R$.
We obtain an infinite series which
(in the sense of Term $1$)
might converge or not.

Understanding power series is best done by considering everything as a complex power series $\infsum[k = 0]a_kz ^ k$ with $a_k, z \in \C$.
However,
we will leave this to the Complex Analysis II course.

\subsection{A quick review of infinite series}

For $(a_k)_{k \in \Z_{\geq 0}}$ we define the associated
(infinite)
series by
\[
\infsum[k = 0]a_k = \liminfty[n]\left(\sum_{k = 0}^{n}a_k\right)
\]
as the limit of the sequence of the partial sums $s_n = \sum_{k = 0}^{n}a_k$
(which might or might not exist).
\begin{example}[Geometric series]
    For $|q| < 1$ we have
    \[
    \infsum[k = 0]q ^ k = \frac{1}{1 - q},
    \]
    while the series diverges for $|q| \geq 1$.
\end{example}

\begin{example}[$p$-series]
    \[
    \infsum\frac{1}{k ^ p}\text{ converges} \iff p > 1.
    \]
\end{example}

\begin{lemma}
    If $\infsum[k = 0]a_k$ is convergent,
    then $\liminfty[k]a_k = 0$.
    \begin{proof}
        \textbf{Prove!}
    \end{proof}
\end{lemma}

\begin{example}
    The harmonic series $\infsum\frac{1}{k}$ is divergent but $\limas[k]{\frac{1}{k}}{0}$.
\end{example}

\begin{theorem}[COLT for series]
    Assume $\infsum[k = 0]a_k = a$ and $\infsum[k = 0]b_k = b$.
    Also let $c \in \R$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item $\infsum[k = 0]{a_k + b_k} = a + b$;
        \item $\infsum[k = 0]{ca_k} = ca$.
    \end{enumerate}
    \begin{proof}
        \textbf{Prove!}
    \end{proof}
\end{theorem}

\begin{theorem}[Comparison Test]
    Assume $0 \leq a_k \leq b_k$ for all $k$.
    \begin{enumerate}[label = (\roman*)]
        \item If $\infsum[k = 0]b_k$ is convergent with limit $b$,
        then $\infsum[k = 0]a_k$ is also convergent with limit $a \leq b$.
        \item If $\infsum[k = 0]a_k$ is divergent,
        then so is $\infsum[k = 0]b_k$.
    \end{enumerate}
    \begin{proof}
        \textbf{Prove!}
    \end{proof}
\end{theorem}

\begin{definition}
    If the series $\infsum[k = 0]|a_k|$ converges,
    we say that the series $\infsum[k = 0]a_k$ converges absolutely
    (is absolutely convergent).
\end{definition}

\begin{theorem}[Absolute Convergence Theorem]
    Every absolutely convergent series $\infsum[k = 0]a_k$ is also convergent.
    \begin{proof}
        \textbf{Prove!}
    \end{proof}
\end{theorem}

\begin{theorem}[Alternating Sign Test]
    Let $\seq$ be a monotone decreasing sequence of positive numbers with $a_k \rightarrow 0$.
    Then the alternating series $\infsum(-1) ^ {k + 1}a_k$ is convergent,
    to say $s ^ {*}$.
    Moreover,
    $|s ^ {*} - s_n| \leq a_{n + 1}$.
    \begin{proof}
        \textbf{Prove!}
    \end{proof}
\end{theorem}

\begin{remark}\label{pre:analy:remark:leibnizlogformula}
    Leibniz famously computed
    \begin{align*}
        \infsum\frac{(-1) ^ {k + 1}}{k} &= 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \dotsi = \log{2}, \\
        \infsum\frac{(-1) ^ {k + 1}}{2k - 1} &= 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \dotsi = \frac{\pi}{4}.
    \end{align*}
    We will revisit these later.
\end{remark}

\begin{theorem}[Ratio Test]
    Let $(a_k)$ be a sequence with $a_k \neq 0$ for all but possibly finitely many $k$.
    \begin{enumerate}[label = (\roman*)]
        \item If $\liminfty[k]\frac{|a_{k + 1}|}{|a_k|} < 1$,
        then $\infsum[k = 0]a_k$ converges absolutely.
        \item If $\liminfty[k]\frac{|a_{k + 1}|}{|a_k|} > 1$,
        then $\infsum[k = 0]a_k$ is divergent.
    \end{enumerate}
    \textit{Note:
    If the limit of the ratio is $1$ then we get no information.}
    \begin{proof}
        \textbf{Prove!}
    \end{proof}
\end{theorem}

\begin{theorem}[Root Test]
    For a sequence $(a_k)$ set $a = \limsup_k|a_k| ^ {1 / k}$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item If $a < 1$,
        then $\infsum[k = 0]a_k$ converges absolutely.
        \item If $a > 1$,
        then $\infsum[k = 0]a_k$ is divergent.
    \end{enumerate}
\end{theorem}

\begin{remark}
    The statement of course works if $\liminfty[k]|a_k| ^ {1 / k}$ exists
    (which however may not be the case).
    The above version has the advantage of a uniform formulation and that it always applies.
\end{remark}

\begin{theorem}
    Let $\infsum a_k$ be an absolutely convergent series and be rearranged at will without affecting the convergence or its value.
\end{theorem}
\textit{Pretty sure that the above is just a restatement of the Riemann Rearrangement Theorem.}

\subsection{Radius of convergence}

\begin{definition}
    A
    (real)
    power series is an infinite series of the form $\infsum[k = 0]a_kx ^ k$ with real $a_k$ and $x \in \R$.
\end{definition}

While a power series always converges for $x = 0$ it might not converge in all of $\R$ or even at any other point.
We have
\begin{theorem}[Cauchy-Hadamard]
    Let $\infsumo a_kx ^ k$ be a power series.
    Then there exists a constant $R \in [0, \infty]$ such that
    \begin{enumerate}[label = (\roman*)]
        \item If $R = 0$,
        then $\infsumo a_kx ^ k$ converges only for $x = 0$.
        \item If $R > 0$,
        then
        \begin{align*}
            &\infsumo a_kx ^ k \text{ converges absolutely for $x \in (-R, R)$}; \\
            &\infsumo a_kx ^ k \text{ diverges for $|x| > R$}.
        \end{align*}
    \end{enumerate}
    In fact,
    if we set
    \[
    \limsup_{k}\sqrt[k]{|a_k|} = c \in [0, \infty],
    \]
    then $R$ is explicitly given by
    \[
    R = \frac{1}{c} \in [0, \infty].
    \]
    We call $R$ the radius of convergence of the power series.
    \begin{proof}
        First assume $c = \limsup_{k}\sqrt[k]{|a_k|} \neq 0, \infty$.
        We apply the root test to $\infsumo a_kx ^ k$.
        Then $\sqrt[k]{|a_k||x ^ k|} = \sqrt[k]{|a_k|}|x|$.
        Thus for $x \neq 0$,
        we see
        \[
        \limsup_{k}\sqrt[k]{|a_k|}|x| = c|x| \begin{cases}
            < 1 & \text{if } |x| < R = \frac{1}{c}; \\
            > 1 & \text{if } |x| > R = \frac{1}{c}.
        \end{cases}
        \]
        Hence by the root test we see that $\infsumo a_kx ^ k$ converges absolutely if $|x| < R = 1 / c$ and diverges if $|x| > R = 1 / c$.
        If $c = 0$,
        then $c|x| = 0 < 1$ for all $x$,
        and $\infsumo a_kx ^ k$ converges absolutely for all $x$
        (so $R = \infty$),
        while if $c = \infty$,
        then $\infsumo a_kx ^ k$ diverges for any $x \neq 0$
        (so $R = 0$).
    \end{proof}
\end{theorem}

\begin{remark}\phantom{}
    \begin{itemize}
        \item In practice,
        one can usually compute the radius of convergence by using either the ratio or root test.
        That is,
        if $\liminfty[k]\frac{|a_{k + 1}|}{|a_k|} = c$,
        then (for $x \neq 0$),
        \[
        \liminfty[k]\frac{|a_{k + 1}|x| ^ {k + 1}}{|a_k||x| ^ k} = \liminfty[k]\frac{|a_{k + 1}|}{|a_k|}|x| = c|x|,
        \]
        and the same reasoning as in the proof of the theorem applies and gives $R = \frac{1}{c}$.

        The advantage of formulating and proving the above theorem lies in the fact that the $\limsup$ of a sequence always exists and that the root test allows for a uniform formulation.

        \item The behaviour of $\sum a_kx ^ k$ at the endpoints $x = \pm R$ has to be checked separately.
        Anything can happen.
        Do NOT use the ratio/root test to check this
        (it will be inconclusive),
        but use one of the other tests.
    \end{itemize}
\end{remark}

\begin{remark}
    The notion ratio of convergence comes from the analogous situation for a complex power series $\infsumo a_kz ^ k$ with $z \in \C$.
    Then same result holds,
    that is,
    the power series converges absolutely for all complex values $z \in \C$ with $|z| < R$ and diverges for all complex values $|z| > R$.
    Note that the set $\{z \in \C\,|\, |z| < R\}$ describes the interior of a circle of radius $R$ around the origin.
    This is the reason for the name radius of convergence.
\end{remark}

Here are some examples

\begin{example}\phantom{}
    \begin{itemize}
        \item Every polynomial $\sum_{k = 0}^{n}a_kx ^ k$ is a power series with $a_k = 0$ for $k \geq n + 1$.
        In this case the radius of convergence is of course $R = \infty$,
        since the sum is a finite value for every choice of $x \in \R$.
        (Note $c = 0$).

        \item Recall that the geometric series is given by $\infsumo x ^ k$.
        Here we have $a_k = 1$ for all $k$.
        Note that we have for $|x| < 1$ the identity
        \[
        \infsumo x ^ k = \frac{1}{1 - x},
        \]
        and divergence for $|x| > 1$.
        So the radius of convergence here is $R = 1$.
        The power series represents the function $f(x) = 1 / (1 - x)$ for $|x|$ but note that,
        while the power series diverges for $|x| > 1$,
        the function $f$ is defined everywhere except for $x = 1$.

        \item Consider $\infsumo k ^ kx ^ k$.
        In this case we have $a_k = k ^ k$ and $\limas[k]{\sqrt[k]{|a_k|} = k}{\infty}$.
        Thus $c = \infty$,
        and the series only converges for $x = 0$,
        that is,
        $R = 0$.

        \item Consider $\infsumo\frac{2 ^ k}{k}x ^ k$,
        so $a_k = 2 ^ k / k$.
        Then we have
        \[
        \left|\frac{a_{k + 1}}{a_k}\right| = \frac{2k}{k + 1} \rightarrow 2,
        \]
        so the radius of convergence is $R = \frac{1}{2}$.
        The root test would have worked fine too.
        
        \textit{Note: if we use $x = \pm 1 / 2$ we get $\infsumo\frac{(\pm 1) ^ k}{k}$,
        hence we have divergence at $x = R$,
        but
        (conditional)
        convergence at $x = -R$.}

        For $\infsumo\frac{2 ^ k}{k}x ^ k$ we also obtain $R = 1 / 2$ but with convergence at both end points,
        while for $\infsumo 2 ^ kk x ^ k$ we have again $R = 1 / 2$ with divergence at both endpoints.
        
        \item Let $a_k = k ^ k / k!$ for $k \geq 0$.
        Then we have
        \[
        \left|\frac{a_{k + 1}}{a_k}\right| = \frac{(k + 1) ^ {k + 1}}{(k + 1)k ^ k} = \left(\frac{k + 1}{k}\right) = \left(1 + \frac{1}{k}\right) ^ k \rightarrow e,
        \]
        so the radius of convergence is $R = 1 / e$.
        
        It is a challenge to determine convergence at the endpoints.
    \end{itemize}
\end{example}

It is worthwhile to record the following
\begin{corollary}
    Assume $\infsumo a_kc ^ k$ converges for some $c \neq 0$.
    Then $\infsumo a_kx ^ k$ converges absolutely for all $|x| < |c|$.
    \begin{proof}
        Let $R > 0$ be the radius of convergence.
        Then $c$ must lie in the
        (closed)
        interval $[-R, R]$
        (we cannot exclude the endpoints as the series might converge there).
        But then we have absolute convergence in the open interval $(-|c|, |c|) \subseteq (-R, R)$.
    \end{proof}
\end{corollary}

\begin{lemma}\label{pre:analy:lem:powseriescalc}
    Let $\infsumo a_kx ^ k$ be a power series with radius of convergence $R$.
    Then the
    \begin{align*}
        \textbf{formal derivative}\quad&\infsum a_kkx ^ {k - 1} = \frac{1}{x}\infsum a_kk x ^ {k}; \\
        \textbf{formal anti-derivative}\quad&\infsumo\frac{a_k}{k + 1}x ^ {k + 1} = x\infsumo\frac{a_k}{k + 1}x ^ {k}
    \end{align*}
    also have radius of convergence $R$.
    \begin{proof}
        Set $c = \limsup_{k}\sqrt[k]{|a_k|}$.
        But then since $\liminfty[k]\sqrt[k]{k} = 1$ also $\liminfty[k]\sqrt[k]{1 / (k + 1)} = 1$ as well we see directly
        \[
        \limsup_{k}\sqrt[k]{|a_k|k} = \limsup_{k}\sqrt[k]{|a_k| / (k + 1)} = c.
        \]
        The claim now follows by Cauchy-Hadamard.
    \end{proof}
\end{lemma}

\begin{remark}
    So far the formal derivative is really only formal.
    We haven't shown that the power series define differentiable functions and that their derivatives are obtained by differentiation term by term.
\end{remark}

\subsection{Power series as a function}

So far we considered the
(convergent)
power series $\infsumo a_kx ^ k$
(implicitly)
for fixed $x$.
In this section,
we view $\infsumo a_kx ^ k$ as a function $f(x)$ and study its properties.
For this and in fact,
as a principle,
we consider the power series as a limit of a sequence of polynomials,
\[
f(x) = \liminfty f_n(x),
\]
where
\[
f_n(x) := \sum_{k = 0}^{n}a_kx ^ k.
\]
This point of view,
constructing complicated/interesting functions as the limit of "easier" functions is a guiding principle in Analysis.

Let us start with the following proposition.
\begin{proposition}[Power series are continuous]\label{pre:analy:prop:powserarecont}
    Let $\infsumo a_kx ^ k$ be a power series with radius of convergence $R > 0$ defining a function $f(x)$ in the interval $(-R, R)$.
    Then $f$ is continuous on $(-R, R)$.

    In fact,
    $f$ is locally Lipschitz continuous on $(-R, R)$.
    More precisely,
    for all positive constants $r$ with $r < R$ we have that $f(x)$ is Lipschitz continuous on the interval $[-r, r]$.
    That is,
    there exists a constant $M = M_r > 0$
    (depending on $r$)
    such that
    \[
    |f(x) - f(y)| \leq M|x - y|
    \]
    for all $x$ and $y$ in $[-r, r]$.
    \begin{proof}
        Pick $n$ and define $f_n(x) := \sum_{k = 0}^{n}a_kx ^ k$.
        Then
        \begin{align*}
            |f_n(x) - f_n(y)| &= \left|\sum_{k = 0}^{n}a_k(x ^ k - y ^ k)\right| \\
            &= \left|\sum_{k = 1}^{n}a_k(x ^ k - y ^ k)\right| \\
            &= |x - y|\left|\sum_{k = 1}^{n}a_k\left(x ^ {k - 1} + x ^ {k - 2}y + \dotsi + xy ^ {k - 2} + y ^ {k - 1}\right)\right| \\
            &\leq |x - y|\sum_{k = 1}^{n}|a_k|\left(|x ^ {k - 1}| + |x ^ {k - 2}||y| + \dotsi + |x||y ^ {k - 2}| + |y ^ {k - 1}|\right) \\
            &\leq |x - y|\sum_{k = 1}^{n}|a_k|kr ^ {k - 1}.
        \end{align*}
        But by \autoref{pre:analy:lem:powseriescalc} the limit as $n \rightarrow \infty$ on the right hand side exists,
        and we obtain
        \[
        |f(x) - f(y)| \leq |x - y|\infsum|a_k|kr ^ {k - 1}.
        \]
        Thus $f(x)$ is Lipschitz on $[-r, r]$ with Lipschitz constant $M = \infsum|a_k|kr ^ {k - 1}$.
        This implies that $f$ is locally Lipschitz on $(-R, R)$.
        Given $c \in (-R, R)$ find $r > 0$ such that $c \in (-r, r) \subset (-R, R)$ and then $f$ is Lipschitz in the open neighbourhood $(-r, r)$ of $c$.
        In \textbf{Problem Sheet $1$} we have seen that this implies that $f$ is continuous on the entire interval $(-R, R)$.
    \end{proof}
\end{proposition}

\begin{remark}
    For the statement of the lemma we really need to restrict ourselves to the closed subinterval $[-r, r]$ of $(-R, R)$.
    In general/Typically,
    the power series will not be Lipschitz on the entire interval $(-R, R)$.
    
    For example,
    assume $R < \infty$.
    Then if $f(x) = \infsumo a_kx ^ k$ was Lipschitz on $(-R, R)$,
    then it would be bounded.
    Indeed,
    picking $y = 0$,
    we see
    \[
    |f(x) - f(0)| \leq M|x| \leq MR
    \]
    so that $|f(x)| \leq |f(0)| + MR = a_0 + MR$.
    However,
    a power series is not necessarily bounded on $(-R, R)$.
    For example,
    the geometric series $\infsumo x ^ k = \frac{1}{1 - x}$ is unbounded as $x \rightarrow 1 ^ {-1}$.
\end{remark}

The next result shows that power series are differentiable within the radius of convergence and that we can differentiate power series term by term.

\begin{theorem}\label{pre:analy:thm:funcofpowseriesinfdiff}
    Let $f(x) = \infsumo a_kx ^ k$ be a power series and $R \in (0, \infty]$ be its radius of convergence.
    Then $f$ is differentiable infinitely many times at all points $x \in (-R, R)$,
    and we can differentiate term by term:
    \[
    f'(x) = \infsum ka_kx ^ {k - 1}.
    \]
    Furthermore,
    we have
    \[
    f ^ {(n)}(0) = n! \cdot a_n
    \]
    for all $n \geq 0$.
    \begin{proof}
        Let $c \in (-R, R)$.
        In the proof of \autoref{pre:analy:prop:powserarecont} by taking the limit we saw
        \[
        f(x) = f(c) + (x - c)\infsum a_k(x ^ {k - 1} + x ^ {k - 2}c + \dotsi + xc ^ {k - 2} + c ^ {k - 1}).
        \]
        Set
        \[
        f_1(x) = \infsum a_k(x ^ {k - 1} + x ^ {k - 2}c + \dotsi + xc ^ {k - 2} + c ^ {k - 1}).
        \]
         Note that $f_1(c) = \infsum a_kkc ^ {k - 1}$.
         Considering first order Taylor what remains to be shown to obtain differentiability is that $f_1(x)$ is continuous at $x = c$
         (in fact a continuous function on $(-R, R)$).
         Then $f'(c) = f_1(c)$.
         This can be done 'by hand',
         similarly to the proof of \autoref{pre:analy:prop:powserarecont} but we will refrain from doing so here since it is fairly tedious and since there is a
         (much)
         more conceptual approach which we will discuss later.

         Assuming this,
         we now keep taking derivatives and see that $f$ is infinitely many times differentiable at all points $x \in (-R, R)$ and that we have
         \begin{align*}
             f ^ {(n)}(x) &= \infsum[k = n](k - n + 1)(k - n + 2)\dotsi(k - 1)ka_kx ^ {k - n} \\
             &= \infsumo(k + 1)(k + 2)\dotsi(k + n)a_{k + n}x ^ k.
         \end{align*}
         Evaluation at $x = 0$ yields $f ^ {(n)}(0) = n! \cdot a_n$.
    \end{proof}
\end{theorem}

Using the previous theorem we now show that power series representing a function within its radius of convergence are unique.
\begin{theorem}[Identity Theorem for Power Series]
    Let $\infsumo a_kx ^ k$ and $\infsumo b_kx ^ k$ be two power series with positive radii of convergence $R_a, R_b > 0$,
    respectively.
    If both series agree as functions on an interval $(-r, r)$ with $0 < r < \min\{R_a, R_b\}$,
    then we have $a_k = b_k$ for all $k$.
    \begin{proof}
        Let $f(x) = \infsumo a_kx ^ k$ and $g(x) = \infsumo b_kx ^ k$.
        Since $f = g$ on $(-r, r)$,
        we have
        \[
        k! \cdot a_k = f ^ {(k)}(0) = g ^ {(k)}(0) = k! \cdot b_k,
        \]
        i.e.,
        $a_k = b_k$.
    \end{proof}
\end{theorem}

The following we will mention without proof.
\begin{theorem}[Abel's Limit Theorem]\label{pre:analy:thm:abellimthm}
    Let $f(x) = \infsumo a_kx ^ k$ be a power series with radius of convergence $R > 0$.
    Assume that the power series converges at the endpoint $x = R: \infsumo a_kR ^ k$ converges.
    Then $f$ is
    (left)
    continuous at $x = R$,
    that is,
    \[
    \lim_{x \rightarrow R ^ {-}}f(x) = \infsumo a_kR ^ k.
    \]
    (The analogous result holds at $x = -R$ if the series converges at that point).
\end{theorem}

One can use this to establish Leibniz' formulas for $\log{2}$ and $\arctan(1) = \frac{\pi}{4}$.
However,
we will give direct proofs later.

\subsection{The exponentials, logarithm, and trigonometric functions}
Here we will reintroduce the exponential and trigonometric functions via power series.
This provides a natural and rigorous approach to these functions.

\subsubsection{The exponential function}
We of course have already constructed the exponential function $e ^ x$ and have derived its standard properties.
In particular,
we have seen $(e ^ x)' = e ^ x$ and also
\[
e ^ x = \infsumo\frac{x ^ k}{k!}.
\]
We will now ignore this and rather take a different starting point:
\begin{definition}[Exponential function]
    We define the exponential function $\exp(x)$ by
    \[
    \exp(x) := \infsumo\frac{x ^ k}{k!} = 1 + x + \frac{1}{2}x ^ 2 + \frac{1}{6}x ^ 3 + \frac{1}{24}x ^ 4 + \dotsi
    \]
    We have already seen
    (several times)
    that this power series converges absolutely for all $x \in \R$.
\end{definition}

\begin{theorem}
    \begin{enumerate}[label = (\roman*)]
        \item $\exp(0) = 1$.
        \item The exponential function is infinitely often differentiable on $\R$ with
        \[
        \exp'(x) = \exp(x).
        \]
        \item For all $x, y \in \R$ we have
        \begin{align*}
            \exp(x + y) &= \exp(x)\exp(y); \\
            \exp(-x) &= \frac{1}{\exp(x)}.
        \end{align*}
        \item $\exp(x) > 0$ for all $x \in \R$.
        \item $\exp(x)$ is strictly monotone increasing.
        \item
        \[
        \liminfty[x]\exp(x) = \infty\qquad\text{and}\qquad\lim_{x \rightarrow -\infty}\exp(x) = 0.
        \]
    \end{enumerate}
    \begin{proof}
        (i) is clear.
        For (ii) we differentiate term-wise
        (by \autoref{pre:analy:thm:funcofpowseriesinfdiff})
        and obtain
        \[
        \exp'(x) = \infsum k\frac{x ^ {k - 1}}{k!} = \infsumo(k + 1)\frac{x ^ k}{(k + 1)!} = \infsumo\frac{x ^ k}{k!} = \exp(x).
        \]
        For (iii),
        we consider $f(t) := \exp(x + t)\exp(y - t)$.
        Note that $f$ is differentiable on $\R$ with $f(0) = \exp(x)\exp(y)$ and $f(y) = \exp(x + y)$.
        But now
        \[
        f'(t) = \exp(x + t)\exp(y - t) - \exp(x + t)\exp(y - t) = 0.
        \]
        Thus $f$ is constant by \autoref{pre:analy:thm:contdiffthenfunchasprops}.
        This proves (ii).
        (iv) is clear for positive $x$,
        while for $x < 0$,
        this follows from (iii).
        (v) follows from (ii) and (iv) using \autoref{pre:analy:thm:contdiffthenfunchasprops}.
        For (vi),
        we note $\exp(x) > 1 + x$ for $x > 0$.
        This gives $\liminfty[x]\exp(x) = \infty$ and the limit as $x \rightarrow -\infty$ follows from (iii).
    \end{proof}
\end{theorem}

\begin{remark}
    Of course we have $e ^ x$ is equal to $\exp(x)$,
    either since their power series expansion are the same or by using that they satisfy the same differential equation $f' = f$ with initial condition $f(0) = 1$
    (see problem sheets).
\end{remark}
We proceed with
\begin{definition}[Euler's number]
    We define
    \[
    e := \exp(1) = \infsumo\frac{1}{k!} = 1 + \frac{1}{2} + \frac{1}{6} + \frac{1}{24} + \dotsi
    \]
\end{definition}

\begin{remark}
    With this definition we then in our setup need to show that $e = \liminfty[k](1 + 1 / k) ^ k$
    (which was the previous definition we had for Euler's number).
    But this is harmless
    (after having introduced the logarithm as the inverse of the exponential as usual).
    We have
    \[
    (1 + 1 / k) ^ k = \exp(k\log(1 + 1 / k)) \rightarrow \exp(1) = e
    \]
    by continuity of $\exp$ and $\liminfty[k]k\log(1 + 1 / k) = \liminfty[k]\frac{\log(1 + 1 / k)}{1 / k} = 0$ using \lhopital's Rule.
\end{remark}

\subsubsection{The logarithm}
Once we have defined the exponential function and derived its properties then we can introduce the logarithm $\log(x), a ^ x$,
and $\log_a(x)$ in exactly the same way as we did before.
For completeness we will give the line of reasoning.
Since $\exp(x)$ is strictly monotone increasing mapping $(-\infty, \infty)$ to $(0, \infty)$ it has an inverse function which we will denote by the logarithm:
\[
\log(x) : (0, \infty) \rightarrow (-\infty, \infty).
\]
Thus
\begin{align*}
    \exp(\log(x)) &= x&\text{for all } x > 0; \\
    \log(\exp(x)) &= x&\text{for all } x \in \R.
\end{align*}
We have
\begin{theorem}\label{pre:analy:thm:propoflogarithmspowseries}
    \begin{enumerate}[label = (\roman*)]
        \item $\log(1) = 0$.
        \item The logarithm is differentiable with $\log'(x) = \frac{1}{x}$.
        \item For all $x, y > 0$ we have
        \begin{align*}
            \log(xy) &= \log(x) + \log(y); \\
            \log\left(\frac{1}{x}\right) &= -\log(x).
        \end{align*}
        \item $\log(x)$ is strictly monotone increasing.
        \item
        \[
        \liminfty[x]\log(x) = \infty\qquad\text{and}\qquad\lim_{x \rightarrow 0 ^ {+}}\log(x) = -\infty.
        \]
    \end{enumerate}
    \begin{proof}
        (i),
        (iv),
        (v) follow from the analogous properties of the exponential function.
        For (ii) we apply \autoref{pre:analy:prop:contfuncderivofinverse}
        (nothing $\exp(x)$ is continuous and differentiable with $\exp'(x) = \exp(x) \neq 0$ for all $x$)
        and obtain
        \[
        \log'(x) = \frac{1}{\exp(\log(x))} = \frac{1}{x}.
        \]
        For
        (iii)
        we could use the addition law of $\exp$.
        Here is a different argument:
        For fixed $y$ consider both sides of $\log(xy) = \log(x) + \log(y)$ as a function of $x$.
        Not for $x = 1$ both sides take the value $\log(y)$.
        Now take derivatives:
        The right hand side gives $1 / x$ while the left hand side using the chain rule also gives $y\frac{1}{xy} = 1 / x$.
        Hence the functions coincide by \autoref{pre:analy:thm:contdiffthenfunchasprops}.
    \end{proof}
\end{theorem}

Finally,
we define for $a > 0$ the function
\[
a ^ x := \exp(\log(a)x),
\]
From the definition and the properties of the exponential we immediately obtain
\begin{theorem}\label{pre:analy:thm:thmsonatopowx}
    \begin{enumerate}[label = (\roman*)]
        \item $a ^ 0 = 1$,
        $a ^ 1 = a$,
        $a ^ {-1} = \frac{1}{a}$.
        
        \item $a ^ x$ is differentiable with $(a ^ x)' = \log(a)a ^ x$.

        \item For all $x, y \in \R$ we have
        \begin{align*}
            a ^ {x + y} &= a ^ xa ^ y; \\
            a ^ {-x} &= \frac{1}{a ^ x}.
        \end{align*}
        
        \item $a ^ x > 0$ for all $x \in \R$.

        \item $a ^ x$ is strictly monotone increasing if $a > 1$,
        constant if $a = 1$,
        and strictly monotone decreasing $a \in (0, 1)$.
    \end{enumerate}
\end{theorem}

We define $\log_a(x)$ as the inverse function of $a ^ x$
(if $a \neq 1$).
From the definition of $a ^ x$ we directly see
\[
\log_a(x) = \frac{\log(x)}{\log(a)}.
\]
\begin{remark}
    Recall one defines $n \in \N$
    (and arbitrary $a$)
    the $n$-th power $a ^ n$ as $a \dotsi a$.
    We recover this for $a > 0$ from \autoref{pre:analy:thm:thmsonatopowx}(iii) and (i)
    ($a ^ 1 = a$).
    From this we also see $a ^ {-n} = \frac{1}{a ^ n}$ which was like $a ^ 0 = 1$.
    Note that we cannot define $a ^ x$ for negative $a$.
\end{remark}

\subsubsection{The trigonometric functions}

Here we will give a rigorous introduction to trigonometric functions using power series.
We begin with
\begin{definition}
    We define the sine and cosine function by
    \begin{align*}
        \sin(x) := \infsumo\frac{(-1) ^ k}{(2k + 1)!}x ^ {2k + 1};&\cos(x) := \infsumo\frac{(-1) ^ k}{(2k)!}x ^ {2k}.
    \end{align*}
    By the ratio test
    (or comparison with $\exp(x)$)
    we see that the radius of convergence is $R = \infty$.
\end{definition}

\begin{theorem}
    We have
    \begin{enumerate}[label = (\roman*)]
        \item $\sin(0) = 0$ and $\cos(0) = 1$.
        
        \item The sine-function is odd;
        cosine is even.
        
        \item Sine and cosine are infinitely often differentiable on $\R$ with
        \[
        \sin'(x) = \cos(x)\qquad\text{and}\qquad\cos'(x) = -\sin(x).
        \]

        \item For all $x, y \in \R$ we have
        \begin{align*}
            \sin(x + y) &= \sin(x)\cos(y) + \cos(x)\sin(y); \\
            \cos(x + y) &= \cos(x)\cos(y) - \sin(x)\sin(y).
        \end{align*}

        \item For all $x \in \R$ we have
        \[
        \sin ^ 2(x) + \cos ^ 2(x) = 1.
        \]
        In particular,
        $|\sin(x)| \leq 1$;
        $|\cos(x)| \leq 1$ for all $x \in \R$.
    \end{enumerate}
    \begin{proof}
        (i) and (ii) are clear by definition.
        For (iii) differentiate term-wise
        (allowed by \autoref{pre:analy:thm:funcofpowseriesinfdiff}).
        For (iv),
        fix $x$ and $y$ and consider the function
        \[
        f(t) := \sin(x + t)\cos(y - t) + \cos(x + t)\sin(y - t).
        \]
        Note $f(0) = \sin(x)\cos(y) + \cos(x)\sin(y)$ and $f(y) = \sin(x + y)$.
        Now differentiate:
        \begin{align*}
            f'(t) = &\cos(x + t)\cos(y - t) + \sin(x + t)\sin(y - t) \\
            &-\sin(x + t)\sin(y - t) - \cos(x + t)\cos(y - t) = 0.
        \end{align*}
        Hence $f$ is constant by \autoref{pre:analy:thm:contdiffthenfunchasprops}.
        The addition law for cosine follows by differentiating the one for sine
        (in $x$).
        (v) holds for $x = 0$.
        Now differentiate both sides and use again \autoref{pre:analy:thm:contdiffthenfunchasprops}.
    \end{proof}
\end{theorem}

\begin{theorem}\label{pre:analy:thm:coshasasmallestpossol}
    The equation $\cos(x) = 0$ has a smallest positive solution.
    \begin{proof}
        We consider $\cos(2)$.
        We have
        \[
        \infsumo\frac{(-1) ^ k}{(2k)!}2 ^ {2k} = 1 - 2 + \frac{16}{24} + \infsum[k = 3]\frac{(-1) ^ k}{(2k)!}2 ^ {2k} = -\frac{2}{3} + \infsum[k = 3]\frac{(-1) ^ k}{(2k)!}2 ^ {2k}.
        \]
        The 'tail' $\infsum[k = 3]\frac{(-1) ^ k}{(2k)!}2 ^ {2k}$ is an alternating series with decreasing terms:
        \[
        \frac{2 ^ {2(k + 1)}}{(2(k + 1))!} = \frac{4}{(2k + 2)(2k + 1)}\frac{2 ^ {2k}}{(2k)!} < \frac{2 ^ {2k}}{(2k)!}.
        \]
        By the alternating series test and its proof we see that its value must be a negative number.
        Since $\cos(0) = 1 > 0$ by the intermediate value theorem the cosine function has a root in the interval $(0, 2)$.
        Hence the set $X := \{x > 0; \cos(x) = 0\}$ is non-empty and bounded below.
        Thus $x_0 = \inf{X}$ exists.
        Now take a sequence $x_n \in X$ with $\cos(x_n) = 0$ and $\liminfty x_n = x_0$.
        By continuity we conclude $\cos(x_0) = 0$.
        Thus $x_0$ is the smallest positive root of $\cos(x)$.
    \end{proof}
\end{theorem}

This leads to
\begin{definition}
    We denote the 'Weltkonstante' given by twice the smallest positive root of cosine by $\pi$,
    that is,
    \[
    \cos\left(\frac{\pi}{2}\right) = 0.
    \]
    Note $\cos(x) > 0$ for all $x \in [0, \pi / 2)$.
\end{definition}

Note that the same argument in the proof of \autoref{pre:analy:thm:coshasasmallestpossol} shows that $\cos(x) < 0$ for $x = 8 / 5$,
but $\cos(x) > 0$ for all positive $x \leq 3 / 2$
(break up the sum at $k = 4$).
This gives a very crude first estimate $3 < \pi < 3.2$.

We continue with
\begin{theorem}
    \[
    \sin\left(\frac{\pi}{2}\right) = 1.
    \]
    $\sin(x)$ is strictly monotone increasing on $[0, \pi / 2)$,
    mapping this interval to $[0, 1]$.
    \begin{proof}
        Since $\sin ^ 2(x) + \cos ^ 2(x) = 1$ we have $\sin(\pi / 2) = \pm 1$.
        But $\sin(x)$ is strictly monotone increasing in $[0, \pi / 2)$ since its derivative $\cos(x)$ is positive in this region,
        see $\autoref{pre:analy:thm:contdiffthenfunchasprops}$.
        Thus $\sin(x)$ is positive in $[0, \pi / 2]$ as $\sin(0) = 0$.
    \end{proof}
\end{theorem}

This also implies that $\cos(x)$ is strictly monotone decreasing on $[0, \pi / 2)$
(since its derivative $-\sin(x)$ is negative in this interval,
again using \autoref{pre:analy:thm:contdiffthenfunchasprops}),
mapping this interval to $[0, 1]$ as well.

\begin{theorem}
    \[
    \sin(x) = \cos\left(x - \frac{\pi}{2}\right)\qquad\text{and}\qquad\cos(x) = \sin\left(x + \frac{\pi}{2}\right).
    \]
    \begin{proof}
        This follows now from using the addition laws.
    \end{proof}
\end{theorem}

The addition laws
(using $\pi = \pi / 2 + \pi / 2$)
also give
\[
\sin(\pi) = 0\qquad\text{and}\qquad\cos(\pi) = -1,
\]
and with this using the addition laws again
\begin{theorem}
    \[
    \sin(x + \pi) = -\sin(x)\qquad\text{and}\qquad\cos(x + \pi) = -\cos(x),
    \]
    which implies $\sin(2\pi) = 0$ and $\cos(2\pi) = 1$.
    Which in turn implies
    \[
    \sin(x + 2\pi) = \sin(x)\qquad\text{and}\qquad\cos(x + 2\pi) = \cos(x),
    \]
\end{theorem}
From this it is also not hard to see that $2\pi$ is indeed the smallest period of sine and cosine,
and thus we have derived all
(basic)
properties of sine and cosine!

\begin{remark}
    As already mentioned the standard geometric approach from an axiomatic point of view lacks the rigorous introduction of geometry.
    In particular,
    one would need to introduce and define angle,
    arclength,
    area etc.

    Moreover,
    in the context of Calculus/Analysis the principal problem is to justify $\lim_{x \rightarrow 0}\frac{\sin(x)}{x} = 1$
    (which admittedly is not too hard).
    Still it requires a bit of geometry.

    The point is there is the need to base the trigonometric functions on a solid foundation.
    The approach via convergent power series provides such a foundation,
    and it works nicely.
    However,
    one does pay a price with a loss of geometric intuition,
    at least initially.
\end{remark}

\subsection{Taylor series}

We first remark all results in the section radius of convergence and the section power series as a function also holds for power series of the form
\[
f(x) := \infsumo a_k(x - c) ^ k
\]
for some $c \in \R$.
The radius of convergence $R$ is unaffected by the shift from $x$ to $x - c$,
and the interval of convergence is now symmetric around $c$.
Furthermore,
$f(x)$ is again differentiable infinitely often,
and for the coefficients $a_k$ we have
\[
a_k = \frac{f ^ {(k)}(c)}{k!}.
\]
We now turn this around to make the following definition

\begin{definition}[Taylor Series]
    Let $f(x)$ be a smooth function on an
    (open)
    interval $I$,
    that is,
    $f$ can be differentiated infinitely often.
    We define its Taylor series at $c \in I$ by
    \[
    T_{f, c}(x) = \infsum\frac{f ^ {(k)}(c)}{k!}(x - c) ^ k.
    \]
    If $c = 0$ one also calls the Taylor series the Maclaurin series of $f$.
    Note we have
    \[
    T_{f, c}(x) = \liminfty T_{f, c} ^ {(n)}(x)
    \]
    where
    \[
    T_{f, c}^{(n)}(x) = \sum_{k = 0}^{n}\frac{f ^ {(k)}(c)}{k!}(x - c) ^ k = f(c) + f'(c)(x - c) + \dotsi + \frac{f ^ {(n)}(c)}{n!}(x - c) ^ n
    \]
    is the $n$-th Taylor polynomial of $f$ at $c$.
\end{definition}

Given a Taylor series for $f$,
we can use the Ratio test or the Root test to calculate its radius of convergence $R$.

We will begin the discussion with the following statement which follows directly from the property of power series and is the motivating fact for the definition of Taylor series:
\begin{proposition}
    A function given as a power series centred at $c$ is equal to its Taylor series at $c$.
    \begin{proof}
        The coefficients $a_k$ of $\infsumo a_k(x - c) ^ k$ satisfy $a_k = \frac{f ^ {(k)}(c)}{k!}$.

        It is natural to except/hope that the Taylor series represents the function $f$.
        However,
        this is not always the case!
        The Taylor series of $f$ may not agree with the original function $f$.
    \end{proof}
\end{proposition}

\begin{enumerate}[label = (\roman*)]
    \item Let $f : \R \rightarrow \R$ be defined as
    \[
    f(x) = \begin{cases}
        e ^ {-1 / x ^ 2} & \text{if }x \neq 0, \\
        0 & \text{if }x = 0,
    \end{cases}
    \]
    In this case,
    you checked in one of the written assignments that $f$ is infinitely many times differentiable and $f ^ {(k)}(0) = 0$ for all $k \geq 0$.
    Thus the Taylor series of $f$ at $0$ is $\infsumo 0 \cdot x ^ k$,
    i.e.,
    the zero function and,
    therefore,
    has radius of convergence $R = \infty$.
    But the Taylor series of $f$ agrees with $f$ only at the origin $x = 0$.

    \item Another example of this kind is the 'take-off function'
    \[
    f(x) = \begin{cases}
        e ^ {-1 / x} & \text{if }x > 0, \\
        0 & \text{if }x \leq 0,
    \end{cases}
    \]
    which might strike you as even more odd.
    Again the Taylor series at $c = 0$ vanishes identically
    (which can be checked using the same method as in (i)).

    \item Finally,
    the 'bump function'
    \[
    f(x) = \begin{cases}
        e ^ {-1 / (1 - x ^ 2)} & \text{if } |x| < 1, \\
        0 & \text{if } |x| \geq 1.
    \end{cases}
    \]
    Here the Taylor series at $c = \pm 1$ vanish identically.
\end{enumerate}

\begin{remark}
    You might wonder:
    is this 'bad' news?
    In fact,
    the existence of such functions in real analysis allows for some very interesting constructions.
\end{remark}

In the above examples Taylor series were "nice",
but they don't need to be:

\begin{example}
    There exist smooth functions whose Taylor series at $c$ only converge at $x = c$.
    In fact,
    one can show that given any sequence $a_k$ of real numbers there exists a smooth function $f$ such that $f ^ {(k)}(0) = a_kk!$ so that the Taylor series of $f$ is equal to $\infsumo a_kx ^ k$.
    For $a_k = k!$ we have seen that the radius of convergence is $0$.
\end{example}

The next example shows a different phenomenon:
\begin{example}
    Let
    \[
    f(x) = \frac{1}{1 + x ^ 2}.
    \]
    Then $f$ is smooth on all of $\R$.
    Now using the geometric series we have
    \[
    \frac{1}{1 + x ^ 2} = \frac{1}{1 - (-x ^ 2)} = \infsumo(-1) ^ kx ^ {2k}.
    \]
    This is a power series,
    hence $\infsumo(-1) ^ k x ^ {2k}$ is also the Taylor series of $f$.
    However,
    $\infsumo(-1) ^ kx ^ {2k}$ only converges for $|x| < 1$ so the Taylor series only represents $f$ in $(-1, 1)$.

    A similar example is the geometric series $\infsumo x ^ k$ which is the Taylor series for $\frac{1}{1 - x}$ but only converges for $|x| < 1$ while of course $\frac{1}{1 - x}$ is defined for all $x \neq 1$.

    This will be clarified/explained in Complex Analysis.
\end{example}

So when does the Taylor series represent the function?
This can be viewed as a question about the behaviour of the remainder term in the Taylor's theorem we have previously mentioned,
see \autoref{pre:analy:thm:taylorsthm} and \autoref{pre:analy:cor:polypartoftaylor}.
Namely we have using the Lagrange remainder
\[
f(x) - T_{f, c} ^ {(n)}(x) = \frac{f ^ {(n + 1)}(\xi)}{(n + 1)!}(x - c) ^ {n + 1}
\]
for some $\xi$ between $c$ and $x$.
So the question when and for which $x$ the right hand side vanishes as $n \rightarrow \infty$.

We will not give a general criterion,
but rather continue by discussing several examples.
\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item A polynomial $f(x) = a_nx ^ n + \dotsc + a_1x + a_0$ is its own Taylor series since $f ^ {(k)}(0) = k! \cdot a_k$ for all $k$. But we can do this at any point $c$ and obtain
        \[
        f(x) = f(c) + f'(c)(x - c) + \dotsi + \frac{f ^ {(n)}(c)}{n!}(x - c) ^ n,
        \]
        since all higher derivatives
        (and thus the Lagrange remainder)
        vanish.

        \item For $f(x) = \sin(x), \cos(x), e ^ x$,
        the Taylor series at any point $c$ converges in all of $\R$ and is equal to $f$.
        Indeed,
        since $|\sin(x)| \leq 1$ and $|\cos(x)| \leq 1$ and all derivatives of $\sin(x)$ and $\cos(x)$ are given by $\pm\sin(x)$ and $\pm\cos(x)$ we have and $|x - c| \leq r$
        \[
        \left|\frac{f ^ {(n + 1)}(\xi)}{(n + 1)!}(x - c) ^ {n + 1}\right| \leq \frac{r ^ {n + 1}}{(n + 1)!} \rightarrow 0
        \]
        as $n \rightarrow \infty$ since $r ^ n / n! \rightarrow 0$ for any $r \in \R$.
        For $e ^ x$ the argument is similar:
        \[
        \left|\frac{f ^ {(n + 1)}(\xi)}{(n + 1)!}(x - c) ^ {n + 1}\right| \leq e ^ {c + r}\frac{r ^ {n + 1}}{(n + 1)!} \rightarrow 0.
        \]
    \end{enumerate}
\end{example}
This actually holds in general:
\begin{proposition}
    Let $f(x) = \infsumo a_kx ^ k$ be a function defined by a power series with radius of convergence $R > 0$,
    then the Taylor series of $f$ at any other point $c \in (-R, R)$ converges to $f(x)$ with radius of convergence at least $R - |c|$.
    That is,
    \[
    f(x) = \infsumo \frac{f ^ {(k)}(c)}{k!}(x - c) ^ k\qquad(|x - c| < R - |c|).
    \]
    \begin{proof}
        We will show that ? vanishes as $n \rightarrow \infty$.
        Let $R' = R - |c|$.
        Fix $x \in (c - R', c + R')$.
        Note then $|x - c| < R'$.
        Then choose $r > 0$ such that $|x - c| \leq r$ and also $\xi$ in ? satisfies $|\xi| \leq r$.
        Then
        \begin{align*}
            \left|\frac{f ^ {(n + 1)}(\xi)}{(n + 1)!}(x - c) ^ {n + 1}\right| &\leq \frac{1}{(n + 1)!}\left(\infsum[k = n + 1](n + 1)!|a_k||\xi| ^ {k - n - 1}\right)|x - c| ^ {n + 1} \\
            &\leq \infsum[k = n + 1]|a_k|r ^ {k - n - 1}r ^ {n + 1} \\
            &= \infsum[k = n + 1]|a_k|r ^ k.
        \end{align*}
        But this goes to zero as $n \rightarrow \infty$.
        Indeed,
        set $s ^ {*} = \infsumo|a_k|r ^ k$ and let $s_n = \sum_{k = 0}^{n}|a_k|r ^ k$ be the $n$-th partial sum so that $\liminfty s_n = s ^ {*}$.
        Thus
        \[
        \limas{\infsum[k = n + 1]|a_k|r ^ k = s ^ {*} - s}{0}.
        \]
    \end{proof}
\end{proposition}

\begin{example}
    \[
    \cos(x ^ 2) = \infsumo\frac{(-1) ^ k}{(2k)!}(x ^ 2) ^ {2k} = \infsumo\frac{(-1) ^ k}{(2k)!}x ^ {4k}.
    \]
    Let $f(x) := \frac{\sin(x)}{x}$ for $x \neq 0$.
    Then
    \[
    f(x) = \frac{1}{x}\infsumo\frac{(-1) ^ k}{(2k + 1)!}x ^ {2k + 1} = \infsumo\frac{(-1) ^ k}{(2k + 1)!}x ^ {2k} = 1 - \frac{1}{6}x ^ 2 + \frac{1}{120}x ^ 4 + \dotsi.
    \]
    In particular,
    $f$ extends to a differentiable function on all of $\R$ with $f(0) = 1$.
\end{example}

\begin{example}[Logarithm]
    We consider the logarithm $f(x) = \log(1 + x)$ which is defined for $x > -1$.

    We will not use the Lagrange remainder approach but argue differently.
    Recall the geometric series $\frac{1}{1 + x} = \frac{1}{1 - (-x)} = \infsumo(-x) ^ k$ which converges for $|x| < 1$.
    Then we can take the anti-derivative of this power series term-wise
    (\autoref{pre:analy:thm:funcofpowseriesinfdiff} allows for this)
    and obtain $\log(1 + x) = C + \infsum\frac{(-1) ^ {k + 1}x ^ k}{k}$ for a constant $C$ which is of course zero,
    since $f(0) = 0$.
    Thus
    \[
    \log(1 + x) = \infsum\frac{(-1) ^ {k + 1}}{k}x ^ k\qquad(|x| < 1).
    \]
    Now
    \begin{align*}
        \log\left(\frac{1}{2}\right) &= \infsum\frac{(-1) ^ {k + 1}}{k}\left(-\frac{1}{2}\right) ^ k \\
        &= -\infsum\frac{1}{k}\left(\frac{1}{2}\right) ^ k \\
        &= -\frac{1}{2} - \frac{1}{8} - \frac{1}{24} - \frac{1}{64} - \frac{1}{160} - \frac{1}{384} - \frac{1}{896} - \infsum[k = 8]\frac{1}{k}\left(\frac{1}{2}\right) ^ k.
    \end{align*}
    Hence
    \[
    \log(2) \sim \frac{1}{2} + \frac{1}{8} + \frac{1}{24} + \frac{1}{64} + \frac{1}{160} + \frac{1}{384} + \frac{1}{896} \sim 0.6927.
    \]
    In general,
    the error made stopping after the $n$-th term is
    \[
    \infsum[k = n + 1]\frac{1}{k}\left(\frac{1}{2}\right) ^ k < \frac{1}{n + 1}\frac{1}{2 ^ {n + 1}}\infsumo\left(\frac{1}{2}\right) ^ k = \frac{1}{(n + 1)2 ^ n}.
    \]
    So in our case $n = 7$,
    we underestimate $\log(2)$ by at most $\frac{1}{1024} < 0.001$.
    Hence
    \[
    0.6927 < \log(2) < 0.6937.
    \]
    Note $\log(2) \sim 0.69315$
    (This is actually not such a great way of computing $\log(2) = -\log(1 / 2)$ since the error only roughly halves at each step).
    
    Now in a problem sheet we considered the approach via Taylor polynomials and the Lagrange remainder and showed that our approximation for $\log(1 + x)$ holds for $0 \leq x \leq 1$ by showing that the Lagrange remainder given by
    \[
    \frac{f ^ {(n + 1)(\xi)}}{(n + 1)!}x ^ {n + 1} = (-1) ^ n\frac{1}{n + 1}\frac{x ^ {n + 1}}{(1 + \xi) ^ k}
    \]
    with some $\xi$ strictly between $0$ and $x$ vanishes in the limit for $x$ in that range.
    (For $-1 / 2 \leq x \leq 0$ it is also not too difficult to show that the remainder vanishes in the limit as well but for $-1 < x < -1 / 2$ it is not clear how that would work.)
    Hence the Taylor series for the logarithm holds for $x = 1$ as well,
    and we obtain the beautiful result:
    The alternating harmonic series converges to $\log(2)$;
    \[
    \log(2) = \infsum\frac{(-1) ^ {k + 1}}{k} = 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \dotsi
    \]
    as mentioned earlier,
    see \autoref{pre:analy:remark:leibnizlogformula}.
    This should be viewed in the context of Abel's limit theorem,
    see \autoref{pre:analy:thm:abellimthm},
    which is concerned with the behaviour of power/Taylor series at the endpoints of the interval of convergence.
    
    You might appreciate the subtlety of the situation noting that while the Taylor series of $\log(1 + x)$ converges at $x = 1$,
    the series for its derivative does not.
\end{example}

\begin{example}[Binomial Series]
    For $c \in \R$ we consider the function $f(x) = (1 + x) ^ c$ with $x > -1$.
    (The restriction arises since $a ^ c$ is in general only defined for $a > 0$).
    We have
    \[
    f ^ {(k)}(x) = c(c - 1)(c - 2)\dotsi(c - k + 1)(1 + x) ^ {c - k}.
    \]
    We define the generalised binomial coefficient by
    \[
    \binom{c}{k} = \frac{c(c - 1)(c - 2)\dotsi(c - k + 1)}{k!}
    \]
    for $k \geq 1$ and $\binom{c}{0} = 1$.
    Hence $f ^ {(k)}(0) = k!\binom{c}{k}$.
    Furthermore,
    the Taylor series does not represent the function,
    that is,
    \[
    (1 + x) ^ c = \infsumo\binom{c}{k}x ^ k\qquad(|x| < 1).
    \]
    We will consider this in the problem sheet.
    (The method of estimating the Lagrange remainder looks quite inconvenient in this case).

    Here are some special cases:
    \begin{enumerate}[label = (\roman*)]
        \item If $c = n \in \N$,
        then $\binom{n}{k} = 0$ for $k > n$,
        and the power series is the polynomial
        \[
        (1 + x) ^ n = \infsumo\binom{n}{k}x ^ k = \sum_{k = 0}^{n}\binom{n}{k}x ^ k
        \]
        which of course is valid for all $x$.
        
        \item Since $\binom{-1}{k} = (-1) ^ k$,
        we recover the geometric series
        \[
        \frac{1}{1 + x} = \infsumo(-x) ^ k.
        \]

        \item We have for $c = 1 / 2$
        \[
        \sqrt{1 + x} = 1 + \frac{1}{2}x - \frac{1}{8}x ^ 2 + \frac{3}{16}x ^ 3 - \dotsi\quad\text{for } |x| < 1.
        \]
    \end{enumerate}
\end{example}

\subsection{Complex power series}
When we consider $\infsumo a_kz ^ k$ respectively $\infsumo a_k(z - c) ^ k$ for $a_k, c \in \C$ and a complex variable $z \in \C$ essentially everything holds as in the real case with the same proof.
\begin{enumerate}[label = (\roman*)]
    \item Cauchy-Hadamard still applies and power series converge in a disk of radius $R$ around $0$ respectively $c$.

    \item Lipschitz continuity and continuity for power series hold in the same way
    (with the real absolute value replaced with the complex one).

    \item Power series can be
    (complex)
    differentiated term by term and the radius of convergence doesn't change.

    \item Taylor series are defined in the same way.
    However,
    it is one of the key results that all complex differentiable functions can be written
    (locally)
    as a power/Taylor series.
    The counter-examples given in the real case fail to be complex differentiable.
    This makes complex analysis much more rigid
    (and actually more beautiful)
    than real analysis.

    \item
    The complex exponential,
    sine,
    and cosine functions carry over as before with the same derivatives and addition laws.
    However,
    some of the properties are different.
    \begin{enumerate}[label = (\alph*)]
        \item We have
        \[
        \exp(ix) = e ^ {ix} = \cos(x) + i\sin(x),
        \]
        which can be directly seen by considering the defining power series.
        As a consequence one obtains the de Moivre laws.

        \item We have
        \[
        \cos(ix) = \cosh(x)\qquad\text{and}\qquad\sin(ix) = i\sinh(x),
        \]
        where $\cosh(x)$ and $\sinh(x)$ are the hyperbolic trigonometric functions.

        In particular,
        the complex sine and cosine are unbounded.

        \item For $z = x + iy$,
        we have
        \[
        \exp(z) = \exp(x + iy) = e ^ xe ^ {iy} = e ^ x(\cos(y) = i\sin(y)),
        \]
        and from this one sees that $\exp(z)$ takes all non-zero complex values,
        including negative ones.
    \end{enumerate}
\end{enumerate}
\newpage

\section{Sequences of functions, uniform convergence, and limit theorems}

In this section,
we will consider sequences $(f_n)$ of functions and will discuss different notions of convergence for these sequences to a limit function $f$.

Here are some motivations why one considers sequences of functions and their convergence properties:
\begin{itemize}
    \item \textbf{Construction of functions}

    We have already seen that many interesting functions $f(x)$ cannot be obtained in an 'elementary' way,
    but rather arise by a limit process;
    for example,
    $\liminfty\left(1 + \frac{x}{n}\right) ^ n$.
    Particularly attractive are infinite series of functions $\infsumo g_k(x)$,
    viewed as the limit of partial sums $f_n(x) := \sum_{k = 0}^{n}g_k(x)$.
    For example,
    $\infsumo a_kx ^ k$ are of this type.

    \item \textbf{Representations of functions}

    Given a function $f$ one can also seek to represent it by an infinite series of
    (simpler)
    functions,
    e.g.,
    its Taylor series or for periodic functions its Fourier series.

    \item \textbf{Approximation of (complicated) functions by (easier) functions}

    In the next section we will use step functions to approximate continuous functions on a compact interval uniformly to arbitrary precision and use this then in the following chapter to extend the notion of integration for step functions
    (signed area of rectangles)
    to the one for continuous functions
    (signed area under the graph).

    \item \textbf{Preservation of properties}

    One fundamental question in this process is under which circumstances the limit function $f$ inherits the properties of the functions $f_n$,
    say continuity or differentiability.
\end{itemize}

\subsection{Notions of convergence of functions: pointwise and uniform}
The simplest notion of convergence is this one

\begin{definition}[Pointwise convergence]
    Let $(f_n)$ be a sequence of functions on an interval $I$.
    We say that this sequence has a pointwise limit if for all $x \in I$,
    the limit $\liminfty f_n(x)$ exists
    (as a regular sequence of real numbers).
    In that case,
    the limit function $f : I \rightarrow \R$ is defined as
    \[
    f(x) = \liminfty f_n(x).
    \]
    Note that "$f_n \rightarrow f$ pointwise" can be expressed as follows with the help of quantifiers and the $(\varepsilon, \delta)$-formalism:
    \[
    \forall x \in I\ \forall \varepsilon > 0\ \exists N \in \N\ \forall n \geq N:\ |f_n(x) -f(x)| < \varepsilon.
    \]
    Note how '$N$' not only depends on $\varepsilon$ but also on the individual point $x \in I$.
\end{definition}

\begin{example}
    Power series in their interval of convergence $(-R, R)$
    (for $R > 0$)
    converge pointwise:
    For each fixed $x$ we apply root/ratio test to obtain convergence.
\end{example}

\begin{example}
    Let $f_n : [0, 1] \rightarrow \R$ be defined as $f_n(x) = x ^ n$.
    Then for fixed non-negative $x < 1$,
    we have
    \[
    \liminfty f_n(x) = \liminfty x ^ n = 0. 
    \]
    Moreover,
    \[
    \liminfty f_n(1) = \liminfty 1 = 1.
    \]
    So the pointwise limit of $(f_n)$ is the function $f : [0, 1] \rightarrow \R$
    \[
    f(x) = \begin{cases}
        0, & \text{if } x \in [0, 1), \\
        1, & \text{if } x = 1.
    \end{cases}
    \]
    Note that the limit function $f : [a, b] \rightarrow \R$ is discontinuous at $x = 1$,
    even though ll the $f_n$ are continuous.
\end{example}

There is a stronger notion of convergence of functions $f_n$,
uniform convergence:
\begin{definition}[Uniform convergence]
    Let $(f_n)$ be a sequence of functions on $I$.
    We say that $f_n$ converges uniformly to $f$ if,
    for every $\varepsilon > 0$ we can find $N \in \N$ such that for all $n \geq N$ and all $x \in I$,
    we have
    \[
    |f(x) - f_n(x)| < \varepsilon.
    \]
    If $f_n$ converges uniformly to $f$,
    we write "$f_n \rightarrow f$ uniformly".

    Note that "$f_n \rightarrow f$ uniformly" is expressed as follows:
    \[
    \forall \varepsilon > 0\ \exists N \in \N\ \forall n \geq N\ \forall x \in I:\ |f_n(x) - f(x)| < \varepsilon,
    \]
    we highlight that in contrast to the definition of pointwise convergence 'N' does not depend on the individual $x \in I$,
    but works for all $x$ at the same time.
\end{definition}

In terms of the graph of $f$ and the ones for $(f_n)$,
uniform convergence means that for every $\varepsilon > 0$,
there exists $N \in \N$ such that the graph of $f_n$ stays within the graphs of $f - \varepsilon$ and $f + \varepsilon$,
for all $n \geq N$.
We write $\|f - f_n\| < \varepsilon$.

Clearly uniform convergence is a stronger property than pointwise convergence:

\begin{proposition}
    Assume $(f_n)$ converges uniformly to $f$,
    then $(f_n)$ converges also pointwise to $f$.
    \begin{proof}
        This follows clearly from the fact that $N$ doesn't depend on $x$ for uniform convergence whereas it does for uniform pointwise convergence. 
    \end{proof}
\end{proposition}

\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item Let $f_n(x) = x ^ 2 + \frac{\sin{nx}}{n}$ on $I = \R$.
        Then $f_n \rightarrow f$ with $f(x) = x ^ 2$ uniformly since,
        for every $\varepsilon > 0$ there exists $N \in \N$ with $1 / N < \varepsilon$ and we have for all $n \geq N$ and $x \in \R$:
        \[
        |f_n(x) - f(x)| = \frac{|\sin{nx}|}{n} \leq \frac{1}{N} < \varepsilon.
        \]

        \item The sequence $f_n(x) = x ^ n$ on $[0, 1)$,
        see above,
        does not converge uniformly to its limit function $f = 0$.
        Assume it did,
        then for $\varepsilon = 1 / 4$,
        there would exist an $N$ such that $|f_n(x) - f(x)| = x ^ n < \varepsilon$ for all $0 < x < 1$ and all $n \geq N$.
        But now pick $x = x_n = \sqrt[n]{1 / 2} = \frac{1}{2 ^ {1 / n}} < 1$.
        Then
        \[
        |f_n(x_n) - f(x_n)| = f_n(x_n) = (x_n) ^ n = \left(\frac{1}{2 ^ {1 / n}}\right) ^ n = \frac{1}{2} > \frac{1}{4} = \varepsilon.
        \]
        So such an $N$ cannot exist.

        However,
        $f_n$ converges uniformly to $0$ on every compact subinterval $[0, r]$ with $r < 1$.
        Indeed,
        let $\varepsilon > 0$ and find $N$ such that $r ^ n < \varepsilon$ for all $n \geq N$
        (which exists since $r ^ n \rightarrow 0$).
        Then for all $x \in [0, r]$ and $n \geq N$
        \[
        |f_n(x)| = x ^ n \leq r ^ n < \varepsilon.
        \]
        This might look a bit confusing to you - because it is!
        The point is that in the counterexample for uniform convergence on the entire interval $[0, 1)$
        (or $[0, 1]$),
        the elements $x_n = \sqrt[n]{1 / 2}$ get closer and closer to $1$,
        so at some point would be larger than $r$.

        In terms of the graphs of $f_n$,
        the $\varepsilon$-strip around the graph of the limit function $f = 0$ is just the horizontal line $y = \varepsilon$,
        and the graph of $f_n$ leave that strip at some point
        (since $\lim_{x \rightarrow 1}x ^ n = 1$).
        However,
        if one restricts to $[0, r]$,
        the there exists an $N$ such that the graphs of $f_n$ for all $n \geq N$ leave the $\varepsilon$-strip to the right of $r$ and thus lie completely inside the strip for all $x \in [0, r]$.
    \end{enumerate}
\end{example}

This kind of 'funny' occurrence of the 'auxiliary' number $r$ might remind you of something similar for power series.
We will come back to this below.
Rather we give another example.

\begin{example}
    The geometric series $\infsumo x ^ k = \liminfty\sum_{k = 0}^{n}x ^ k$ converges pointwise for $|x| < 1$ to $f(x) := \frac{1}{1 - x}$ but not uniformly.
    Set $f_n(x) = \sum_{k = 0}^{n}x ^ k$.
    Then
    \[
    |f(x) - f_n(x)| = \left|\infsum[k = n + 1]x ^ k\right| = \frac{|x| ^ {n + 1}}{|1 - x|}.
    \]
    For uniform convergence we need this to be less than $\varepsilon$ for all $n$ sufficiently large and all $x$.
    But for $x_n = \sqrt[n + 1]{1 / 2} \in (-1, 1)$ we have
    \[
    |f(x_n) - f_n(x_n)| = \frac{1 / 2}{1 - \sqrt[n + 1]{1 / 2}},
    \]
    which is unbounded.
    However the convergence is uniform on the compact subinterval $[-r, r]$ for any $0 < r < 1$.
    Indeed,
    pick $\varepsilon > 0$.
    Then for any $x \in [-r, r]$ we have
    \[
    |f(x) - f_n(x)| = \left|\infsum[k = n + 1]x ^ k\right| \leq \infsum[k = n + 1]|x| ^ k \leq \infsum[k = n + 1]r ^ k = \frac{r ^ {n + 1}}{1 - r} < \varepsilon
    \]
    for all $n$ sufficiently large
    (since $\liminfty\frac{r ^ {n + 1}}{1 - r} = 0$).
    Note that the estimate is independent of the individual $x \in [-r, r]$.
\end{example}

Of course the geometric series is a special case of a power series.
We will see that power series with radius of
(pointwise)
convergence $R > 0$ will always converge uniformly in $[-r, r]$ for all $0 < r < R$
(but not necessarily uniform on the entire interval $(-R, R)$).

This phenomena turns out to be a central one for Analysis.

\begin{definition}[Uniform convergence in compact subsets]
    Let $f_n(x)$ be a sequence of functions on an
    (arbitrary)
    interval $I$ converging pointwise to a limit function $f(x)$.
    We say $f_n$ converges uniformly in compact subintervals
    (or compact subsets)
    if $f_n$ converges uniformly to $f$ on all compact subintervals $[a, b]$ of $I$.
\end{definition}

We finally give an easy criterion for
(non)-uniform convergence which we
(implicitly)
already used above.

\begin{lemma}[(Non)-uniform convergence criterion]
    Let $f_n$ be a sequence on the interval $I$ converging pointwise to $f$.
    \begin{enumerate}[label = (\roman*)]
        \item If there exists a sequence $(a_n)$ of positive numbers with $\liminfty a_n = 0$ such that
        \[
        |f(x) - f_n(x)| \leq a_n\qquad\text{for all } x \in I
        \]
        then $f_n \rightarrow f$ uniformly.
        (Note that the $a_n$ cannot depend on the individual $x \in I$ - but will
        (typically)
        depend on the data defining $I$.)

        \item If there exists $\varepsilon > 0$ and a sequence $x_n \in I$ such that for all $n$ sufficiently large
        \[
        |f(x_n) - f_n(x_n)| \geq \varepsilon
        \]
        then $f_n$ does not converge uniformly to $f$.
        This is in particular what happens when the sequence of numbers $|f(x_n) - f_n(x_n)|$ converges to a number differently from zero or is unbounded.
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
            \item Take $\varepsilon > 0$.
            Since $a_n \rightarrow 0$,
            we can find an $N \in \N$ such that $a_n < \varepsilon$ for all $n \geq N$.
            Thus
            \[
            |f(x) - f_n(x)| \leq a_n < \varepsilon\qquad\text{for all } x \in I\text{ and } n \geq N.
            \]

            \item Take that $\varepsilon > 0$.
            For uniform convergence we would need an $N$ such that $|f(x) - f_n(x)| < \varepsilon$ for all $x \in I$ and all $n \geq N$.
            But this is impossible since for every $n$ we have $|f(x_n) - f_n(x_n)| \geq \varepsilon$.
        \end{enumerate}
    \end{proof}
\end{lemma}

\subsection{Uniform convergence preserves continuity}

In this section we discuss the behaviour and properties of uniform convergence with respect to continuity,
integration,
and differentiability.

\begin{theorem}\label{pre:analy:thm:uniflimofseqofcontfuncsiscont}
    Let $f_n$ be a sequence of continuous functions on an interval $I$ such that $f_n \rightarrow f$ uniformly.
    Then the limit function $f$ is also continuous.
    \begin{proof}
        Let $c \in I$ and $\varepsilon > 0$ be given.
        Since $f_n \rightarrow f$ uniformly,
        we can find $N \in \N$ such that for all $n \geq N$:
        \begin{equation}\label{pre:analy:eq:3}
            |f(x) - f_n(x)| < \varepsilon\qquad\text{for all } x \in I.
        \end{equation}
        Now we use continuity of the function $f_N$:
        Since $f_N$ is continuous at $c \in I$,
        we find $\delta > 0$ such that for all $x \in I$ with $|x - c| < \delta$ we have
        \begin{equation}\label{pre:analy:eq:4}
            |f_N(x) - f_N(c)| < \varepsilon.
        \end{equation}
        Combining \eqref{pre:analy:eq:3} and \eqref{pre:analy:eq:4} with the triangle inequality implies that we have for all $x \in I$ with $|x - c| < \delta$:
        \begin{align*}
            |f(x) - f(c)| &\leq |f(x) - f_N(x)| + |f_N(x) - f_N(c)| + |f_N(c) - f(c)| \\
            &< \varepsilon + \varepsilon + \varepsilon = 3\varepsilon,
        \end{align*}
        which shows that $f$ is continuous at $c$.
        Since $c \in I$ was arbitrary,
        $f$ is continuous on $I$.
    \end{proof}
\end{theorem}

\begin{remark}
    \autoref{pre:analy:thm:uniflimofseqofcontfuncsiscont} implies
    (again)
    that the pointwise convergence of $x ^ n = f_n$ in $[0, 1]$ cannot be a uniform convergence,
    since the limit function is discontinuous at $x = 1$.
\end{remark}

\begin{remark}
    The same proof can be slightly modified to show that uniform convergence preserves one-sided continuity.
\end{remark}

But continuity is a local property.
We obtain the stronger result.

\begin{theorem}\label{pre:analy:thm:limofseqcontunifconviscont}
    Let $f_n$ be a sequence of continuous functions on an interval $I$ such that $f_n$ converges to $f$ uniformly in all compact subsets of $I$.
    Then the limit function $f$ is also continuous.

    \begin{proof}
        Let $c \in I$ be arbitrary.
        Then find a compact interval $[a, b]$ containing $c$.
        Then by \autoref{pre:analy:thm:uniflimofseqofcontfuncsiscont} the limit function $f$ is then continuous on $[a, b]$,
        in particular at $c$.
    \end{proof}
\end{theorem}

\begin{example}
    We say that $f_n(x) = x ^ n$ converges pointwise to $f = 0$ on $[0, 1)$ but not uniformly on $[0, 1)$.
    However,
    we have uniform convergence on intervals of the form $[0, r]$ for all $r < 1$.
    Since every compact interval $[a, b]$ of $[0, 1)$ is contained in one of those
    (namely,
    $[a, b] \subseteq [0, b]$)
    we have uniform convergence on compact subsets and hence we can see that the limit function is continuous
    (which in the given case is of course trivial as the limit function is $0$).
\end{example}

We now discuss the relationship between uniform convergence and differentiation.
This is a bit more subtle.

\begin{theorem}\label{pre:analy:thm:contdifffuncsconvas}
    Let $(f_n)$ be a sequence of continuously differentiable functions on an interval $I$.
    Assume that
    \begin{enumerate}[label = (\roman*)]
        \item the sequence $f_n$ converges pointwise to a limit function $f$;
        \item the sequence $f_n'$ converges uniformly in $I$.
    \end{enumerate}
    Then $f$ is continuously differentiable on $I$,
    and we have
    \[
    f'(x) = \liminfty f_n'(x).
    \]
    Since differentiability is also a local property we can weaken condition (ii) to
    \begin{enumerate}[label = (\roman*)]
        \item[(ii)] the sequence $f_n'$ converges uniformly in compact subsets of $I$,
    \end{enumerate}
    with the same conclusion.
\end{theorem}

We will not give a proof here but rather revisit the issue in the next subsection when we discuss infinite series of functions.

\begin{example}
    We again consider $f_n(x) = x ^ n$ on $[0, 1]$.
    The derivative is given by $f_n'(x) = nx ^ {n - 1}$ which also converges pointwise to $0$ and uniformly on intervals of the form $[0, r]$ for all $r < 1$.
    Thus we have uniform convergence on compact subsets for $f_n'$ and hence we can see that the limit function is differentiable with derivative $0$.
\end{example}

\begin{remark}
    \autoref{pre:analy:thm:contdifffuncsconvas} looks a little cumbersome and unsatisfying
    (we would rather have a statement like 'uniform limit of differentiable functions is differentiable' etc)
    and it is,
    see the next example.

    However,
    the complications go away in complex analysis where a much simpler statement holds.
\end{remark}

\begin{example}
    Let
    \[
    f_n(x) = \frac{\sim(nx)}{n}.
    \]
    Then since $|f_n(x)| \leq 1 / n$ we see that $f_n \rightarrow f = 0$ uniformly on $\R$.
    However,
    we cannot interchange the limit and differentiation!
    Indeed,
    $f'(x) = 0$ but
    \[
    f_n'(x) = \cos(nx),
    \]
    which does not converge pointwise unless $x \in 2\pi\Z$.
    So \autoref{pre:analy:thm:contdifffuncsconvas} does not apply.
\end{example}

\subsection{Uniform convergence for series of functions: Weierstrass M-Test}

In this section we will discuss the question of uniform convergence for series of functions.
This is a particularly attractive way of constructing interesting functions and greatly generalises the concept of power series.

We first develop a very powerful and highly usable test which ensures uniform
(and absolute)
convergence for a series of functions $\infsumo f_k(x)$.

\begin{theorem}[Weierstrass M-Test]\label{pre:analy:thm:weierstrassmtest}
    Let $I \subset \R$ be an interval and $(f_k)$ be a sequence of functions $f_k : I \rightarrow \R$.
    Let $M_k$ be a sequence of real numbers
    (not depending on $x \in I$!)
    satisfying
    \begin{enumerate}[label = (\roman*)]
        \item $|f_k(x)| \leq M_k$ for all $x \in I$;
        \item $\displaystyle\infsumo M_k$ is convergent.
    \end{enumerate}
    Then $\infsumo f_k(x)$ converges uniformly
    (and absolutely)
    to a limit function $f(x)$.

    \begin{proof}
        For each $x \in I$,
        the series $\sum f_k(x)$ converges pointwise
        (and absolutely)
        by comparison with the series $\sum M_k$.
        Hence we can define the function $f(x)$ on $I$ by
        \[
        f(x) := \infsumo f_k(x).
        \]
        Now we need to show $\sum_{k = 0}^{n}f_k(x) =: F_n(x) \rightarrow f$ uniformly in $I$.
        Let $\varepsilon > 0$.
        Since $\sum M_k$ is convergent,
        i.e.,
        $\limas{\sum_{k = 0}^{n}M_k}{L}$ for some $L$,
        we can find an $N \in \N$ such that
        \[
        \infsum[k = n + 1]M_k = \left|L - \sum_{k = 0}^{n}M_k\right| < \varepsilon
        \]
        for all $n \geq N$.
        But this implies for all $x \in I$ and all $n \geq N$ that
        \begin{align*}
            |f(x) - F_n(x)| &= \left|f(x) - \sum_{k = 0}^{n}f_k(x)\right| \\
            &= \left|\infsum[k = n + 1]f_k(x)\right| \\
            &\leq \infsum[k = n + 1]|f_k(x)| \\
            &\leq \infsum[k = N + 1]M_k \\
            &< \varepsilon.
        \end{align*}

        The shows that $F_n$ converges uniformly to $f$ in $I$.
    \end{proof}
\end{theorem}

Combining the Weierstrass M-Test
(\autoref{pre:analy:thm:weierstrassmtest})
with the
(local version of the)
limit theorem \autoref{pre:analy:thm:limofseqcontunifconviscont} provides a powerful tool to establish continuity of infinite series of functions!
But in fact,
we can also use it to finally prove the differentiability of power series!

\begin{theorem}
    Let $f(x) = \infsumo a_kx ^ k$ be a real power series with radius of convergence $R > 0$.
    \begin{enumerate}[label = (\roman*)]
        \item Then the power series $\infsumo a_kx ^ k$ converges uniformly in compact subsets of $(-R, R)$
        (and hence we give another proof that $f(x)$ is a continuous function in $(-R, R)$).

        \item Further $f$ is
        (infinitely often)
        differentiable on $(-R, R)$ with
        \[
        f'(x) = \infsumo(k + 1)a_{k + 1}x ^ k.
        \]
    \end{enumerate}
    \begin{proof}
        (i).
        Let $[a, b]$ be a compact interval in $(-R, R)$.
        So we can find $0 < r < R$ such that $[a, b] \subseteq [-r, r] \subset (-R, R)$.
        We apply the M-test for $f_k(x) = a_kx ^ k$ for the interval $I = [-r, r]$.
        Take $M_k = |a_k|r ^ k$.
        Then $|f_k(x)| \leq M_k$ for all $x \in I$ and $\infsumo|a_k|r ^ k$ converges since power series converge absolutely within their interval of convergence.
        So $\infsumo a_kx ^ k$ converges uniformly in $[-r, r]$,
        in particular in $[a, b]$.

        For (ii).
        In the proof of \autoref{pre:analy:prop:powserarecont} by taking the limit we saw
        \[
        f(x) = f(c) + (x - c)\infsum a_k(x ^ {k - 1} + x ^ {k - 2}c + \dotsi + xc ^ {k - 2} + c ^ {k - 1}).
        \]
        Set
        \[
        g_1(x) = \infsum a_k(x ^ {k - 1} + x ^ {k - 2}c + \dotsi + xc ^ {k - 2} + c ^ {k - 1}).
        \]
        Note that $g_1(c) = \infsum a_kkc ^ {k - 1}$.
        Considering first order Taylor what remains to be shown to obtain differentiability that $g_1(x)$ is continuous at $x = c$
        (in fact a continuous function on $(-R, R)$).
        Then $f'(c) = g_1(c)$.
        We proceed as above.
        Namely,
        we apply the M-test for $f_k(x) = a_k(x ^ {k - 1} + x ^ {k - 2}c + \dotsi + xc ^ {k - 2} + c ^ {k - 1})$ for the interval $I = [-r, r]$ containing $c$ as the inner point $(|c| < r < R)$.
        Take $M_k = |a_k|rr ^ {k - 1}$.
        Then
        \begin{align*}
            |f_k(x)| &\leq |a_k|\left(|x| ^ {k - 1} + |x| ^ {k - 2}|c| + \dotsi + |x||c| ^ {k - 2} + |c| ^ {k - 1}\right) \\
            &\leq |a_k|(r ^ {k - 1} + r ^ {k - 2}r + \dotsi + rr ^ {k - 2} + r ^ {k - 1}) \\
            &= |a_k|kr ^ {k - 1} = M_k
        \end{align*}
        for all $x \in I$.
        But now $\infsum|a_k|r ^ {k - 1}$ converges since the formal derivative $\infsumo(k + 1)a_{k + 1}x ^ k$ converges absolutely within their interval of convergence.
        Hence $g_1(x)$ is continuous and therefore $f(x)$ is differentiable at $c$ with $f'(c) = \infsum a_kkc ^ {k - 1}$ as desired.
    \end{proof}
\end{theorem}

The above proof suggests a possible route to give a criterion/general way to establish differentiability of $\infsumo f_k(x)$ via its characterisation by first order Taylor.
This indeed works!

\begin{theorem}\label{pre:analy:thm:convserofseqdifffuncs}
    Let $I \subset \R$ be an interval and $(f_k)$ be a sequence of differentiable functions $f_k : I \rightarrow \R$.
    Assume
    \begin{enumerate}[label = (\roman*)]
        \item $\displaystyle\infsumo f_k(x)$ converges pointwise in $I$ to a function $f(x)$;
        \item $\displaystyle\infsumo f_k'(x)$ converges uniformly in all compact subsets of $I$.
    \end{enumerate}
    Then $f(x) = \infsumo f_k(x)$ is a differentiable function on $I$ with
    \[
    f'(x) = \infsumo f_k'(x).
    \]
    \begin{proof}
        We only give a proof under the additional assumption that $\infsumo f_k'(x)$ satisfies the Weierstrass M-test locally,
        that is for all compact intervals $[a, b] \subseteq I$ there exists constants $M_k$ such that $|f_k'(x)| \leq M_k$ with $\infsumo M_k < \infty$.
        In practice this is not a significant restriction since the Weierstrass M-test is the typical way for establishing uniform convergence of an infinite series functions anyway!

        Let $c \in I$ and let $[a, b] \subseteq I$ be a compact interval containing $c$ as an interior point.
        We set
        \[
        g_k(x) = \begin{cases}
            \frac{f_k(x) - f_k(c)}{x - c} & \text{if } x \neq c; \\
            f_k'(c) & \text{if } x = c.
        \end{cases}
        \]
        Then by first order Taylor we have
        \[
        f_k(x) = f_k(c) + (x - c)g_k(x)
        \]
        with $g_k(x)$ being continuous at $x = c$ with $g_k(c) = f_k'(c)$.
        We consider
        \[
        g(c) := \infsumo g_k(x).
        \]
        We will show that $g(x)$ converges absolutely in $[a, b]$ to a continuous function;
        in particular $g(c) = \infsumo f_k'(c)$.
        Since
        \[
        f(x) = f(c) - (x - c)g(x),
        \]
        differentiability of $f$ at $c$ with $f'(c) = \infsumo f_k'(c)$ follows from first order Taylor.
        By the Mean Value Theorem we have for $x \neq c$
        \[
        g_k(x) = \frac{f_k(x) - f_k(c)}{x - c} = f_k'(\xi_k)
        \]
        for some $\xi_k$ between $x$ and $c$ and hence for all $x \in [a, b]$,
        \[
        |g_k(x)| = |f_k'(\xi_k)| \leq M_k.
        \]
        (For $x = c$ the estimate also holds).
        Hence by the Weierstrass M-test $g(x)$ converges absolutely in $[a, b]$ to a continuous function,
        and this completes the proof of the theorem!
    \end{proof}
\end{theorem}

\begin{example}[Weierstrass function]\label{pre:analy:examp:weierstrassfunc}
    Consider the function
    \[
    f(x) = \infsumo\frac{1}{2 ^ k}\cos(15 ^ k\pi x).
    \]
    Then
    \[
    \left|\frac{1}{2 ^ k}\cos(15 ^ k\pi x)\right| \leq \frac{1}{2 ^ k}
    \]
    and since $\infsumo\frac{1}{2 ^ k} = 2$ we conclude by the Weierstrass M-test that $\infsumo\frac{1}{2 ^ k}\cos(15 ^ k\pi x)$ is uniformly convergent on all of $\R$ to a function $f(x)$.
    But furthermore,
    since $\frac{1}{2 ^ k}\cos(15 ^ k\pi x)$ is continuous we see that $f(x)$ is continuous by \autoref{pre:analy:thm:uniflimofseqofcontfuncsiscont}.
    The
    (formal)
    derivative is given by $\infsumo\frac{15 ^ k\pi}{2 ^ k}\sin(15 ^ k\pi x)$ which obviously does not converge so that \autoref{pre:analy:thm:convserofseqdifffuncs} does not apply.
    This by itself does not show that $f(x)$ couldn't be differentiable,
    see the example of $f_n(x) = \sin(nx) / n$ earlier.
    However,
    Weierstrass showed in 1872 that $f(x)$ is not differentiable at any point.
\end{example}

\begin{remark}
    \autoref{pre:analy:thm:convserofseqdifffuncs} can also be used to give a proof for \autoref{pre:analy:thm:contdifffuncsconvas},
    the relationship between uniform convergence of a sequence of functions and differentiability.
    Namely,
    given a sequence $f_n(x)$ of functions converging
    (pointwise)
    to a function $f(x)$,
    define a new sequence by $g_1(x) = f_1(x)$ and $g_k(x) = f_k(x) - f_{k - 1}(x)$ for $n \geq 2$.
    Then $\sum_{k = 1}^{n}g_k(x) = f_n(x)$ and hence one can apply \autoref{pre:analy:thm:convserofseqdifffuncs} on series to the sequence $f_n(x)$ to obtain \autoref{pre:analy:thm:contdifffuncsconvas}.
    (It is a bit artificial).
\end{remark}

\subsection{Complex case}

Everything in this chapter holds in the complex case,
that is for sequences $f_n(z)$ respectively infinite series $\infsumo f_k(z)$ of complex-valued functions on $\C$.
The interval $I$ is replaced by a 'domain',
an open and connected subset of $\C$.

With one exception,
\autoref{pre:analy:thm:convserofseqdifffuncs} becomes much cleaner:
It turns out that the uniform limit of complex differentiable functions is again differentiable,
and the derivative is the limit of the derivatives.
The counterexample given by \autoref{pre:analy:examp:weierstrassfunc} does not apply since for complex entries the sequence resp. series actually doesn't converge
(uniformly)
since the complex trigonometric functions are not bounded:
Consider $ix$ with $x \in \R$.
Then $\sin(inx) / n = i\sinh(nx) / n$ is unbounded for $x \neq 0$,
so $\sin(inx) / n$ does not even converge
(pointwise).

\newpage

\section{Fonctions rgles (Regulated functions)}

Roughly speaking,
these are functions on a compact interval $I$ which can be uniformly approximated to arbitrary precision by step functions which are functions constant on each subinterval of a partition of $I$.
Among other things we will see that continuous functions are regulated.

We have two main motivations
\begin{itemize}
    \item In the next section we will define integrals for regulated functions.
    \item We will use this to learn more about important concepts and ideas of analysis,
    in particular about uniform convergence and the principle of constructing interesting functions as limits of easier functions.

    In a sense we have already seen this when we constructed power series as limits of polynomials.
\end{itemize}

\subsection{Definition of regulated functions}
We will begin by introducing a selection of remarkably simple yet useful functions.
\begin{definition}[Step functions]\label{pre:analy:def:stepfuncs}
    A function $f$ on a compact interval $[a, b]$ is called a step function if there exists a partition
    \[
    a = x_0 < x_1 < \dotsi < x_{N - 1} < x_N = b
    \]
    of $[a, b]$ such that $f$ is constant on each open subinterval $(x_k, x_{k + 1})$ $(k = 0, \dotsc, N - 1)$.

    Note that the values at the division points $x_k$ can be arbitrary.

    Also note that the partition is not unique.
    $f$ would also be a step function for any finer partition $a = y_0 < y_1 < \dotsi < y_{M - 1} < y_M = b$ which contains the $x_k$.
    In particular the function value for two adjacent intervals could be the same!
\end{definition}

The following properties are immediate
\begin{proposition}
    Let $f$ and $g$ be two step functions on $[a, b]$.
    Let $c \in \R$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item $cf(x)$ and $f(x) + g(x)$ are step functions.
        
        In particular,
        the set of regulated functions forms an $\R$-vector space!

        \item The product $f(x)g(x)$ is a step function.

        \item The absolute value $|f(x)|$ is a step function.

        In particular,
        $\max\{f, g\} = \frac{1}{2}(f + g + |f - g|)$ and $\min\{f, g\} = \frac{1}{2}(f + g - |f - g|)$ are step functions.
    \end{enumerate}
\end{proposition}

\begin{definition}[Regulated function]
    Let $f(x)$ be a function on a compact interval $[a, b]$.
    We call $f(x)$ a regulated function if there exists a sequence of step functions $f_n(x)$ which converges uniformly to $f(x)$.

    That is,
    for all $\varepsilon > 0$ there exists an $N \in \N$ such that
    \[
    |f_n(x) - f(x)| < \varepsilon\qquad\text{for all } x \in [a, b]\text{ and all } n \geq N.
    \]
\end{definition}

\begin{remark}
    The sequence of step functions converging
    (uniformly)
    to the function is by no means unique.
\end{remark}

\begin{lemma}[Sequence-free criterion for being regulated]\label{pre:analy:lem:seqfreecritforbereg}
    Let $f(x)$ be a function on a compact interval $[a, b]$.
    Then $f$ is regulated if and only if for all $\varepsilon > 0$ there exists a step function $g(x)$ on $[a, b]$ such that
    \[
    |f(x) - g(x)| < \varepsilon\qquad\text{for all } x \in [a, b].
    \]
    \begin{proof}
        One direction is clear by the definition of a regulated function.
        Now assume for all $\varepsilon > 0$ there exists a step function $g(x)$ such that $|f(x) - g(x)| < \varepsilon$ for all $x \in [a, b]$.
        In particular,
        for $\varepsilon = 1 / n$ we can find a step function $f_n(x)$ such that $|f(x) - f_n(x)| < 1 / n$ for all $x \in [a, b]$.
        This is the required sequence of step functions.
        Indeed,
        given $\varepsilon > 0$ find $N$ such that $1 / N < \varepsilon$.
        Then for all $n \geq N$ we have $|f(x) - f_n(x)| < 1 / n \leq 1 / N < \varepsilon$,
        as desired.
    \end{proof}
\end{lemma}

An important class of regulated functions are continuous functions!

\begin{theorem}[Continuous functions are regulated]
    Let $f$ be a continuous function on a compact interval $[a, b]$.
    Then $f$ is regulated.
    \begin{proof}
        Since $f$ is continuous on $[a, b]$,
        it is uniformly continuous by \autoref{pre:analy:thm:contoncompactthenunicontonint},
        that is,
        \[
        \forall\varepsilon > 0\,\exists\delta > 0\,\forall x, y \in I\quad(|x - y| < \delta \implies |f(x) - f(y)| < \varepsilon).
        \]
        So take $\varepsilon > 0$ and find a $\delta > 0$ such that $|f(x) - f(y)| < \varepsilon$ for all $X, y$ with $|x - y| < \delta$.
        Now let
        \[
        a = x_0 < x_1 < \dotsi < x_{N - 1} < x_N = b
        \]
        be any partition of $[a, b]$ such that $x_{k + 1} - x_k < \delta$.
        We let $x_k ^ {*}$ be any point in $[x_k, x_{k + 1})$,
        for example $x_k ^ {*} = \frac{x_k + x_{k + 1}}{2}$,
        the midpoint.
        We then define a step function $g(x)$ on $[a, b]$ by
        \[
        g(x) = f(x_k ^ {*})\quad\text{for } x \in [x_k, x_{k + 1}).
        \]
        We set $g(b) = g(x_{N - 1} ^ {*})$.
        Thus the function $g$ is constant on the subintervals $[x_k, x_{k + 1})$ with
        (potential)
        discontinuities at $x_1, \dotsc, x_{N - 1}$.
        We claim
        \begin{equation}\label{pre:analy:eq:5}
            |f(x) - g(x)| < \varepsilon\quad\text{for all } x \in [a, b].
        \end{equation}
        Indeed,
        let $x \in [a, b]$.
        Then
        (if $x < b$)
        we can find a $k$ such that $x \in [x_k, x_{k + 1})$,
        hence $|x - x_k ^ {*}| < \delta$.
        Thus by uniform continuity we obtain
        \[
        |f(x) - g(x)| = |f(x) - f(x_k ^ {*})| < \varepsilon.
        \]
        If $x = b$,
        the same works.
        Then \eqref{pre:analy:eq:5} shows that $f$ is regulated by \autoref{pre:analy:lem:seqfreecritforbereg}.
    \end{proof}
\end{theorem}

\subsection{Examples}
Now we will discuss regulated functions via important examples.

\textbf{How to explicitly construct a sequence of step functions in case of Lipschitz}

Assume $f : [a, b] \rightarrow \R$ is Lipschitz continuous with Lipschitz constant $M$:
$|f(x) - f(y)| \leq M|x - y|$ for all $x, y \in [a, b]$.
(Say,
if $f$ is continuously differentiable,
can take $M = \max\{|f'(x)|\}$).
Recall that in that case given $\varepsilon > 0$ one can take $\delta = \varepsilon / M$ to establish uniform continuity.

In that case it is particularly simple to construct the required sequence of step functions $f_n$ converging uniformly to $f$.

Namely,
given $n$,
consider the equi-distant partition $a = x_0 < x_1 < \dotsi < x_{n - 1} < x_n = b$ given by $x_k = a + (b - a)\frac{k}{n}$ with $k = 0, \dotsc, n$.
Note that each subinterval $[x_k, x_{k + 1}]$ has length $x_{k + 1} - x_k = (b - a) / n$.
Then let $x_k ^ {*}$ be any point in $[x_k, x_{k + 1}]$ define the step function $f_n$ on $[a, b]$ by
\[
f_n(x) = \begin{cases}
    f(x_k ^ {*}) &\text{if } x \in [x_k, x_{k + 1}) \\
    f(x_{n - 1} ^ {*}) &\text{if } x = b
\end{cases}
\]
($f_n(b) = f(b)$ would also be perfectly fine).
Then given $\varepsilon > 0$,
\[
|f_n(x) - f(x)| < \varepsilon\qquad\text{for all } x \in [a, b]\text{ and $n$ sufficiently large.}
\]
Indeed,
if $x \in [x_k, x_{k + 1})$ then $x - x_k < x_{k + 1} - x_k = (b - a) / n$ and hence
\[
|f(x) - f_n(x)| = |f(x) - f(x_k ^ {*})| \leq M|x - x_k ^ {*}| < M\frac{b - a}{n}.
\]
Similarly,
for $x = b$ we have $|f(b) - f_n(b)| = |f(b) - f(x_{n - 1})| \leq M|b - x_{n - 1}| = M(b - a) / n$.
Then $|f_n(x) - f(x)| < \varepsilon$ for $n > (b - a) / \varepsilon$ as desired.


\textbf{Not only continuous functions are regulated}
\begin{itemize}
    \item  Step functions themselves are obviously regulated
    (approximate $f$ by the constant sequence of functions $f_n = f$).
    Famous step functions are the following:
    The so-called Heaviside function:
    \[
    H(x) = u(x) = \begin{cases}
        0 & \text{if } x < 0; \\
        1 & \text{if } x \geq 0.
    \end{cases}
    \]
    Another example is the
    (Dirac)
    Delta function
    \[
    \delta(x) = \begin{cases}
        0 & \text{if } x \neq 0; \\
        1 & \text{if } x = 0.
    \end{cases}
    \]
    
    \item A piecewise continuous function is a function $f$ with only finitely many discontinuities
    (or which are discrete if one allows for unbounded intervals)
    such that at the points of discontinuity the left-hand and the right-hand limit exist
    (as finite numbers).
    Then $f$ is regulated.
    This can be seen directly by approximating the function on each interval of continuity.
\end{itemize}

\textbf{Not all functions are regulated}
\begin{itemize}
    \item Let $d(x)$ be the Dirichlet function on $[0, 1]$:
    \[
    d(x) = \begin{cases}
        0 & \text{if } x \in \Q; \\
        1 & \text{else}.
    \end{cases}
    \]
    Then $d(x)$ is not regulated.
    Indeed,
    let $g(x)$ be any step function with value $C$ on a non-trivial interval $(a, b)$ inside $[0, 1]$.
    But this interval contains
    (infinitely many)
    rational and irrational numbers.
    Hence $g(x) - d(x)$ takes on any open interval exactly the values $C$ and $C - 1$.
    Thus
    \[
    \max_{x \in (a, b)}\{|d(x) - g(x)|\} = \max\{|C|, |1 - C|\} \geq 1 / 2.
    \]
    Hence $g(x)$ violates \autoref{pre:analy:lem:seqfreecritforbereg} and no sequence of step functions can converge uniformly to $d(x)$.

    \item Let
    \[
    f(x) = \begin{cases}
        \sin(1 / x) & \text{if } x \in (0, 1]; \\
        1 & \text{if } x = 0.
    \end{cases}
    \]
    In a similar way as (i) $f(x)$ is not regulated
    (and no other value of $f$ at $x = 0$ will fix this).
    see problem sheets.
\end{itemize}

\textbf{Regulated functions can have infinitely many discontinuities}

Step functions have only finitely many
(potential)
discontinuities
(at the division points $x_k$).
That's not necessarily the case for regulated functions.

Let $(a_k)$ be a convergent sequence with limit $a$
(e.g.,
$a_k = 1 / k$ and $a = 0$).
Define a function on $[0, 1]$ by
\[
f(x) = a_k\qquad\text{if } x \in \left(\frac{1}{k + 1}, \frac{1}{k}\right]
\]
ad $f(0) = a$.
Note that $f$ has infinitely many
(potential)
discontinuities at $x = 1 / k$,
$k \geq 2$.
Hence it is not a step function in the above sense.
However,
$f$ is a regulated function:
Define $f_n(x)$ by
\[
f_n(x) = \begin{cases}
    f(x) &\text{if } x > 1 / n \\
    a &\text{if } x \leq 1 / n.
\end{cases}
\]
Then
\[
|f_n(x) - f(x)| = \begin{cases}
    0 &\text{if } x > 1 / n \\
    |a - a_k| &\text{if } x \in \left(\frac{1}{k + 1}, \frac{1}{k}\right]\text{ and } k \geq n.
\end{cases}
\]
But for sufficiently large $n$ we have $|a - a_k| < \varepsilon$ for all $k \geq n$.


\textbf{Monotone functions are regulated}

We give another class of regulated functions:
\begin{proposition}\label{pre:analy:prop:iffmonotonethenfreg}
    Let $f$ be a monotone function on $[a, b]$.
    Then $f$ is regulated.
    \begin{proof}
        Assume $f$ is increasing
        (the decreasing case works in the same way).
        Then $M := f(b)$ is the maximum and $m := f(a)$ is the minimum of $f$.
        Set $\Delta = M - m$.
        For $n \in \N$ we define division points $x_k$ by
        \[
        x_k := \sup\{x \in [a, b];\, f(x) < m + k\Delta / n\}.
        \]
        Since $f$ is monotone increasing we see directly that in the interval $[x_k, x_{k + 1})$ we have $f(x) \in [m + k\Delta / n, m + (k + 1)\Delta / n)$,
        a interval of length $\Delta / n$.
        So all values of $f$ in $[x_k, x_{k + 1})$ differ by at most $\Delta / n$.
        Then let $m_k \in [m + k\Delta / n, m + (k + 1)\Delta / n)$ be any number,
        say $m_k = f(x_{k} ^ {*})$ for some arbitrary point $x_{k} ^ {*}$ in $[x_k, x_{k + 1})$,
        and define $f_n$ by
        \[
        f_n(x) = m_k\qquad\text{if } x \in [x_k, x_{k + 1}).
        \]
        and $f(b) = m_{n - 1}$.
        Then for $x \in [x_k, x_{k + 1})$ we have
        \[
        |f(x) - f_n(x)| = |f(x) - m_k| < \Delta / n,
        \]
        and the same at $x = b$.
        Then for $n > 1 / \varepsilon\Delta$ we have $|f(x) - f_n(x)| < \varepsilon$ for all $x \in [a, b]$,
        as needed.
    \end{proof}
\end{proposition}

\subsection{Further properties of regulated functions}

\begin{lemma}\label{pre:analy:lem:regfuncisuniversallybounded}
    Let $f$ be a regulated function on $[a, b]$.
    Then $f$ and any sequence $f_n$ of step functions converging uniformly to $f$ are universally bounded.
    That is,
    there exists a constant $C$ such that $|f(x)| \leq C$ and $|f_n(x)| \leq C$ for all $x \in [a, b]$.
    \begin{proof}
        Let $(f_n(x))$ be a sequence of step functions which converge uniformly to $f(x)$.
        Take $\varepsilon = 1$.
        Then $|f(x) - f_n(x)| < 1$ for all $x \in [a, b]$ and all $n \geq N$.
        But $f_N(x)$ is certainly bounded since it takes only finitely many values,
        say by $M$,
        Using the triangle inequality we conclude $|f(x)| \leq |f_N(x)| + 1 \leq M + 1$.
        Since $|f_n(x) - f(x)| < 1$ this shows that the $f_n$ for $n \geq N$ are also bounded,
        namely $|f_n(x)| \leq |f_n(x) - f(x)| + |f(x)| < 1 + M + 1 = M + 2$.
        Now bound the finitely many step functions $f_1, \dotsc, f_{N - 1}$,
        say by $M'$.
        Then $C = \max\{M + 2, M'\}$ does it.
    \end{proof}
\end{lemma}

\begin{proposition}[Regulated COLT]\label{pre:analy:prop:regulatedcolt}
    Let $f$ and $g$ be two regulated functions on $[a, b]$,
    say $f, g$ respectively are the uniform limit of the step functions $f_n, g_n$ respectively.
    Let $c \in \R$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item
        \[
        cf(x)\text{ and } f(x) + g(x)\text{ are regulated}.
        \]
        In fact,
        $cf(x)$ is the uniform limit of $cf_n(x)$ and $f(x) + g(x)$ is the one of $f_n(x) + g_n(x)$.
        
        In particular,
        the set of regulated functions forms an $\R$-vector space!
        
        \item
        \[
        \text{The product } f(x)g(x)\text{ is regulated}.
        \]
        In fact,
        $f(x)g(x)$ is the uniform limit of $f_n(x)g_n(x)$.

        \item 
        \[
        \text{The absolute value } |f(x)|\text{ is regulated}.
        \]
        In fact,
        $|f(x)|$ is the uniform limit of $|f_n(x)|$.
        In particular,
        $\max\{f, g\} = \frac{1}{2}(f + g + |f - g|)$ and $\min\{f, g\} = \frac{1}{2}(f + g - |f - g|)$ are regulated.
    \end{enumerate}
    \begin{proof}
        First note that the statements are all true for 'regulated functions' replaced by 'step functions'.
        So let $f_n(x)$ and $g_n(x)$ be sequences of step functions which converge uniformly to $f(x)$ and $g(x)$.
        So given $\varepsilon > 0$ we have an $N$ such that
        \[
        |f(x) - f_n(x)| < \varepsilon\quad\text{and}\quad|g(x) - g_n(x)| < \varepsilon\qquad\text{for all } x \in [a, b] \text{ and all } n \geq N.
        \]
        Now the proofs are the same as for the corresponding results for COLT for sequences.
        Indeed,
        for all $x \in [a, b]$ and all $n \geq N$ we have
        \begin{align*}
            |cf(x) - cf_n(x)| &< c\varepsilon; \\
            |(f(x) + g(x)) - (f_n(x) + g_n(x))| &\leq |f(x) - f_n(x)| + |g(x) - g_n(x)| < 2\varepsilon; \\
            ||f(x)| - |f_n(x)|| &\leq |f(x) - f_n(x)| < \varepsilon.
        \end{align*}
        This proves (i) and (iii).
        For (ii),
        we first note all functions involved $f, f_n, g, g_n$ are all bounded by a universal constant $C$ by \autoref{pre:analy:lem:regfuncisuniversallybounded}.
        Then for all $x \in [a, b]$ and all $n \geq N$ we have
        \begin{align*}
            |f(x)g(x) - f_n(x)g_n(x)| &= |f(x)g(x) - f_n(x)g(x) + f_n(x)g(x) - f_n(x)g_n(x)| \\
            &\leq |g(x)||f(x) - f_n(x)| + |f_n(x)||g(x) - g_n(x)| \\
            &\leq 2C\varepsilon.
        \end{align*}
        This gives (ii).
    \end{proof}
\end{proposition}

\begin{remark}
    Given $f$ how does one find a sequence of step functions which converges uniformly to $f$?

    In general this is
    (of course)
    hard business,
    but we have seen two constructions which are quite/fairly explicit.
    \begin{enumerate}[label = (\arabic*):]
        \item The case when $f$ is Lipschitz continuous on $[a, b]$.
        \item The construction in the proof of \autoref{pre:analy:prop:iffmonotonethenfreg} for $f$ monotone.
    \end{enumerate}
\end{remark}

In all examples for regulated functions we discussed the one-sided limits $\lim_{x \rightarrow c ^ {\pm}}f(x)$ still existed at all points $c \in [a, b]$:
For
(piecewise)
continuous functions this is true by definition.
For the one constructed with infinitely many discontinuities it is obvious,
as it consists of patches of constant functions,
while we have seen in the problem sheets that monotone functions on a compact interval also have this property.
On the other hand,
for the non-examples the one-sided limits do not  always exist

This is no accident.
In fact,
this characterises regulated functions.
We state without proof the following characterisation of regulated functions but will not make
(serious)
use of it:

\begin{theorem}
    Let $f$ be a function on $[a, b]$.
    Then $f$ is regulated if and only if for all points $c \in [a, b]$ the left-sided and the right-sided limits $\lim_{x \rightarrow c ^ {-}}f(x)$ respectively $\lim_{x \rightarrow c ^ {+}}f(x)$ exist
    (but don't have to coincide).
\end{theorem}

One can also show that the points of discontinuity of a regulated function are at most countable,
that is,
if there are infinitely many then there exists a sequence $(x_n)$ such all discontinuities are located at the $x_n$.
Viewed differently,
the fourth example in the example section is typical
(except the function doesn't need to be constant in the intervals of continuity).

\newpage

\section{Integration}
We know the integral $\int_{a}^{b}f(x)\,dx$ as describing the signed area between the graph of $f$ and the $x$-axis on the interval $[a, b]$.
In fact,
the integral can be defined in different ways which all lead to the result if $f$ is "sufficiently nice",
e.g.,
if $f$ is continuous.
The best known definitions of integrals are the Riemann integral and the Lebesgue integral.
Whilst the Lebesgue integral is a more general concept,
it needs a lot more time to be introduced properly.
You have seen
(some version of)
the Riemann integral before.

We will not follow either direction but follow a different approach.
This integral we will follow is called the regulated integral which is defined
(as the name suggests)
for regulated functions.
It is somewhat more restrictive than the Riemann integral but
(much)
simpler to handle.

In general,
integration theories
(on $\R$)
tackle two interrelated questions:
\begin{enumerate}[label = (\roman*)]
    \item Define the integral $\int_{a}^{b}f(x)\,dx$ to a large
    (and 'well-behaved')
    space of functions.
    It should also have 'good' properties,
    in particular for a limit $f(x) = \liminfty f_n(x)$ of functions.

    \item Give a meaning to the notion of 'length' $\ell(A)$ for as large as possible class of subsets of $\R$ extending the notion of length of an interval.
    In terms of integrals this would be $\ell(A) = \int_A 1\,dx$,
    integrating the constant function $1$ over $A$.
    This very naturally leads to 'measure theory' which is central to probability and statistics.
\end{enumerate}

We will be mainly concerned with (i) and
(largely)
ignore (ii).

\subsection{Regulated integral: Definition}
In this section we will introduce the integral for regulated functions.
We proceed in stages:
First we define the integral for step functions,
then the one for regulated functions as the limit of the one for step functions.
This is a very standard procedure in Analysis.

\begin{definition}[Integral of step functions]
    Let $f$ be a step function on $[a, b]$ associated to a partition $a = x_0 < x_1 < \dotsi < x_{N - 1} < x_N = b$ and let $x_k ^ {*}$ be any point in the subinterval
    ($x_k. x_{k + 1}$
    (on which $f$ is constant)).
    We then define the integral $I(f)$ of $f$
    (over $[a, b]$)
    by
    \[
    I(f) = I(f; a, b) = \sum_{k = 0}^{N - 1}f(x_k ^ {*})(x_{k + 1} - x_k).
    \]
    Now recall that the choice of the underlying partition for $f$ is not unique,
    see \autoref{pre:analy:def:stepfuncs},
    but $I(f)$ is independent of this.
    More precisely,
    let $a = y_0 < y_1 < \dotsi < y_{M -1} < y_M = b$ be a finer partition,
    that is,
    $\{x_k\}$ is a subset of the $\{y_k\}$.
    Then it is easy to visualise/see that we also have
    \[
    I(f) = \sum_{k = 0}^{M - 1}f(y_k ^ {*})(y_{k + 1} - y_k),
    \]
    where $y_k ^ {*}$ is any point in the interval $(y_{k + 1}, y_k)$.
\end{definition}

\textit{Note how this definition coincides with our intuitive notion of the integral as a signed area,
in this case of rectangles.}

\begin{definition}[Integral of regulated functions]
    Let $f$ be a regulated function on $[a, b]$,
    say $f$ is the uniform limit of the sequence of step functions $f_n$.
    We then define the integral of $f$ by
    \[
    I(f) = \liminfty I(f_n).
    \]
\end{definition}
There are two things to check for this definition to make sense:
\begin{enumerate}[label = (\roman*)]
    \item The sequence of $I(f_n)$ of the integrals of the step functions converges.
    \item The limit $\liminfty I(f_n)$ is independent of the choice of the $f_n$.
\end{enumerate}

Before we do this,
a small comment which relates to the definition of the Riemann integral.
Namely,
given $f_n \rightarrow f$ uniformly,
we have
\[
I(f_n) = \sum_{k = 0}^{N - 1}f_n(x_k ^ {*})(x_{k + 1} - x_k),
\]
and we call this sum the/a Riemann sum associate to $f$ relative to the partition $\{x_k\}$ and $f_n$.
In fact,
often or usually in our constructions we have that the constant value of $f_n$ on the subinterval $(x_k, x_{k + 1})$ is given by a value $f(x'_k)$ of $f$ on some number $x'_k \in [x_k, x_{k + 1}]$.
In that case,
we obtain
\[
\sum_{k = 0}^{N - 1}f(x'_k)(x_{k + 1} - x_k),
\]
what usually denotes a Riemann sum.

We now consider (1):
\begin{lemma}
    Let $f$ be a regulated function on $[a, b]$,
    say $f$ is the uniform limit of the sequence of step functions $f_n$.
    Then the sequence $I(f_n)$ of the step function integrals converges.

    \begin{proof}
        We will show that the sequence $I(f_n)$ of real numbers is a Cauchy sequence,
        hence converges.
        Let $\varepsilon > 0$ and take $N$ such that $|f(x) - f_n(x)| < \varepsilon$ for all $x \in [a, b]$ and all $n \geq N$.
        Let $n, m \geq N$.
        Let $\{x_k; k = 0, \dotsc, M\}$ be a common refinement of the partitions underlying both $f_n$ and $f_m$
        (the union would do).
        Then
        \begin{align*}
            |I(f_n) - I(f_m)| &= \left|\sum_{k = 0}^{M - 1}f_n(x_k ^ {*})(x_{k + 1} - x_k) - \sum_{k = 0}^{M - 1}f_m(x_k ^ {*})(x_{k + 1} - x_k)\right| \\
            &\leq \sum_{k = 0}^{M - 1}|f_n(x_k ^ {*}) - f_m(x_k ^ {*})||(x_{k + 1} - x_k)| \\
            &= \sum_{k = 0}^{M - 1}|f_n(x_k ^ {*}) - f(x_k ^ {*}) + f(x_k ^ {*}) - f_m(x_k ^ {*})|(x_{k + 1} - x_k) \\
            &\leq \sum_{k = 0}^{M - 1}(|f_n(x_k ^ {*}) - f(x_k ^ {*})| + |f(x_k ^ {*}) - f_m(x_k ^ {*})|)(x_{k + 1} - x_k) \\
            &\leq \sum_{k = 0}^{M - 1}2\varepsilon(x_{k + 1} - x_k) \\
            &= 2(b - a)\varepsilon.
        \end{align*}
        Hence the $I(f_n)$ indeed form a Cauchy sequence.
    \end{proof}
\end{lemma}

Now we consider (2):
\begin{lemma}
    Let $f$ be a regulated function on $[a, b]$,
    say $f$ is the uniform limit of the sequence of step functions $f_n$.
    Then the limit $\displaystyle\liminfty I(f_n)$ is independent of the choice of the $f_n$.

    \begin{proof}
        So let $f_n$ and $g_n$ be two different sequences of step functions converging uniformly to $f$.
        We need to show $\liminfty I(f_n) - I(g_n) = 0$.
        We proceed in exactly the same way as above.
        Let $\varepsilon > 0$ and take $N$ such that $|f(x) - f_n(x)| < \varepsilon$ and also $|f(x) - g_n(x)| < \varepsilon$ for all $x \in [a, b]$ and all $n \geq N$.
        Let $\{x_k; k = 0, \dotsc, M\}$ be a common refinement of the partitions underlying both $f_n$ and $g_n$.
        Then
        \begin{align*}
            |I(f_n) - I(g_n)| &= \left|\sum_{k = 0}^{M - 1}f_n(x_k ^ {*})(x_{k + 1} - x_k) - \sum_{k = 0}^{M - 1}g_n(x_k ^ {*})(x_{k + 1} - x_k)\right| \\
            &\leq \sum_{k = 0}^{M - 1}|f_n(x_k ^ {*}) - g_n(x_k ^ {*})|(x_{k + 1} - x_k) \\
            &= \sum_{k = 0}^{M - 1}|f_n(x_k ^ {*}) - f(x_k ^ {*}) + f(x_k ^ {*}) - g_n(x_k ^ {*})|(x_{k + 1} - x_k) \\
            &\leq \sum_{k = 0}^{M - 1}(|f_n(x_k ^ {*}) - f(x_k ^ {*})| + |f(x_k ^ {*}) - g_n(x_k ^ {*})|)(x_{k + 1} - x_k) \\
            &\leq \sum_{k = 0}^{M - 1}2\varepsilon(x_{k + 1} - x_k) \\
            &= 2(b - a)\varepsilon.
        \end{align*}
        This gives the lemma.
    \end{proof}
\end{lemma}

\begin{remark}
    Of course we usually write
    \[
    I(f) = \int_{a}^{b}f(x)\,dx.
    \]
    Note that the variable '$x$' doesn't matter:
    $\int_{a}^{b}f(x)\,dx = \int_{a}^{b}f(t)\,dt = \int_{a}^{b}f(u)\,du$.
    It is a 'dummy' variable and hence $\int_{a}^{b}f$ might be better notation.
    The traditional notation goes back to Leibniz which while a bit superfluous is truly a strike of genius and very useful
    (like $\frac{df}{dx}$ for the derivative).
\end{remark}

\begin{example}
    Let's compute $\int_{a}^{b}x\,dx$,
    that is,
    $f(x) = x$.
    As the sequence of step functions we take the partition $x_k = a + \frac{k}{n}(b - a)$ which divides the interval $[a, b]$ into $n$ subintervals of length $(b - a) / n$ each.
    Take $x_k ^ {*} = x_k$ and set $f_n(x) = f(x_k ^ {*}) = x_k$ for $x \in [x_k, x_{k + 1})$.
    Also set $f_n(b) = f(x_{n - 1})$.
    Hence $|f(x) - f_n(x)| = |x - x_k ^ {*}| \leq (b - a) / n$,
    and the sequence of the $f_n$ indeed converges uniformly to $f$.
    Then
    \begin{align*}
        \int_{a}^{b}f_n(x)\,dx &= \sum_{k = 0}^{n - 1}\frac{b - a}{n}f(x_k ^ {*}) \\
        &= \frac{b - a}{n}\sum_{k = 0}^{n - 1}\left(a + \frac{k}{n}(b - a)\right) \\
        &= (b - a)a + (b - a) ^ 2 \frac{1}{n ^ 2}\sum_{k = 0}^{n - 1}k \\
        &= (b - a)a + (b - a) ^ 2\frac{n(n - 1)}{2n ^ 2} \\
        &\rightarrow (b - a)a + \frac{1}{2}(b - a) ^ 2 \\
        &= \frac{1}{2}(b ^ 2 - a ^ 2) \text{ as } n \rightarrow \infty.
    \end{align*}
\end{example}

We have shown
\[
\int_{a}^{b}x\,dx = \frac{1}{2}(b ^ 2 - a ^ 2),
\]
which should come as a relief!

\subsection{Comparison with the Riemann integral}
We now sketch briefly the more traditional Riemann integral which for regulated functions coincides with the integral we introduced.

The starting point for the Riemann integral is also a partition $\mathcal{P} = \{x_0 = a, \dotsc, x_N = b\}$ of the interval $[a, b]$.
One chooses points $x_k ^ {*} \in [x_k, x_{k + 1}]$ and defines the associated Riemann sum by
\[
\sum_{k = 0}^{N - 1}f(x_k ^ {*})(x_{k + 1} - x_k).
\]
Then roughly speaking one calls $f$ Riemann integrable if the limit of all Riemann sums exists as the mesh of the partition
(the length of the largest sub-interval)
goes to zero.
More precisely,
$f$ is called Riemann integrable if there exists a number $s$
(the integral of $f$)
such that for all $\varepsilon > 0$ there exists $\delta > 0$ such that for all Riemann sums associated to a partition with mesh less than $\delta$ we have
\[
\left|s - \sum_{k = 0}^{N - 1}f(x_k ^ {*})(x_{k + 1} - x_k)\right| < \varepsilon.
\]
This is fairly difficult to practically impossible to deal with,
and typically in textbooks one considers the following concept which turns out to be equivalent to Riemann's original definition.
Going back to Darboux.
Starting from a partition $\mathcal{P}$ as above one sets $m_k = \inf_{x \in [x_k, x_{k + 1}]}\{f(x)\}$ and $M_k = \sup_{x \in [x_k, x_{k + 1}]}\{f(x)\}$
(Here one needs to assume $f$ is bounded)
and defines lower and upper Riemann sums by
\[
L(f, \mathcal{P}) = \sum_{k = 0}^{N - 1}m_k(x_{k + 1} - x_k)\quad\text{and}\quad U(f, \mathcal{P}) = \sum_{k = 0}^{N - 1}M_k(x_{k + 1} - x_k).
\]
One then defines the upper and lower Riemann integrals by
\[
U_f = \inf_{\mathcal{P}}U(f, \mathcal{P})\quad\text{and}\quad L_f = \sup_{\mathcal{P}}L(f, \mathcal{P}),
\]
and calls $f$ Riemann-integrable
(Darboux-integrable)
if $U_f = L_f$.

\textit{Note how one can view the integral of a step function derived from $f$ as a special kind of Riemann sum.}

\textbf{Advantages of the Riemann integral}
\begin{itemize}
    \item The Riemann integrable might be more intuitive for the purpose of defining the integral of $f$ as the signed area.
    
    \item The Riemann integral is a
    (slightly)
    more general concept:
    Every regulated function is Riemann integrable but not vice-versa.
    That is,
    there are functions which cannot be approximated well by step functions but its area under the graph can be approximated well by rectangles.
    For example,
    \[
    f(x) = \begin{cases}
        \sin(1 / x) &\text{if } x \in (0, 1]; \\
        1 &\text{if } x = 0
    \end{cases}
    \]
    is not regulated
    (see above),
    but one can show that the Riemann integral exists.
\end{itemize}

\textbf{Disadvantages of the Riemann integral}
\begin{itemize}
    \item The definition of Riemann integrable functions is somewhat tautological:
    These are the functions for which the construction works.
    
    \item As a result Riemann integrable functions are harder to work with
    (as a concept).
    Its properties are harder to establish,
    in particular,
    in regard to limit theorems for sequences of functions.

    \item Regulated functions and their integrals are more conceptual,
    in particular in view of concepts like uniform convergence etc.

    \item The definition of the integral for regulated functions already directs towards the far reaching Lebesgue integral where one uses so-called simple functions to define the integral of 'Lebesgue integrable functions'.
\end{itemize}

\subsection{Regulated integral: Basic properties}
We now show how one can establish from our point of view some of the basic properties of the integral.

\begin{theorem}[Properties of the regulated integral - 'COLT for integrals']\label{pre:analy:thm:coltforintegrals}
    Let $f$ and $g$ be regulated functions on $[a, b]$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item\textbf{Linearity}:
        For $c \in \R$,
        \begin{align*}
            \int_{a}^{b}cf(x)\,dx &= c\int_{a}^{b}f(x)\,dx; \\
            \int_{a}^{b}f(x) + g(x)\,dx &= \int_{a}^{b}f(x)\,dx + \int_{a}^{b}g(x)\,dx. \\
        \end{align*}

        \item\textbf{Monotonicity}:
        If $f(x) \leq g(x)$ for all $x \in [a, b]$ then
        \[
        \int_{a}^{b}f(x)\,dx \leq \int_{a}^{b}g(x)\,dx.
        \]
        In particular,
        if $f(x) \geq 0$ for all $x \in [a, b]$,
        then also
        \[
        \int_{a}^{b}f(x)\,dx \geq 0.
        \]
        Furthermore,
        set $m = \inf_{x \in [a, b]}\{f(x)\}$ and $M = \sup_{x \in [a, b]}\{f(x)\}$.
        Then
        \[
        m(b - a) \leq \int_{a}^{b}f(x)\,dx \leq M(b - a).
        \]
        Finally,
        \[
        \left|\int_{a}^{b}f(x)\,dx\right| \leq \int_{a}^{b}|f(x)|\,dx.
        \]

        \item\textbf{Additivity}:
        For any $c \in (a, b)$,
        $f$ is also regulated on $[a, c]$ and on $[c, b]$,
        and we have
        \[
        \int_{a}^{b}f(x)\,dx = \int_{a}^{c}f(x)\,dx +  \int_{c}^{b}f(x)\,dx.
        \]
    \end{enumerate}
\end{theorem}

\begin{remark}
    Note that in \autoref{pre:analy:prop:regulatedcolt} we have already shown that $f$ and $g$ are regulated then $cf$,
    $f + g$ and $|f|$ are also regulated and hence their integrals exist.
    This is one of the advantages of the approach to integrals via regulated functions.
    For the Riemann integral the analogous statements are more tedious to prove since one always needs to establish the Riemann-integrability of the function involved as well.
\end{remark}

\begin{proof}
    The strategy for all of this is the same:
    We first note that the statements are all true for $f$ and $g$ step functions.
    To see this one uses a common refinement of the underlying partitions for $f$ and $g$
    (and for (iii) those which include $c$).

    Now let $f$ and $g$ be arbitrary regulated functions.
    We let $f_n(x)$ and $g_n(x)$ be sequences of step functions which converge uniformly to $f(x)$ and $g(x)$.
    So given $\varepsilon > 0$ we have an $N$ such that
    \[
    |f(x) - f_n(x)| < \varepsilon\quad\text{and}\quad|g(x) - g_n(x)| < \varepsilon\qquad\text{for all } x \in [a, b]\text{ and all } n \geq N.
    \]
    Again we can use a common refinement of the underlying partitions for $f_n$ and $g_n$ for their integrals.

    For (i) we note that $cf_n(x)$ and $f_n(x) + g_n(x)$ are step functions and converge uniformly to $cf(x)$ and $f(x) + g(x)$ respectively,
    see \autoref{pre:analy:prop:regulatedcolt}.
    Then
    \begin{align*}
        I(cf) &= \liminfty I(cf_n) = \liminfty cI(f_n) = c\liminfty I(f_n) = cI(f); \\
        I(f + g) &= \liminfty I(f_n + g_n) = \liminfty I(f_n) + I(g_n) = \liminfty I(f_n) + \liminfty I(g_n) \\
        &= I(f) + I(g).
    \end{align*}
    For (iii),
    we first note that $f_n$ are step functions on both $[a, c]$ and $[c, b]$ and converge uniformly to $f$ on both subintervals.
    Thus $f$ is indeed regulated on $[a, c]$ and $[c, b]$.
    Furthermore,
    \[
    I(f_n; a, b) = I(f_n; a, c) + I(f_n; c, b) \rightarrow I(f; a, c) + I(f; c, b).
    \]
    For (ii) we use linearity to reduce to the case $f(x) \geq 0$.
    ($I(f) \geq I(g) \iff I(f - g) = I(f) - I(g) \geq 0$).
    In that case we have $f_n(x) > -\varepsilon$ for all $x \in [a, b]$ and $n \geq N$.
    Hence
    \[
    I(f_n) = \sum_{k = 0}^{M - 1}f_n(x_k ^ {*})(x_{k + 1} - x_k) > -\varepsilon\sum_{k = 0}^{M - 1}(x_{k + 1} - x_k) = -(b - a)\varepsilon.
    \]
    But this means that we can make $I(f_n)$ bigger than any negative number.
    Hence in the limit we must have $I(f) = \liminfty I(f_n) \geq 0$ as claimed.
    The other claims follow from that.
    Indeed,
    we have $m \leq f(x) \leq M$ and hence $m(b - a) = I(m) \leq I(f) \leq I(M) = M(b - a)$.
    Finally,
    $|f(x)| \geq \pm f(x)$ and hence $I(|f|) \geq I(\pm f) = \pm I(f)$.
    Thus $I(|f|) \geq |I(f)|$.
\end{proof}

\begin{theorem}[Mean Value Theorem for integrals]
    Let $f$ be a continuous function on $[a, b]$.
    Then there exists $c \in [a, b]$ such that
    \[
    \int_{a}^{b}f(x)\,dx = f(c)(b - a).
    \]
    \begin{proof}
        This follows directly from \autoref{pre:analy:thm:coltforintegrals}(ii) and the intermediate value theorem:
        There must exist a constant $C$ such that $\int_{a}^{b}f(x)\,dx = C(b - a)$ with $\min_{x \in [a, b]}\{f(x)\} \leq C \leq \max_{x \in [a, b]}\{f(x)\}$.
        But since $f$ is continuous there exists a $c \in (a, b)$ such that $f(c) = C$.
    \end{proof}
\end{theorem}

\textit{Note that we can view the mean value theorem for integrals as a way of expressing the area under the graph by the area of a rectangle.
Furthermore we can view $f(c)$ as the 'mean value' of $f$ in $[a, b]$.}

\begin{example}
    The condition '$f$ continuous' in the theorem is necessary.
    Consider the step function $f : [0, 1] \rightarrow \R$ given by
    \[
    f(x) = \begin{cases}
        0 & \text{if } x \in [0, 1 / 2], \\
        1 & \text{if } x \in (1 / 2, 1].
    \end{cases}
    \]
    Note $f$ as a step function is not continuous.
    Then we have $\int_{0}^{1}f(x)\,dx = 1 / 2$ but $f$ only takes the values $0$ and $1$.
\end{example}

The following is a very useful sharpening of \autoref{pre:analy:thm:coltforintegrals}(ii) for continuous functions.

\begin{proposition}
    Let $f(x)$ be a continuous function on $[a, b]$ such that $f(x) \geq 0$ for all $x \in [a, b]$ and $f(c) > 0$ for some $c \in [a, b]$.
    Then
    \[
    \int_{a}^{b}f(x)\,dx > 0.
    \]
    \begin{proof}
        By continuity there exists a non-trivial interval $[\alpha, \beta]$ containing $c$
        (possibly as an endpoint)
        such that $f(x) \geq C / 2$ where $C = f(c) > 0$.
        (Indeed,
        pick $\varepsilon = C / 2$ and find $\delta > 0$ such that $|f(x) - C| < C / 2$ for all $x \in [a, b]$ with $|x - c| < \delta$.
        Then $\alpha = \max\{c - \delta, a\}$ and $\beta = \min\{c + \delta, b\}$ does it).
        Then
        \[
        \int_{a}^{b}f(x)\,dx = \int_{a}^{\alpha}f(x)\,dx + \int_{\alpha}^{\beta}f(x)\,dx + \int_{\beta}^{b}f(x)\,dx \geq 0 + \frac{C}{2}(\beta - \alpha) + 0 > 0,
        \]
        as claimed.
    \end{proof}
\end{proposition}

\begin{example}
    The condition '$f$ continuous' in the proposition is necessary.
    Consider the Dirac Delta function $\delta(x)$ viewed as the step function on $[-1, 1]$
    \[
    \delta(x) = \begin{cases}
        0 &\text{if } x \neq 0, \\
        1 &\text{if } x = 0.
    \end{cases}
    \]
    (Yes,
    this is a step function:
    $x_0 = -1, x_1 = 0, x_2 = 1$,
    the function is constant on $(x_0, x_1) = (-1, 0)$ and $(x_1, x_2) = (0, 1)$).
    Obviously $\delta(x) \geq 0$ with $\delta(0) = 1$ but $\int_{-1}^{1}\delta(x)\,dx = \delta(x_0 ^ {*})(x_1 - x_0) + \delta(x_1 ^ {*})(x_2 - x_1) = 0 \cdot 1 + 0 \cdot 1 = 0$.
\end{example}

\begin{remark}
    Let $f : [a, b] \rightarrow \R$ be a regulated function.
    It is useful to introduce the integral $\int_{b}^{a}f(x)\,dx$ by setting
    \[
    \int_{b}^{a}f(x)\,dx = -\int_{a}^{b}f(x)\,dx.
    \]
    Then it is straightforward to check that \autoref{pre:analy:thm:coltforintegrals}(iii) extends to arbitrary $a, b, c \in \R$.
\end{remark}

\subsection{Fundamental theorem of calculus}

In this section we consider the function
\[
F(x) := \int_{a}^{x}f(t)\,dt
\]
for a regulated function $f(x)$ on the interval $[a, b]$.
We start with the following,
perhaps surprising,
but easy.
\begin{theorem}\label{pre:analy:thm:definintislipschitzcontonint}
    Let $f$ be a regulated function on $[a, b]$.
    Then the function $F(x) = \int_{a}^{x}f(t)\,dt$ is Lipschitz continuous on $[a, b]$ with Lipschitz constant $M := \sup_{x \in [a, b]}\{|f(x)|\}$,
    so in particular,
    $F(x)$ is continuous.
    \begin{proof}
        So let $x, y \in [a, b]$,
        say $x > y$.
        Then
        \begin{align*}
            |F(x) - F(y)| &= \left|\int_{a}^{x}f(t)\,dt - \int_{a}^{y}f(t)\,dt\right| \\
            &= \left|\int_{y}^{x}f(t)\,dt\right| \leq \int_{y}^{x}|f(t)|\,dt \leq \int_{y}^{x}M\,dt = M|x - y|.
        \end{align*}
    \end{proof}
\end{theorem}

What is surprising here is that $f$ was assumed only to be regulated,
so with potentially many discontinuities,
but its integral is continuous,
in fact even stronger,
Lipschitz.

However,
for differentiability of $F$ we do have to assume a bit more.
We
(finally)
state
\begin{theorem}[Fundamental Theorem of Calculus]\footnote{FTOC.}
    Let $f$ be a continuous function on $[a, b]$.
    Then $F(x) := \int_{a}^{x}f(t)\,dt$ is a differentiable function on $[a, b]$
    (one-sided at $a$ and $b$),
    and we have $F'(x) = f(x)$ for all $x \in [a, b]$.
    \begin{proof}
        Let $c \in [a, b]$.
        We need to show $\lim_{x \rightarrow c}\frac{F(x) - F(c)}{x - c} = f(c)$.
        For $x \neq c$ we have by the mean value theorem for integrals
        \[
        \frac{F(x) - F(c)}{x - c} = \frac{\int_{a}^{x}f(t)\,dt - \int_{a}^{c}f(t)\,dt}{x - c} = \frac{\int_{c}^{x}f(t)\,dt}{x - c} = \frac{f(\xi_x)(x - c)}{x - c} = f(\xi_x)
        \]
        for some $\xi_x$ between $x$ and $c$.
        But now as $x \rightarrow c$ we also have $\xi_x \rightarrow c$ and hence by continuity of $f$ we conclude $\lim_{x \rightarrow c}f(\xi_x) = f(c)$.
        Note how we used the continuity of $f$ in a neighbourhood of $c$ to prove the result.

        We now give a more involved proof which however shows something stronger:
        If $f$ is regulated and continuous at the single point $x = c$ then $F(x) = \int_{a}^{x}f(t)\,dt$ is differentiable at $x = c$ with $F'(c) = f(c)$.

        We again want to show $\lim_{x \rightarrow c}\frac{F(x) - F(c)}{x - c} = f(c)$.
        So fix $\varepsilon > 0$.
        We need to find $\delta > 0$ such that
        \[
        \left|\frac{F(x) - F(c)}{x - c} - f(c)\right| < \varepsilon\qquad\text{for all } x \in [a, b] \setminus \{c\}\text{ with } |x - c| < \delta.
        \]
        We multiply through with $(x - c)$.
        Then
        \begin{align*}
            F(x) - F(c) - f(c)(x - c) &= \int_{a}^{x}f(t)\,dt - \int_{a}^{c}f(t)\,dt - f(c)(x - c) \\
            &= \int_{c}^{x}f(t)\,dt - \int_{c}^{x}f(c)\,dt \\
            &= \int_{c}^{x}(f(t) - f(c))\,dt.
        \end{align*}
        But now since $f$ is continuous at $c$ we find $\delta > 0$ such that $|f(t) - f(c)| < \varepsilon$ for all $t \in [a, b]$ with $|t - c| < \delta$.
        Thus for all $x$ with $|x - c| < \delta$ we have
        \begin{align*}
            |F(x) - F(c) - f(c)(x - c)| &\leq \left|\int_{c}^{x}f(t) - f(c)\,dt\right| \\
            &\leq\left|\int_{c}^{x}|f(t) - f(c)|\,dt\right| \\
            &\leq\left|\int_{c}^{x}\varepsilon\,dt\right| \\
            &= |x - c|\varepsilon.
        \end{align*}
        (Here we kept the modulus outside the integral to account for the case that $x$ might be smaller than $c$).
        Diving both sides by $|x - c|$ then gives the claim.
    \end{proof}
\end{theorem}

Another formulation of the Fundamental Theorem of Calculus reads as follows:
If $g$ is continuous on $[a, b]$ and $g = f'$ for some function $f : [a, b] \rightarrow \R$,
then
\[
f(x) = f(a) + \int_{a}^{x}g(t)\,dt.
\]
Recall that $F(x)$ is called an anti-derivative of $f(x)$.
We have shown that for continuous functions on an interval the anti-derivative exists!
Furthermore,
it is unique up to an additive constant
(as an application of the mean value theorem),
and we have
\[
\int_{a}^{b}f(x)\,dx = F(x)\mid_{a}^{b} = F(b) - F(a).
\]

\begin{remark}
    You might wonder what can be said about differentiability of $F$ when $f$ is only regulated.
    In that case one uses \autoref{pre:analy:thm:definintislipschitzcontonint} to show that for $F(x)$ at each point $c$ the left and right sided derivative $\lim_{x \rightarrow c ^ {\pm}}\frac{F(x) - F(c)}{x - c}$ exists and is equal to $f(c ^ {\pm}) := \lim_{x \rightarrow c ^ {\pm}}f(x)$
    (which might or might not coincide).
    The second proof given above extends;
    one only has to replace $f(c)$ with $f(c ^ {\pm})$ and assume $x > c$ respectively $x < c$.
\end{remark}

For completeness we record the basic integration techniques \textit{Substitution} and \textit{Integration by Parts}.

\begin{proposition}[Substitution Rule]
    Let $f$ be a continuous function on an interval $I$ and let $g : [a, b] \rightarrow I$ continuously differentiable.
    Then
    \[
    \int_{a}^{b}f(g(x))g'(x)\,dx = \int_{g(a)}^{g(b)}f(t)\,dt.
    \]
    \begin{proof}
        Let $F$ be an anti-derivative of $f$ on $I$
        (which exists by the fundamental theorem of calculus).
        Then by the chain rule $F(g(x))$ is an anti-derivative of $f(g(x))g'(x)$ on $[a, b]$.
        Hence the left hand side is equal to $F(g(b)) - F(g(a))$ which is also the right hand side since the image of $g$ contains the interval $[g(a), g(b)]$
        (or $[g(b), g(a)]$ if $g(b) < g(a)$)
        by the intermediate value theorem.
    \end{proof}
\end{proposition}

\begin{proposition}[Integration by Parts]
    Let $f$ and $g$ be two continuous functions on $[a, b]$ with $g$ being continuously differentiable.
    Let $F$ be an anti-derivative of $f$ on $[a, b]$.
    Then
    \[
    \int_{a}^{b}f(x)g(x)\,dx = F(x)g(x)\mid_{a}^{b} - \int_{a}^{b}F(x)g'(x)\,dx.
    \]
    \begin{proof}
        By the product rule of differentiation we note that $F(x)g(x)$ is an anti-derivative of $f(x)g(x) + F(x)g'(x)$.
        The claim follows by the fundamental theorem of calculus and linearity of the integral.
    \end{proof}
\end{proposition}

We leave at that and will not discuss other techniques like \textit{partial fractions} etc.

\subsection{Functions via integrals}

It is very attractive to construct functions as integrals
\[
F(x) := \int_{a}^{x}f(t)\,dt,
\]
in particular when there is no 'explicit' or 'elementary' anti-derivative of $f(x)$.
We give one example by discussing in detail
\[
L(x) := \int_{1}^{x}\frac{1}{t}\,dt\qquad(x > 0).
\]
Of course $L(x) = \log(x)$ but we will ignore this for the moment but rather derive all the properties of the logarithm and its inverse,
the exponential,
from this.
This approach to the logarithm and exponential has the advantage that we don't need to use any power series.
Analogous to \autoref{pre:analy:thm:propoflogarithmspowseries} we have
\begin{theorem}\phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item $L(1) = 0$.
        \item The logarithm is differentiable with $L'(x) = \frac{1}{x}$.
        \item For all $x, y > 0$ we have
        \begin{align*}
            L(xy) &= L(x) + L(y); \\
            L\left(\frac{1}{x}\right) &= -L(x).
        \end{align*}
        \item $L(x)$ is strictly monotone increasing.
        \item
        \[
        \liminfty[x]L(x) = \infty\qquad\text{and}\qquad\lim_{x \rightarrow 0 ^ {+}}L(x) = -\infty.
        \]
    \end{enumerate}
    \begin{proof}
        (i),
        (ii),
        and (iv) are clear.
        For (iii) we could use the argument given in \autoref{pre:analy:thm:propoflogarithmspowseries}
        (fix $y$ and take derivatives with respect to $x$),
        but we can also use the substitution rule and obtain
        \begin{align*}
            L(xy) &= \int_{1}^{xy}\frac{1}{t}\,dt \\
            &= \int_{1 / x}^{y}\frac{1}{t'}\,dt' \\
            &= \int_{1}^{y}\frac{1}{t'}\,dt' + \int_{1 / x}^{1}\frac{1}{t'}\,dt' \\
            &= \int_{1}^{y}\frac{1}{t'}\,dt' + \int_{1}^{x}\frac{1}{u}\,du \\
            &= L(y) + L(x).
        \end{align*}
        ($t' = t / x$; $u = xt'$).
        Then consider $y = 1 / x$ to see $L(1 / x) = -L(x)$.
        For (v) note that the inequality holds for $x = 0$.
        But then taking derivatives gives $1 / (1 + x) \leq 1$ which is true.
        For (vi) note $\log(2) = \int_{1}^{2}\frac{1}{t}\,dt \geq \int_{1}^{2}\frac{1}{2}\,dt = 1 / 2$.
        Hence by (iii) we conclude $\log(2 ^ n) = n\log(2) > n / 2$.
        For (vii) we need to show that given $C$ there exists an $N$ such that $\log(x) \geq C$ for all $x \geq N$.
        For this find by (vi) an $n$ such that $\log(2 ^ n) \geq C$.
        Then since the logarithm is increasing we have $\log(x) \geq \log(2 ^ n) \geq C$ for all $x \geq 2 ^ n$.
        The second claim now follows from $L\left(\frac{1}{x}\right) = -L(x)$.
    \end{proof}
\end{theorem}

Of course we have $L(x) = \log(x)$,
where $\log(x)$ is defined as the inverse function of $\exp(x)$ as we have in previous sections.
Indeed,
their value at $x = 1$ coincide and so do their derivatives.
In any case,
since $L(1) = 0$ and $\liminfty L(x) = \infty$ and using the intermediate value theorem we can now make the following
\begin{definition}[Euler's constant again]
    We denote the unique number $x > 0$ such that
    \[
    L(x) = 1
    \]
    by $e$.
\end{definition}

We can then define the inverse function $E(x)$ of $L(x)$
(which is of course equal to $\exp(x) = e ^ x$)
and derive all the properties from the ones of the logarithm.
For example,
$E(x)$ as the inverse of a differentiable function on an interval with non-zero derivative is differentiable with
\[
E'(x) = \frac{1}{L'(E(x))} = \frac{1}{1 / E(x)} = E(x).
\]
We will not carry this out further.
\begin{remark}
    In a similar way we can consider
    \[
    x \mapsto \int_{0}^{x}\frac{1}{\sqrt{1 - t ^ 2}}\,dt,\qquad x \mapsto \int_{0}^{x}\frac{1}{1 + t ^ 2}\,dt,
    \]
    derive their properties and then introduce $\sin(x)$ and $\tan(x)$ as their inverse functions etc.

    In the problem sheets we see that this works out nicely except for one crucial aspect:
    One doesn't see the periodicity of the trigonometric functions directly.
    For this one would need to consider $F(x)$ as a function of a complex variable $x$
    (at least implicitly).
\end{remark}

\subsection{Uniform convergence and integration}

\begin{theorem}
    Let $I = [a, b]$ and $(f_n)$ be a sequence of regulated functions on $I$ with $f_n \rightarrow f$ uniformly.
    Then:
    \begin{enumerate}[label = (\roman*)]
        \item The limit function $f$ is regulated and
        
        \item
        \[
        \int_{a}^{b}f(x)\,dx = \liminfty\int_{a}^{b}f_n(x)\,dx.
        \]
    \end{enumerate}
    \begin{proof}
        Starting with (i).
        Let $\varepsilon > 0$.
        Since $f_n \rightarrow f$ uniformly,
        we can find $N$ such that for all $n \geq N$ and all $x \in I$ we have
        \[
        |f(x) - f_n(x)| < \varepsilon.
        \]
        Furthermore,
        for each regulated function $f_n$ there exists a step function $g_n$ such that $|f_n(x) - g_n(x)| < \varepsilon$ for all $x \in [a, b]$,
        see \autoref{pre:analy:lem:seqfreecritforbereg}.
        We claim that $g_n \rightarrow f$ uniformly.
        Indeed,
        for all $x \in I$ and $n \geq N$,
        \[
        |f(x) - g_n(x)| \leq |f(x) - f_n(x)| + |f_n(x) - g_n(x)| < \varepsilon + \varepsilon = 2\varepsilon.
        \]
        This gives (i).

        For (ii) we have for $n \geq N$
        \begin{align*}
            \left|\int_{a}^{b}f(x)\,dx - \int_{a}^{b}f_n(x)\,dx\right| &= \left|\int_{a}^{b}f(x) - f_n(x)\,dx\right| \\
            &\leq \int_{a}^{b}|f(x) - f_n(x)|\,dx \\
            &< \int_{a}^{b}\varepsilon\,dx \\
            &= (b - a)\varepsilon.
        \end{align*}
    \end{proof}
\end{theorem}

\begin{remark}
    If $f_n$ is a sequence of step functions converging uniformly to $f$,
    then the above theorem is the definition of the integral of a regulated function.
\end{remark}

\begin{remark}
    The pointwise limit of a sequence of regulated functions need not be regulated.
    Set
    \[
    f_n(x) = \begin{cases}
        1 &\text{if } x \in \left[0, \frac{1}{(2n + 1 / 2)\pi}\right); \\
        \sin(1 / x) &\text{if } x \in \left[\frac{1}{(2n + 1 / 2)\pi}, 1\right].
    \end{cases}
    \]
    Then all $f_n$ are continuous,
    in particular regulated,
    but we have seen that the limit function is not.
\end{remark}

\begin{example}[The Weierstrass function integrated]
    Earlier we considered $f(x) = \infsumo\frac{1}{2 ^ k}\cos(15 ^ k\pi x)$ and used the Weierstrass $M$-test to show that $f(x)$ converges uniformly to a continuous function
    (and indicated that it is not differentiable at any point).

    But we can integrate $f(x)$!
    By uniform convergence we obtain
    \begin{align*}
        \int_{0}^{1 / 2}f(x)\,dx &= \infsumo\int_{0}^{1 / 2}\frac{1}{2 ^ k}\cos(15 ^ k\pi x)\,dx \\
        &= \infsumo\frac{1}{30 ^ k}\sin(15 ^ k\pi / 2) \\
        &= \frac{1}{\pi}\infsumo\frac{(-1) ^ k}{30 ^ k} \\
        &= \frac{1}{\pi}\frac{1}{1 + 1 / 30} \\
        &= \frac{30}{31\pi}.
    \end{align*}
    Here we used $\sin(15 ^ k\pi / 2) = (-1) ^ k$ which arises from a little fact from elementary number theory that we can write $15 ^ k = 4\ell + (-1) ^ k$ for some $\ell \in \N$.
    Of course,
    in general we cannot expect to get such a simple answer from the integral of a complicated function.
\end{example}

\subsection{Pointwise convergence and integration}


























































\subsection{Improper integrals}















\end{document}