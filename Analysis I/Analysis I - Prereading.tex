\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\newcommand{\seq}[1][x]{(#1_n)_{n \in \N}}
\newcommand{\dseq}[2][n]{(#2_#1)_{#1 \in \N}}
\newcommand{\limas}[3][n]{#2 \rightarrow #3 \text{ as } #1 \rightarrow \infty}
\newcommand{\lhopital}[0]{L' H\^opital}

\title{Analysis I \\
    \large Prereading}
\author{Luke Phillips}
\date{December 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Functions, Limits, and Continuity}

\subsection{Open and closed sets}
Let $a \leq b$.
Recall that we call
\[
(a, b) := \{t \in \R;\,a < t < b\}
\]
an open interval inside $\R$.
We do allow $a = -\infty$ or $b = \infty$.
Similarly,
we have the closed
(or compact)
interval inside $\R$
\[
[a, b] := \{t \in \R;\, a \leq t \leq b\}.
\]
We also have half-open intervals:
$[a, b)$ or $(a, b]$.

We call a subset $X \subseteq \R$ open if for each $c \in X$ there exists an open interval $(c - \delta, c + \delta)$ with $\delta > 0$ which is contained in $X : (c - \delta, c + \delta) \subseteq X$.
\textit{Note:
that $\delta$ typically depends on the point $c$ and that $\delta$ isn't certainly unique.}

If you have found one $\delta > 0$ then all positive reals less than $\delta$ will also work.
\textit{Note:
This definition of an open interval $(a, b)$ is indeed open:
Given $c \in (a, b)$ one can take $\delta = \min\{b - c, c - a\}$.}

Finally,
$c \in X$ is an interior point if there exists an open subset $U$ or an open interval $(a, b)$ containing $c$ which lies completely in $X$.

\hfill

\begin{lemma}\label{pre:analy:lem:limofconvseqinclosedintinint}
    Let $\seq$ be a convergent sequence in the closed interval $[a, b]$.
    Then its limit $L$ lies also in $[a, b]$.
    \begin{proof}
        Assume $L \notin [a, b]$.
        Let $\varepsilon = \min\{L - b, a - L\} > 0$.
        Then by the definition of the limit there exists an $N$ such that $|x_n - L| < \varepsilon$ for all $n \geq N$.
        But this means in particular $x_n \notin [a, b]$ for $n \geq N$.
        Contradiction.
    \end{proof}
\end{lemma}
\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
The proof is sort of saying
(by my understanding)
that if $\varepsilon$ is in the interval of the limit and $L$ is outside of the limit,
then $x_n$ must not be in $[a, b]$,
but this contradicts our assumption.
\end{minipage}
}
\end{center}
\hfill

Of course this does not hold for open intervals $(a, b)$.
The sequence $(1/n)_{n \in \N}$ lies on $(0, 1]$ but has limit $0$ outside the set.
However,
considering $(a, b) \subset [a, b]$ we immediately obtain.

\begin{corollary}
    Let $\seq$ be a converging sequence in the open interval $(a, b)$.
    Then its limit $L$ lies in the closed interval $[a, b]$.
\end{corollary}

$[a, b]$ are the only potential limit points for a sequence in $(a, b)$.
We generalise now that a given set $X \subseteq \R$ we let $\bar{X}$ be the set of all limits of all convergent sequences in $X$.
We call $\bar{X}$ the closure of $X$.

In general,
this can be quite complicated to describe but we will be mainly concerned with situations where $X$ is an interval or finite union of such.

\begin{corollary}[Bolzano-Weierstrass]
    Let $\seq$ be a sequence in the compact interval $[a, b]$.
    Then the sequence has a subsequence which converges in $[a, b]$.

    \begin{proof}
        The sequence $\seq$ is bounded,
        hence by Bolzano-Weierstrass a converging subsequence.
        By \autoref{pre:analy:lem:limofconvseqinclosedintinint} its limit must lie in $[a, b]$.
    \end{proof}
\end{corollary}

\subsection{Limits of functions}
Throughout we let $f : X \rightarrow \R$ be a function on a subset $X \subseteq \R$ of the reals.
Sometimes we might also write $D = D(f)$ for the domain of $f$.

\begin{definition}
    Let $f : X \rightarrow \R$ be a function and assume $X$ contains the open interval $(a, b)$ with the possible exception of a point $c \in (a, b)$.
    We say
    \[
    \lim_{x \rightarrow c}f(x) = L
    \]
    for some $L \in \R$,
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - L| < \varepsilon
    \]
    for all $x \in (a, b) \setminus \{c\}$ with
    \[
    |x - c| < \delta.
    \]
\end{definition}

\textit{Note:
if $c \in X$,
then $L$ can be $f(c)$,
but need not be.
(If it is we will call $f$ continuous at $c$,
see below).}
One often also writes $\lim_{x \rightarrow c}f(x) = L$ in the form
\[
f(x) \rightarrow L\quad\text{as}\quad x \rightarrow c.
\]

\begin{proposition}\label{pre:analy:prop:funclimcriterion}
    Let $f : X \rightarrow \R$ be a function and assume $X$ contains the open interval $(a, b)$ with the possible exception of a point $c \in (a, b)$.
    
    Then $\lim_{x \rightarrow c}f(x) = L$ if and only if for all sequences $\seq$ in $X \setminus \{c\}$ with $x_n \rightarrow c$ as $n \rightarrow \infty$ we have $\lim_{n \rightarrow \infty}f(x_n) = L$.
    \begin{proof}
        First assume $\lim_{x \rightarrow c}f(x) = L$ and let $(x_n)$ be a sequence in $X \setminus \{c\}$ converging to $c$.
        Pick $\varepsilon > 0$.
        Then by assumption there exists a $\delta > 0$ such that
        \[
        |f(x) - L| < \varepsilon\quad\text{provided that}\quad|x - c| < \delta
        \]
        and $x \in (a, b) \setminus \{c\}$.
        Since $x_n \rightarrow c$ as $n \rightarrow \infty$ there exists an $N \in \N$ with
        \[
        |x_n - c| < \delta
        \]
        for all $n \geq N$.
        But then $|f(x_n) - L| < \varepsilon$ for all $n \geq N$.
        This means that $f(x_n) \rightarrow L$ as $n \rightarrow \infty$.

        Now let us assume that $\lim_{x \rightarrow c}f(x) \neq L$
        (or does not exist).
        We need to find a sequence $(x_n)$ in $X \setminus \{c\}$ converging to $c$ such that the sequence $(f(x_n))$ does not converge to $L$.
        By assumption there exists an $\varepsilon > 0$ such that for all $\delta > 0$ there exists a $x \neq c$ with $|x - c| < \delta$ and $|f(x) - L| \geq \varepsilon$.
        So given this $\varepsilon$ we can choose $\delta = \frac{1}{n}$ and then there exists an $x_n \neq c$ with $|x_n - c| < \frac{1}{n}$,
        but $|f(x_n) - L| \geq \varepsilon$.
        But then $x_n \rightarrow c$ as $n \rightarrow \infty$,
        while $f(x_n) \nrightarrow L$.
    \end{proof}
\end{proposition}

We now will extend the definition of a limit of a function to one-sided limits and also to $c = \pm\infty$.
\begin{definition}[Right-sided Limit]
    Let $f : (a, b) \rightarrow \R$ be a function.
    We define the right-sided limit as
    \[
    \lim_{x \rightarrow a ^ {+}}f(x) = L
    \]
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - L| < \varepsilon\qquad\text{for all } x \in (a, b)\text{ with }|x - a| = x - a < \delta.
    \]
\end{definition}

\begin{definition}[Left-sided Limit]
    Let $f : (a, b) \rightarrow \R$ be a function.
    We define the left-sided limit as
    \[
    \lim_{x \rightarrow a ^ {-}}f(x) = L
    \]
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - L| < \varepsilon\qquad\text{for all } x \in (a, b)\text{ with }|x - b| < \delta.
    \]
\end{definition}

We can also define the left/right-sided limit for an interior point $c \in (a, b)$ in same way
(formally,
pick $a = c$ respectively.
$b = c$ in the definition.
Naturally,
the one-sided limits if they exist don't have to be the same!)

If the domain $X$ of $f$ is not bounded above or below
(containing intervals of the form $(b, \infty)$ or $(-\infty, a)$),
we say
\begin{definition}
    \[
    \lim_{x \rightarrow \infty}f(x) = L
    \]
    if for every $\varepsilon > 0$ there exists a $K > 0$ such that
    \[
    |f(x) - L| < \varepsilon\text{ for every } x \in X\text{ with } x \geq K.
    \]
    If $X$ is not bounded below,
    we can define
    \[
    \lim_{x \rightarrow -\infty}f(x) = L
    \]
    analogously.
\end{definition}

We can use the limit criterion \autoref{pre:analy:prop:funclimcriterion} to easily carry over COLT to the limit of functions
\begin{theorem}
    Let $f, g : X \rightarrow \R$ be two functions with $\lim_{x \rightarrow c}f(x) = L_1$ and $\lim_{x \rightarrow c}g(x) = L_2$.
    \begin{enumerate}[label = (\roman*)]
        \item For any $\alpha, \beta \in \R$ the limit $\lim_{x \rightarrow c}\alpha\cdot f(x) + \beta\cdot g(x)$ exists and is equal to $\alpha L_1 + \beta L_2$.
        \item The limit $\lim_{x \rightarrow c}f(x)g(x)$ exists and is equal to $L_1L_2$.
        \item Assume $L_2 \neq 0$.
        The limit $\lim_{x \rightarrow c}f(x) / g(x)$ exists and is equal to $L_1 / L_2$.
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}[label = (\roman*)]
        \item
            We will use the criterion \autoref{pre:analy:prop:funclimcriterion}.
            For this let $(x_n)$ be a sequence in $X$ with $\limas{x_n}{c}$.
    
            By COLT for sequences and the hypothesis we then have that $\alpha \cdot f(x_n) + \beta \cdot g(x_n)$ converges to $\alpha \cdot L_1 + \beta \cdot L_2$ which then implies the first claim by \autoref{pre:analy:prop:funclimcriterion}.
        
        \textit{The other claims follow in the same way.}
        \item \textbf{Prove!}
        \item \textbf{Prove!}
        \end{enumerate}
    \end{proof}
\end{theorem}

Similarly we also have,
\begin{theorem}
    Let $X \subset \R$,
    $c \in X$ and $f, g, h : X \rightarrow \R$ be three functions $f(x) \leq g(x) \leq h(x)$ for all $x \in X$ and also $\lim_{x \rightarrow c}f(x) = \lim_{x \rightarrow c}h(x) = L$.
    Then also
    \[
    \lim_{x \rightarrow c}g(x) \text{ exists with } \lim_{x \rightarrow c}g(x) = L.
    \]
    \begin{proof}
        This follows from \autoref{pre:analy:prop:funclimcriterion} in exactly the same way.
        So let $(x_n)$ be a sequence in $X$ with $\limas{x_n}{c}$.
        Hence by hypothesis $f(x_n) \leq g(x_n) \leq h(x_n)$ for all $x \in X$ and also $\liminfty f(x_n) = \liminfty h(x_n) = L$.
        Now apply squeezing for sequences to conclude that $\liminfty g(x_n)$ exists and is equal to $L$.
        This implies the claim by \autoref{pre:analy:prop:funclimcriterion}.
    \end{proof}
\end{theorem}

\subsection{Continuity}

\begin{definition}
    Let $f : X \rightarrow \R$ be a function and let $c$ be an interior point of $X$.
    Then $f$ is called continuous at $c \in X$ if
    \[
    \lim_{x \rightarrow c}f(x) = f(c).
    \]
    That is,
    if for all $\varepsilon > 0$ there exists a $\delta > 0$ such that
    \[
    |f(x) - f(c)| < \varepsilon\quad\text{for all $x \in X$}\quad\text{with } |x - c| < \delta.
    \]
    The function $f : X \rightarrow \R$ is called continuous,
    if it is continuous for all $c \in X$.
\end{definition}

\begin{remark}\phantom{}
    \begin{itemize}
        \item 
        $f$ must be defined at the point $c$ for it to be continuous at $c$.

        \item
        Additionally,
        continuity at a point $c$ is a local property,
        it only depends on the behaviour of $f$ in a
        (small)
        (open)
        interval around $c$.
        So the function defined on $X$ is continuous at $c$ if and only if the function $f$ is restricted to an interval around $c$ is continuous at $c$.
        
        This can make things simpler if $f$ is restricted to a closed and bounded interval around $c$.
    
        \item 
        We can define left-continuity and right-continuity analogously as for the limit of functions.
        It is easy to see that $f$ is continuous at $c$ if and only if it is left and right continuous at $c$.

        In this way one can also define continuity for a function $f$ on the closed interval $[a, b]$ at the boundary points $a$ and $b$.
    \end{itemize}
\end{remark}

\begin{example}
    The identity function $f : \R \rightarrow \R$ given by $f(x) = x$ is continuous.
    \begin{proof}
        Given $c \in \R$ and $\varepsilon > 0$,
        we can choose $\delta = \varepsilon$.
        This completes the proof.
    \end{proof}
\end{example}

\begin{example}
    The function $f : \R \setminus \{0\} \rightarrow \R$ given by $f(x) = 1 / x$ is also continuous.

    This function is not defined at $x = 0$,
    so it is not a question whether $f$ is continuous at $c = 0$.

    At $c \neq 0$ we have
    \[
    \left|\frac{1}{x} - \frac{1}{c}\right| = \left|\frac{c - x}{cx}\right|.
    \]
    This needs to be smaller than the given $\varepsilon > 0$.
    The $c - x$ term is harmless,
    the $c$ in the denominator is just a constant.
    Our problem is the $x$ in the denominator which if we are close to zero can become large.
    We need to restrict ourselves away from $x = 0$.
    Consider the function in the restricted region $|x| > |c| / 2$.
    Note $c$ is certainly still an interior point.
    So now for $|x| > |c| / 2$ have
    \[
    \left|\frac{1}{x} - \frac{1}{c}\right| = \left|\frac{c - x}{cx}\right| \leq \frac{2}{c ^ 2}|c - x|\footnotemark.
    \]
    \footnotetext{This step is a handful at first,
    think about the $|x| > |c| / 2$ inequality hard enough then you should be able to understand this ridiculous transition.}
    This needs to be less than $\varepsilon$.
    We can pick $\delta = \frac{c ^ 2 \varepsilon}{2}$.
    Then if $|x - c| < \delta = \frac{c ^ 2 \varepsilon}{2}$
    (and $|x| > |c| / 2$)
    we get
    \[
    \left|\frac{1}{x} - \frac{1}{c}\right| = \left|\frac{c - x}{cx}\right| \leq \frac{2}{c ^ 2}|c - x| < \frac{2}{c ^ 2}\frac{c ^ 2\varepsilon}{2} = \varepsilon,
    \]
    as required.
    We have shown this works for all $c$ so we have shown that $\frac{1}{x}$ is continuous on all of $\R \setminus \{0\}$\footnote{It may seem weird but since $c$ can get really close to $0$ it technically covers all of our domain,
    I hate it too.}.

    Alternatively,
    if you don't like the trick of restricting the function away from the bad point $x = 0$,
    you could take $\delta = \min\{|c| / 2, c ^ 2\varepsilon / 2\}$,
    and the same calculation will work.
    However,
    I find this quite over the top,
    if you think about the first method hard enough it seems more intuitive.
\end{example}

\begin{example}
    The function $f : [0, \infty) \rightarrow \R$ given by $f(x) = \sqrt{x}$ is continuous.
    First consider $c = 0$.
    Given $\varepsilon > 0$,
    choose $\delta = \varepsilon ^ 2$.
    Then for $|x| = |x - 0| < \delta = \varepsilon$ we get
    \[
    |\sqrt{x} - \sqrt{0}| = |\sqrt{x}| < \sqrt{\delta} = \varepsilon,
    \]
    so $f$ is
    (right)
    continuous at $c = 0$.
    At $c \neq 0$,
    we can write
    \[
    |\sqrt{x} - \sqrt{c}| = \frac{|x - c|}{\sqrt{c} + \sqrt{x}} \leq \frac{|x - c|}{\sqrt{c}}.
    \]
    So given $\varepsilon > 0$,
    choose $\delta = \sqrt{c}\varepsilon$.
\end{example}
\textit{Note:
in the above example we have to consider $c = 0$ as a separate case since we cannot use the same argument we used for $c \neq 0$ when $c = 0$ as we would be dividing by $0$!}

\begin{example}
    Consider the sign function $\mathrm{sgn} : \R \rightarrow \R$ given by
    \[
    \mathrm{sgn}(x) = \begin{cases}
        -1 & x < 0 \\
        0 & x = 0 \\
        1 & x > 0.
    \end{cases}
    \]
    Then $\mathrm{sgn}$ is continuous for every $c \neq 0$,
    but not at $c = 0$.
\end{example}

\begin{example}
    Consider $d : \R \rightarrow \R$ given by
    \[
    d(x) = \begin{cases}
        0 & x \in \Q \\
        1 & x \notin \Q.
    \end{cases}
    \]
    Then $d$ is not continuous at any $c \in \R$.
\end{example}

From \autoref{pre:analy:prop:funclimcriterion} we obtain the very useful criterion.
\begin{proposition}\label{pre:analy:prop:contfuncoflimeqlimofcontfunc}
    Let $X \subset \R$,
    $c \in X$,
    and $f : X \rightarrow \R$ a function.
    Then $f$ is continuous at $c$ if and only if for all sequences $\seq$ in $X$ with $\limas{x_n}{c}$ we have $\limas{f(x_n)}{f(c)}$.
    That is,
    \[
    \liminfty f(x_n) = f\left(\liminfty x_n\right).
    \]
\end{proposition}

We can use this to carry over COLT to continuous functions.
\begin{theorem}\label{pre:analy:thm:coltforcontfuncs}
    Let $X \subset \R$,
    $c \in X$ and $f, g : X \rightarrow \R$ be continuous at $c$.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item $h(x) := \alpha \cdot f(x) + \beta \cdot g(x)$ is continuous at $c$ for any $\alpha, \beta \in \R$.
        \item $h(x) := f(x) \cdot g(x)$ is continuous at $c$.
        \item $h(x) := f(x) / g(x)$ is continuous at $c$,
        provided that $g(c) \neq 0$.
    \end{enumerate}
    \begin{proof}
        To show that $h(x)$ is continuous at $c$,
        let $(x_n)$ be a sequence in $X$ with $\limas{x_n}{c}$.
        By COLT and \autoref{pre:analy:prop:contfuncoflimeqlimofcontfunc}
        (applied to the continuous functions $f$ and $g$),
        $\limas{h(x_n)}{h(c)}$.
        By \autoref{pre:analy:prop:contfuncoflimeqlimofcontfunc} $h$ is continuous at $c$.
    \end{proof}
\end{theorem}

\begin{example}
    Given two polynomials $p, q : \R \rightarrow \R$,
    the rational function $r : X \rightarrow \R$ given by
    \[
    r(x) = \frac{p(x)}{q(x)}
    \]
    is continuous,
    where $X = \{x \in \R\,\mid\,q(x) \neq 0\}$ by COLT for continuous functions.
\end{example}

\begin{theorem}\label{pre:analy:thm:contfunccompcontfunciscont}
    Let $X, Y \subset \R$,
    $c \in X$,
    $f : X \rightarrow \R$,
    $g : Y \rightarrow \R$ with $f(X) \subset Y$.
    If $f$ is continuous at $c \in X$ and $g$ is continuous at $f(c) \in Y$,
    then $g \circ f$ is continuous at $c \in X$.

    \begin{proof}
        Let $(x_n)$ be a sequence in $X$ converging to $c$.
        Then $y_n = f(x_n) \rightarrow f(c)$ by \autoref{pre:analy:prop:contfuncoflimeqlimofcontfunc},
        since $f$ is continuous at $c$.
        Then $g(y_n) \rightarrow g(f(c))$ by \autoref{pre:analy:prop:contfuncoflimeqlimofcontfunc},
        since $g$ is continuous at $f(c)$.
        This means $\limas{g \circ f(x_n)}{g \circ f(c)}$,
        and $g \circ f$ is continuous at $c$ by \autoref{pre:analy:prop:contfuncoflimeqlimofcontfunc}.
    \end{proof}
\end{theorem}

\begin{example}
    The exponential function $f : \R \rightarrow \R$ given by $f(x) = e ^ x$ is continuous.
    \begin{proof}
        First we look at $c = 0$,
        so $f(0) = e ^ 0 = 1$.
        Recall the following inequality,
        for $x < 1$
        \[
        1 + x \leq e ^ x \leq \frac{1}{1 - x}.
        \]
        Hence by the squeezing theorem $\lim_{x \rightarrow 0}e ^ x = 1 = e ^ 0$.
        So $e ^ x$ is continuous at $c = 0$.

        Now let $c \neq 0$.
        Then $e ^ {x - c}$ is continuous at $x = c$ as an application of \autoref{pre:analy:thm:contfunccompcontfunciscont}
        (with $f(x) = e ^ x$ and $g(x) = x - c$)\footnote{This can be thought of very slightly easier as $\exp{(x - c)}$.}.
        Using the addition law for the exponential we conclude
        \[
        \lim_{x \rightarrow c}e ^ x = e ^ c \lim_{x \rightarrow c}e ^ {x - c} = e ^ ce ^ 0 = e ^ c,
        \]
        as claimed.
    \end{proof}
\end{example}

\subsection{The Intermediate Value Theorem and the Extreme Value Theorem}

If a function $f : [a, b] \rightarrow \R$ is continuous on a closed interval,
the Intermediate Value Theorem states that $f$ takes on any value between $f(a)$ and $f(b)$.
For example,
if $f(a)$ is negative and $f(b)$ is positive,
there has to be a point $c \in (a, b)$ with $f(c) = 0$.
More precisely,
we have

\begin{theorem}[Intermediate Value Theorem]
    Let $a, b \in \R$ with $a < b$ and $f : [a, b] \rightarrow \R$ be continuous.
    If $f(a) < f(b)$ and $d \in [f(a), f(b)]$,
    then there exists $c \in [a, b]$ with $f(c) = d$.
    Likewise,
    if $f(b) < f(a)$ and $d \in [f(b), f(a)]$,
    then there exists $c \in [a, b]$ with $f(c) = d$.

    \begin{proof}
        We assume that $f(a) < f(b)$.
        We can also assume $d < f(b)$
        (otherwise pick $c = b$)\footnote{As $f(c) = d = f(b)$ but $d < f(b)$.}.
        Let
        \[
        X = \{x \in [a, b]\,|\,f(x) \leq d\}.
        \]
        Then $X$ is bounded above by $b$,
        and $a \in X$.
        So $c = \sup{X}$ exists.
        Hence by
        (some result I cannot remember the name of,
        plus the completeness axiom)
        there is a sequence $(x_n)$ in $X$ with $x_n \rightarrow c$.
        By continuity and \autoref{pre:analy:lem:limofconvseqinclosedintinint} we have $f(x_n) \rightarrow f(c) \leq d < f(b)$;
        in particular $c < b$.
        If we had $f(c) < d$,
        we could find by continuity $\delta > 0$ with $c + \delta < b$ and
        \[
        |f(x) - f(c)| < \frac{d - f(c)}{2}(:= \varepsilon)
        \]
        for all $x \in [a, b]$ with $|x - c| < \delta$.
        But then $f(c + \delta / 2) < d$,
        contradicting that $c$ is the supremum of $X$.
        Therefore $f(c) = d$.
    \end{proof}
\end{theorem}

If $f : X \rightarrow \R$ is a function,
the range of $f$ is the set
\[
f(X) = \{y \in \R\mid y = f(x)\text{ for some } x \in X\}.
\]
If $X$ is an interval and $a < b$ both in $X$,
we can restrict $f$ to $[a, b]$ and get that all values between $f(a)$ and $f(b)$ are taken.
This means that $f(X)$ is an interval as well.

\textit{Note that if $f(X)$ is not bounded above,
the upper endpoint of the interval is $\infty$.}

If $f(X)$ is bounded above,
the upper endpoint of the interval is given by the supremum of $f(X)$.
This supremum may or may not be in the range,
but this is okay for an interval.
The lower endpoint is treated similarly,
so we get

\begin{corollary}\label{pre:analy:corl:funcofintisint}
    Let $I$ be an interval,
    and $f : I \rightarrow \R$ be continuous.
    Then the image $f(I)$ of $I$ under $f$ is an interval.
\end{corollary}

\begin{example}
    Let $f : (0, 2) \rightarrow \R$ be given by
    \[
    f(x) = \frac{x - 1}{x(2 - x)}.
    \]
    Then the range is the interval $(-\infty, \infty) = \R$.
\end{example}

If the domain is a closed interval $[a, b]$,
this cannot happen,
and the range is also a closed interval.

\begin{theorem}
    Let $a, b \in \R$ with $a < b$ and $f : [a, b] \rightarrow \R$ be continuous.
    Then
    
    Every continuous function on a compact interval attains its maximum and minimum.

    Formulated differently,
    the image of $f$ is given by $[c, d]$ for some $c, d \in \R$.
    In particular,
    there exist $u, v \in [a, b]$ with $f(u) = c$ and $f(v) = d$.
    So in conclusion.

    The continuous image of a compact interval is again a compact interval.

    \begin{proof}
        We first show that $f([a, b])$ cannot be unbounded above.
        For otherwise there exists a sequence $\seq$ in $[a, b]$ such that $f(x_n) \geq n$.
        By Bolzano-Weierstrass there is a subsequence $(x_{n_j})$ which is convergent to a $c \in [a, b]$.
        By continuity we have $f(x_{n_j}) \rightarrow f(c) \in \R$.
        But this is a contradiction to $f(x_{n_j}) \geq n_j$ for all $j$.

        Therefore $f([a, b])$ is bounded above,
        and $d = \sup{f([a, b])}$ exists.
        By
        (the same result that I cannot remember)
        there exists a sequence $(y_n)$ in $f([a, b])$ with $\limas{y_n}{d}$.
        But there also exists a sequence $(x_n)$ in $[a, b]$ with $f(x_n) = y_n$.
        By Bolzano-Weierstrass there is a convergent subsequence $(x_{n_j})$ with limit $c \in [a, b]$.
        Then
        \[
        f(c) = f(\liminfty[j]x_{n_j}) = \liminfty[j]f(x_{n_j}) = \liminfty[j]y_{n_j} = d.
        \]
        So $d \in f([a, b])$.
        Similarly we get that $c = \inf{f([a, b])} \in f([a, b])$,
        and by \autoref{pre:analy:corl:funcofintisint} we get $f([a, b]) = [c, d]$.
    \end{proof}
\end{theorem}

\subsection{Uniform continuity}
Recall the definition of continuity for a function $f$ on an interval $I$
\[
\forall c \in I\, \forall \varepsilon > 0\, \exists\delta > 0\, \forall x \in I\quad (|x - c| < \delta \implies |f(x) - f(c)| < \varepsilon)
\]

\begin{remark}\phantom{}
    \begin{itemize}
        \item Note that $\delta$ not only depends on the initial choice of $\varepsilon > 0$,
        but also
        (potentially)
        on the actual point $c$.
        \item Recall that if one $\delta > 0$ does the job,
        then so will any other positive number less than $\delta$.
        \item Recall that continuity at a point $c$ is a local property,
        that is,
        it only depends on the behaviour of $f$ in a
        (small)
        neighbourhood of $c$ and not on the entire domain of definition $I$.
        So if we restrict $f$ to a small(er) interval $J$ containing $c$ as an inner point then the question of continuity at $c$ is unaffected.
    \end{itemize}
\end{remark}

\begin{definition}[Uniform Continuity]
    Let $f$ be a function on an interval $I$.
    We call $f$ uniformly continuous on $I$  if
    \[
    \forall \varepsilon > 0\,\exists \delta > 0\,\forall c, x \in I \quad (|x - c| < \delta \implies |f(x) - f(c)| < \varepsilon).
    \]
    The point is that now $\delta$ has to be the same for all $c \in I$,
    that is,
    $\delta$ no longer depends on the individual point $c$
    (but of course still on $\varepsilon$ and naturally the function $f$ itself).
    In particular,
    uniform continuity is no longer a local concept but a 'global' one as it depends on the entire interval $I$.
\end{definition}

\begin{theorem}
    Let $f$ be a continuous function on a compact interval $[a, b]$.
    Then $f$ is uniformly continuous on $[a, b]$.
    \begin{proof}
        Assume $f$ is not uniformly continuous on $[a, b]$.
        This means
        \[
        \exists\varepsilon > 0\, \forall\delta > 0\, \exists x, y \in [a, b]\text{ with } |x - y| < \delta \text{ but } |f(x) - f(y)| \geq \varepsilon.
        \]
        Fix such an $\varepsilon > 0$ and take $\delta = \delta_n = \frac{1}{n}$ for each $n \in \N$.
        We find $x_n, y_n \in [a, b]$ such that
        \[
        |x_n - y_n| < \frac{1}{n}\quad\text{but}\quad|f(x_n) - f(y_n)| \geq \varepsilon.
        \]
        The key is,
        by Bolzano-Weierstrass we can find a subsequence $(x_{n_j})$ of $(x_n)$ which converges to a point $x*$ in $[a, b]$.
        (Here we use the fact that we are dealing with a compact interval).
        We claim that also $y_{n_j} \rightarrow x*$.
        Indeed let $\varepsilon' > 0$.
        Then there exists an $N$ such that for all $n_j > N$ we have $|x_{n_j} - x*| < \varepsilon'$ and hence for all $n_j > \max\{1 / \varepsilon', N\}$ we have
        \[
        |y_{n_j} - x*| = |y_{n_j} - x_{n_j} + x_{n_j} - x*| \leq |y_{n_j} - x_{n_j}| + |x_{n_j} - x*| < \varepsilon' + \frac{1}{n_j} < 2\varepsilon'.
        \]
        Thus the $(y_{n_j})$ also converge to $x*$.
        But now we
        (finally)
        use the continuity of $f$ at $x*$.
        There exists a $\delta > 0$ such that $|f(x*) - f(x)| < \varepsilon / 2$ for all $x \in [a, b]$ with $|x - x*| < \delta$.
        Now for $n_j$ sufficiently large we have $|x_{n_j} - x*| < \delta$ and also $|y_{n_j} - x*| < \delta$ and then
        \[
        |f(x_{n_j} - f(y_{n_j})| \leq |f(x_{n_j}) - f(x*)| + |f(x*) - f(y_{n_j})| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon
        \]
        in contradiction to $1$.
    \end{proof}
\end{theorem}

\begin{remark}\phantom{}
    \begin{itemize}
        \item We have seen\footnote{In a problem sheet that I cannot be bothered to find.} that a Lipschitz continuous function
        ($|f(x) - f(y)| \leq M|x - y|$ for all $x, y \in I$)
        is uniformly continuous.

        Later we will see that differentiable functions with bounded derivative are Lipschitz and hence are uniformly continuous.

        \item The function $f(x) = \sqrt{x}$ is continuous on $[0, 1]$ but not Lipschitz.
        However,
        the theorem states that it is uniformly continuous.
        So Lipschitz continuity is really a stronger concept than uniform continuity.

        \item The function $f(x) = \frac{1}{x}$ is not uniformly continuous on $(0, 1]$.
        So one really needs a compact interval in the theorem to conclude that continuity implies uniform continuity.

        \item The function $\exp(x) = e ^ x$ is not uniformly continuous on $\R$.
        However,
        $\log(x)$ is uniformly continuous on $[1, \infty)$
        (even though both functions diverge as $x \rightarrow \infty$).
    \end{itemize}
\end{remark}

\subsection{Injective continuous functions and their inverses}

A function $f : X \rightarrow \R$ is said to be strictly increasing if $f(x) < f(y)$ for all $x, y \in X$ with $x < y$.
Similarly,
it is said to be strictly decreasing if $f(x) > f(y)$ for all $x, y \in X$ with $x < y$.
Also recall that $f$ is called injective
(or one-to-one)
if $f(x_1) = f(x_2)$ implies $x_1 = x_2$.
Of course strictly increasing/decreasing functions on $\R$ are injective.
The converse does not always hold.
However,
\begin{proposition}
    Let $f : I \rightarrow \R$ be continuous and injective on an interval $I$.
    Then $f$ is either strictly increasing or strictly decreasing.
    \begin{proof}
        If $f$ was neither we would find elements $a, b, c \in I$ with $a < c < b$ but with $f(c)$ bigger
        (smaller goes the same)
        than both $f(a)$ and $f(b)$,
        say for example $f(a) < f(b) < f(c)$.
        But then $f$ must take the value $f(b)$ in the interval $[a, c]$
        (which does not contain $b$)
        again.
        Thus $f$ would not be one-to-one.
    \end{proof}
\end{proposition}

Assume $f : X \rightarrow \R$ is injective.
Recall we can form its inverse $f ^ {-1} : f(X) \rightarrow \R$ by $f ^ {-1}(x) = y$,
where $y \in X$ is the unique element with $f(y) = x$.
In particular,
the domain of $f ^ {-1}$ is the range of $f$ and the range of $f ^ {-1}$ is the domain $X$ of $f$.
Note $f(f ^ {-1}(c)) = c$ for all $c \in f(X)$ and $f ^ {-1}(f(d)) = d$ for all $d \in X$.

\begin{example}
    The logarithm was defined as the inverse function of the exponential function.
    So with $f(x) = e ^ x$,
    we get $f ^ {-1}(x) = \log{x}$.
    Notice that $f ^ {-1}$ is only defined for $x > 0$,
    since $e ^ x > 0$ for all $x \in \R$.
\end{example}

\begin{theorem}\label{pre:analy:thm:imgofinvcontinjfunciscont}
    Let $f : I \rightarrow \R$ be continuous and injective on a non-trivial interval $I$ with image $J := f(I)$.
    Then $f ^ {-1} : J \rightarrow \R$ is also continuous.

    \begin{proof}
        Let $d \in J$,
        say $d = f(c)$ for some $c \in I$.
        We need to show that given $\varepsilon > 0$ there exists $\delta > 0$ so that $|f ^ {-1}(d) - f ^ {-1}(y)| < \varepsilon$ for all $y \in J$ with $|y - d| < \delta$.

        Assume for simplicity that $f$ is strictly increasing
        (and hence also $f ^ {-1}$).
        Assume for the moment that $c$ is an interior point of $I$.
        Then we can assume $[c - \varepsilon / 2, c + \varepsilon / 2] \subset I$.
        Then under $f$ this becomes $[f(c - \varepsilon / 2), f(c + \varepsilon / 2)]$ with $f(c - \varepsilon / 2) < d < f(c + \varepsilon / 2)$.
        Now find $\delta > 0$ such that $(d - \delta, d + \delta) \subset [f(c - \varepsilon / 2), f(c + \varepsilon / 2)]$.
        Then for every $y \in J$ with $|y - d| < \delta$ we have $f ^ {-1}(y) \in [c - \varepsilon / 2, c + \varepsilon / 2]$.
        Hence $|f ^ {-1}(y) - f ^ {-1}(d)| = |f ^ {-1}(y) - f ^ {-1}(f(c))| = |g(y) - c| \leq \varepsilon / 2 < \varepsilon$ as desired.
        If $c$ is an endpoint of $I$
        (if it exists),
        say the upper one,
        then so is $d \in J$,
        and the argument carries through with $c + \varepsilon / 2$ and $d + \delta$ replaced by $c$ and $d$ respectively.
        The lower endpoint goes the same.
    \end{proof}
\end{theorem}

\begin{example}\phantom{}
    \begin{itemize}
        \item The logarithm $\log(x) : \R_{+} \rightarrow \R$ is continuous.
        \item It is important to note that $f$ is defined on an interval in \autoref{pre:analy:thm:imgofinvcontinjfunciscont} to get the continuity of the inverse function.

        Consider $f : [0, 1] \cup (2, 3] \rightarrow \R$ given by
        \[
        f(x) = \begin{cases}
            2x & \text{for } x \in [0, 1] \\
            x & \text{for } x \in (2, 3].
        \end{cases}
        \]
        This is easily seen to be continuous and injective
        (note that $2$ is not in the domain of $f$).
        The inverse function $f ^ {-1} : [0, 2] \rightarrow \R$ is given by
        \[
        f ^ {-1}(x) = \begin{cases}
            \frac{1}{2}x & \text{for } x \in [0, 2] \\
            \frac{1}{2}x & \text{for } x \in (2, 3]
        \end{cases}
        \]
        and it is not continuous at $2$,
        as $f ^ {-1}(2 + 1 / n) \rightarrow 2 \neq f(2)$.

        It is instructive to revisit the proof of the continuous inverse proposition above and to investigate what goes wrong for this function.
        It is fairly subtle at $d = 2 = f(1)$.
        Namely,
        while $c = 1$ is the endpoint of one interval of the domain of $f$,
        the value $d = 2$ is not for the range.
    \end{itemize}
\end{example}

\begin{remark}
    In general,
    one want would like to be true is that if the function $f$ is "nice"
    (has a certain property),
    then the inverse function $f$ should be equally "nice"
    (has the same property).
    Unfortunately,
    this does not always hold;
    one usually has to require an extra condition.
    We have already seen this for continuous functions on the real line to itself
    (in a mild way).
\end{remark}

\newpage

\section{Differentiable Functions}

\subsection{Basics on differentiable functions}

Let us first define differentiability
\begin{definition}
    Let $X \subseteq \R$ be open and $f : X \rightarrow \R$ be a function.
    $f$ is differentiable at $c \in X$ if $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}$ exists.
    We denote this limit by $f'(c)$ and call $f'(c)$ the derivative of $f$ at $c$.
    $f$ is called a differentiable function if $f$ is differentiable at all points $c \in X$.
\end{definition}

One often encounters the formulation
\[
f'(c) = \lim_{h \rightarrow 0}\frac{f(c - h) - f(c)}{h},
\]
which is the same
(set $h = x - c$).
Sometimes this formulation is more convenient.

\begin{remark}\phantom{}
    \begin{itemize}
        \item The expression $\frac{f(x) - f(c)}{x - c}$ has an important geometric interpretation:
        It is the slope of the straight line passing through the two points $(c, f(c)) \in \R ^ 2$ and $(x, f(x)) \in \R ^ 2$ of the graph of $f$.
        As $x \rightarrow c$,
        the second point $(x, f(x))$ approaches the first point $(c, f(c))$,
        and the limit describes the slope of the tangent of the graph of $f$ at the point $(c, f(c)) \in \R ^ 2$.

        \item Differentiability at $c \in X$ can also be described in the $(\varepsilon, \delta)$-formalism.
        $f$ is differentiable at $c \in X$ if there exists a number $L \in \R$
        (the derivative $f'(c)$)
        such that there exists for every $\varepsilon > 0$ a positive number $\delta > 0$ such that
        \[
        \left|\frac{f(c) - f(x)}{c - x} - L\right| < \varepsilon\qquad\forall x \in X \text{ with } |x - c| < \delta.
        \]

        \item We can also define
        (one-sided)
        differentiability for a function defined on a closed interval $[a, b]$ at the endpoints $a$ and $b$ by considering the one-sided limit of the difference quotient.

        We now give another equivalent formulation for differentiability which is not only handy but it also generalises to higher dimensions.
    \end{itemize}
\end{remark}

\begin{lemma}[First order Taylor]\label{pre:analy:lem:firstordertaylorindiffdef}
    Let $f : X \rightarrow \R$ be a function as above.
    Then $f$ is differentiable at $c \in X$ if and only if the following holds:

    There exists a constant $m \in \R$ an a function $r(x)$ on $X$ such that
    \[
    f(x) = f(c) + m(x - c) + r(x)(x - c)
    \]
    such that
    \[
    r(x)\text{ is continuous at } c \qquad\text{and}\qquad\lim_{x \rightarrow c}r(x) = r(c) = 0.
    \]
    In this case $m = f'(c)$.
\end{lemma}
Before we get into the juicy proof of this,
let us have a few remarks.
\begin{remark}\phantom{}
    \begin{itemize}
        \item We can view differentiability as the attempt to approximate $f(x)$ in the neighbourhood of $c$ by a linear function $L(x) = f(c) + m(x - c)$.
        Then differentiability stipulates that the error $r(x)(x - c)$ is of 'higher order'
        (not linear)
        as $\lim_{x \rightarrow c}r(x) = 0$.

        \item From the perspective of the previous,
        we can view continuity as the 'zero order Taylor'.
        The function $f$ is continuous at $c$ if $f(x) = f(c) + \tilde{r}(x)$ with $\lim_{x \rightarrow c}\tilde{r}(x) = 0$.
        We approximate $f$ by the constant function $f(c)$ and the error vanishes in the limit.
    \end{itemize}
\end{remark}

\begin{remark}
    In practice one often considers $f_1(x) = m + r(x)$ which is continuous at $c$ with value $f_1(c) = m$.
    Thus differentiability at $c$ is equivalent to:

    There exists a function $f_1 : X \rightarrow \R$ such that
    \[
    f(x) = f(c) + (x - c)f_1(x)
    \]
    and $f_1$ is continuous at $c$.
    Then $\lim_{x \rightarrow c}f_1(x) = f_1(c) = f'(c)$.
    (In this formulation one doesn't need to know the value of the derivative in advance).
\end{remark}

\begin{lemma}[continues = pre:analy:lem:firstordertaylorindiffdef]
    \begin{proof}
        Assume $f$ is differentiable at $c$,
        so the limit $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}$ exists.
        Let's denote this limit by $m$ and set
        \[
        r(x) = \begin{cases}
            \frac{f(x) - (f(c) + m(x - c))}{x - c} & \text{if } x \neq c; \\
            0 & \text{if } x = c.
        \end{cases}
        \]
        Then if $r(x)$ is continuous at $c$ holds,
        and we only need to check continuity at $c$:
        But
        \[
        \lim_{x \rightarrow c}r(x) = \lim_{x \rightarrow c}\frac{f(x) - (f(c) + m(x - c))}{x - c} = \lim_{x \rightarrow c}\left(\frac{f(x) - f(c)}{x - c} - m\right) = m - m = 0,
        \]
        as required.

        Conversely,
        if $\lim_{x \rightarrow c}r(x) = r(c) = 0$ holds with $r$ continuous at $c$ and $r(c) = 0$,
        then
        \[
        0 = r(c) = \lim_{x \rightarrow c}r(x) = \lim_{x \rightarrow c}\frac{f(x) - (f(c) + m(x - c))}{x - c} = \lim_{x \rightarrow c}\left(\frac{f(x) - f(c)}{x - c} - m\right).
        \]
        But this is only possible if $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}$ exists and is equal to $m$.
    \end{proof}
\end{lemma}

\begin{example}\phantom{}
    \begin{itemize}
        \item Let us prove differentiability of $f(x) = \frac{1}{x}$ on $(0, \infty)$.
        Let $c \in (0, \infty)$.
        Then we have
        \[
        \frac{f(x) - f(c)}{x - c} = \frac{c - x}{xc(x - c)} = -\frac{1}{xc},
        \]
        which implies that
        \[
        \lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c} = -\lim_{x \rightarrow c}\frac{1}{xc} = -\frac{1}{c ^ 2}.
        \]
        This shows that $f'(c) = -\frac{1}{c ^ 2}$.
        Now for the 'remainder' $r(x)$ we have
        \[
        r(x) = \frac{f(x) - f(c)}{x - c} - f'(c) = -\frac{1}{xc} + \frac{1}{c ^ 2} = \frac{x - c}{xc ^ 2},
        \]
        which indeed is continuous at $c$ with limit $0$.

        \item Let $f(x)$ be a polynomial.
        Then $f(x) - f(c)$ is also a polynomial which has a zero at $x = c$.
        Thus by polynomial division we can write
        \[
        f(x) - f(c) = (x - c)f_1(x)
        \]
        for some polynomial $f_1(x)$.
        In particular,
        $f_1(x)$ is continuous.
        We therefore have shown that all polynomials are differentiable!
        (Of course,
        we haven't shown what the derivative actually is;
        for $x ^ n$ it is fairly direct to determine $f_1(x)$ and its limit $nc ^ {n - 1}$ as $x \rightarrow c$.).
    \end{itemize}
\end{example}

Differentiability is a stronger property than continuity:
\begin{theorem}\label{pre:analy:thm:iffdiffatcthencontatc}
    Let $X \subset \R$ and $f : X \rightarrow \R$.
    If $f$ is differentiable at $c \in X$ then $f$ is also continuous at $c$.
    \begin{proof}
        We have
        \[
        f(x) - f(c) = (x - c) \cdot \frac{f(x) - f(c)}{x - c} \rightarrow 0 \cdot f'(c) = 0\text{ as } x \rightarrow c.
        \]
        This shows that $\lim_{x \rightarrow c}f(x) = f(c)$,
        in other words,
        $f$ is continuous at $c$.
    \end{proof}
\end{theorem}

\begin{remark}
    There are continuous functions which are not differentiable.
    The easiest example is the function $f(x) = |x|$,
    which is continuous on $\R$ but not differentiable at $c = 0$.
    However,
    the construction of a function $f : \R \rightarrow \R$ which is everywhere continuous but nowhere differentiable is a much more difficult task.
    
    Historically,
    there was a belief that every continuous function should be differentiable everywhere except a set of isolated points.
\end{remark}

\begin{theorem}\label{pre:analy:thm:sumscalarprodcomprulesfordiff}
    \begin{enumerate}[label = (\alph*)]
        \item 
        Let $f, g$ be two functions which are differentiable at $c$ and let $\alpha \in \R$ be a constant.
        Then $f + g$ and $\alpha f$ are also differentiable at $c$,
        and we have
        \begin{align*}
            (f + g)'(c) &= f'(c) + g'(c) \\
            (\alpha f)'(c) &= \alpha f'(c)
        \end{align*}
        Similarly,
        their product $fg$ is also differentiable at $c$ with
        \[
        (fg)'(c) = f(c)g'(c) + f'(c)g(c).
        \]

        \item
        Let $f, g$ be two functions such that $g$ is differentiable at $c$ and $f$ is differentiable at $g(c)$.
        Then the composition $f \circ g$ is also differentiable at $c$ and we have
        \[
        (f \circ g)'(c) = f'(g(c))g'(c).
        \]
        
        \item 
        Let $f$ be a function which is differentiable at $c$ and we have $f(c) \neq 0$.
        Then $1 / f$ is also differentiable at $c$ and we have
        \[
        \left(\frac{1}{f}\right)'(c) = -\frac{f'(c)}{f ^ 2(c)}.
        \]
    \end{enumerate}
    \begin{proof}
        We only prove the product of the functions and the composition of functions.
        This can be proven by the equivalent formulation given in \autoref{pre:analy:lem:firstordertaylorindiffdef} and its subsequence remark.
        \begin{enumerate}[label = (\alph*)]
            \item
            We have $f(x) = f(c) + (x - c)f_1(x)$ and $g(x) = g(c) + (x - c)g_1(x)$.
            This implies that
            \[
            (fg)(x) = (fg)(c) + (x - c)h(x).
            \]
            With $h(x) = f(c)g_1(x) + f_1(x)g(c) + (x - c)f_1(x)g_1(x)$.
            It is easy to see that $h$ is continuous at $c$ and,
            therefore,
            $fg$ is differentiable at $c$ with derivative
            \[
            (fg)'(c) = \lim_{x \rightarrow c}h(x) = f(c)g_1(c) + f_1(c)g(c) = f(c)g'(c) + f'(c)g(c).
            \]
            \item
            We can write $f(y) = f(g(c)) + (y - g(c))f_1(y)$ and $g(x) = g(c) + (x - c)g_1(x)$ with $f_1$ continuous at $g(c)$ and $g_1$ continuous at $c$.
            Then we have
            \begin{multline*}
                (f \circ g)(x) = f(g(x)) = f(g(c)) + (g(x) - g(c))f_1(g(x)) = \\
                f(g(c)) + (x - c)g_1(c)f_1(g(x)) = (f \circ g)(c) + (x - c)h(x)
            \end{multline*}
            with $h(x) = (f_1 \circ g)(c)g_1(c)$.
            Since $f_1$ is continuous at $g(c)$ and $g$ is continuous at $c$ by \autoref{pre:analy:thm:iffdiffatcthencontatc},
            the composition $f_1 \circ g$ is continuous at $c$.
            Using again the equivalent formulation for differentiability,
            we conclude that $f \circ g$ is differentiable at $c$ and we have
            \[
            (f \circ g)'(c) = \lim_{x \rightarrow c}h(x) = f_1(g(c))g_1(c) = f'(g(c))g'(c).
            \]
        \end{enumerate}

        The rest of the proof is an \textbf{exercise}.
    \end{proof}
\end{theorem}

\begin{remark}
    One may be tempted to prove the chain rule easily in the following way:
    \[
    \frac{f(g(x)) - f(g(c))}{x - c} = \frac{f(g(x)) - f(g(c))}{x - c} \times \frac{g(x) - g(c)}{x - c}
    \]
    and argue that in the limit the first fraction goes to $f'(g(c))$,
    while the second goes to $g'(c)$.

    In some sense this is how Leibniz' differential calculus would work.


    However,
    this is problematic since one cannot rule out $g(x) - g(c) = 0$,
    e.g.,
    if $g$ is constant in a neighbourhood of $c$ or if there is a sequence $x_n$ with $\liminfty x_n = c$ and $g(x_n) - g(c) = 0$.
    In general,
    the above approach can be made to work if $g'(c) \neq 0$,
    but $g'(c) = 0$ becomes a bit painful to consider.
    The above proof avoids these complications.
\end{remark}

\begin{example}
    \begin{itemize}
        \item The derivative of $f(x) = x$ is clearly $f'(x) = 1$.
        Then by \autoref{pre:analy:thm:sumscalarprodcomprulesfordiff}(a) we obtain the standard formula for the derivative of polynomials and hence using (c) the one for rational functions.

        \item We now show that the derivative of $e ^ x$ is $e ^ x$ itself.

        We start again at $c = 0$.

        Using the $e ^ x$ inequality lemma
        (which needs adding in here)
        again we have for $x < 1$ we have
        \[
        x \leq e ^ x - 1 \leq \frac{x}{1 - x}.
        \]
        Hence by squeezing $\lim_{x \rightarrow 0}\frac{e ^ x - 1}{x} = 1$.
        For general $c$ we have
        \[
        \frac{e ^ x - e ^ c}{x - c} = e ^ c\frac{e ^ {x - c} - 1}{x - c} \rightarrow e ^ c
        \]
        as $x \rightarrow c$ by the same argument
        ($x - c$ goes to $0$;
        if you don't like that take any sequence $x_n \rightarrow c$ so that the sequence $x_n - c$ goes to zero and apply the sequence criterion for limit of functions).

        In conclusion:
        $e ^ x$ is differentiable on all of $\R$ being its own derivative!

        \item Consider $c ^ x = e ^ {\log(c)x}$ for $c > 0$.
        Then by the chain rule $(c ^ x)' = \log(c)c ^ x$.
    \end{itemize}
\end{example}

\subsection{Inverse functions}

We want for $f$ being differentiable and the one-to-one inverse $f ^ {-1}$ also being differentiable.
However,
while this is a natural expectation it does not always have a positive answer.
We begin with the following warning:

When $f$ is differentiable and invertible with inverse $g$ then this does not necessarily imply that $g$ is differentiable!
The function $f(x) = x ^ 3$ on $\R$ is continuous and differentiable and strictly monotone increasing and hence has a continuous inverse $g(x) = \sqrt[3]{x}$,
the third root.
However $g$ is not differentiable at zero where the horizontal tangent line of $f$ turns into a vertical tangent line of $g$!

However,
we have
\begin{proposition}
    Let $f : I \rightarrow \R$ be continuous on the interval $I$,
    differentiable at a point $d$.
    Assume that $f$ is invertible with inverse $f ^Y {-1}$ and that $f'(d) \neq 0$.

    Then $f ^ {-1}$ is differentiable at $c := f(d)$ with
    \[
    (f ^ {-1})'(c) = \frac{1}{f'(f ^ {-1}(c))}.
    \]
    \begin{proof}
        Write $g := f ^ {-1}$.
        We need to show that for any sequence $(y_n)$ with limit $\liminfty y_n = c$ we have $\liminfty \frac{g(y_n) - g(c)}{y_n - c} = \frac{1}{f'(g(c))}$.
        Now set $x_n = g(y_n)$.
        Since $g$ is continuous by \autoref{pre:analy:thm:imgofinvcontinjfunciscont} we have $\liminfty x_n = \liminfty g(y_n) = g(\liminfty y_n) = g(c) = d$.
        Then
        \[
        \liminfty\frac{g(y_n) - g(c)}{y_n - c} = \liminfty \frac{x_n - d}{f(x_n) - f(d)} = \liminfty\frac{1}{\frac{f(x_n) - f(d)}{x_n - d}} = \frac{1}{f'(d)}.
        \]
    \end{proof}
\end{proposition}

\begin{remark}
    If for one reason one already knows or assumes that the inverse $g$ is differentiable one has a very quick argument to obtain the formula for $g'$.
    Namely,
    we use the chain rule to differentiate the equation $x = f \circ g(x)$ and obtain
    \[
    1 = g'(x)f'(g(x)),
    \]
    which then gives $g'(x) = 1 / f'(g(x))$.
    But of course,
    for this one needs to know in advance that $g$ is differentiable.
    There is however still another merit in this calculation:
    It identifies a necessary condition for the $g$ being differentiable at $x: f'(g(x))$ has to be non-zero.
\end{remark}

\begin{example}
    We know $\log(x)$ is the inverse function of $e ^ x$.
    Using $(e ^ x)' = e ^ x$ we immediately see
    \[
    \log'(x) = \frac{1}{e ^ {\log{x}}} = \frac{1}{x}.
    \]
    Using this we can now compute the derivative if $f(x) := x ^ \alpha = e ^ {\alpha\log(x)}$ for $\alpha \in \R$ and $x > 0$.
    Namely,
    by the chain rule we see
    \[
    f'(x) = \frac{\alpha}{x}x ^ \alpha = \alpha x ^ {\alpha - 1},
    \]
    as desired!
\end{example}

\subsection{Mean value theorems}

We have already seen the following fact and its proof in the Calculus course:
\begin{proposition}\label{pre:analy:prop:fdiffatcthenlocmaxorminatdifffc}
    If $f$ is differentiable at $c$ and has a local maximum or minimum at $c$ then $f'(c) = 0$.

    \begin{proof}
        The difference quotient $\frac{f(x) - f(c)}{x - c}$ must have opposite signs for $x > c$ and $x < c$.
        Hence for the one-sided limits we obtain $\geq 0$ and $\leq 0$ respectively.
        But the limits must be the same which is only possible with $0$.
    \end{proof}
\end{proposition}

We have also already seen Rolle's Theorem and the Mean Value Theorem.
However,
for completeness we will restate them.
\begin{theorem}[Rolle's Theorem]\label{pre:analy:thm:rollesthm}
    Let $f : [a, b] \rightarrow \R$ be continuous and differentiable on $(a, b)$ and suppose that $f(a) = f(b)$.
    Then there exists $c \in (a, b)$ such that $f'(c) = 0$.
    \begin{proof}
        As a continuous function on a closed interval $f$ attains its minimum and maximum.
        If one of them is in the interior we are done by the pervious proposition        
        (\autoref{pre:analy:prop:fdiffatcthenlocmaxorminatdifffc}).
        Otherwise,
        minimum and maximum occur at the endpoints.
        Since $f(a) = f(b)$ this implies $f$ must be constant in which case $f'(x) = 0$ for all $x \in (a, b)$.
    \end{proof}
\end{theorem}

\begin{theorem}[Mean Value Theorem]\label{pre:analy:thm:meanvaluethm}
    Let $f : [a, b] \rightarrow \R$ be continuous and differentiable on $(a, b)$.
    Then there exists $c \in (a, b)$ such that
    \[
    f'(c) = \frac{f(b) - f(a)}{b - a}.
    \]
    \begin{proof}
        Set
        \[
        g(x) := f(x) - \frac{f(b) - f(a)}{b - a}(x - a).
        \]
        Then $g$ is continuous on $[a, b]$ and differentiable on $(a, b)$ with
        \[
        g'(x) = f'(x) - \frac{f(b) - f(a)}{b - a}.
        \]
        Moreover,
        $g(a) = f(a)$ and $g(b) = f(a)$.
        Hence by Rolle there exists a $c \in (a, b)$ such that
        \[
        0 = f'(c) - \frac{f(b) - f(a)}{b - a},
        \]
        as claimed.
    \end{proof}
\end{theorem}

An immediate consequence of the Mean Value Theorem is the following
\begin{theorem}
    Let $f : I \rightarrow \R$ be a continuous function on an interval $I$,
    differentiable in its interior points.
    Then
    \begin{enumerate}[label = (\roman*)]
        \item If $f'(x) = 0$ for all $x$,
        then $f$ is constant.
        \item If $f'(x) \leq 0$ ($\leq 0$) for all $x$,
        then $f$ is monotone increasing
        (decreasing).
        \item If $f'(x) > 0$ ($< 0$) for all $x$,
        then $f$ is strictly monotone increasing
        (decreasing).
    \end{enumerate}
    \begin{proof}
        Let $c < d$ in $I$ be two arbitrary points.
        Then by the Mean Value Theorem there exists $\alpha$ in the interval $(c, d)$ such that
        \[
        f(d) - f(c) = (d - c)f'(\alpha) \begin{dcases*}
            = 0 & for case (i); \\
            \geq 0 & for case (ii); \\
            > 0 & for case (iii).
        \end{dcases*}
        \]
        Thus
        \[
        \begin{dcases*}
            f(c) = f(d) & for case (i); \\
            f(c) \leq f(d) & for case (ii); \\
            f(c) < f(d) & for case (iii).
        \end{dcases*}
        \]
        The claim follows since $c$ and $d$ were arbitrarily chosen.
    \end{proof}
\end{theorem}

The requirement on $I$ being an interval is absolutely essential!
For example $f(x) = 1 /x$ is differentiable on $\R* = (-\infty, 0) \cup (0, \infty)$,
the union of two intervals.
Now $f'(x) = -1 / x ^ 2 < 0$ for all $x$,
but $f(x)$ is certainly not overall decreasing,
but only on the two individual intervals $(-\infty, 0)$ and $(0, \infty)$.

Th issue is that an interval is connected\footnote{Don't worry about this concept at the moment,
it is next years problem.}.

Rolle's theorem implies a more general mean value theorem
(from which the regular Mean Value follows by setting $g(x) = x$).

\begin{theorem}[Cauchy's Generalised Mean Value Theorem]\label{pre:analy:thm:genmeanvalthm}
    Let $f, g : [a, b] \rightarrow \R$ be continuous and differentiable on $(a, b)$.
    Assume that $g'(x) \neq 0$ for all $x \in (a, b)$.
    Then there exists $c \in (a, b)$ such that
    \[
    \frac{f'(c)}{g'(c)} = \frac{f(b) - f(a)}{g(b) - g(a)}.
    \]
    \begin{center}
    \fbox{
    \begin{minipage}{0.9\textwidth}
    You may wonder why this is not a simple corollary of the Mean Value theorem.
    But applying \autoref{pre:analy:thm:meanvaluethm} to both functions $f$ and $g$ would lead to
    \[
    \frac{f'(c_1)}{g'(c_2)} = \frac{f(b) - f(a)}{g(b) - g(a)}.
    \]
    The important point of this theorem is that we can choose $c_1 = c_2$ on the left hand side.
    \end{minipage}
    }
    \end{center}
    \hfill
    \begin{proof}
        Consider the function
        \[
        h(x) = (g(b) - g(a))f(x) - (f(b) - f(a))g(x).
        \]
        Then we have
        \[
        h(a) = g(b)f(a) - f(b)g(a) = h(b).
        \]
        $h$ is obviously continuous on $[a, b]$ and differentiable on $(a, b)$,
        and we can apply Rolle's theorem to $h$.
        Therefore,
        there exists $c \in (a, b)$ such that
        \begin{equation}
            0 = h'(c) = (g(b) - g(a))f'(c) - (f(b) - f(a))g'(c).
        \end{equation}
        Since $g'(x) \neq 0$ for all $x \in (a, b)$,
        we can conclude from \autoref{pre:analy:thm:meanvaluethm} that $g(b) - g(a) \neq 0$,
        and we can rewrite ($1$) as
        \[
        \frac{f'(c)}{g'(c)} = \frac{f(b) - f(a)}{g(b) - g(a)}.
        \]
    \end{proof}
\end{theorem}

We define higher derivatives in the usual way.
\textit{Note that for $f''(c)$ as the derivative of $f'(x)$ at $x = c$ we need to assume that $f$ is differentiable in an interval around $c$.}

However,
we will not be discussing the role of the second derivatives.

\subsection{L' H\^opital's rule}

An application of \autoref{pre:analy:thm:genmeanvalthm} is L' H\^opital's Rule.
We give the following version
(There exists some with weaker hypotheses,
e.g. the one in Spivak's Calculus book)

\begin{theorem}[L' H\^opital's Rule]\label{pre:analy:thm:lhopitalsrule}
    Let $f$ and $g$ be two differentiable functions in some interval $(a, b)$.

    Assume that
    (the one-sided limits)
    $\lim_{x \rightarrow a ^ {+}}f(x) = 0$ and $\lim_{x \rightarrow a ^ {+}}g(x) = 0$ and $g(x) \neq 0$,
    $g'(x) \neq 0$ for all $x$.

    Then if $\lim_{x \rightarrow a ^ {+}}f'(x) / g'(x)$ exists,
    then also $\lim_{x \rightarrow a ^ {+}}f(x) / g(x)$ exists,
    and we have
    \[
    \lim_{x \rightarrow a ^ {+}}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a ^ {+}}\frac{f'(x)}{g'(x)}.
    \]
    \begin{proof}
        We can extend $f$ and $g$ continuously to $x = a$ by setting $f(a) = g(a) = 0$.
        Let's write $L = \lim_{x \rightarrow a ^ {+}}f'(x) / g'(x)$
        ($L = \pm\infty$ is actually ok).
        Let $(x_n)$ be any sequence in $(a, b)$ such that $\liminfty x_n = a$.
        We need to show
        \[
        \liminfty\frac{f(x_n)}{g(x_n)} = L.
        \]
        By the generalised mean value theorem for each $n$ we can find $y_n$ with $a < y_n < x_n$ such that
        \[
        \frac{f'(y_n)}{g'(y_n)} = \frac{f(x_n) - f(a)}{g(x_n) - g(a)} = \frac{f(x_n)}{g(x_n)}.
        \]
        By squeezing we see $\liminfty y_n = a$,
        and hence
        \[
        L = \liminfty\frac{f'(y_n)}{g'(y_n)} = \liminfty\frac{f(x_n)}{g(x_n)},
        \]
        as claimed.
    \end{proof}
\end{theorem}

\begin{remark}
    There exists a simple proof for L' H\^opital's Rule in the special case $f(c) = g(c) = 0$ and differentiable at $c$ with $g'(c) \neq 0$.
    Then
    \[
    \lim_{x \rightarrow c}\frac{f(x)}{g(x)} = \lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}\frac{x - c}{g(x)} = \lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c}\lim_{x \rightarrow c}\frac{x - c}{g(x)} = \frac{f'(c)}{g'(c)}.
    \]
    \textit{Note how we needed the extra assumptions in order to apply COLT}.
\end{remark}

\begin{remark}
    \begin{enumerate}[label = (\roman*)]
        \item There is of course a version for the other one-sided limit and for regular two-sided limits as well.
        \item The cases $a = -\infty$ or $b = \infty$ are also ok.
        This can be done by considering $f(1 / x)$ and $g(1 / x)$.
        Indeed,
        assuming $\lim_{x \rightarrow \pm\infty}f'(x) / g'(x)$ exists we compute
        \[
        \lim_{x \rightarrow \pm\infty}\frac{f(x)}{g(x)} = \lim_{x \rightarrow 0 ^ {\pm}}\frac{f(1 / x)}{g(1 / x)} = \lim_{x \rightarrow 0 ^ {\pm}}\frac{-\frac{1}{x ^ 2}f'(1 / x)}{-\frac{1}{x ^ 2}g'(1 / x)} = \lim_{x \rightarrow \pm\infty}\frac{f'(x)}{g'(x)}.
        \]
        \item Limits of the form "$\frac{\infty}{\infty}$" could in principle be handled by considering $\frac{f(x)}{g(x)} = \frac{1 / g(x)}{1 / f(x)}$,
        but this only works in special examples.
        Rather one can upgrade the given proof and obtain
        \[
        \lim_{x \rightarrow a ^ {\pm}}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a ^ {\pm}}\frac{f'(x)}{g'(x)}.
        \]
        \item Limits of the form "$0 \cdot \infty$" can often be handled by considering $f(x)g(x) = \frac{f(x)}{1 / g(x)}$.
        \item In applications one sometimes has to apply L' H\^opital's Rule more than once to get a limit of the ratio of higher derivatives.
    \end{enumerate}
\end{remark}
Be careful that all conditions of L' H\^opital's Rule are satisfied,
in particular the condition that $f, g$ are differentiable and that $f(c) = g(c) = 0$.
Without checking these conditions,
L' H\^opital's Rule may lead to absolutely wrong results,
like the following:
\[
\lim_{x \rightarrow 1}\frac{x ^ 2 + x + 2}{x - 2} "=" \lim_{x \rightarrow 1}\frac{2x + 1}{1} = 3.
\]
In fact,
the correct limit is $-4$ by continuity.
\begin{example}
    \begin{enumerate}[label = (\roman*)]
        \item Tautological use.
        Using L' H\^opital's Rule we see
        \[
        \lim_{x \rightarrow 0}\frac{e ^ {x} - 1}{x} \lim_{x \rightarrow 0}\frac{e ^ x}{1} = e ^ 0 = 1.
        \]
        This is a bit of tautology or circular reasoning since this is exactly the calculation to show that $e ^ x$ is differentiable
        (at $0$)
        in the first place!

        In general,
        for $f$ differentiable computing
        \[
        \lim_{x \rightarrow c}\frac{f(x) - f(c)}{x -c} = \lim_{x \rightarrow c}\frac{f'(x)}{1} = f'(c).
        \]
        appears to be a bit silly.
        However,
        if one only knows that $f$ is differentiable in $(c, d)$ but not yet at $c$ itself,
        then this calculation shows that $f$ is differentiable at $c$
        (if the limit exists!)
        
        \item A special case of Taylor's theorem.
        Using L' H\^opital's Rule twice we see
        \[
        \lim_{x \rightarrow 0}\frac{\log(1 + x) - x}{x ^ 2} = \lim_{x \rightarrow 0}\frac{1 / (1 + x) - 1}{2x} = \lim_{x \rightarrow 0}\frac{-1 / (1 + x) ^ 2}{2} = -\frac{1}{2}.
        \]
        We can rewrite this as
        \[
        \log(1 + x) = x - \frac{1}{2}x ^ 2 + r(x)x ^ 2
        \]
        with $\lim_{x \rightarrow 0}r(x) = 0$.
        This version of Taylor's theorem which we consider in the next subsection.

        \item Using \lhopital's Rule twice - in detail.

        The exact reasoning for using \lhopital's Rule twice as above goes as follows:
        Let $f(x) = \log(1 + x) - x$ and $g(x) = x ^ 2$.
        Then $f$ and $g$ are differentiable and we have $f(0) = g(0) = 0$,
        so the first assumption of \autoref{pre:analy:thm:lhopitalsrule} is satisfied.
        But we still do not know whether $\lim_{x \rightarrow 0}f'(x) / g'(x)$ exists,
        since $\lim_{x \rightarrow 0}f'(x) = \lim_{x \rightarrow 0}g'(x) = 0$.
        But the first assumption of \autoref{pre:analy:thm:lhopitalsrule} is still satisfied for the differentiable functions $f'(x) = 1 / (1 + x) - 1$ and $g'(x) = 2x$,
        since $f'(0) = 0$ and $g'(0) = 0$,
        and we see that
        \[
        \lim_{x \rightarrow 0}\frac{f''(x)}{g''(x)} = \lim_{x \rightarrow 0}\frac{-1 / (1 + x) ^ 2}{2} = -\frac{1}{2}
        \]
        exists.
        Arguing backwards,
        this shows that $\lim_{x \rightarrow 0}f'(x) / g'(x)$ exists and then also $\lim_{x \rightarrow 0}f(x) / g(x)$ exists etc.

        The point is that one really needs to read the above calculation backwards.
        
        \item Powers beat logs
        \begin{enumerate}[label = (\alph*)]
            \item 
            Let $\alpha > 0$.
            Consider
            \[
            \liminfty[x]\frac{\log(x)}{x ^ {\alpha}}.
            \]
            This is of the type "$\frac{\infty}{\infty}$".
            Taking derivatives gives
            \[
            \liminfty[x]\frac{1 / x}{\alpha x ^ {\alpha - 1}} = \liminfty[x]\frac{1}{\alpha x ^ \alpha} = 0.
            \]
            By \lhopital we conclude
            \[
            \liminfty[x]\frac{\log(x)}{x ^ {\alpha}} = 0.
            \]

            \item
            Let $\alpha > 0$.
            Consider
            \[
            \lim_{x \rightarrow 0 ^ {+}}\log(x)x ^ {\alpha}.
            \]
            This is of the type "$\infty \cdot 0$".
            We turn this into "$\frac{\infty}{\infty}$" by writing $x ^ {\alpha} = 1 / x ^ {-\alpha}$.
            \[
            \lim_{x \rightarrow 0 ^ {+}}\frac{\log(x)}{x ^ {-\alpha}} = \lim_{x \rightarrow 0 ^ {+}}\frac{1 / x}{-\alpha x ^ {-\alpha - 1}} = \lim_{x \rightarrow 0 ^ {+}}\frac{1}{-\alpha x ^ {-\alpha}} = \lim_{x \rightarrow 0 ^ {+}}x ^ {\alpha} = 0.
            \]
            We have shown
            \[
            \lim_{x \rightarrow 0 ^ {+}}\log(x)x ^ {\alpha} = 0.
            \]
            We could have deduced this directly by using ($a$):
            \[
            \liminfty[x] = \frac{\log(x)}{x ^ {\alpha}} = \lim_{x \rightarrow 0 ^ {+}}\frac{\log(1 / x)}{(1 / x) ^ {\alpha}} = \lim_{x \rightarrow 0 ^ {+}}\frac{-\log(x)}{x ^ {-\alpha}}.
            \]
        \end{enumerate}
        \item Exponentials beat powers.

        Let $\alpha > 0$.
        Consider
        \[
        \liminfty[x]x ^ {\alpha}e ^ {-x} = \liminfty\frac{x ^ {\alpha}}{e ^ x}.
        \]
        We apply \lhopital repeatedly
        ($n$ times)
        until $\alpha - n \leq 0$.
        Then
        \[
        \liminfty[x]\frac{x ^ {\alpha}}{e ^ x} = \liminfty[x]\frac{\alpha x ^ {\alpha - 1}}{e ^ x} = \dotsi = \alpha(\alpha - 1)\dotsi(\alpha - n + 1)\liminfty\frac{x ^ {\alpha - n}}{e ^ x} = 0.
        \]
        So
        \[
        \liminfty[x]x ^ {\alpha}e ^ {-x} = 0.
        \]
        Alternatively,
        we could have substituted $x = \log(y)$ and obtained
        \[
        \liminfty[x]x ^ {\alpha}e ^ {-x} = \alpha\liminfty[y]\frac{\log(y)}{y} = 0
        \]
        by (iv)(a).
    \end{enumerate}
\end{example}

\subsection{Taylor's theorem}

Above we saw that we can consider the definition of differentiability of a function $f$ at a point as closely related to approximating a function by a linear function.
It is hence a very natural question to approximate $f$ by higher degree polynomials.
This is addressed by the following
\begin{theorem}[Taylor's Theorem]\label{pre:analy:thm:taylorsthm}
    Let $f : I \rightarrow \R$ be a function which is differentiable $n$ times in the interval $I$.
    Then there exists a function $r_n(x)$
    (depending on $c$)
    such that
    \begin{equation}
        f(x) = f(c) + f'(c)(x - c) + \frac{f''(c)}{2}(x - c) ^ 2 + \dotsi + \frac{f ^ {(n)}(c)}{n!}(x - c) ^ n + r_n(x)(x - c) ^ n
    \end{equation}
    with $\lim_{x \rightarrow c}r_n(x) = 0$.
    This is called the Peano form of the remainder
    (and the direct generalisation of differentiability at $x = c$).

    If in addition $f$ is $n + 1$ times differentiable in $I$ then we write
    \[
    f(x) = f(c) + f'(c)(x - c) + \frac{f''(c)}{2}(x - c) ^ 2 + \dotsi + \frac{f ^ {(n)}(c)}{n!}(x - c) ^ n + \frac{f ^ {(n + 1)}(\xi)}{(n + 1)!}(x - c) ^ {n + 1},
    \]
    for a number $\xi$ between $x$ and $c$
    (depending on $x$ and $c$).
    This is called the Lagrange form of the remainder.
    \begin{proof}
        The first form is a direct application of \lhopital's Rule.
        Namely,
        consider
        \[
        F(x) := f(x) - \left(f(c) + f'(c)(x - c) + \frac{f''(c)}{2}(x - c) ^ 2 + \dotsi + \frac{f ^ {(n)}(c)}{n!}(x - c) ^ n\right).
        \]
        Then we directly see $F(0) = F'(c) = F''(c) = \dotsi = F ^ {(n)}(c) = 0$.
        Then
        \[
        r_n(x) = \frac{F(x)}{(x - c) ^ n}.
        \]
        We can apply \lhopital's Rule $n$-times and obtain
        \[
        \lim_{x \rightarrow c}r_n(x) = \lim_{x \rightarrow c}\frac{F(x)}{(x - c) ^ n} = \lim_{x \rightarrow c}\frac{F'(x)}{n(x - c) ^ {n - 1}} = \dotsi = \lim_{x \rightarrow c}\frac{F ^ {(n)}(x)}{n!} = 0,
        \]
        as claimed.

        To obtain the Lagrange remainder we fix $x$ and introduce the auxiliary function for $t$ between $x$ and $c$ by
        \[
        F(t) := f(x) - \left(f(t) + f'(t)(x - t) + \frac{f''(t)}{2}(x - t) ^ 2 + \dotsi + \frac{f ^ {(n)}(t)}{n!}(x - t) ^ n\right)
        \]
        so that $F(c) = r_n(x)(x - c) ^ n$ for which we need to find a $\xi$ such that it equals $\frac{f ^ {(n + 1)}(\xi)}{(n + 1)!}(x - c) ^ {n + 1}$.
        By a little calculation we obtain
        \[
        F'(t) = -\frac{f ^ {(n + 1)}(t)}{n!}(x - t) ^ {n}.
        \]
        Now we apply Cauchy's Generalised Mean Value Theorem
        (\autoref{pre:analy:thm:genmeanvalthm})
        for $F$ and
        \[
        G(t) = (x - t) ^ {n + 1};\quad G'(t) = -(n + 1)(x - t) ^ n,
        \]
        and obtain
        \begin{align*}
            \frac{F(c)}{(x - c) ^ {n + 1}} &= \frac{F(c)}{G(c)} \\
            &= \frac{F(c) - F(x)}{G(c) - G(x)} \\
            &= \frac{F'(\xi)}{G'(\xi)} \\
            &= \frac{\frac{f ^ {(n + 1)}(\xi)}{n!}(x - \xi) ^ n}{(n + 1)(x - \xi) ^ n} \\
            &= \frac{1}{(n + 1)!}f ^ {(n + 1)}(\xi)
        \end{align*}
        for some $\xi$ between $x$ and $c$.
        But this is the claim.
    \end{proof}
\end{theorem}

\begin{corollary}
    The polynomial part $T_{f, c}^{(n)}(x)$ on the right hand side of (2)
    (the first definition of $f(x)$ in \autoref{pre:analy:thm:taylorsthm}) 
    is called the $n$-th Taylor polynomial associated to $f$ at $c$.
\end{corollary}

Assume we have $|f ^ {(n + 1)}(x)| \leq M_{n + 1}$ for some constant $M_{n + 1}$
(say if $f ^ {(n + 1)}$ is continuous on $I = [a, b]$).
Then
\[
|f(x) - T_{f, c}^{(n)}(x)| \leq \frac{M_{n + 1}}{(n + 1)!}|x - c| ^ {n + 1}.
\]
So from this perspective Taylor's theorem describes how well one can approximate a
(complicated)
function by an
(easy)
polynomial.

\begin{example}
    The $4$-th Taylor polynomial for $e ^ x$ at $x = 0$ is given by
    \[
    1 + x + \frac{1}{2}x ^ 2 + \frac{1}{6}x ^ 3 + \frac{1}{24}x ^ 4.
    \]
    This approximates $e ^ x$ between $[-1 / 2, 1 / 2]$ with an error
    (take $M = 2 > e ^ {1 / 2}$)
    \[
    \left|e ^ {x} - \left(1 + x + \frac{1}{2}x ^ 2 + \frac{1}{6}x ^ 3 + \frac{1}{24}x ^ 4\right)\right| \leq \frac{2}{5!}\left(\frac{1}{2}\right) ^ 5 = \frac{1}{1920} < 0.001.
    \]
\end{example}

\begin{remark}
    Taylor's theorem gives a different way of considering \lhopital's Rule in the case $f, g$ are
    (sufficiently)
    often differentiable at $x = c$.
    Let $n$ be the first index such that $f ^ {(n)}(c) \neq 0$ and let $m$ be the first index such that $g ^ {(m)}(c0 \neq 0$
    \[
    \lim_{x \rightarrow c}\frac{f(x)}{g(x)} = \begin{cases}
        0 & \text{if } n > m; \\
        \frac{f ^ {(n)}(c)}{g ^ {(n)}(c)} & \text{if } n = m; \\
        \pm\infty & \text{if } n < m.
    \end{cases}
    \]
\end{remark}

\newpage

\section{Power Series}

\subsection{A quick review of infinite series}







































\subsection{Radius of convergence}

\subsection{Power series as a function}

\subsection{The exponentials, logarithm, and trigonometric functions}

\subsubsection{The exponential function}

\subsubsection{The logarithm}

\subsubsection{The trigonometric functions}

\subsection{Taylor series}

\subsection{Complex power series}

\newpage

\section{Sequences of functions, uniform convergence, and limit theorems}

\subsection{Notions of convergence of functions: pointwise and uniform}

\subsection{Uniform convergence preserves continuity}

\subsection{Uniform convergence for series of functions: Weierstrass M-Test}

\subsection{Complex case}

\newpage

\section{Fonctions rgles (Regulated functions)}

\subsection{Definition of regulated functions}

\subsection{Examples}

\subsection{Further properties of regulated functions}

\newpage

\section{Integration}

\subsection{Regulated integral: definition}

\subsection{Comparison with the Riemann integral}

\subsection{Regulated integral: basic properties}

\subsection{Fundamental theorem of calculus}

\subsection{Functions via integrals}

\subsection{Uniform convergence and integration}

\subsection{Pointwise convergence and integration}

\subsection{Improper integrals}















\end{document}