\documentclass[10pt, a4paper]{article}
\usepackage{preamble}
\usepackage{bbm}

\declaretheorem[style = avgstyle, name = Counting Principle]{countprinc}
\declaretheorem[style = avgstyle, name = Property, numberwithin = section]{property}

\title{Probability I \\
    \large Prereading}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}

\subsection{Sets}

A set is an unordered collection of distinguishable objects. 

\begin{definition}[Empty Set]
    The set with no outcomes is called the empty set denoted as $\emptyset$.
    \[
    \emptyset = \{\}
    \]
\end{definition}

\begin{definition}[Subset]
    For two sets $A$ and $B$, $A$ is a subset of $B$, and we write $A \subseteq B$ (or $B \supseteq A$), whenever every element in $A$ also belongs to $B$, for all $x \in A$ we have $x \in B$.
\end{definition}

\begin{example}
    For instance, $\{2, 4, 5\} \subseteq \{1, 2, 3, 4, 5\}$.    
\end{example}

\begin{definition}[Pwer Set]
    The power set is the set consisting of all subsets of a set $A$, this is denoted as $2 ^ A$\footnote{I have also seen it denoted as $\P(A)$.}.
    \[
    2 ^ A = \{B: B \subseteq A\}.
    \]
\end{definition}

\begin{example}
    The power set of the set $A = \{1, 2, 3\}$ is
    \[
    2 ^ A = \{\emptyset, \{1\}, \{2\}, \{3\}, \{1, 2\}, \{1, 3\}, \{2, 3\}, \{1, 2, 3\}\}.
    \]
\end{example}

\subsection{Sample space and events}
A sample space is a set of all outcomes for this scenario such that one and only one will occur.

The usual notation for a sample space will be $\Omega$, and a generic outcome is $\omega \in \Omega$.

\begin{definition}
    A set $A$ is countable if either:
    \begin{enumerate}[label = (\roman*)]
        \item $A$ is finite, or
        \item there is a bijection (one-to-one and onto mapping) between $A$ and the set of natural numbers $\N$.
    \end{enumerate}
\end{definition}

An even is just a collection of possible outcomes, i.e., a subset of $\Omega$.

\begin{definition}[Events]
    Associated to our sample space $\Omega$ is a collection $\mathcal{F}$ of all events:
    \[
    A \subseteq \Omega \text{ for every } A \in \mathcal{F}.
    \]
    We say that an event $A$ occurs when the outcome that occurs at the end of the scenario is in the set $A$.
\end{definition}

If $\Omega$ is discrete, we can always take $\mathcal{F} = 2 ^ \Omega$, so that every subset of $\Omega$ is an event.

The empty set $\emptyset$ represents the impossible event.

The sample space $\Omega$ represents the certain event, i.e. it will always occur.

\begin{definition}[Complement]
    For an event $A \in \mathcal{F}$, we define its complement, denoted $A ^ c$ (or sometimes $\overline{A}$) and read "not $A$", to be $A ^ c := \Omega \setminus A = \{\omega \in \Omega : \omega \notin A\}$.
\end{definition}

\begin{definition}[Disjoint]
    We say that events $A$ and $B$ are disjoint, mutually exclusive, or incompatible if $A \cap B = \emptyset$, i.e. it is impossible for $A$ and $B$ both to occur.
\end{definition}

We can simplify unions and intersections of multiple sets to being written as:
\[
\bigcup_{i = 1}^{n}{A_i} := A_1 \cup A_2 \cup \dotsi \cup A_n = \{\omega \in \Omega : \omega \in A_i \text{ for at least one } i \in \{1,\dots, n\}\}
\]
and
\[
\bigcap_{i = 1}^{n}{A_i} := A_1 \cap A_2 \cap \dotsi \cap A_n = \{\omega \in \Omega : \omega \in A_i \text{ for every } i \in \{1,\dots, n\}\}.
\]

Occasionally, we will need to take infinite unions and intersections over sequences of sets:
\[
\bigcup_{i = 1}^{\infty}{A_i} := A_1 \cup A_2 \cup A_3 \cup \dotsi = \{\omega \in \Omega : \omega \in A_i \text{ for at least one } i \in \N\}
\]
and
\[
\bigcap_{i = 1}^{\infty}{A_i} := A_1 \cap A_2 \cap A_3 \cap \dotsi = \{\omega \in \Omega : \omega \in A_i \text{ for every } i \in \N\}.
\]

Additionally, we can use De Morgan's Laws for a collection of events $A_i$,
\begin{enumerate}[label = (\alph*)]
    \item $\left(\bigcup_i A_i\right) ^ c = \bigcap_i A_i ^ c$, and
    \item $\left(\bigcap_i A_i\right) ^ c = \bigcup_i A_i ^ c$.
\end{enumerate}

\subsection{The axioms of probability}

\begin{definition}[Probability]
    A probability $\P$ on a sample space $\Omega$ with collection $\mathcal{F}$ of events is a function mapping every event $A \in \mathcal{F}$ to a real number $\P(A)$, obeying the following axioms:
    \begin{enumerate}[label = A\arabic*]
        \item $\P(A) \geq 0$ for every $A \in \mathcal{F}$
        \item $\P(\Omega) = 1$ and
        \item if $A$ and $B$ are disjoint events (i.e. if $A, B \in \mathcal{F}$ have $A \cap B = \emptyset$) then
        \[
        \P(A \cup B) = \P(A) + \P(B).
        \]
        We call the number $\P(A)$ the probability of $A$.
        \item Countable additivity. For any infinite sequence $A_1, A_2, \dots$ of pairwise disjoint events (so $A_i \cap A_j = \emptyset$ for all $i \neq j$),
        \[
        \P\left(\bigcup_{i = 1}^{\infty}A_i\right) = \sum_{i = 1}^{\infty}\P(A_i).
        \]
    \end{enumerate}
\end{definition}

\subsection{Consequences of the axioms}
\begin{enumerate}[label = C\arabic*]
    \item For any two events $A$ and $B$,
    \[
    \P(B \setminus A) = \P(B) - \P(A \cap B).
    \]
    \item For any event $A$, $\P(A ^ c) = 1 - \P(A)$.
    \item $\P(\emptyset) = 0$.
    \item For any event $A,\ \P(A) \leq 1$.
    \item Monotonicity. If $A \subseteq B$ then $\P(A) \leq \P(B)$.
    \item For any two events $A$ and $B$,
    \[
    \P(A \cup B) = \P(A) + \P(B) - \P(A \cap B).
    \]
    \item Finite additivity. If $A_1, A_2, \dots, A_k$ are pairwise disjoint (so $A_i \cap A_j = \emptyset$ if $i \neq j$) then
    \[
    \P\left(\bigcup_{i = 1}^{k}A_i\right) = \sum_{i = 1}^{k}\P(A_i).
    \]
    \item Boole's inequality. For any events $A_1, A_2, \dots,$ (these need not to be pairwise disjoint),
    \[
    \P\left(\bigcup_{i = 1}^{\infty}A_i\right) \leq \sum_{i = 1}^{\infty}\P(A_i).
    \]
    \item Continuity along monotone limits. If $A_1 \subseteq A_2 \subseteq \dotsi$ is an increasing sequence of events, then
    \[
    \P\left(\bigcup_{n = 1}^{\infty}A_n\right) = \lim_{n \rightarrow \infty}\P(A_n).
    \]
    If $A_1 \supseteq A_2 \supseteq \dotsi$ is an decreasing sequence of events, then
    \[
    \P\left(\bigcap_{n = 1}^{\infty}A_n\right) = \lim_{n \rightarrow \infty}\P(A_n).
    \]
    \item
    If $E_1, E_2, \dotsc, E_k$ form a partition then
    \[
    \sum_{i = 1}^{k}\P(E_i) = 1.
    \]
\end{enumerate}

\begin{definition}[Partition]
    We say that the events $E_1, E_2, \dotsc, E_k \in \mathcal{F}$ form a (finite) partition of the sample space $\Omega$ if:
    \begin{enumerate}[label = (\roman*)]
        \item they all have positive probability, i.e. $\P(E_i) > 0$ for all $i$;
        \item they are pairwise disjoint, i.e. $E_i \cap E_j = \emptyset$ whenever $i \neq j$ and
        \item their union is the whole sample space
        \[
        \bigcup_{i = 1}^{k}E_i = \Omega.
        \]
    \end{enumerate}
    The definition extends to countably infinite partitions.
    We say that $E_1, E_2, \dotsc \in \mathcal{F}$ form an infinite partition of $\Omega$ if:
    \begin{enumerate}[label = (\roman*)]
        \item $\P(E_i) > 0$ for all $i$;
        \item $E_i \cap E_j = \emptyset$ whenever $i \neq j$; and
        \item
        \[
        \bigcup_{i = 1}^{\infty}E_i = \Omega.
        \]
    \end{enumerate}
\end{definition}
Now we will provide a good example to solidify what a partition is.
\begin{example}
    Consider a sample space $\Omega = \{1, 2, 3, 4, 5, 6\}$.
    Possible partitions are:
    \begin{multline*}
        \{1\}, \{2\}, \{3\}, \{4\}, \{5\}, \{6\} \\
        \{1, 2\}, \{3, 4\}, \{5, 6\} \\
        \{1, 2, 3\}, \{4, 5, 6\} \\
        \{1\}, \{2, 3\}, \{4, 5, 6\} \\
        \{1, 2, 3, 4, 5, 6\}
    \end{multline*}
    and so on.
\end{example}
Effectively we can see that a partition is a collection of subsets of the sample space such that each subset is pairwise disjoint.

\section{Equally likely outcomes and counting principles}

\subsection{Classical probability}
Suppose we have a finite sample space $\Omega$, $m = |\Omega|$ possible outcomes. In the equally likely outcomes model (also sometimes known as classical probability) we suppose that each outcome has the same probability:
\[
\P(\omega) = \frac{1}{|\Omega|}\text{ for each } \omega \in \Omega,
\]
or, $\P(\omega_i) = \frac{1}{m}$ for each $i$.

Then by the axioms of probability the probability of any event $A \subseteq \Omega$ by C7
\[
\P(A) = \sum_{\omega \in A}\P(\omega) = \frac{|A|}{|\Omega|}\text{ for any event } A \subseteq \Omega.
\]

\begin{definition}[Equally likely outcomes]
    Consider a scenario with $m$ equally likely outcomes enumerated as $\Omega = \{\omega_1,\dots,\omega_m\}$. In the equally likely outcomes model, the probability of an event $A \subseteq \Omega$ is declared to be
    \[
    \P(A) = \frac{|A|}{|\Omega|}
    \]
\end{definition}

\subsection{Counting principles}

\begin{countprinc}[Multiplication principle]
    Suppose that we must make $k$ choices in succession where they are:
    \begin{itemize}
        \item $m_1$ possibilities for the first choice,
        \item $m_2$ possibilities for the second choice, \\
        \vdots
        \item $m_k$ possibilities for the $k$th choice,
    \end{itemize}
    and the number of possibilities at each stage does not depend on the outcomes of any previous choices. The total number of distinct possible selections is
    \[
    m_1 \times m_2 \times m_3 \times \dotsi \times m_k = \prod_{i = 1}^{k}m_i.
    \]
\end{countprinc}

\begin{example}
    PINs are made up of $4$ digits ($0$-$9$) with the exceptions that (i) they cannot be four repetitions of a single digit; (ii) they cannot form increasing or decreasing consecutive sequences, e.g. $3456$ and $8765$ are excluded. How many possible four-digit PINs are there?

    There are a total of $10 ^ 4$ PINs (including the excluded ones). There are $10$ PINs with only repeated digits e.g. $0000,\,1111,\,\dotsc,\,9999$. Additionally, consecutive increasing sequences must begin with $0,\,,1,\,2,\,\dotsc,\,6$, and consecutive decreasing sequences must begin with $9,\,8,\,7,\,\dotsc,\,3$, hence there are $7$ consecutive increasing sequences and $7$ consecutive decreasing sequences. Therefore, $10 + 7 + 7 = 24$ PINs are excluded from the $10 ^ 4$ total PINs. There are $10 ^ 4 - 24$ possible PINs. 
\end{example}

\begin{countprinc}[Ordered choices of distinct objects with replacement]
    Suppose that we have a collection of $m$ distinct objects and we select $r$ of them with replacement. The number of different ordered lists (ordered $r$-tuples) is
    \[
    \underbrace{m \times \dotsi \times m}_{r \text{ times}} = m ^ r
    \]
\end{countprinc}

\begin{countprinc}[Ordered choices of distinct objects without replacement]
    Suppose that we have a collection of $m$ distinct objects and we select $r \leq m$ of them without replacement. The number of different ordered lists (ordered $r$-tuples) is
    \[
    (m)_r := \underbrace{m \times (m - 1) \times (m - 2) \times \dotsi \times (m - r + 1)}_{r \text{ terms}} = \frac{m!}{(m - r)!}.
    \]
\end{countprinc}
So, the falling factorial notation $(m)_r$ (sometimes also denoted $m ^ {\underline{r}}$ is simply a convenient way to write $\frac{m!}{(m - r)!}$. In the case where $r = m$ we set $0! = 1$ and then $(m)_m = m!$ is the number of permutations of the $m$ objects. If $m$ is large, and $r$ is much smaller than $m$, then $(m)_r \approx m ^ r$.

\begin{example}[Birthday problem]
    There are $n < 365$ people in a room. Let $B$ be the event that (at least) two of them have the same birthday. (We ignore leap years.) What is $\P(B)$? How big must $n$ be so that $\P(B) > \frac{1}{2}$?

    The number of possible outcomes is
    \[
    365 \times 365 \times \dotsi \times 365 = 365 ^ n.
    \]
    This is the denominator of the probability. For the numerator, we must work out how many outcomes there are in $B$. In order to do this it is easier to count the number of outcomes in $B ^ c$, where everyone has a different birthday. The number of outcomes in $B ^ c$ is
    \[
    365 \times 364 \times \dotsi \times (365 - n + 1) = (365)_n.
    \]
    So
    \[
    \P(B) = 1 - \P(B ^ c) = 1 - \frac{(365)_n}{365 ^ n}.
    \]

    To find for what $n$ $\P(B) > \frac{1}{2}$ we can consider the following,
    \[
    \frac{(365)_n}{365 ^ n} = 1 \times \left(1 - \frac{1}{365}\right) \times \left(1 - \frac{2}{365}\right) \times \dotsi \times \left(1 - \frac{n - 1}{365}\right).
    \]
    For this we can see that $1 - x \leq e ^ {-x}$, in fact this is close to equality for $x = \frac{1}{365}$\footnote{Since $e^{-x}$ shrinks so rapidly and $\frac{1}{365}$ is already quite close to $0$.}, being close to zero.
    \begin{align*}
        \frac{(365)_n}{365 ^ n} &\leq e ^ {-x} e ^ {-2x} e ^ {-3x} \dotsi e^{-(n - 1)x} \\
        &= \exp{-(1 + 2 + \dotsi + n - 1)x} \\
        &= \exp{-\frac{(n - 1)n}{2 \times 365}}.
    \end{align*}
\end{example}

\begin{countprinc}[Unordered choices of distinct objects without replacement]
    Suppose that we have a collection of $m$ distinct objects and we select a subset of $r \leq m$ of them without replacement. The number of distinct subsets of size $r$ is
    \[
    \binom{m}{r} := \frac{(m)_r}{r!} = \frac{m!}{r!(m - r)!}.
    \]
\end{countprinc}

The expression $\binom{m}{r}$ is the binomial coefficient for choosing $r$ objects from $m$ and is often called $m$-choose-$r$. Note that
\[
\binom{m}{r} = \binom{m}{m - r}
\]

\begin{example}
    What is the probability of finding no aces in a four-card hand dealt from a well-shuffled deck?

    There are $52$ cards in a deck and $4$ aces in a deck. To choose $4$ cards from a deck there are $\binom{52}{4}$ ways of doing this. Now, assume that we have taken all of the aces out of the deck, then there are $\binom{48}{4}$ ways of choosing $4$ cards which are not aces. Finding the probability of choosing a hand with aces we can see that it is the number of chooses without the aces compared to the chooses with aces, i.e.
    \[
    \frac{\binom{48}{4}}{\binom{52}{4}} = \frac{(48)_4}{(52)_4}.
    \]
    This is approximately $0.7$.
\end{example}

\begin{countprinc}[Ordered choices of $2$ types of objects]
    Suppose that we have $m$ objects, $r$ of type $1$ and $m - r$ of type $2$, where objects are indistinguishable from others of their type. The number of distinct, ordered choices of the $m$ objects is
    \[
    \binom{m}{r}.
    \]
\end{countprinc}

\begin{countprinc}[Ordered grouping of indistinguishable objects]
    The number of ways to divide $m$ indistinguishable objects into $k$ distinct groups is
    \[
    \binom{m + k - 1}{m} = \binom{m + k - 1}{k - 1}.
    \]
\end{countprinc}

\begin{example}
    How many ways are there to divide $6$ (identical) pound coins amongst $3$ (distinguished) people?

    There are $\binom{6 - 3 - 1}{6} = \binom{8}{2} = 28$ ways to divide the coins amongst the people.
\end{example}

\section{Conditional probability and independence}

\subsection{Conditional probability}
\begin{definition}
    For events $A, B \subseteq \Omega$, the conditional probability of $A$ given $B$ is
    \[
    \P(A | B) := \frac{\P (A \cap B)}{\P (B)}\text{ whenever } \P(B) > 0.
    \]
\end{definition}
The usual interpretation is that $\P(A | B)$ represents our probability for $A$ after we have observed $B$.

\begin{example}
    Roll a fair dice and let $A$ be the event that the score is odd and $B$ be the event that the score is at most $3$. What is $\P (A | B)$?

    If $A = \{1, 3, 5\}$ and $B = \{1, 2, 3\}$ we have $A \cap B = \{1, 3\}$ so
    \[
    \P (A | B) = \frac{\P(\{1, 3\})}{\P(\{1, 2, 3\})} = \frac{\frac{2}{6}}{\frac{3}{6}} = \frac{2}{3}.
    \]
\end{example}

\subsection{Properties of conditional probability}

\begin{enumerate}[label = P\arabic*]
    \item For any event $B \subseteq \Omega$ for which $\P (B) > 0$, $\P(\cdot | B)$ satisfies axioms A1-4 (i.e., is a probability on $\Omega$) and therefore satisfies also C1-10.

    \item The multiplication rule for probabilities (simple version): for any events $A$ and $B$ with $\P (A) > 0$ and $\P(B) > 0$,
    \[
    \P (A \cap B) = \P (B) \P(A | B) = \P(A) \P(B | A).
    \]
    More generally, for any $A, B$ and $C$:
    \[
    \P(A \cap B | C) = \P(B | C) \P(A | B \cap C),\text{ if } \P(B \cap C) > 0.
    \]

    \item The multiplication rule for probabilities (general version): for any events $A_0, A_1, \dotsc, A_k$ with $\P\left(\bigcap_{i = 0}^{k - 1}A_i\right) > 0$,
    \begin{align*}
    \P\left(\bigcap_{i = 1}^{k}A_i \,\middle|\, A_0\right) &= \P(A_1 | A_0) \times \P(A_2 | A_1 \cap A_0) \times \dotsi \\
    &\dotsi\times \P\left(A_{k - 2} \,\middle|\, \bigcap_{i = 0}^{k - 3}A_i\right) \times \P\left(A_{k - 1} \,\middle|\, \bigcap_{i = 0}^{k - 2}A_i\right) \times \P\left(A_{k} \,\middle|\, \bigcap_{i = 0}^{k - 1}A_i\right).
    \end{align*}
    \item The partition theorem or law of total probability. If $E_1, E_2, \dotsc, E_k$ form a partition then, for any event $A$, we have
    \[
    \P(A) = \sum_{i = 1}^{k}\P(E_i)\P(A | E_i).
    \]
    More generally, if $\P(B) > 0$,
    \[
    \P(A | B) = \sum_{i = 1}^{k}\P(E_i | B)\P(A | E_i \cap B)
    \]
    \item Bayes's theorem (first version): for any events $A$ and $B$ with $\P(A) > 0$ and $\P(B) > 0$
    \[
    \P(A | B) = \frac{\P(A)\P(B | A)}{\P(B)}.
    \]
    More generally, if $\P(A | C) > 0$ and $\P(B | C) > 0$,
    \[
    \P(A | B \cap C) = \frac{\P(A | C)\P(B | A \cap C}{\P(B | C)}.
    \]
    \item Bayes's theorem (second version):
    for any partition $A_1, \dotsc, A_k$ and any $B$ with $\P(B) > 0$,
    \[
    \P(A_i\,|\,B) = \frac{\P(A_i)\P(B\,|\,A_i)}{\sum_{j = 1}^{k}\P(A_j)\P(B\,|\,A_j)}.
    \]
    More generally,
    if $\P(B\,|\,C) > 0$,
    \[
    \P(A_i\,|\,B \cap C) = \frac{\P(A_i\,|\,C)\P(B\,|\,A_i \cap C)}{\sum_{j = 1}^{k}\P(A_j\,|\,C)\P(B\,|\,A_j \cap C)}.
    \]
\end{enumerate}
P6 is an immediate consequence of P4 and P5.

To get the grasp of some of these consequences,
we will provide an example involving two of them.
\begin{example}
    Pat ends up in the pub of an evening with probability $\frac{3}{10}$.
    If she goes to the pub,
    she will get drunk with probability $\frac{1}{2}$.
    If she stays in,
    she will get drunk with probability $\frac{1}{5}$.
    What is the probability that she gets drunk?
    Given that she does get drunk,
    what is the probability that she went to the pub?
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        Firstly,
        we can see that we have two events,
        the event that she goes to the pub and the event that she gets drunk.
        Let $P$ be the event that she goes to the pub.
        Let $D$ be the event that she gets drunk.
        Then we are told that $\P(P) = \frac{3}{10}$, $\P(D\,|\,P) = \frac{1}{2}$ and $\P(D\,|\,P ^ c) = \frac{1}{5}$.
        We are told that she either stays in or goes out,
        so we can use partitions $P$ and $P ^ c$ to get the following
        \[
        \P(D) = \P(P)\P(D\,|\,P) + \P(P ^ c)\P(D\,|\,P ^ c) = \frac{3}{10}\cdot\frac{1}{2} + \frac{7}{10}\cdot\frac{1}{5} = \frac{29}{100}.
        \]
        To find the probability that she went to the pub given that she gets drunk,
        we can use Bayes's theorem to see that
        \[
        \P(P\,|\,D) = \frac{\P(D\,|\,P)\P(P)}{\P(D)} = \frac{\frac{3}{10}\cdot\frac{1}{2}}{\frac{29}{100}} = \frac{15}{29}.
        \]
        
    \end{proof}
\end{example}


\subsection{Independence of events}

\begin{definition}[Independence of two events]
    We say that two events $A$ and $B$ are independent whenever
    \[
    \P(A \cap B) = \P(A)\P(B).
    \]
    We say that two events $A$ and $B$ are conditionally independent given a third event $C$ with $\P(C) > 0$ whenever
    \[
    \P(A \cap B | C) = \P(A | C)\P(B | C).
    \]
\end{definition}

\begin{theorem}
    Consider any two events $A$ and $B$ with $\P(A) > 0$ and $\P(B) > 0$. The following statements are equivalent.
    \begin{enumerate}[label = (\roman*)]
        \item $\P(A \cap B) = \P(A)\P(B)$.
        \item $\P(A | B) = \P(A)$.
        \item $\P(B | A) = \P(B)$.
    \end{enumerate}
\end{theorem}

\begin{theorem}
    Consider any three events $A,\ B$, and $C$, with $\P(A \cap B \cap C) > 0$. The following statements are equivalent.
    \begin{enumerate}[label = (\roman*)]
        \item $\P(A \cap B | C) = \P(A | C)\P(B | C)$.
        \item $\P(A | B \cap C) = \P(A | C)$.
        \item $\P(B | A \cap C) = \P(B | C)$.
    \end{enumerate}
\end{theorem}

\begin{definition}[Independence of multiple events]\label{pre_prob_def_indepomulev}
    A (possible infinite) collection of events $\mathcal{A} \subseteq \mathcal{F}$ are mutually independent if for every finite non-empty $\mathcal{C} \subseteq \mathcal{A}$
    (that is, $\mathcal{B}$ is a finite subcollection of the events in question),
    \[
    \P\left(\bigcap_{A \in \mathcal{C}} A \,\middle|\, B\right) = \prod_{A \in \mathcal{C}}\P(A | B).
    \]
    A collection of events $\mathcal{A} \subseteq \mathcal{F}$ are mutually conditionally independent given another event $B$ if for every finite non-empty subcollection $\mathcal{C} \subseteq \mathcal{A}$,
    \[
    \P\left(\bigcap_{A \in \mathcal{C}} A \,\middle|\, B\right) = \prod_{A \in \mathcal{C}}\P(A | B).
    \]
\end{definition}

\newpage

\section{Interpretations of probability}

\subsection{Equally likely outcomes interpretation}
The equally likely outcomes interpretation of probability has some obvious limitations in practice.
Often we cannot find a set of outcomes that it is reasonable to think of as a priori equally likely.

\subsection{Relative frequency interpretation}
This interpretation applies to trials giving chance outcomes of an experiment that can be repeated indefinitely under essentially unchanged conditions and which exhibits long term regularity.
Suppose that we run $n$ trials of an experiment with a known list of possible outcomes,
and the number of trials on which event $A$ occurs is $n_A$ ($A$ is again the set of possible outcomes).
The relative frequency of occurrence of $A$ is $\frac{n_A}{n}$.

\subsection{Betting interpretation}
A different way of interpreting probability is by considering probability as a quantification of someone's belief that an event will occur.
Here is a simple way of doing this.
Your subjective probability that $A$ will occur is measured by the amount 
$\pounds p_A$ that you would consider to be a fair price for the following gable:
\begin{itemize}
    \item if $A$ occurs, you receive $\pounds1$;
    \item if $A$ does not occur, you receive nothing.
\end{itemize}
In this interpretation,
there are no true probabilities.
Different individuals will have different information relevant to a problem
and so may validly make different probability assessments.

\subsection{Interpretation and the axioms}
We claimed that the axioms of probability are the same regardless of the interpretation of the probabilities that we are using.
A1 and A2 are clearly very sensible in any interpretation.
The justification of A3 (and A4) needs some more thought.
A3 is obvious for the classical model by its relation to counting:
in the classical model if $A$ contains $m_A$ outcomes and $B$ contains $m_B$ outcomes,
with none in common with $A$,
then $m_{A \cup B} = m_A + m_B$.
The argument is similar for the relative frequency model and slightly more subtle for the betting model.

\newpage

\section{Some applications of probability}

\subsection{Reliability of networks}
Reliability theorem concerns mathematical models of systems that are made up of individual components that may be faulty.
If components fail randomly,
a key objective of this theory is to determine the probability that the system as a whole works.

In this course,
to demonstrate the application of the probabilistic ideas we have covered so far,
we address the basic question:
Given a system made up of finitely many components,
what is the probability that the system works?

\begin{definition}[Reliability network]
    A reliability network is a diagram of nodes and arcs.
    The nodes represent components of a multi-component system,
    where each node is either working or is broken,
    and where the entire system works if it is possible to get from the left end to the right end of the diagram through working components only.
\end{definition}

Suppose the $i$th component functions with probability
$p_i,\, i \in \{1, 2, \dotsc, k\}$,
and different components are independent.
The probability that the system works is then a function of the probabilities
$p_1, \dotsc, p_k$.
We denote this function by
$r(p_1, p_2, \dotsc, p_k)$,
and call it the reliability function.
It is determined by the layout of the reliability network.

\subsection{Genetics}
Inherited characteristics are determined by genes.
The mechanism governing inheritance is random and so the laws of probability are crucial to understanding genetics.
Cells contain $23$ pairs of chromosomes,
each containing many genes.
The genes take different forms called alleles.
Of the $23$ pairs of chromosomes,
$22$ pairs are homologous
(each of the pair has an allele for any gene located on this pair).
People with different alleles are grouped by visible characteristics into phenotypes;
often one allele,
$A$ say,
is dominant and another,
$a$,
is recessive in which case $AA$ and $Aa$ are of the same phenotype while $aa$ is distinct.
Sometimes,
the recessive gene is rare and the corresponding phenotype is harmful,
for example haemophilia or sickle-cell anaemia.

\textbf{Basic principle of genetics}:
For each gene on a homologous chromosome,
a child receives one allele from each parent,
where each allele received is chosen independently and at random from each parent's two alleles for that gene.

It is extremely important to note that genotypes of siblings are dependent if not conditioned on parental genotypes.
For example,
if two black mice
(which may each be BB or Bb)
have $100$ black offspring,
you may conclude that the next offspring is overwhelmingly likely to also be black,
because i is very likely that at least one parent is BB.


\subsection{Hardy-Weinberg equilibrium}
Consider a population of a large number of individuals evolving over successive generations.
Consider a gene with two alleles $A$ and $a$ and genotypes $\{AA, Aa, aa\}$.
Suppose the genotype of the proportions in the populations (uniformly for males and females) at generation $n = 0, 1, 2, \dotsc$ are
\[
AA \rightarrow u_n\quad Aa \rightarrow 2v_n\quad aa \rightarrow w_n
\]
where we have $u_n + 2v_n + w_n = 1$.
Suppose that the proportions of the alleles in the population are
\[
A \rightarrow p\quad a \rightarrow q
\]
where $p_n + q_n = 1$.
It is evident that
\[
p_n = \frac{2u_n + 2v_n}{2u_n + 4v_n + 2w_n} = u_n + v_n
\]
and, similarly, $q_n = v_n + w_n$.
Suppose that
\begin{itemize}
    \item the gene is neutral, meaning that different genotypes have equal reproductive success;
    \item there is random mating with respect to this gene,
    meaning that each individual in generation $n + 1$ draws randomly two parents whose genotypes are independently in the proportions $u_n, 2v_n, w_n$.
\end{itemize}
How do the genotype proportions evolve over successive generations?

Consider the offspring of generation $0$.
Let $FA = $ event that child gets allele $A$ from father,
$MA = $ event that child gets allele $A$ from mother,
$F_{AA} = $ event that father is $AA$,
$F_{Aa} = $ event that father is $Aa$,
$F_{aa} = $ event that father is $aa$. Then
\begin{align*}
    \P(FA) &= \P(F_{AA})\P(FA | F_{AA}) + \P(F_{Aa})\P(FA | F_{Aa}) + \P(F_{aa})\P(FA | F_{aa}) \\
    &= 1 \cdot u_0 + \frac{1}{2} \cdot 2v_0 + 0 \cdot w_0 = u_0 + v_0 = p_0.
\end{align*}
Similarly, $\P(MA) = p_0$.
In particular, since parents contribute alleles independently,
the probability distribution of the genotype of an individual in generation $1$ is
\[
AA \rightarrow p_0 ^ 2\quad Aa \rightarrow 2p_0(1 - p_0)\quad aa \rightarrow (1 - p_0) ^ 2.
\]
Provided that the population is large enough these will also be the generation $1$ proportions of $AA, Aa, aa$, i.e.
\[
u_1 = p_0 ^ 2,\quad v_1 = p_0(1 - p_0),\quad w_1 = (1 - p_0) ^ 2.
\]
Now let $p_1 = u_1 + v_1$ be the proportion of $A$ in the gene pool at generation $1$.
Substituting the values of $u_1, v_1$ we find that
\[
p_1 = u_1 + v_1 = p_0 ^ 2 + p_0(1 - p_0) = p_0,
\]
i.e. the proportions of $A$ and $a$ in the gene pool are constant.
The same argument applies for later generations,
so that $p_n = p_0$ for all $n$ i.e.,
the proportions of the two alleles in the gene pool remain constant.
This means that, for $n \geq 1$,
\[
u_n = p_0 ^ 2,\quad v_n = p_0(1 - p_0),\quad w_n = (1 - p_0) ^ 2,
\]
so that the proportions of the three genotypes in the population remain constant in every generation after the first.
This is called the Hardy-Weinberg equilibrium.

\newpage

\section{Random variables}

\subsection{Definition and notation}

\begin{definition}[Random Variable on a Sample Space]
    A random variable on $\Omega$ is a mapping from the sample space $\Omega$ to some set of possible values
    $X(\Omega) := \{X(\omega) \,:\, \omega \in \Omega\}$:
    \[
    X : \Omega \rightarrow X(\Omega) \text{ given by } \omega \mapsto X(\omega).
    \]
\end{definition}
Typically we find the following situations:
$X(\Omega) \subseteq \R$,
where $X$ is a real-valued random variable.
Or, $X(\Omega) \subseteq \R ^ d$,
where $X$ is a vector-valued random variable.

\begin{example}
    Consider throwing two standard dice, with sample space
    \[
    \Omega = \{(i, j) : i \in \{1, 2, 3, 4, 5, 6\}, j \in \{1, 2, 3, 4, 5, 6\}\},
    \]
    so elements of $\Omega$ correspond to pairs $(i, j)$.
    The sum of the numbers that show on the dice corresponds to a real-valued random variable $X$ defined by:
    \[
    X(i, j) := i + j,\text{ for all } (i, j) \in \Omega.
    \]
\end{example}

Given a probability distribution $\P$ on the sample space $\Omega$,
a random variable
$X : \Omega \rightarrow X(\Omega)$ induces a probability distribution $\P_X$ on the sample space $X(\Omega)$ as follows.

\begin{theorem}
    The function $\P_X$,
    mapping sets $B \subseteq X(\Omega)$ to a real number $\P_x(B)$,
    defined by
    \[
    \P_X(B) := \P(X \in B) = \P(\{\omega \in \Omega:\, X(\omega) \in B\}),
    \]
    is a probability on $X(\Omega)$,
    that is,
    $\P_X$ satisfies the probability axioms A1-A4.
    \begin{proof}
        
    \end{proof}
\end{theorem}
Note that $\P_X$ therefore also has properties C1-C10.

\subsection{Discrete random variables}
The function $\P_X$ tells us everything we might need to know about the random variable $X$: it is called the distribution of $X$.

\begin{definition}[Discrete random variable and probability mass function]\label{pre_prob_def_drvpmf}
    A random variable $X : \Omega \rightarrow X(\Omega)$ is said to be discrete when there is a finite or countable set of values $\mathcal{X} \subseteq X(\Omega)$ such that $\P(X \in \mathcal{X}) = 0$.
    The function $p: \mathcal{X} \rightarrow [0, 1]$ defined by
    \[
    p(x) = \P(X = x),\text{ for all } x \in \mathcal{X},
    \]
    is called the probability mass function of $X$.
\end{definition}

\begin{theorem}
    Suppose that $X$ is a discrete random variable and $p : \mathcal{X} \rightarrow [0, 1]$ is its probability mass function. Then
    \[
    \P(X \in B) = \sum_{x \in B}p(x),\text{ for all } B \subseteq \mathcal{X},
    \]
    and
    \[
    \sum_{x \in \mathcal{X}}p(x) = 1.
    \]
    \begin{proof}
        Any $A \subseteq \mathcal{X}$ is finite or countable (because it is a subset of a finite or countable set) and so
        \[
        \{X \in B\} = \bigcup_{x \in B}\{X = x\},
        \]
        this union runs over a countable number of events.
        Hence, we may apply A4 to get
        \[
        \P(X \in B) = \sum_{x \in B}\P(X = x) = \sum{x \in B}p(x).
        \]
        In particular, take $B = \mathcal{X}$ and we get $\sum_{x \in \mathcal{X}}p(x) = \P(X \in \mathcal{X}) = 1$.
    \end{proof}
\end{theorem}
The probability mass function of a discrete random variable summarises all information we have about $X$.
Specifically, it allows us to calculate the probability of every event of the form $\{X \in B\}$.

\begin{theorem}
    A random variable $X : \Omega \rightarrow X(\Omega)$ is discrete whenever (i) $X(\Omega)$ is finite or countable,
    or (ii) $\Omega$ is finite or countable.
    \begin{proof}
        Clearly, (ii) implies (i),
        therefore if (i) then the statement holds which can be seen immediately from \autoref{pre_prob_def_drvpmf} as we can take $\mathcal{X} = X(\Omega)$.
    \end{proof}
\end{theorem}

\subsection{The binomial and geometric distributions}
Consider the following random experiment,
called a binomial scenario:
\begin{itemize}
    \item A sequence of $n$ trials will be carried out, where $n$ is known in advance of the experiment.
    \item Trials are independent.
    \item Each trial has only two outcomes, usually denoted 'success' or 'failure'.
    \item Each trial succeeds independently with the same probability $p$.
\end{itemize}
Consider the random variable $X$, the total number of successes in the $n$ trials.
The usual sample space $\Omega$ for the binomial scenario is the set of all possible length-$n$ sequences of successes and failures; if we represent success and failure by $1$ and $0$ respectively, then each $\omega \in \Omega$ is a string 
$\omega = \omega_1 \omega_2 \dotsi \omega_n$ with each $\omega_i \in \{0, 1\}$.
The random variable $X$ takes values in $X(\Omega) := \{0, 1, \dotsc, n\}$,
$X(\omega) = \sum_{i = 1}^{n}\omega_i$, the total numbers of $1$s in the string.
For each $x \in \{0, 1, \dotsc, n\}$:
\begin{itemize}
    \item because trials are independent (see \autoref{pre_prob_def_indepomulev}, every sequence $\omega \in \Omega$ with exactly $x$ successes and $n - x$ failures has probability $\P(\{\omega\}) = p ^ x (1 - p) ^ {n - x}$; and
    \item there are $\binom{n}{x}$ sequences with exactly $x$ successes.
\end{itemize}
By C7, we can sum the probabilities of the outcomes in the event
\[
\{X = x\} = \{\omega \in \Omega : \sum_i \omega_i = x\}
\]
to obtain
\[
p(x) = \P(X = x) = \sum_{\omega \in \Omega: \sum_i \omega_i = x}\P(\omega).
\]
Putting everything to get the following definition.
\begin{definition}[Binomial distribution]
    We say that a discrete random variable $X$ is binomially distributed with parameters $n \in \N$ and $p \in [0, 1]$,
    and we write $X \sim \Bin(n, p)$,
    when $\mathcal{X} = \{0, 1, \dotsc, n\}$ and
    \[
    p(x) = \binom{n}{x}p ^ x (1 - p) ^ {n - x}\text{ for all } x \in \{0, 1, 2, \dotsc, n\}.
    \]
\end{definition}
In case of just a single trial ($n = 1$),
the binomial scenario is often referred to as a Bernoulli trial,
and $X \sim \Bin(1, p)$ is often referred to as a Bernoulli random variable with parameter $p$.

Suppose we extend the binomial scenario indefinitely,
to an unlimited number of trials,
and we repeat the trials until we obtain the first success.
The number of trials up to an including the first success is called the geometric distribution.
\begin{align*}
    \P(\text{first success occurs on trial } n) &= \P(\text{first } n - 1\text{ trials are failure, then trail } n \text{ is a success}) \\
    &(1 - p) ^ {n - 1} p,
\end{align*}

\begin{definition}
    We say that a discrete random variable $X$ is geometrically distributed with parameter $p \in (0, 1]$, and we write $X \sim \Geo(p)$, when $\mathcal{X} = \N := \{1, 2, 3, \dotsc\}$ and
    \[
    p(x) = (1 - p) ^ {x - 1} p,\text{ for all } x \in \{1, 2, 3, \dotsc\}.
    \]
\end{definition}

\subsection{The Poisson distribution}

\begin{definition}[Poisson distribution]
    We say that a discrete random variable $X$ is Poisson distributed with parameter $\lambda$,
    and we write $X \sim \Po(\lambda)$,
    when $\mathcal{X} = \Z_+ := \{0, 1, 2, \dotsc\}$ and
    \[
    p(x) = \frac{e^{-\lambda}\lambda ^ x}{x!}\quad\text{for } x \in \Z_+.
    \]
\end{definition}
The Poisson distribution is used to model counts of events which occur randomly in time at a constant average rate $r$ per unit time,
under some natural assumptions. Specifically,
\[
\P(\text{event occurs in } (x, x + h)) \approx rh,\ \text{for small } h.
\]
If $X$ is the count of number of events over a period of length $t$ then it has distribution $\Po(rt)$.
Thus, $\lambda$ in $\Po(\lambda)$ is interpreted as the average number events.

\begin{theorem}
    Consider any $\lambda > 0$. Let $X_n \sim \Bin(n, p_n)$ where $\lim_{n \rightarrow \infty} np_n = \lambda$, and let $Y \sim \Po(\lambda)$. Then for all $x \in \Z_+$,
    \[
    \lim_{n \rightarrow \infty}p_{X_n}(x) = p_Y(x).
    \]
    We describe this by saying that $X_n$ converges in distribution to $Y$.
    \begin{proof}
        Notice since $np_n \rightarrow \lambda$ we have $p_n \rightarrow 0$.
        For fixed $x$ we have that, for $n \geq x$,
        \[
        \P(X_n = x) = \binom{n}{x}(p_n) ^ x (1 - p_n) ^ {n - x} = n ^ {-x}\frac{n!}{(n - x)!x!}(np_n) ^ x (1 - p_n) ^ {n - x},
        \]
        where we observe that, by some COLT,
        \begin{align*}
            \lim_{n \rightarrow \infty} (np_n) ^ x &= \lambda ^ x, \\
            \lim_{n \rightarrow \infty} n ^ {-x} \frac{n!}{(n - x)!} &= \lim_{n \rightarrow \infty} \frac{n}{n} \cdot \frac{n - 1}{n} \dotsi \frac{n - x + 1}{n} = 1, \\
            \lim_{n \rightarrow \infty} (1 - p_n) ^ n &= \lim_{n \rightarrow \infty}\left(1 - \frac{np_n}{n}\right) ^ n = e ^ {-\lambda}, \\
            \lim_{n \rightarrow \infty} (1 - p_n) ^ {-x} &= 1.
        \end{align*}
        Collecting up terms gives
        \[
        \lim_{n \rightarrow \infty}\P(X_n = x) = \frac{e ^ {-\lambda}\lambda ^ x}{x!},
        \]
        as claimed.
    \end{proof}
\end{theorem}

\subsection{Continuous random variables}
\begin{definition}[Continuous random variable and probability density function]\label{pre_prob_def_crvandpdf}
Consider a real-valued random variable $X : \Omega \rightarrow \R$.
We say that $X$ is a continuous random variable,
or that $X$ has a continuous probability distribution,
or that $X$ is continuously distributed,
when there is a non-negative function $f : \R \rightarrow \R$ such that
\[
\P(X \in [a, b]) = \int_{a}^{b}f(t)\,dt,
\]
for all $[a, b] \subseteq \R$.
In this case,
$f$ is called the probability density function of $X$.
\end{definition}

All of the continuous random variables in this course have a probability density function that is piecewise continuous.
If the random variable under consideration is not clear from the context,
we may write $f_X$ for the probability density function of $X$.

\begin{theorem}
    If $X$ is continuously distributed with probability density function $f$,
    then for any $B \subseteq \R$ that is a finite union of intervals:
    \[
    \P(X \in B) = \int_Bf(x)\, dx
    \]
\end{theorem}
The probability density function of a continuous random variable summarises practically all the information we have about $X$.
Specifically, it allows us to calculate the probability of every event of the form $\{X \in B\}$ where $B$ is a finite union of intervals.

\begin{corollary}
    Let $X$ be a continuous random variable.
    Then its probability density function $f$ integrates to one:
    \[
    \int_{-\infty}^{\infty}f(x)\,dx = 1.
    \]
\end{corollary}

\subsection{The uniform distribution}
\begin{definition}[Uniform distribution]
Let $a$ and $b$ be real numbers with $a < b$.
We say a continuous random variable $X$ is uniformly distributed on $[a, b]$,
and we write $X \sim \mathrm{U}(a, b)$,
when
\[
f(x) = \begin{cases}
    \frac{1}{b - a} & \text{for all } x \in [a, b]. \\
    0 & \text{elsewhere}.
\end{cases}
\]
\end{definition}

When $X \sim \mathrm{U}(a, b)$ then $X$ can take any value in the continuous range of values from $a$ to $b$ and the probability of finding $X$ in any interval $[x, x + h] \subseteq [a, b]$ does not depend on $x$.

In effect,
finding the cumulative probabilities for uniform distributions is just an integral.
This is intuitively implied by the fact that it is a continuous random variable.

\subsection{The exponential distribution}
Suppose a bell chimes randomly in time at rate $\beta > 0$.
Let $T > 0$ denote the time of the first chime
(a random variable).
The events $\{\text{no chimes in } [0, \tau]\}$ and $\{T > \tau\}$ are the same.
We know that the number of chimes in the interval $[0, \tau]$ is $\Po(\beta\tau)$ and so the probability of no chimes is $e ^ {-\beta\tau}$.
Hence
\[
\P(T > \tau) = e ^ {-\beta\tau}\quad\text{or}\quad\P(T \leq \tau) = 1 - e ^ {-\beta\tau}\quad\text{for all } \tau \geq 0.
\]
As $1 - e ^ {-\beta\tau} = \int_0^\tau\beta e ^ {-\beta t}\,dt$ we see that $T$ is a continuous random variable with probability density function
$f(t) = \beta e ^ {-\beta t}$ for all $t \geq 0$.

\begin{definition}[Exponential distribution]
    Let $\beta > 0$.
    We say a continuous random variable $X$ is exponentially distributed with parameter $\beta$,
    and we write $X \sim \Exp(\beta)$, when
    \[
    f(x) = \begin{cases}
        \beta e ^ {-\beta x} & \text{for all } x \geq 0, \\
        0 & \text{elsewhere}.
    \end{cases}
    \]
\end{definition}

The exponential distribution describes the time between the event first happening,
so in our example it describes the time taken for the bell to first chime.

\subsection{The normal distribution}
This is one of the most important probability distributions (so pay attention!).
\begin{definition}[Normal distribution]
    Let $\mu,\,\sigma$ be real numbers with $\sigma > 0$.
    We say a continuous random variable $X$ is normally distributed with parameters $\mu$ and $\sigma ^ 2$,
    and we write $X \sim \mathcal{N}(\mu,\,\sigma ^ 2)$, when
    \[
    f(x) =\frac{1}{\sigma\sqrt{2\pi}}e ^ {-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right) ^ 2}\text{ for all } x \in \R.
    \]
\end{definition}
This is sometimes called the Gaussian distribution.
The probability density function is bell shaped.
In rough terms,
the parameter $\mu$ determines the location of the bell,
and $\sigma$ determines the spread of the bell,
e.g. for smaller $\sigma$,
the bell is narrower.

Formally,
we would refer to $\sigma$ as the standard deviation,
and $\mu$ as the expectation;
but alas we have not reached such terminology yet.

\subsection{Cumulative distribution functions}
\begin{definition}[Cumulative distribution function]
    For any real-valued random variable $X$,
    the function $F: \R \rightarrow [0, 1]$ defined by
    \[
    F(x) := \P(X \leq x)\text{ for } x \in \R
    \]
    is called the cumulative distribution function of $X$.
\end{definition}

\textit{Note: If the random variable under consideration is not clear from the context,
we may write $F_X$ for the cumulative distribution function of $X$}.

By C1,
it follows immediately that:
\[
\P(X \in (a, b]) = \P(X \leq b) - \P(X \leq a) = F(b) - F(a).
\]

For a continuous random variable $X$ this is the same as $\P(X \in (a, b)), \P(X \in [a, b])$,
and so on.

\begin{theorem}
    Suppose that $X$ is a continuously distributed random variable on $\R$ with probability density function $f$.
    Then $F$ is a continuous function and,
    for all $x \in \R$,
    \[
    F(x) = \int_{-\infty}^{x}f(t)\,dt,\qquad f(x) = \frac{dF}{dx}(x)\textit{ when $f$ is continuous at $x$}.
    \]
    \begin{proof}
        The first equality follows from \autoref{pre_prob_def_crvandpdf},
        and it follows that $F$ is continuous.
        The second equality is a consequence of the fundamental theorem of calculus (the correspondence between derivative and integral).
    \end{proof}
\end{theorem}

\subsection{Standard normal tables}
\begin{definition}[Standard normal distribution]
    A continuous random variable $Z$ is standard normally distributed when $Z \sim \mathcal{N}(0, 1)$.
\end{definition}
In other words,
a standard normal is normal with $\mu = 0$ and $\sigma = 1$.
We use a special symbol to denote its probability density function and cumulative distribution function
\[
\phi(z) := f(z) = \frac{1}{\sqrt{2\pi}}e ^ {-\frac{z ^ 2}{2}},\qquad\Phi(z) := F(z) = \int_{-\infty}^{z}\phi(t)\,dt.
\]
As already mentioned,
$\Phi$ has no closed analytical form,
however,
it can be tabulated.
Such tabulation is called a standard normal table.
Because $\phi(z) = \phi(-z)$,
it follows that $\Phi(z) = 1 - \Phi(-z)$.
and so we only need to tabulate $\Phi$ for non-negative values of $z$.

\subsection{Functions of random variables}
Suppose $X : \Omega \rightarrow X(\Omega)$ is a a random variable,
and $g : X(\Omega) \rightarrow \mathcal{S}$ is some function.
Then $g(X)$ is also a random variable,
namely the outcome to a 'new experiment' obtained by running the 'old experiment' to produce a value $x$ for $X$,
and then evaluating $g(x)$.
Formally,
as a function of $\omega \in \Omega$,
$g(X) := g \circ X$, i.e.,
\[
g(X)(\omega) := g(X(\omega))\text{ for all } \omega \in \Omega.
\]
For example,
\[
\P(g(X) \in B) = \P(\{\omega \in \Omega : g(X(\omega)) \in B\})\text{ for all } B \subseteq \mathcal{S}.
\]

\begin{theorem}[Standardising the normal distribution]
    Suppose $\mu \in \R$ and $\sigma > 0$.
    If $X \sim \mathcal{N}(\mu, \sigma^ 2)$ and $Z \sim \mathcal{N}(0, 1)$,
    then
    \[
    \frac{X - \mu}{\sigma} \sim \mathcal{N}(0, 1),\quad\text{and}\quad\sigma Z + \mu \sim \mathcal{N}(\mu, \sigma ^ 2).
    \]
    \begin{proof}
        We can prove this is via change of variable in the integral for the cumulative distribution function: see Exercise 114.
        A shorter proof goes via the moment generating function,
        which will be introduced later.
    \end{proof}
\end{theorem}

\begin{corollary}
    If $X \sim \mathcal{N}(\mu, \sigma ^ 2)$ then
    \[
    F(x) = \Phi\left(\frac{x - \mu}{\sigma}\right).
    \]
\end{corollary}

\newpage

\section{Multiple random variables}

\subsection{Joint probability distributions}
A bivariate random variable is a mapping from $\Omega$ into a Cartesian product of two sets,
i.e.,
a random variable whose values are ordered pairs of the form $(x, y)$.

\begin{definition}[Bivariate random variable]
    Consider random variables $X$ and $Y$ defined on the same sample space $\Omega$,
    $X : \Omega \rightarrow X(\Omega)$ and $Y : \Omega \rightarrow Y(\Omega)$.
    The mapping $(X, Y) : \Omega \rightarrow (X, Y)(\Omega)$ defined by
    \[
    (X, Y)(\omega) := (X(\omega), Y(\omega))
    \]
    is then a bivariate random variable.
\end{definition}

\begin{definition}[Independence of two random variables]
    Two random variables $X$ and $Y$ on the same sample space $\Omega$ are independent if
    \[
    \P(X \in A, Y \in B) = \P(X \in A)\P(Y \in B)\text{ for all } A \subseteq X(\Omega)\text{ and } B \subseteq Y(\Omega).
    \]
    In other words,
    $X$ and $Y$ are independent (as random variables) if and only if $\{X \in A\}$ and $\{Y \in B\}$ are independent as events for all sets $A$ and $B$.
\end{definition}

Three random variables,
$X, Y$, and $Z$,
say, are independent if the events $\{X \in A\}, \{Y \in B\}, \{Z \in C\}$ are mutually independent.

\begin{example}
    On sample space $\Omega = \{1, 2, 3, 4, 5, 6\}$,
    define random variables $X$ and $Y$ by
    \begin{table}[H]
        \centering
        \begin{tabular}{c|cccccc}
             $\omega$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ \\
             \hline
             $X(\omega)$ & $0$ & $0$ & $0$ & $1$ & $1$ & $1$ \\
             $Y(\omega)$ & $0$ & $1$ & $0$ & $2$ & $0$ & $3$
        \end{tabular}
    \end{table}
    Then the bivariate random variable $(X, Y)$ is given by
    \begin{table}[H]
        \centering
        \begin{tabular}{c|cccccc}
             $\omega$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ \\
             \hline
             $(X, Y)(\omega)$ & $(0, 0)$ & $(0, 1)$ & $(0, 0)$ & $(1, 2)$ & $(1, 0)$ & $(1, 3)$
        \end{tabular}
    \end{table}
    Note that $X(\Omega) = \{0, 1\}, Y(\Omega) = \{0, 1, 2, 3\}$,
    and $(X, Y)(\Omega) = \{(0, 0), (0, 1), (1, 0), (1, 2), (1, 3)\}$ which is a strict subset of $\{0, 1\} \times \{0, 1, 2, 3\}$
    (the outcomes $(0, 3)$ does not appear, for example).
\end{example}

\begin{definition}[Independence of two random variables]
    Two random variables $X$ and $Y$ on the same sample space $\Omega$ are independent if
    \[
    \P(X \in A, Y \in B) = \P(X \in A)\P(Y \in B)\text{ for all } A \subseteq X(\Omega) \text{ and } B \subseteq Y(\Omega).
    \]
    In other words,
    $X$ and $Y$ are independent (as random variables) if and only if $\{X \in A\}$ and $\{Y \in B\}$ are independent as events for all sets $A$ and $B$.
\end{definition}

\subsection{Jointly distributed discrete random variables}

\begin{definition}[Joint probability mass function]
    Let $(X, Y)$ be a bivariate discrete random variable with $\P((X, Y) \in \mathcal{Z}) = 1$ for a finite or countable $\mathcal{Z} \subseteq (X, Y)(\Omega)$.
    The joint probability mass function $p$ of $X$ and $Y$ is defined by
    \[
    p(x, y) := \P(X = x, Y = y)\quad\text{ for all } (x, y) \in \mathcal{Z}.
    \]
\end{definition}
The above is simply stating the probability mass function for a bivariate discrete random variable.

\begin{theorem}
    Let $X$ and $Y$ be two random variables on the same sample space $\Omega$.
    Then the bivariate random variable $(X, Y)$ is discrete if and only if $X$ and $Y$ are both discrete.
    Moreover,
    if $X$ and $Y$ are both discrete with $\P(X \in \mathcal{X}) = \P(Y \in \mathcal{Y}) = 1$ for finite or countable $\mathcal{X}$ and $\mathcal{Y}$,
    then their marginal probability mass functions are given in terms of the joint probability mass function by
    \begin{align*}
        p_X(x) = \sum_{y \in \mathcal{Y}}p(x, y)\text{ for all } x \in \mathcal{X}, & p_Y(y) = \sum_{x \in \mathcal{X}}p(x, y)\text{ for all } y \in \mathcal{Y}.
    \end{align*}
    \begin{proof}
        First suppose that $X$ and $Y$ are discrete.
        Then there exist finite or countable sets of values $\mathcal{X}$ and $\mathcal{Y}$ such that $\P(X \in \mathcal{X}) = \P(Y \in \mathcal{Y}) = 1$.
        Hence $\P(X \in \mathcal{X}, Y \in \mathcal{Y}) = 1$.
        The bivariate random variable $(X, Y)$ thus has $\P((X, Y) \in \mathcal{X} \times \mathcal{Y}) = 1$.
        Since $\mathcal{X}$ and $\mathcal{Y}$ are finite or countable,
        the Cartesian product $\mathcal{X} \times \mathcal{Y}$ is also finite or countable.
        Hence $(X, Y)$ is discrete.
        On the other hand,
        suppose that $(X, Y)$ is discrete.
        The possible values in $(X, Y)(\Omega)$ are ordered pairs of the form $(x, y)$,
        and there is a finite or countable set $\mathcal{Z} \subseteq (X, Y)(\Omega)$ such that $\P((X, Y) \in \mathcal{Z}) = 1$.
        But if we set $\mathcal{X} = \{x : (x, y) \in \mathcal{Z}\}$ we have $\P(X \in \mathcal{X}) = \P((X, Y) \in \mathcal{Z}) = 1$,
        and $\mathcal{X}$ is finite or countable,
        so $X$ is discrete.
        Similarly for $Y$.
        It remains to notice that,
        for example, with the same definition of $\mathcal{X}$,
        \begin{align*}
            \P(Y = y) &= \P(X \in \mathcal{X}, Y = y) \\
            &= \P\left(\bigcup_{x \in \mathcal{X}}\{X = x, Y = y\}\right) \\
            &= \sum_{x \in \mathcal{X}}p(x, y),
        \end{align*}
        where we have used A4,
        the fact that $\mathcal{X}$ is countable,
        and $\P(X \in \mathcal{X}) = 1$.
    \end{proof}
\end{theorem}

\begin{theorem}\label{pre_prop_thm_discretevarcountsumeqone}
    Let $X$ and $Y$ be discrete random variables with $\P((X, Y) \in \mathcal{Z}) = 1$ for a finite or countable $\mathcal{Z}$.
    Then we have that
    \[
    \P((X, Y) \in A) = \sum_{(x, y) \in A}p(x, y)\quad\text{for all } A \subseteq \mathcal{Z}.
    \]
    In particular,
    \[
    \sum_{(x, y) \in \mathcal{Z}}p(x, y) = 1.
    \]
\end{theorem}

\begin{definition}[Conditional probability mass function]
    Let $X$ and $Y$ be discrete random variables.
    For $y \in \mathcal{Y}$,
    the conditional probability mass function of $X$ given $Y = y$ is defined by
    \[
    p_{X | Y}(x\,|\,y) := \P(X = x\,|\, Y = y) = \frac{p_{X, y}(x, y)}{p_{Y}(y)}
    \]
    for all $x \in \mathcal{X}$ and $y \in \mathcal{Y}$ such that $p_Y(y) > 0$,
    and similarly,
    the conditional probability mass function of $Y$ given $X = x$ is
    \[
    p_{Y\,|\,X}(y\,|\,x) := \P(Y = y\,|\,X = x) = \frac{p_{X, Y}(x, y)}{p_X(x)}
    \]
    for all $y \in \mathcal{Y}$ and $x \in \mathcal{X}$ such that $p_X(x) > 0$.
\end{definition}

\begin{theorem}[Partition theorem for discrete random variables]
    Let $X$ and $Y$ be discrete random variables.
    Then,
    \[
    p_X(x) = \sum_{y \in \mathcal{Y}}p_{X\,|\,Y}(x\,|\,y)p_Y(y).
    \]
    Moreover,
    if $X$ is real-valued,
    then for any functions $g : \mathcal{Y} \rightarrow \R$ and $h : \mathcal{Y} \rightarrow \R$ with $g \leq h$,
    we can write
    \begin{align*}
        \P(g(Y) \leq X \leq h(Y)) &= \sum_{y \in \mathcal{Y}}\P(g(y) \leq X \leq h(y)\,|\, Y = y)\P(Y = y) \\
        &= \sum_{y \in \mathcal{Y}}\sum_{x \in [g(y), h(y)] \cap \mathcal{X}}p_{X|Y}(x\,|\,y)p_Y(y).
    \end{align*}
\end{theorem}

\begin{lemma}[Independence of discrete random variables]
    Two discrete random variables $X$ and $Y$ on the same sample space $\Omega$ and independent if and only if
    \[
    p_{X, Y}(x, y) = p_X(x)p_Y(y)\text{ for all } x \in \mathcal{X}\text{ and } y \in \mathcal{Y}.
    \]
    \begin{proof}
        By definition of independence,
        we have that
        \[
        \P(X = x, Y = y) = \P(X = x)\P(Y = y),
        \]
        as required.
        On the other hand,
        suppose that $p_{X, Y}(x, y) = p_X(x)p_Y(y)$.
        Then by \autoref{pre_prop_thm_discretevarcountsumeqone},
        for any sets $A$ and $B$,
        \begin{align*}
            \P(X \in A, Y \in B) &= \P((X, Y) \in A \times B) = \sum_{x \in A}\sum_{y \in B}p_{X, Y}(x, y) \\
            &= \sum_{x \in A}p_X(x)\sum_{y \in B}p_Y(y) \\
            &= \P(X \in A)\P(Y \in B),
        \end{align*}
        so $X$ and $Y$ are independent.
    \end{proof}
\end{lemma}

\subsection{Jointly continuously distributed random variables}
\begin{definition}[Joint probability density function]
    Consider two real-valued random variables $X : \Omega \rightarrow \R$ and $Y : \Omega \rightarrow \R$.
    We say that $X$ and $Y$ are jointly continuously distributed when there is a non-negative piecewise continuous function $f: \R ^ 2 \rightarrow \R$,
    called the joint probability density function,
    such that
    \[
    \P(X \in [a, b], Y \in [c, d]) = \int_a^b\left(\int_c^d f(x, y)\,dy\right)\,dx
    \]
    for all $[a, b] \times [c, d] \subseteq \R ^ 2$.
\end{definition}

We sometimes write $f_{X, Y}(x, y)$ for the joint probability density of $X$ and $Y$.

A type I region is of the form
\[
D = \{a_0 \leq x \leq a_1,\, \phi_1(x) \leq y \leq \phi_2(x)\}
\]
for some continuous functions $\phi_1$ and $\phi_2$,
and a type II region is of the form
\[
D = \{\psi_1(y) \leq x \leq \psi_2(y),\, b_0 \leq y \leq b_1\}
\]
for some continuous functions $\psi_1$ and $\psi_2$.

\begin{theorem}
    If $X$ and $Y$ are jointly continuously distributed,
    then for any $A \subseteq \R ^ 2$ that is a finite union of type I and type II regions:
    \[
    \P((X, Y) \in A) = \iint\limits_{A}f(x, y)\,dxdy.
    \]
\end{theorem}
Again as before,
$f(x, y)$ integrates to one.
\begin{corollary}
    Let $X$ and $Y$ be jointly continuously distributed random variables.
    Then their joint probability density function integrates to one:
    \[
    \iint\limits_{\R ^ 2}f(x, y)\,dxdy = 1.
    \]
\end{corollary}

\begin{corollary}[Marginal probability density function]
    Let $X$ and $Y$ be continuously distributed random variables.
    Then $X$ and $Y$ are
    (each separately)
    continuously distributed,
    with
    \begin{align*}
    f_X(x) = \int_{-\infty}^{\infty}f(x, y)\,dy, \text{ for all } x \in \R, &f_Y(y) = \int_{-\infty}^{\infty}f(x, y)\,dx, \text{ for all } y \in \R.
    \end{align*}
\end{corollary}

In a multivariate context,
the probability density functions $f_X(x)$ and $f_Y(y)$ are also called marginal probability density functions.

\begin{definition}[Conditional probability density function]
    Let $X$ and $Y$ be jointly continuously distributed random variables.
    The conditional probability density function of $X$ at $Y = y$ is defined by
    \[
    f_{X|Y}(x | y) := \frac{f_{X, Y}(x, y)}{f_Y(y)}\qquad\text{for all $x, y \in \R$ such that $f_Y(y) > 0$}.
    \]
    Similarly,
    the conditional probability density function of $Y$ at $X = x$ is
    \[
    f_{Y|X}(y | x) := \frac{f_{X, Y}(x, y)}{f_X(x)}\qquad\text{for all $x, y \in \R$ such that $f_X(x) > 0$}.
    \]
\end{definition}

\begin{theorem}[Partition theorem for jointly continuous random variables]
    Let $X$ and $Y$ be jointly continuously distributed random variables.
    Then,
    \[
    f_X(x) = \int_{-\infty}^{+\infty}f_{X|Y}(x | y)f_Y(y)\,dy.
    \]
    Moreover,
    for any piecewise continuous functions $g : \R \rightarrow \R$ and $h : \R \rightarrow \R$ with $g \leq h$,
    we can write,
    \[
    \P(g(Y) \leq X \leq h(Y)) = \int_{-\infty}^{\infty}\left(\int_{g(y)}^{h(y)}f(x | y)\,dx\right)f(y)\,dy.
    \]
\end{theorem}

\begin{lemma}[Independence of jointly continuous random variables]
    Two jointly continuously distributed random variables $X$ and $Y$ are independent if and only if
    \[
    f_{X, Y}(x, y) = f_X(x)f_Y(y)\text{ for all $x$ and $y \in \R$}.
    \]
\end{lemma}

\subsection{Functions of multiple random variables}
Suppose $X : \Omega \rightarrow X(\Omega)$ and $Y : \Omega \rightarrow Y(\Omega)$ are
(discrete or continuous)
random variables,
and $g: X(\Omega) \times Y(\Omega) \rightarrow \mathcal{S}$ is some function assigning a value $g(x, y) \in \mathcal{S}$ to each point $(x, y)$.
Then $g(X, Y)$ is also a random variable,
namely the outcome to a 'new experiment' obtained by running the 'old experiments' to produce values $x$ for $X$ and $y$ for $Y$,
and the evaluating $g(x, y)$.

Formally,
$g(X, Y) := g \circ (X, Y)$,
or in more specific terms,
the random variable $g(X, Y) : \Omega \rightarrow \mathcal{S}$ is defined by:
\[
g(X, Y)(\omega) := g(X(\omega), Y(\omega))\text{ for all } \omega \in \Omega.
\]

\newpage

\section{Expectation}

\subsection{Definition and interpretation}
Let $X : \Omega \rightarrow \R$ be a real-valued random variable for the outcome of $n$ observed trials.
Let $x_i$ denote the observed value of $X$ in the $i$th trial;
the sequence of observations $x_1, x_2, \dotsc, x_n$ is called a sample.
The sample mean is then simply $\frac{1}{n}\sum_{i = 1}^{n}x_i$.
As $n$ tends to infinity we call the sample mean
(of $X$)
the expectation of $X$.

This idea of expectation leads us into a definition for the expectation for real-valued random variables.
\begin{definition}[Expectation]
    For any real-valued random variable $X$,
    the expectation
    (also called expected value or mean)
    of $X$,
    denoted as $\mathbb{E}(X)$,
    is defined as:
    \begin{align}
        \E(X) &:= \sum_{x \in \chi}x p(x)&\text{if $X$ is discrete, and} \\
        \E(X) &:= \int_{-\infty}^{\infty}x f(x)\,dx&\text{if $X$ is continuously distributed,}
    \end{align}
    provided that the sum or integral exists.
\end{definition}

\begin{example}
    Suppose that $X$ is discrete with probability mass function
    \begin{table}[H]
        \centering
        \begin{tabular}{c|cc}
             $x$ & $1$ & $2$ \\
             \hline
             $p(x)$ & $\frac{1}{2}$ & $\frac{1}{2}$
        \end{tabular}
    \end{table}
    Then $\E(X) = \frac{1}{2} \cdot 1 + \frac{1}{2} \cdot 2 = \frac{3}{2}$
\end{example}

\begin{example}
    To find the expectation of a discrete random variable $X$ where $p(x) = \frac{1}{n}$ for $x \in \{1, 2, \dotsc, n\}$,
    we compute
    \begin{align*}
        \E(X) := \sum_{x = 1}^{n}xp(x) \\
        &= (1 + 2 + \dotsi + n)\frac{1}{n} = \frac{n + 1}{2}.
    \end{align*}
\end{example}

\begin{example}
    If $X \sim U(a, b)$ then
    \[
    \E(X) = \int_a^b\frac{x}{b - a}\,dx = \left[\frac{\frac{x ^ 2}{2}}{b - a}\right]_a^b = \frac{a + b}{2}.
    \]
\end{example}

\begin{example}
    If $Z \sum \mathcal{N}(0, 1)$ then
    \[
    \E(Z) = \int_{-\infty}^{\infty}z\phi(z)\,dz = 0,
    \]
    since the integrand is an odd function
    ($\phi(z) = \phi(-z)$).
\end{example}

\subsection{Expectation of functions of random variables}

Let $\chi$ be a discrete random variable with $\P(X \in \chi) = 1$ for a finite or countable set $\chi$,
and let $g : \chi \rightarrow \R$ be a real-valued function.
$g(X) := g \circ X$ is a random variable.
$g(X)$ is a real-valued random variable,
so we can define its expectation.

To find the expectation of $g(X)$,
according to the definition we need to find the probability mass function $p_{g(X)}$ first.
However,
we can express $\E(g(X))$ directly in terms of $p_X$,
hence saving us the effort of having to calculate $p_{g(X)}$ from $p_X$.

For any $y \in g(\chi)$,
\[
p_{g(X)}(y) = \P(g(X) = y) = \sum_{x \in \chi}\P(g(X) = y\,|\,X = x)\P(X = x) = \sum_{x \in \chi:g(x) = y}p(x),
\]
since
\[
\P(g(X) = y\,|\,X = x) = \begin{cases}
    1 &\text{if } y = g(x), \\
    0 &\text{otherwise}.
\end{cases}
\]
It follows that
\begin{align*}
    \E(g(X)) &= \sum_{y \in g(\chi)}yp_{g(X)}(y)
    = \sum_{y \in g(\chi)}y\left(\sum_{x \in \chi: g(x) = y}p(x)\right)
    = \sum_{y \in g(\chi)}\left(\sum_{x \in \chi : g(x) = y}yp(x)\right) \\
    &= \sum_{x \in \chi}\left(\sum_{y \in g(\chi) : y = g(x)}yp(x)\right)
    = \sum_{x \in \chi}\left(\sum_{y \in g(\chi) : y = g(x)}y\right)p(x)
    = \sum_{x \in \chi}g(x)p(x),
\end{align*}
where we applied the definition of expectation,
the previous equation,
distributivity, change of order of summation, and distributivity again.
A similar result can be proven when $X$ is continuously distributed.

The following result, sometimes known as the Law of the Unconscious Statistician, follows from what we have just shown.
\begin{theorem}[Expectation of a function of a random variable]\label{pre_prob_thm_lawofunconsciousstatistician}
    For any discrete random variable $X$ taking values in $\chi$,
    and any function $g : \chi \rightarrow \R$,
    \begin{equation}
        \E(g(X)) = \sum_{x \in \chi}g(x)p(x),
    \end{equation}
    provided that the sum exists.
    Similarly,
    for any continuous random variable $X$ and any function $g : \R \rightarrow \R$,
    \begin{equation}
        \E(g(X)) = \int_{-\infty}^{\infty}g(x)f(x)\,dx,
    \end{equation}
    provided that the integral exists.
\end{theorem}

\begin{example}
    Suppose that $X$ takes values $0, 1, 2, 3, 4$ each with probability $\frac{1}{5}$.
    Then
    \begin{align*}
        \E((X - 3) ^ 2) &= \sum_{x = 0}^{4}(x - 3) ^ 2p(x) \\
        &= \frac{1}{5}((-3) ^ 2 + (-2) ^ 2 + (-1) ^ 2 + (0) ^ 2 + (1) ^ 2) \\
        &= \frac{1}{5}(15) \\
        &= 3.
    \end{align*}
\end{example}

For multiple random variables,
the Law of the Unconscious Statistician reads as follows:
\begin{theorem}[Expectation of a function of multiple random variables]\label{pre_prob_thm_multilawofunconsciousstatistician}
    For any discrete random variables $X$ and $Y$ taking values in $\mathcal{X}$ and $\mathcal{Y}$,
    and for any function $g : \mathcal{X} \times \mathcal{Y} \rightarrow \R$,
    \[
    \E(g(X, Y)) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}}g(x, y)p(x, y),
    \]
    provided that the sum exists.
    Similarly,
    for any jointly continuously distributed random variables $X$ and $Y$,
    and any function $g : \R ^ 2 \rightarrow \R$,
    \[
    \E(g(X, Y)) = \iint\limits_{\R ^ 2}g(x, y)f(x, y)\,dxdy,
    \]
    provided that the integral exists.
\end{theorem}

\subsection{Linearity of expectation}
Remember that summation and integration are linear operators,
i.e.,
\[
\sum_{i}\alpha f(x_i) + \beta g(x_i) = \alpha\sum_{u}f(x_i) + \beta\sum_{i}g(x_i),
\]
and
\[
\int_A(\alpha f(x) + \beta g(x))\,dx = \alpha\int_Af(x)\,dx + \beta\int_Ag(x)\,dx.
\]
Consequently:
\begin{theorem}[Linearity of expectation I]
    For any real-valued random variable $X$,
    and any constants $\alpha$ and $\beta \in \R$,
    \[
    \E(\alpha X + \beta) = \alpha\E(X) + \beta.
    \]
\end{theorem}
A similar,
but deeper,
result is the following.
\begin{theorem}[Linearity of expectation II]
    For any two real-valued random variables $X$ and $Y$ on the same sample space $\Omega$,
    \[
    \E(X + Y) = \E(X) + \E(Y).
    \]
    More generally,
    for any real-valued random variables $X_1, X_2, \dotsc, X_n$,
    \[
    \E\left(\sum_{i = 1}^{n}X_i\right) = \sum_{i = 1}^{n}\E(X_i).
    \]
    \begin{proof}
        We give the proof in the case where $X$ and $Y$ are discrete.
        Consider the multiple random variable $(X, Y)$ and the function $g(x, y) = x + y$.
        By the Law of the Unconscious Statistician
        (\autoref{pre_prob_thm_lawofunconsciousstatistician})
        we get
        \begin{align*}
            \E(g(X, Y)) &= \sum_{x \in \mathcal{X}}x\sum_{y \in \mathcal{Y}}p(x, y) + \sum_{y \in \mathcal{Y}}y\sum_{x \in \mathcal{X}}p(x, y) \\
            &= \sum_{x \in \mathcal{X}}xp_X(x) + \sum_{y \in \mathcal{Y}}yp_Y(y) = \E(X) + \E(Y),
        \end{align*}
        as claimed.
        A similar calculation applies in the jointly continuous case,
        and the extension to more than two random variables follows by induction.
    \end{proof}
\end{theorem}

\subsection{Variance and covariance}
A popular and mathematically convenient way to measure the variability of $X$ i.e. measure how much $X$ varies from $\E(X)$ in the long run goes via the expectation of the random variable $(X - \E(X)) ^ 2$.

\begin{definition}
    Let $X$ be any real-valued random variable.
    The variance of $X$ is defined as
    \[
    \mathbb{V}\mathrm{ar}(X) := \E((X - \E(X)) ^ 2),
    \]
    and the standard deviation of $X$ is defined as
    \[
    \mathrm{SD}(X) := \sqrt{\mathbb{V}\mathrm{ar}(X)}.
    \]
\end{definition}

From \autoref{pre_prob_thm_lawofunconsciousstatistician} we can see that
\begin{align*}
    \Var(X) &= \sum_{x \in \mathcal{X}}(x - \E(X)) ^ 2p(x)&\text{if $X$ is discrete, and} \\
    \Var(X) &= \int_{-\infty}^{\infty}(x - \E(X)) ^ 2f(x)\,dx&\text{if $X$ is continuously distributed},
\end{align*}
provided that the sum or integral exists.

\begin{definition}[Covariance]
    Let $X$ and $Y$ be two real-valued random variables on the sample space.
    The covariance of $X$ and $Y$ is defined as
    \[
    \mathbb{C}\mathrm{ov}{(X, Y)} := \E((X - \E(X))(Y - \E(Y)).
    \]
\end{definition}
\begin{itemize}
    \item If $\Cov(X, Y) > 0$ it means that $X - \E(X)$ and $Y - \E(Y)$ tend to have the same sign.
    That is,
    if $X > \E(X)$ then it tends to be the case that $Y > \E(Y)$
    (or, conversely,
    if $X < \E(X)$ then it tends to be the case that $Y < \E(Y)$ too).
    In this case we say that $X$ and $Y$ are positively correlated.
    \item If $\Cov(X, Y) < 0$ we say that $X$ and $Y$ are negatively correlated.
    Now $X - \E(X)$ and $Y - \E(Y)$ tend to have opposite signs.
    \item If $\Cov(X, Y) = 0$ we say that $X$ and $Y$ are uncorrelated.
\end{itemize}

A quantification of the correlation is provided by the correlation coefficient,
given by
\[
\rho(X, Y) := \frac{\Cov(X, Y)}{\sqrt{\Var(X)\Var(Y)}}.
\]
It can be proved that
\[
-1 \leq \rho(X, Y) \leq 1.
\]
From \autoref{pre_prob_thm_multilawofunconsciousstatistician},
we can immediately derive the following expressions for the covariance:
\[
\Cov(X, Y) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}}(x - \E(X))(y - \E(Y))p(x, y),
\]
if $X$ and $Y$ are discrete,
and
\[
\Cov(X, Y) = \iint\limits_{\R ^ 2}(x - \E(X))(y - \E(Y))f(x, y)\,dxdy,
\]
if $X$ and $Y$ are jointly continuously distributed,
provided that the double sum or double integral exists.

\begin{property}[Variance as covariance]
    For any real-valued random variable $X$,
    \[
    \Var(X) = \Cov(X, X).
    \]
\end{property}
\begin{property}[Symmetry of covariance]
    For any real-valued random variables $X$ and $Y$,
    \[
    \Cov(X, Y) = \Cov(Y, X).
    \]
\end{property}

\begin{corollary}
    For any real-valued random variable $X$,
    and any constants $\alpha$ and $\beta \in \R$,
    \[
    \Var(\alpha + \beta X) = \beta ^ 2 \Var(X).
    \]
    For any real-valued random variables $X, Y$, and $Z$ and any constants $\alpha, \beta, \gamma$, and $\delta \in \R$,
    \begin{subequations}
        \begin{align}
            \Cov(\alpha + \beta X, \gamma + \delta Y) &= \beta\delta\Cov(X, Y) \\
            \Cov(X + Y, Z) &= \Cov(X, Z) + \Cov(Y, Z) \\
            \Cov(X, Y + Z) &= \Cov(X, Y) + \Cov(X, Z)
        \end{align}
    \end{subequations}
    \begin{proof}
        We give an example of the type of calculation
        \begin{align*}
            \Var(\alpha + \beta X) &= \E((\alpha + \beta X - \E(\alpha + \beta X)) ^ 2) \\
            &= \E((\alpha + \beta X - \alpha + \beta\E(X)) ^ 2) \\
            &= \E((\beta X - \beta\E(X)) ^ 2) \\
            &= \beta ^ 2\E((X - \E(X)) ^ 2) \\
            &= \beta ^ 2\Var(X).
        \end{align*}
        The other statements are similar.
    \end{proof}
\end{corollary}

\begin{corollary}
    For any real-valued random variable $X$,
    \[
    \Var(X) = \E(X ^ 2) - (\E(X)) ^ 2.
    \]
    \begin{proof}
        Observe that,
        by linearity of expectation,
        \begin{align*}
            \Var(X) &= \E((X - \E(X)) ^ 2) \\
            &= \E(X ^ 2 - 2X\E(X) + (\E(X)) ^ 2) \\
            &= \E(X ^ 2) - 2\E(X)\E(X) + (\E(X)) ^ 2) \\
            &= \E(X ^ 2) + (\E(X)) ^ 2),
        \end{align*}
        as required.
    \end{proof}
\end{corollary}

\begin{corollary}
    For any real-valued random variables $X$ and $Y$,
    \[
    \Cov(X, Y) = \E(XY) - \E(X)\E(Y).
    \]
    \begin{proof}
        \begin{align*}
            \Cov(X, Y) &= \E((X - \E(X))(Y - \E(Y))) \\
            &= \E(XY - Y\E(X) - X\E(Y) + \E(X)\E(Y)) \\
            &= \E(XY) - \E(Y)\E(X) - \E(X)\E(Y) + \E(X)\E(Y) \\
            &= \E(XY) - \E(X)\E(Y),
        \end{align*}
        as required.
    \end{proof}
\end{corollary}

\begin{corollary}
    For any real-valued random variables $X, Y$,
    and $Z$ and any $\alpha, \beta, \gamma$,
    and $\delta \in \R$,
    \begin{align*}
        \Cov(\alpha + \beta X, \gamma + \delta Y) &= \beta\delta\Cov(X, Y), \\
        \Cov(X + Y, Z) &= \Cov(X, Z) + \Cov(Y, Z), \\
        \Cov(X, Y + Z) &= \Cov(X, Y) + \Cov(X, Z).
    \end{align*}
\end{corollary}

\begin{theorem}[Variance of a sum]
    For any real-valued random variables $X$ and $Y$ on the same sample space,
    \[
    \Var(X + Y) = \Var(X) + \Var(Y) + 2\Cov(X, Y).
    \]
    More generally,
    for any real=valued random variables $X_1, X_2, \dotsc, X_n$,
    \[
    \Var\left(\sum_{i = 1}^{n}X_i\right) = \sum_{i = 1}^{n}\Var(X_i) + 2\sum_{i = 1}^{n - 1}\sum_{j = i + 1}^{n}\Cov(X_i, X_j).
    \]
    \begin{proof}
        Proving the first statement,
        \begin{align*}
            \Var(X + Y) &= \Cov(X + Y, X + Y) \\
            &= \Cov(X, X + Y) + \Cov(Y, X + Y) \\
            &= \Cov(X, X) + \Cov(X, Y) + \Cov(Y, x) + \Cov(Y, Y) \\
            &= \Var(X) + \Var(Y) + 2\Cov(X, Y),
        \end{align*}
        which gives the result.
        In general,
        \begin{align*}
            \Var\left(\sum_{i = 1}^{n}X_i\right) &= \Cov\left(\sum_{i = 1}^{n}X_i, \sum_{j = 1}^{n}X_j\right) \\
            &= \sum_{i = 1}^{n}\Cov\left(X_i, \sum_{j = 1}^{n}X_j\right) \\
            &= \sum_{i = 1}^{n}\sum_{j = 1}^{n}\Cov(X_i, X_j) \\
            &= \sum_{i = 1}^{n}\Cov(X_i, X_i) + \sum_{i = 1}^{n}\sum_{j \neq i}\Cov(X_i, X_j) \\
            &= \sum_{i = 1}^{n}\Var(X_i) + \sum_{i = 1}^{n}\sum_{j = 1}^{i - 1}\Cov(X_i, X_j) + \sum_{i = 1}^{n}\sum_{j = i + 1}^{n}\Cov(X_i, X_j),
        \end{align*}
        with the convention that an empty sum is zero.
    \end{proof}
\end{theorem}

\subsection{Conditional expectation}
Recall that the indicator random variable of an event $A$ is given by
\[
\ind(\omega) = \begin{dcases*}
    1 & if $\omega \in A$, \\
    0 & otherwise.
\end{dcases*}
\]
Then $\E(\ind) = 1 \cdot \P(A) + 0 \cdot \P(A ^ c) = \P(A)$.

\begin{definition}[Conditional expectation with respect to an event]
    Let $X$ be a rea-valued random variable,
    and let $A \subseteq \Omega$ be an event.
    The conditional expectation of $X$ given $A$ is:
    \[
    \E(X \mid A) := \frac{\E(X\ind)}{\P(A)}\text{ whenever $\P(A) > 0$}.
    \]
\end{definition}

One may also view $\E(\cdot\mid A)$ as expectation with respect to the conditional probability $\P(\cdot\mid A)$.
\begin{theorem}
    For any discrete random variable $X$ and any event $A \subseteq \Omega$,
    \[
    \E(X\mid A) = \sum_{x \in \mathcal{X}}x\P(X = x\mid A).
    \]
    \begin{proof}
        This is an exercise in tracking definitions.
        Indeed,
        if $Y = X\ind$,
        then for any $x \neq 0$,
        \[
        \P(Y = x) = \P(\{X = x\} \cap A) = \P(X = x\mid A)\P(A).
        \]
        Hence
        \begin{align*}
            \E(Y) &= \sum_{x \in \mathcal{X}}X\P(Y = x) \\
            &= \sum_{x \in \mathcal{X}, x \neq 0}x\P(Y = x) \\
            &= \sum_{x \in \mathcal{X}}x\P(X = x\mid A)\P(A), \\
            \intertext{and so}
            \E(X \mid A) &= \frac{\E(Y)}{\P(A)} = \sum_{x \in \mathcal{X}}x\P(X = x\mid A),
        \end{align*}
        as claimed.
    \end{proof}
\end{theorem}

\begin{theorem}[Partition theorem for expectation]\label{pre_prob_thm_partthmforexp}
    Let $X$ be any real-valued random variable.
    Let $E_1, E_2, \dotsc, E_k$ be any events that form a partition.
    Then,
    \[
    \E(X) = \sum_{i = 1}^{k}\E(X \mid E_i)\P(E_i).
    \]
    Similarly,
    if $E_1, E_2, \dotsc$ form an infinite partition
    \[
    \E(X) = \sum_{i = 1}^{\infty}\E(X \mid E_i)\P(E_i).
    \]
    \begin{proof}
        Because $E_1, E_2, \dotsc$ constitute a partition,
        $\bigcup_iE_i = \Omega$ and the $E_i$ are pairwise disjoint,
        so that
        \[
        1 = \ind[\Omega] = \ind[\bigcup_iE_i] = \sum_{i}\ind[E_i],
        \]
        and hence,
        by linearity of expectation,
        \[
        \E(X) = \E\left(X\sum_{i}\ind[E_i]\right) = \sum_{i}\E(X\ind[E_i]),
        \]
        which gives the result.
    \end{proof}
\end{theorem}

\begin{definition}[Conditional expectation with respect to a random variable]
    Let $X$ be a real-valued random variable,
    and let $Y$ be another random variable.
    Define the function $g : Y(\Omega) \rightarrow \R$ by $g(y) := \E(X\mid Y = y)$.
    Then the conditional expectation of $X$ given $Y$ is the random variable denoted by $\E(X\mid Y)$ given by
    \[
    \E(X \mid Y) := g(Y).
    \]
\end{definition}

\begin{theorem}[Partition theorem for expectation II]
\footnote{This is sometimes called the law of iterated expectation}
    For any real=valued random variable $X$ and any random variable $Y$,
    \[
    \E(X) = \E(\E(X \mid Y)).
    \]
    \begin{proof}
        We prove the case when $Y$ is discrete.
        which shows why we call this a partition theorem.
        In this case $\{Y = y\}, y \in \mathcal{Y}$,
        forms a partition,
        and so \autoref{pre_prob_thm_partthmforexp} gives
        \[
        \E(X) = \sum_{y \in \mathcal{Y}}\E(X\mid Y = y)\P(Y = y),
        \]
        but this last expression is the expectation of the discrete random variable $\E(X\mid Y)$ which takes values $g(y) = \E(X\mid Y = y)$ with probabilities $\P(Y = y)$:
        \[
        \E(\E(X \mid Y)) = \E(g(Y)) = \sum_{y \in \mathcal{Y}}g(y)\P(Y = y),
        \]
        by the law of the unconscious statistician.
    \end{proof}
\end{theorem}

\subsection{Independence: multiplication rule for expectation}
\begin{theorem}[Independence means multiply]\label{pre_prob_thm_expindepmeansmult}
    If $X$ and $Y$ are independent real-valued random variables then
    \[
    \E(XY) = \E(X)\E(Y).
    \]
    Moreover, if $g, h : \R \rightarrow \R$,
    \[
    \E(g(X)h(Y)) = \E(g(X))\E(h(Y)).
    \]
    More generally,
    for any mutually independent real-valued random variables $X_1, X_2, \dotsc, X_n$,
    \[
    \E\left(\prod_{i = 1}^{n}X_i\right) = \prod_{i = 1}^{n}\E(X_i).
    \]
    \begin{proof}
        We give a proof only in the discrete case.
        Suppose $X$ and $Y$ are independent discrete random variables with joint probability mass function $p(x, y) = p_X(x)p_Y(y)$.
        Then by the law of the unconscious statistician,
        \[
        \E(g(X)h(Y)) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}}g(x)h(y)p(x, y) = \sum_{x \in \mathcal{X}}g(x)p_X(x)\sum_{y \in \mathcal{Y}}h(y)p_Y(y),
        \]
        which is $\E(g(X))\E(h(Y))$.
    \end{proof}
\end{theorem}

\begin{corollary}
    If $X$ and $Y$ are independent random variables,
    then $\Cov(X, Y) = 0$.
    \begin{proof}
        In the independent case,
        $\Cov(X, Y) = \E(XY) - \E(X)\E(Y) = 0$.
    \end{proof}
\end{corollary}

\begin{corollary}
    Consider random variables $X_1, X_2, \dotsc, X_n$.
    If these random variables are pairwise independent,
    then
    \[
    \Var\left(\sum_{i = 1}^{n}X_i\right) = \sum_{i = 1}^{n}\Var(X_i).
    \]
\end{corollary}

\subsection{Expectation and probability inequalities}
\begin{theorem}[Monotonicity of expectation]\label{pre_prob_thm_monotonicityofexpectation}
    For any random variable $X$,
    and any $a \in \R$,
    if $\P(X \geq a) = 1$ then $\E(X) \geq a$.
\end{theorem}

\begin{corollary}
    For any random variable $X$,
    $\Var(X) \geq 0$.
    \begin{proof}
        We have $\Var(X) = \E((X - \E(X)) ^ 2)$ and the random variable $(X - \E(X)) ^ 2$ is non-negative.
    \end{proof}
\end{corollary}

\begin{corollary}[Markov's inequality]
    If $X \geq 0$ then,
    for any $a > 0$,
    \[
    \P(X \geq a) \leq \frac{\E(X)}{a}.
    \]
    \begin{proof}
        Note that $X \geq a\ind[\{X \geq a\}]$,
        and consequently $0 \leq \E(X - a\ind[\{X \geq a\}]) = \E(X) - a\P(X \geq a)$.
    \end{proof}
\end{corollary}

\begin{corollary}[Chebyshev's inequality]
    For any random variable $X$ and any $a > 0$,
    we have
    \[
    \P(|X - \E(X)| \geq a) \leq \frac{\Var(X)}{a ^ 2}.
    \]
    \begin{proof}
        $\P((X - \E(X)) ^ 2 \geq a ^ 2) \leq \frac{\E((X - \E(X)) ^ 2)}{a ^ 2}$ by Markov's inequality applied to $(X - \E(X)) ^ 2$ at $a ^ 2$.
        Now observe that $\{(X - \E(X)) ^ 2 \geq a ^ 2\} = \{|X - \E(X)| \geq a\}$ and of course $\E((X - \E(X)) ^ 2) = \Var(X)$.
    \end{proof}
\end{corollary}

\newpage

\section{Limit theorems}

\subsection{The weak law of large numbers}
\begin{theorem}[Weak law of large numbers]
    Suppose we have an infinite sequence $X_1, X_2, \dotsc$ of independent random variables with the same mean and variance:
    \[
    \E(X_i) = \mu\text{ and }\Var(X_i) = \sigma ^ 2\text{ for all } i.
    \]
    Consider the sample average $\overline{X}_n := \frac{1}{n}\sum_{i = 1}^{n}X_i$.
    Then,
    for any $\varepsilon > 0$,
    \[
    \lim_{n \rightarrow \infty}\P(|\overline{X}_n - \mu| > \varepsilon) = 0.
    \]
    \begin{proof}
        We use Chebyshev's inequality to bound the probability that we are trying to show is small.
        First note that,
        by linearity of expectation,
        $\E(\overline{X}_n) = \frac{1}{n}\sum_{i = 1}\E(X_i) = \mu$ and,
        by independence,
        $\Var(\overline{X}_n) = \frac{1}{n ^ 2}\sum_{i = 1}^{n}\Var(X_i) = \frac{\sigma ^ 2}{n}$.
        So by Chebyshev's inequality,
        \[
        \P(|\overline{X}_n - \mu| \geq \varepsilon) \leq \frac{\sigma ^ 2}{n\varepsilon ^ 2},
        \]
        which indeed converges to zero as $n$ tends to infinity.
    \end{proof}
\end{theorem}

\begin{example}
    Measure the heights $H_i$ of $n$ randomly selected people from a very large population,
    where $\mu, \sigma ^ 2$ are the average and the variance of heights over the whole population.
    Then $\E(H_i) = \mu$,
    $\Var(H_i) = \sigma ^ 2$ and so,
    as long as the collection of heights is not too asymmetric,
    the chance of the average height $\overline{X}_n$ being more than than a small amount from $\mu$ is very small i.e. almost all large samples have average near $\mu$.
\end{example}

\subsection{The central limit theorem}

\begin{theorem}[Central limit theorem]
    Suppose we have a sequence $X_1, X_2, \dotsc$ of independent and identically distributed random variables.
    Let
    \[
    \mu := \E(X_i) \text{ and } \sigma ^ 2 := \Var(X_i),
    \]
    with $\sigma > 0$.
    Let $S_n := \sum_{i = 1}^{n}X_i$,
    $\overline{X}_n := \frac{S_n}{n}$,
    and
    \[
    Z_n := \frac{S_n - n\mu}{\sigma\sqrt{n}} = \frac{\overline{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}.
    \]
    Then,
    for any $z \in \R$,
    \[
    \lim_{n \rightarrow \infty}F_{Z_n}(z) = \lim_{n \rightarrow \infty}\P(Z_n \leq z) = \Phi(z).
    \]
    We say that $Z_n$ converges in distribution to the standard normal distribution.
\end{theorem}

\begin{example}
    Potatoes with an average weight of $100$g and standard deviation $40$g are packed into bags to contain at least $2500$g.
    What is the chance that more than $30$ potatoes will be needed to fill a given bag?
    
    
    Let $N$ be the number needed to exceed $2500$ and $W$ be the total weight of $30$ potatoes,
    $W = \sum_{i = 1}^{30}X_i$.
    Then $\E(W) = 30 \cdot 100 = 3000$ and $\Var(W) = 30 \cdot 40 ^ 2 = 48000$,
    and the central limit theorem says that
    \[
    \frac{W - 3000}{\sqrt{48000}} \approx \mathcal{N}(0, 1).
    \]
    Also,
    $\{N > 30\} = \{W < 2500\}$ and so
    \begin{align*}
        \P(N > 30) &= \P(W < 2500) \\
        &= \P\left(\frac{W - 3000}{\sqrt{48000}} < \frac{2500 - 3000}{\sqrt{48000}}\right) \\
        &\approx \P(Z < -2.282),
    \end{align*}
    where $Z \sim \mathcal{N}(0, 1)$.
    From the tables,
    this probability is
    \[
    \P(Z < -2.282) = \P(Z > 2.282) = 1 - \Phi(2.282) \approx 0.011.
    \]
\end{example}

\begin{corollary}[Normal approximation to the binomial distribution]
    Let $X \sim \Bin(n, p)$ with $0 < p < 1$,
    $B_n := \frac{X}{n}$,
    and
    \[
    Z_n := \frac{X - np}{\sqrt{np(1 - p}} = \frac{B_n - p}{\sqrt{\frac{p(1 - p)}{n}}}.
    \]
    Then
    \[
    \lim_{n \rightarrow \infty}F_{Z_n}(z) = \lim_{n \rightarrow \infty}\P(Z_n \leq z) = \Phi(z).
    \]
\end{corollary}
This means that,
when $X \sim \Bin(n, p)$ with $0 < p < 1$ and large enough $n$,
then for any $x \in \R$,
we approximately have that
\[
\P(X \leq x) \approx \Phi\left(\frac{x - np}{\sqrt{np(1 - p)}}\right)
\]

\begin{example}
    Consider a multiple choice test with $50$ questions,
    one mark for each correct answer.
    Independently for each question,
    a particular student has chance $\frac{1}{2}$ of answering correctly.
    Find,
    approximately,
    the probability of the student scoring at least $30$ marks?

    Let $X = \text{ student's score}$,
    so $X \sim \Bin\left(50, \frac{1}{2}\right)$.
    Then $\E(X) = \frac{50}{2} = 25$ and $\Var(X) = \frac{50}{4} = \frac{25}{2}$.
    The normal approximation says that $X \approx \mathcal{N}\left(25, \frac{25}{2}\right)$,
    so
    \begin{align*}
        \P(X \geq 30) &= 1 - \P(X < 30) \\
        &\approx 1 - \Phi\left(\frac{30 - 25}{3.54}\right) \\
        &= 1 - \Phi(1.41) \approx 0.1.
    \end{align*}
\end{example}

\subsection{Moment generating functions}

\begin{definition}[Moment generating function]
    For any real-valued random variable $X$,
    the function $M_X : \R \rightarrow [0, +\infty]$ given by
    \[
    M_X(t) := \E(e ^ {tX})
    \]
    is called the moment generating function.
\end{definition}

Because $e ^ {tX} \geq 0$,
we have that $M_X(t) \geq 0$
(\autoref{pre_prob_thm_monotonicityofexpectation}).
From \autoref{pre_prob_thm_lawofunconsciousstatistician},
we immediately derive the following expressions for $M_X(t)$:
\begin{align*}
    M_X(t) &= \sum_{x \in \mathcal{X}}e ^ {tx}p(x) &\text{if $X$ is discrete, and} \\
    M_X(t) &= \int_{-\infty}^{\infty}e ^ {tx}f(x)\,dx &\text{if $X$ is continuously distributed}.
\end{align*}
The above always exist,
but can be $\infty$.

\begin{example}
    If $Y \sim \Po(\lambda)$ then
    \begin{align*}
        M_Y(t) &= \infsum[x = 0]e ^ {tx}p(x) \\
        &= \infsum[x = 0]e ^ {-\lambda}\frac{\lambda ^ x}{x!}e ^ {tx} \\
        &= \infsum[x = 0]e ^ {-\lambda}\frac{(\lambda e ^ t) ^ x}{x!} \\
        &= e ^ {\lambda(e ^ t - 1)}.
    \end{align*}
\end{example}

Here we will state some properties of moment generating functions.
\begin{enumerate}[label = M\arabic*]
    \item (Generates moments.)
    For every $k \in \N$
    \[
    \E(X ^ k) = \frac{d ^ kM_X}{dt ^ k}(0).
    \]
    \item (Moment generating function determines distribution.)
    Consider any two random variables $X$ and $Y$.
    If there is an $h > 0$ such that
    \begin{align*}
        M_X(t) &= M_Y(t) < \infty &\text{for all } t \in (-h, h),
        \intertext{then}
        F_X(x) &= F_Y(x) &\text{for all } x \in \R.
    \end{align*}
    Conversely,
    if $F_X(x) = F_Y(x)$ for all $x \in \R$ then $M_X(t) = M_Y(t)$ for all $t \in \R$.
    \item (Scaling.)
    For any random variable $X$ and any constants $a, b \in \R$,
    \[
    M_{aX + b}(t) = e ^ {bt}M_X(at).
    \]
    \item (Product.)
    Suppose that $X_1, \dotsc, X_n$ are independent random variables and let $Y = \sum_{i = 1}^{n}X_i$.
    Then
    \[
    M_Y(t) = \prod_{i = 1}^{n}M_{X_i}(t).
    \]
    \item (Convergence.)
    Suppose that $X_1, X_2, \dotsc$ is an infinite sequence of random variables,
    and that $X$ is a further random variable.
    If there is an $h > 0$ such that
    \begin{align*}
        \lim_{n \rightarrow \infty}M_{X_n}(t) &= M_X(t) < +\infty & \text{for all } t \in (-h, h),
        \intertext{then}
        \lim_{n \rightarrow \infty}F_{X_n}(x) &= F_X(x) & \text{for all $x \in \R$ where $F_X$ is continuous}, 
    \end{align*}
    i.e.,
    $X_n$ converges in distribution to $X$.
    \begin{proof}
        The proof of $M1$ is easy.
        $M1, M2$,
        and $M5$ use some deeper analysis.
        For $M4$,
        we apply \autoref{pre_prob_thm_expindepmeansmult}
        \begin{align*}
            M_{S_n}(t) &= \E\left(e ^ {t(X_1 + X_2 + \dotsi + X_n)}\right) \\
            &= \E(e ^ {tX_1}e ^ {tX_2}\dotsi e ^ {tX_n}) \\
            &= \E(e ^ {tX_1})\E(e ^ {tX_2}) \dotsi \E(e ^ {tX_n}) \\
            &= \prod_{i = 1}^{n}M_{X_i}(t).
        \end{align*}
    \end{proof}
\end{enumerate}




\end{document}