\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\declaretheorem[style = avgstyle, name = Counting Principle]{countprinc}
\counterwithin*{equation}{subsection}

\title{Probability I}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\section{Introduction}

Probability is based on axioms


Events are based on sets

\begin{definition}
    A set $S$ is a\footnote{Unordered} collection of elements or objects.    
\end{definition}
\begin{example}
    Let $S$ be the set of all even integers from $1$ to $9$.
    \[
    S =  \{2, 4, 6, 8\} = \{4, 6, 2, 8\}.
    \]
\end{example}

$\Z$ is the set of all integers.

$\N$ is the set of all natural numbers.\footnote{Positive integers}

$\N$ can also be defined as follows
\[
\N = \{x \in \Z^+: x > 0\}.
\]

$\varnothing$ denotes the empty set.

\begin{definition}
    Let $A, B$ be sets. $A \subseteq B \iff x \in A \implies x \in B$.

    A strict subset $A \subset B$ means that there is an $x \in B$ such that $x \notin A$ 
\end{definition}

\begin{definition}
    Let $A, B$ be sets. $A = B \iff A\subseteq B \And B \subseteq A$
\end{definition}

\subsection{Basic set operations}

\begin{definition}
    Let $A, B$ be sets. The union of $A, B$ is denoted by $A \cup B$. The union of two sets is the set of elements in $A$ or $B$ or both.
    \[
    A \cup B = \{x: x \in A \text{ or } x \in B \text{ or } x \in A\And B\}.
    \]
\end{definition}

\begin{definition}
    Let $A, B$ be sets. The intersection of $A, B$ is denoted by $A \cap B$. The intersection of two sets is the collection of elements that are in both $A$ and $B$.
    \[
    A \cap B = \{x: x \in A \And x \in B\}.
    \]
\end{definition}

\begin{definition}
    Let $A$ be a set. The complement of a set $A$ is denoted by $A ^ c$. A complement is the collection of elements not in $A$.
    \[
    A ^ c = \{x: x \notin A\}.
    \]
\end{definition}

\begin{example}
    Let $S :=$ collection of integers from $1$ to $9$ 
    $S = \{x \in \N: 1 \leq x \leq 9\}$
    
    Let $A :=$ collection of even integers from $1$ to $9$ 
    $A = \{x \in \N, x \text{ is even}: 1 \leq x \leq 9\}$

    $A \subseteq S\quad A \subseteq \N\quad S\subseteq\N$

    Let $B :=$ collection of odd integers from $1$ to $9$ 
    $B = \{x \in \N, x \text{ is odd}: 1 \leq x \leq 9\}$

    $A \cap B = \varnothing$,\qquad $A ^ c = \{x \in \N: 1\leq x \leq 9, x \notin A\}$

    \begin{align*}
    D &:= \text{All integers in } S \text{ that are divisible by 3} \\
    &= \{3, 6, 9\}
    \end{align*}
    $D \cap B = \{3, 9\}\quad D\cap A = \{6\}$.
\end{example}

\subsection{Cardinality}
\begin{definition}[Cardinality]
    Let $A$ be a finite set. Cardinality or the size of $A$ denoted by
    \[
    |A|
    \]
    is the number of elements it contains.
\end{definition}

If $|A| = m$, $A = \{x_1, x_2, \dots, x_n\}$\footnote{List or sequence or length $m$.}.

\textbf{Types of infinity}:

There are two types of infinity. Countable infinities implies that we can write it as a sequence of possibly infinite length. Uncountable infinities, for example $\R$ cannot be written as a sequence.

$\N = \{1, 2, 3, \dots\}$ can be written as a list.

$[a, b] = \{x \in \R : a \leq x \leq b\}$ for example take $a = 1$ and $b = 2$ and you can keep taking the midpoint between any two numbers infinitely and not find all numbers.

When you have a finite number of sets
\[
A_1, \dots A_n, \quad\text{abbreviation: } \bigcup_{i = 1}^{n}A_i = A_1 \cup A_2 \cup \dotsi \cup A_n\footnote{Finite union.}
\]
Similarly
\[
\bigcap_{i = 1}^{n}A_i = A_1 \cap A_2 \cap \dotsi \cap A_n\footnote{Finite intersection.}
\]

\begin{definition}[Power set]
    For a set $A$, its power set denoted by $2 ^ A$\footnote{or $\mathcal{P}(A)$} is the set containing all subsets of $A$.
    \[
    2 ^ A = \{B : B \subseteq A\}.
    \]
    $\emptyset,\,A \in 2 ^ A$.
\end{definition}


\subsection{Axiomatic Probability}
\begin{enumerate}[label = (\roman*)]
    \item Sample, space \& events

    Throw a  six-sided die, and note the score, that can be a number from $1$ to $6$.

    Outcomes are the score, which we can write as $1, 2, 3, 4, 5, 6$.

    The outcomes are collected in the sample space
    \begin{definition}[Sample space]
        The collection of all outcomes is called the sample space, denoted by $\Omega$.
        \[
        \Omega := \text{ collection of all possible outcomes}.
        \]
        and $\omega \in \Omega$, we say $\omega$\footnote{Also known as a sample point.} is an outcome.
    \end{definition}
    \begin{example}\phantom{}
    
        Throw a die, $\Omega = \{1, 2, 3, 4, 5, 6\}$.\footnote{$|\Omega| = 6$.}

        $6$ is an outcome.
    \end{example}

    In many examples, $\Omega$ will be finite or countably infinite

    \begin{definition}[Event]
        An event $A$ is a subset of the sample space $\Omega$.
    \end{definition}
    \begin{example} \phantom{}
    
        $A := \text{Even score} = \{2, 4, 6\} \subseteq \Omega$.

        $\emptyset := \text{Impossible event}.$ 
    \end{example}

    \begin{definition}[Probability]
        Associated to $\Omega$, there is a collection denoted by $\mathcal{F}$, which is the collection of all possible events.
        \[
        \mathcal{F} = \{A : A \subseteq \Omega\}.
        \]
        Discrete (for finite $\Omega$) we will take $\mathcal{F} = 2 ^ \Omega$.

        Probability is a measure of randomness that is associated that we assign to events $A \in \mathcal{F}$.
    \end{definition}
\end{enumerate}

Set operation is the same as event calculus.

\newpage

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Notations & Set theory language & Prob analogue & Meaning as events \\ [0.1em]
        \hline
        $A \cup B$ & $A$ union $B$ & $A$ or $B$ & $A$ occurs or $B$ occurs or both occur \\
        \hline
        $A \cap B$ & $A$ intersect $B$ & $A$ and $B$ & both $A$ and $B$ occur \\
        \hline
        $A^c$ & $A$ complement & not $A$ & $A$ does not occur \\
        $A \setminus B$ & $A$ set minus $B$ & $A$ but not $B$ & $A$ occurs but $B$ does not \\
        \hline
        $A \subseteq B$ & $A$ subset $B$ & $A$ implies $B$ & if $A$ occurs then $B$ also occurs \\
        \hline
        $\emptyset$ & null set & impossible event & \phantom{1} \\
        \hline
    \end{tabular}
    \caption{Set operations in event calculus}
\end{table}

$A \cap B = \emptyset$, $A$ and $B$ are mutually exclusive.

In order of the event calculus table here are the event notations for each thing. 

\begin{itemize}
    \item $A \cup B = \{\omega : \omega \in A \text{ or } \omega \in B\}$
    \item $A \cap B = \{\omega : \omega \in A \text{ and } \omega \in B\}$
    \item $A \setminus B = \{\omega : \omega \in A \text{ but } \omega \notin B\}$
    \item $A \subseteq B \text{ if } \omega \in A \implies \omega \in B$
\end{itemize}

\newpage

\textbf{De-Morgan's Laws}

\begin{enumerate}[label = (\roman*)]
    \item
    \[
    \left(\bigcup_{i = 1}^{n} A_i\right) ^ c = \bigcap_{i = 1}^{n} A_i ^ c
    \]
    \item
    \[
    \left(\bigcap_{i = 1}^{n} A_i\right) ^ c = \bigcup_{i = 1}^{n} A_i ^ c
    \]
\end{enumerate}

Non-examined
\begin{enumerate}[label = (\roman*)]
    \item
    \[
    \left(\bigcup_{n = 1}^{\infty} A_n\right) ^ c = \bigcap_{n = 1}^{\infty} A_n ^ c
    \]
    \item
    \[
    \left(\bigcap_{n = 1}^{\infty} A_n\right) ^ c = \bigcup_{n = 1}^{\infty} A_n ^ c
    \]
\end{enumerate}

\begin{definition}
    A probability "$\mathbb{P}$" on a sample space $\Omega$, with $\mathcal{F}$ (collection of events) is a mapping such that for every $A$ in $\mathcal{F}$, $\mathbb{P}(A)$ is a real number satisfying.
    \begin{enumerate}[label = A\arabic*]
        \item $\mathbb{P}(A) \geq 0$ (Prob is always non-negative)
        \item $\mathbb{P}(\Omega) = 1$ (Sample space has full probability)
        \item If $A, B \in \mathcal{F}$, and $A$ and $B$ are disjoint\footnote{$A$ and $B$ are mutually exclusive.}, that is $A \cap B = \emptyset$, then \[\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B)\]
        (finite additivity)
        \item (Countable additivity). $A_1, A_2, \dots$ are pairwise disjoint, i.e. $A_i \cap A_j = \emptyset$, for all $i \neq j$, then
        \[
        \mathbb{P}\left(\bigcup_{n = 1}^{\infty}A_n\right) = \sum_{n = 1}^{\infty}\mathbb{P}(A_n).
        \]
    \end{enumerate}
\end{definition}

$(\Omega, \mathcal{F}, \mathbb{P})$, this triplet is known as the probability space.

\begin{example}
    $\Omega = \{\omega_1, \omega_2, \dots, \omega_m\}$, let $\omega_i$'s $1 \leq i \leq m$ be distinct. $A\subseteq \Omega,\ \mathbb{P}(A)$. $A = \{\omega_1, \omega_2, \omega_3\}$.
    \[
    \{\omega_1\} \subseteq \Omega, \{\omega_2\} \subseteq \Omega, \{\omega_3\} \subseteq \Omega.
    \]
    Therefore by (A3)
    \[
    \mathbb{P}(A) = \mathbb{P}(\{\omega_1\} \cup \{\omega_2\} \cup \{\omega_3\}) = \sum_{i = 1}^{3}\mathbb{P}(\{\omega_i\}).
    \]
\end{example}

\textbf{Equally likely outcomes}

\[
\mathbb{P}(\{\omega_i\}) = p_i \geq 0,\quad 1 \leq i \leq m,
\]
such that $\sum_{i = 1}^{m}p_i = 1$. $\Omega = \bigcup_{i = 1}^{m}\{\omega_i\}$, therefore $\mathbb{P}(\Omega) =\footnote{From A3.} \sum_{i = 1}^{m}\mathbb{P}(\{\omega_i\}) = 1\footnote{From A2.}$

Then $A \subseteq \Omega$,
\[
\mathbb{P}(A) = \sum_{i : \omega_i \in A}p_i
\]

Equally likely outcomes it means that $p_i$ are all equal and in this example $p_1 = p_2 = \dotsi = p_m = \frac{1}{m}$

\begin{example}
    Throw a fair die

    $\Omega = \{1, 2, \dots, 6\}$

    Fair = equally likely
    \[
    \mathbb{P}(\{1\}) = \mathbb{P}(\{2\}) = \dotsi = \mathbb{P}(\{6\}) = \frac{1}{6}
    \]
    $A = \{\text{even score}\} = \{2, 4, 6\}$
    \[
    \mathbb{P}(A) = \sum_{i : \omega_i \in A}p_i = \frac{1}{6}|A| = \frac{3}{6} = \frac{1}{2}.
    \]
\end{example}


\begin{definition}[Partition]
    $E_1, E_2,\dotsc,E_k \in \mathcal{F}$ for a finite partition of the sample space $\Omega$, if
    \begin{enumerate}[label = (\roman*)]
        \item $\mathbb{P}(E_i) > 0$ for all $1 \leq i \leq k$,
        \item $E_i\,\&\,E_j$ are pairwise disjoint, i.e. $\forall i \neq j,\,E_i \cap E_j = \emptyset$,
        \item $\displaystyle\Omega = \bigcup_{i = 1}^{k}E_i$.
    \end{enumerate}
\end{definition}
Easily, we can see that
\[
\sum_{i = 1}^{k} \mathbb{P}(E_i) = 1.
\]

\[
\mathbb{P}(\Omega) = \bigcup_{i = 1}^{k}(E_i) \implies 1 = \sum_{i = 1}^{k}\mathbb{P}(E_i)\qquad\text{by A3}
\]


\subsubsection{Sigma algebra}
\begin{definition}
    $\mathcal{F}$, a collection of subsets of $\Omega$ is called a $\sigma$-algebra, if it satisfies
    \begin{enumerate}[label = (S\arabic*)]
        \item $\Omega \in \mathcal{F}$
        \item $A \in \mathcal{F}$, then $A ^ c \in \mathcal{F}$.
        \item if $\underbrace{A_1,\,A_2,\,\dotsc}_{\text{countably infinite}} \in \mathcal{F}$, then $\bigcup_{n = 1}^{\infty}A_n \in \mathcal{F}$.
    \end{enumerate}
\end{definition}

\subsection{Consequences of (A1-A3)}

\begin{proposition} \phantom{} \\
    \begin{enumerate}[label = (C\arabic*)]
        \item $A, B \in \mathcal{F}$, then $\mathbb{P}(B \setminus A) = \mathbb{P}(B) - \mathbb{P}(A \cap B)$.
        \item $A \in \mathcal{F}$, $\mathbb{P}(A ^ c) = 1 - \mathbb{P}(A)$ (A2 + A3)
        \item $\mathbb{P}(\emptyset) = 0$
        \item $\mathbb{P}(A) \leq 1,\quad\text{for } A \in \mathcal{F}$ (Hint: use C2)
        \item Monotonicity: $A \subseteq B$
        \[
        \mathbb{P}(A) \leq \mathbb{P}(B)
        \]
        \item $\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)$
        \item Finite additivity: $A_1, A_2,\dotsc, A_k$ pairwise mutually exclusive
        \[
        \mathbb{P}\left(\bigcup_{i = 1}^{k}\right) = \sum_{i = 1}^{k}\mathbb{P}(A)
        \]
        \item Boole's inequality (if they are not mutually exclusive):
        \[
        \mathbb{P}\left(\bigcup_{i = 1}^{n}A_i\right) \leq \sum_{i = 1}^{n}\mathbb{P}(A_i)
        \]
    \end{enumerate}
    \begin{proof}
        C2:
        $A \cup A ^ c = \Omega$\quad$A \cap A ^ C = \emptyset$

        $\mathbb{P}(A \cup A ^ c) = 1$ (from A2)
        (from A3)
        $\mathbb{P}(A) + \mathbb{P}(A ^ c) = 1$

        C3:
        $\mathbb{P}(\emptyset) = 1 - \mathbb{P}(\Omega) = 1 - 1 = 0$ (by A3)

        C4:
        From 1, $\mathbb{P}(A) + \mathbb{P}(A ^ c) = 1$
        
        $\mathbb{P}(A) \geq 0,\,\mathbb{P}(A ^ c) \geq 0$, hence $\mathbb{P}(A) \leq 1$
    \end{proof}
\end{proposition}

\section{Counting}

\subsection{Equally likely outcomes}
Finite sample space $\Omega = \{\omega_1,\omega_2,\dotsc, \omega_m\}$, $\mathbb{P}(\omega_1) = \mathbb{P}(\omega_2) = \mathbb{P}(\omega_m) = \frac{1}{m}$\footnote{$\mathbb{P}(\{\omega_i\}) = \mathbb{P}(\omega_i)$}, $A$, $\mathbb{P}(A) = \frac{|A|}{m} = \frac{|A|}{|\Omega|}$

\begin{countprinc}[Multiplication principle]
    $k$ choices in succession, where
    \begin{enumerate}[label = (\roman*)]
        \item $m_1$, possibility of first choice
        \item $m_2$, possibility of second choice \\ \vdots
        \item $m_k$, possibility of $k$th choice
    \end{enumerate}
    and at each stage, our choices are not affected by the past.

    The total number of distinct choices = $\underbrace{m_1 \times m_2 \times \dotsi \times m_k}_{k \text{ times}} = \prod_{i = 1}^{k}m_i$.
\end{countprinc}

\begin{example}
    $3$ choices for breakfast, $4$ for lunch and $5$ for dinner. The total number of choices is
    \[
    3 \times 4 \times 5.
    \]
\end{example}

\begin{example}
    Debit Card Pins
    \begin{enumerate}[label = (\roman*)]
        \item Cannot be the same digit at all $4$ places.
        \item Cannot be an increasing or decreasing sequence of consecutive.
    \end{enumerate}

    $A := \text{ Number of possible PINs}$
    
    $A^c := \text{ Number of not possible PINs}$

    The number of sequences satisfying (i) is $10$.

    The number of (ii) increasing consecutive sequences can start with $0,\, 1,\, 2,\,\dotsc,\, 6$ implies $7$ options. Similarly, decreasing consecutive sequences can start with $9,\,8,\,7,\,\dotsc,\,3$.

    The total number of choices satisfying (ii) is $14$.

    $|A ^ c| = 10 + 14 = 24$.

    The number of unrestricted choices $= 10 ^ 4$.

    $|A| = 10 ^ 4 - 24$.
\end{example}

\begin{countprinc}[Ordered choice of distinct objects with replacement]
    $m$ distinct objects, choose $r$ of them with replacement, then the number of different ordered lists (i.e. $r$-tuples) is $\underbrace{m \times m \times \dotsi \times m}_{r \text{ choices}} = m ^ r$.
\end{countprinc}

\begin{countprinc}[Ordered choices of distinct objects without replacement]
    $m$ distinct objects, choose $r$ ($r \leq m$) of them without replacement, then the number of such choices is denoted by $(m)_r$, where $(m)_r = m \times (m - 1) \times \dotsc \times (m - r + 1) = \frac{m!}{(m - r)!}$.
\end{countprinc}

\begin{example}[Birthday Problem]
    There are $n$ people, a year is $365$ days,
    \[
    B = \{\text{At least two people have the same birthday}\}
    \]
    Find $\mathbb{P}(B)$.

    $B ^ c = \{\text{No two people have the same birth day}\}$

    Birthdays of the $n$ people $(D_1, D_2, D_3, \dotsc, D_n)$

    $\Omega$ is the collection of all such tuples

    $|\Omega| = 365 ^ n$

    Let $B_i$ denote the event that exactly $i$ people have the same birthday. Then $B = \displaystyle \bigcup_{i = 1}^{n - 1}B_i$.

    \[
    B ^ c = \bigcap_{i = 1}^{n - 1}B_i ^ c
    \]
    $|B ^ c| = (365)_n$
    \[
    \mathbb{P}(B ^ c) = \frac{(365)_n}{(365) ^ n}, \text{ hence } \mathbb{P}(B) = 1 - \frac{(365)_n}{(365) ^ n}
    \]
\end{example}

\begin{countprinc}
    [Unordered choices of distinct objects without replacement]
    $m$ objects, select $r \leq m$ of them without replacement, the number of distinct subsets that we can form.
    \[
    \binom{m}{r}\footnote{Or $C_r ^ m$} = \frac{m(m - 1)\dotsi(m - r + 1)}{r!} = \frac{m!}{r!(m - r)!}
    \]
    this is read as "$m$ choose $r$".
\end{countprinc}

\[
\binom{m}{r} = \binom{m}{m - r}.
\]

\begin{countprinc}[Ordered choices of two types of objects]
    $m$ objects, $r$ objects of type one, $(m - r)$ of type two. The number of distinct ordered choices is $\binom{m}{r}$.
\end{countprinc}

\begin{example}
    Toss a "fair" coin $7$ times.

    $E := \{\text{There are a total of $3$ heads}\}$.

    Find $\mathbb{P}(E)$

    The number of all possible choices is $|\Omega| = 2 ^ 7$.

    The number of possible outcomes in $E$ are $\binom{7}{3}$
    \[
    \mathbb{P}(E) = \frac{|E|}{|\Omega|} = \frac{\binom{7}{3}}{2 ^ 7}
    \]
\end{example}

\begin{countprinc}[Ordered grouping of indistinguishable objects]
    Suppose we have $m$ indistinguishable objects and $k$ groups. The number of all possible choices is
    \[
    \binom{m + k - 1}{m} = \binom{m + k - 1}{k - 1}.
    \]
\end{countprinc}

\newpage

\section{Conditional probability and independence}

\subsection{Conditional probability}

\begin{definition}
    Let $A,\, B \subseteq \Omega$ be $2$ events. We defined conditional probability of $A$ given $B$, and denote it by $\mathbb{P}(A | B)$, as follows
    \[
    \mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)},\text{ when } \mathbb{P}(B) > 0.
    \]
\end{definition}
The probability of $A$ when we have observed $B$.

\begin{example}
    Roll a fair die.

    $A = \{\text{Score is odd}\} = \{1, 3, 5\}$
    
    $A = \{\text{Score is at most $3$}\} = \{1, 2, 3\}$
    \[
    \mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
    \]
    \[
    A \cap B = \{1, 3\},\quad\mathbb{P}(A \cap B) = \frac{2}{6} = \frac{1}{3} 
    \]
    $\mathbb{P}(B) = \frac{3}{6} = \frac{1}{2}$
    \[
    \mathbb{P}(A | B) = \frac{\frac{1}{3}}{\frac{1}{2}} = \frac{2}{3}
    \]
\end{example}

\begin{example}
    A family has $2$ children, both sexes are equally likely.
    \[
    \Omega = \{\text{BB, BG, GB, GG}\}
    \]
    $A_1 = \{\text{Both girls}\} = \{\text{GG}\} = \frac{1}{4}$
    
    $A_2 = \{\text{At least one girl}\} = \{\text{BG, GB, GG}\} = \frac{3}{4}$

    $A_3 = \{\text{Eldest is a girl}\} = \{\text{GB, GG}\} = \frac{2}{4} = \frac{1}{2}$
    \[
    \mathbb{P}(A) = \frac{|A|}{|\Omega|}
    \]
    \[
    \mathbb{P}(A_1 | A_2) = \frac{\mathbb{P}(A_1 \cap A_2)}{\mathbb{P}(A_2)}
    \]
    $A_1 \subseteq A_2 \implies A_1 \cap A_2 = A_1$. Applying this, we get
    \[
    \mathbb{P}(A_1 | A_2) = \frac{\mathbb{P}(A_1)}{\mathbb{P}(A_2)} = \frac{\frac{1}{4}}{\frac{3}{4}} = \frac{1}{3}.
    \]
    \[
    \mathbb{P}(A_2 | A_1) = \frac{\mathbb{P}(A_1 \cap A_2)}{\mathbb{P}(A_1)} = \frac{\mathbb{P}(A_1)}{\mathbb{P}(A_1)} = 1.
    \]
\end{example}

\subsection{Properties of conditional probability}
\begin{enumerate}[label = P\arabic*]
    \item Let $B$ be an event, such that $\mathbb{P}(B) > 0$. Then $\mathbb{P}(\cdot | B)$ satisfies axioms (A1) - (A3)
\end{enumerate}

We have $(\Omega, \mathcal{F}, \mathbb{P})$

Given the event $B$ with $\mathbb{P}(B) > 0$, let us define

$\mathbb{P}_B : \mathcal{F} \rightarrow \R$, such that for every $A \in \mathcal{F},\,\mathbb{P}_B(A) := \mathbb{P}(A | B)$

$\mathbb{P}_B$ is a probability on $(\Omega, \mathcal{F})$ i.e. $\mathbb{P}_B$ satisfies A1, A2 and A3.

(A1) $\mathcal{P}_B(A) \geq 0$, for any $A \in \mathcal{F}$.
(A1) $\mathcal{P}_B(A) = 1$.

$\mathbb{P}(\cdot | B) \iff \mathbb{P}_B(A) := \mathbb{P}(A | B)$

\begin{enumerate}[label = A\arabic*]
    \item $\mathbb{P}_B(A) \geq 0$
    \item $\mathbb{P}_B(\Omega) = 1$
    \item Let $A_1, A_2$ be $2$ events in $\mathcal{F}$, such that, $A_1 \cap A_2 = \emptyset$, then
    \[
    \mathbb{P}_B(A_1 \cup A_2) = \mathbb{P}_B(A_1) + \mathbb{P}_B(A_2).
    \]
    \begin{proof}
        A3

        \begin{gather}
        \mathbb{P}_B(A_1 \cup A_2) = \frac{\mathbb{P}((A_1 \cup A_2) \cap B}{\mathbb{P}(B)}          
        \end{gather}
        Since $A_1 \cap A_2 = \emptyset$, $(A_1 \cap B) \cap (A_2 \cap B) = \emptyset$, therefore we have
        \begin{align*}
            \mathbb{P}((A_1 \cap A_2) \cap B) &= \mathbb{P}((A_1 \cap B) \cup (A_2 \cap B)) \\
            &= \mathbb{P}(A_1 \cap B) + \mathbb{P}(A_2 \cap B)
        \end{align*}
        $(A_1 \cap B)$ and $(A_2 \cap B)$ are mutually exclusive. Putting this into (1), we get
        \begin{align*}
        \mathbb{P}(A_1 \cup A_2) &= \frac{\mathbb{P}(A_1 \cap B) + \mathbb{P}(A_2 \cap B)}{\mathbb{P}(b)} \\
        &= \frac{\mathbb{P}(A_1 \cap B)}{\mathbb{P}(B)} + \frac{\mathbb{P}(A_2 \cap B)}{\mathbb{P}(B)} \\
        &= \mathbb{P}_B(A_1) + \mathbb{P}_B(A_2).
        \end{align*}
    \end{proof}
\end{enumerate}

Now that we have established that $\mathbb{P}(\cdot | B)$ is a probability, it should satisfy all the consequences.
\begin{enumerate}[label = C\arabic*]
    \item \phantom{}
    \item $\mathbb{P}(A ^ c | B) = 1 - \mathbb{P}(A | B)$

    Recall C2, $A \in \mathcal{F}, \mathbb{P}(A ^ c) = 1 - \mathbb{P}(A)$
    \item \phantom{}
    \item \phantom{}
    \item If $A_1 \subseteq A_2,\, \mathbb{P}_B(A_1) \leq \mathbb{P}_B(A_2)$ or $\mathbb{P}(A_1 | B) \leq \mathbb{P}(A_2 | B)$.
    \item $\mathbb{P}(A_1 \cup A_2 | B) = \mathbb{P}(A_1 | B) + \mathbb{P}(A_2 | B) - \mathbb{P}(A_1 \cup A_2 | B)$
    \item If $A_1, A_2,\dotsc, A_n$ are pairwise disjoint, then
    \[
    \mathbb{P}\left(\bigcup_{i = 1}^{n}A_i|B\right) = \sum_{i = 1}^{n}\mathbb{P}(A_i | B)
    \]
\end{enumerate}

P2: The multiplication rule

For any $2$ events $A, B \in \mathcal{F}$, with $\mathbb{P}(A) \neq 0$, $\mathbb{P}(B) \neq 0$, we have
\[
\mathbb{P}(A \cap B) = \mathbb{P}(A | B)\mathbb{P}(B) = \mathbb{P}(B | A)\mathbb{P}(A)
\]
\begin{proof}
    of the first equality.
    
    Since $\mathbb{P}(A) \neq 0$ and $\mathbb{P} \neq 0$, we can define $\mathbb{P}(B | A)$ and $\mathbb{P}(A | B)$.

    By definition
    \[
    \mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
    \]
    \[
    \mathbb{P}(A \cap B) = \mathbb{P}(A | B)\mathbb{P}(B)
    \]
\end{proof}

More general statement, $\mathbb{P}(A \cap B) | C = \mathbb{P}(B | C)\mathbb{P}(A | B \cap C)$, if $\mathbb{P}(B \cap C) > 0$,.

In the statement, it is enough to assume $\mathbb{P}(B \cap C) > 0$. Because $B \cap C \leq C$, $\mathbb{P}(B \cap C) \leq \mathbb{P}(C)$ follows from monotonicity (C5). Therefore $\mathbb{P}(C) > 0$, and hence we can define conditional probability of events, given $C$.

P3, general multiplication rule. Let $A_0, A_1, \dotsc, A_k$ be events such that
\[
\mathbb{P}\left(\bigcap_{i = 1}^{k}A_i\right) > 0\text{, then}
\]
\[
\mathbb{P}\left(\bigcap_{i = 1}^{k}A_i \middle| A_0\right) = \mathbb{P}(A_1 | A_0)\mathbb{P}(A_2 | A_1 \cap A_0)\mathbb{P}(A_3 | A_2 \cap A_1 \cap A_0) \dotsi \mathbb{P}\left(A_k \middle| \bigcap_{i = 0}^{k - 1}A_i\right)
\]

Recall (what does it mean for $E_i$ to be a partition:
\[
\bigcup_{i = 1}^{k}E_i = \Omega,\text{ where } E_i \cap E_j = \emptyset,\,\forall i, j (i \neq j)
\]
and $\mathbb{P}(E_i) > 0$, for each $1 \leq i \leq k$.

P4:
\begin{theorem}[Partition theorem (Law of total probability]
    Let $E_1, E_2, \dotsc, E_k$ be a partition of $\Omega$. Then for any event $A$, we have
    \[
    \mathbb{P}(A) = \sum_{i = 1}^{k}\mathbb{P}(E_i)\mathbb{P}(A | E_i).
    \]
    \begin{proof}
        \[
        A = \bigcup_{i = 1}^{k}(A \cap E_i)
        \]
        Since $E_i$'s form a partition,
        \[
        (A \cap E_i) \cap (A \cap E_j) = \emptyset, \text{ for all } i \neq j.
        \]
        Hence, we can use A3, to get
        \[
        \mathbb{P}(A) = \sum_{i = 1}^{k}\mathbb{P}(A \cap E_i).
        \]
        Now use (P2) on the previous to get
        \[
        \mathbb{P}(A) = \sum_{i = 1}^{k}\mathbb{P}(E_i)\mathbb{P}(A | E_i).
        \]
    \end{proof}
\end{theorem}

\begin{example}
    There are $3$ machines $A,\, B$ and $C$. $10\%$ of components produced by $A$ are faulty, $20\%$ of $B$ are faulty and $30\%$ of $C$ are faulty.
    Equal number of components are collected from each machine. One component is selected at random. Find $\mathbb{P}(\text{the component is faulty}$.

    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        Let $F := \{\text{the component is faulty}\}$

        Let $M_A := \{\text{component comes from $A$}\}$

        Similarly define $M_B$ and $M_C$.
        \[
        \mathbb{P}(M_A) = \mathbb{P}(M_B) = \mathbb{P}(M_C) = \frac{1}{3}.
        \]
        \[
        \mathbb{P}(F | M_A) = \frac{10}{100},\,\mathbb{P}(F | M_B) = \frac{20}{200},\,\mathbb{P}(F | M_C) = \frac{30}{100}.
        \]
        \begin{align*}
        \mathbb{P}(F) &= \mathbb{P}(M_A)\mathbb{P}(F | M_A) + \mathbb{P}(M_B)\mathbb{P}(F | M_B) + \mathbb{P}(M_C)\mathbb{P}(F | M_C) \\
        &= \frac{1}{3}\times \frac{10 + 20 + 30}{100} = \frac{2}{10} = 0.2.
        \end{align*}
    \end{proof}
\end{example}

P5:
\begin{theorem}[Bayes' Theorem]
    Let $A$ and $B$ be two events, such that $\mathbb{P}(A) > 0,\, \mathbb{P}(A) > 0$, then
    \[
    \mathbb{P}(A | B) = \frac{\mathbb{P}(B | A)\mathbb{P}(A)}{\mathbb{P}(B)}.
    \]

    Bayes' Theorem (general)
    Let $A,\, B$ and $C$ be three events, such that $\mathbb{P}(A | C) > 0,\, \mathbb{P}(B | C) > 0$, then
    \[
    \mathbb{P}(A | B \cap C) = \frac{\mathbb{P}(A | C)\mathbb{P}(B | A \cap C)}{\mathbb{P}(B | C)}.
    \]
\end{theorem}

\begin{example}
    Find $\mathbb{P}(M_A | F)$
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
    \[
    \mathbb{P}(M_A | F) = \frac{\mathbb{P}(F | M_A)\mathbb{P}(M_A)}{\mathbb{P}(F)} = \frac{\frac{10}{100} \times \frac{1}{3}}{\frac{2}{10}} = \frac{1}{6}.
    \]
    \end{proof}
\end{example}

P6:
\begin{theorem}[Bayes' theorem (for partitions)]
   Let $A_1, \dotsc, A_k$ be a partition and let $B$ be an event, with $\mathbb{P}(B) > 0$. Then
   \[
   \mathbb{P}(A_j | B) = \frac{\mathbb{P}(A_j)\mathbb{P}(B | A_j)}{\sum_{i = 1}^{k}\mathbb{P}(B | A_i)\mathbb{P}(A_i)}
   \]
   \begin{proof}
   \[
   \mathbb{P}(A_j | B) = \frac{\mathbb{P}(A_j)\mathbb{P}(B | A_j)}{\mathbb{P}(B)}
   \]
   now use partition theorem to get
   \[
   \mathbb{P}(B) = \sum_{i = 1}^{k}\mathbb{P}(B | A_i)\mathbb{P}(A_i).
   \]
   \end{proof}
\end{theorem}


\begin{example}
    $\mathbb{P}(\text{it rains}) = \frac{1}{2}$ if it rains, Charlie the cat will go out with probability $\frac{1}{10}$.
    If it is day with probability $\frac{3}{5}$.
    If Charlie goes out, find the probability that it rained.
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
       Let $R := \{\text{it rains}\}$

       $C := \{\text{Charlie goes out}\}$

       \[
       \mathbb{P}(R) = \frac{1}{2},
       \]
       we want to find $\mathbb{P}(R | C)$.
       \[
       \mathbb{P}(R | C) = \frac{\mathbb{P}(R)\mathbb{P}(C | R)}{\mathbb{P}(C)} = \frac{\frac{1}{2} \times \frac{1}{10}}{\mathbb{P}(C)}.
       \]
       \begin{align*}
           \mathbb{P}(C) &= \mathbb{P}(C | R)\mathbb{P}( R) + \mathbb{P}(C | R ^ c)\mathbb{P}(R ^ c) \\
           &= \frac{1}{10} \times \frac{1}{2} + \frac{3}{5} \times \frac{1}{2} \\
           &= \frac{1}{2} \times \frac{7}{10}
       \end{align*}
       \[
       \mathbb{P}(R | C) = \frac{1}{7}.
       \]
    \end{proof}
\end{example}

\subsection{Independence of events}

\begin{definition}[Independence of two events]
    Let $A, B$ be $2$ events, then $A$ and $B$ are said to be independent if
    \[
    \mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B).
    \]
\end{definition}
Recall: $\mathbb{P}(A \cap B) = \mathbb{P}(A | B)\mathbb{P}(B)$. Then what the above formula is telling us is,
\[
\mathbb{P}(A | B) = \mathbb{P}(A).
\]

\begin{example}
    Roll a fair die.
    \[
    A_1 = \{2, 4, 6\}(\text{even score}\}\quad A_2 = \{3, 6\}(\text{score divisible by $3$})
    \]
    \[
    \mathbb{P}(A_1 \cap A_2) = \mathbb{P}(\{6\}) = \frac{1}{6}.
    \]
    \[
    \mathbb{P}(A_2)\mathbb{P}(A_2) = \frac{3}{6} \times \frac{2}{6} = \frac{1}{6} = \mathbb{P}(A_1 \cap A_2).
    \]
     Hence, $A_1$ and $A_2$ are independent.

     $A_3 = \{4, 5, 6\}$. Show that $A_1$ and $A_3$ are not independent.
\end{example}

Independence cannot be observed in a Venn diagram.

\begin{theorem}
    Let $A$ and $B$ be two events, with $\mathbb{P}(A) > 0,\,\mathbb{P}(B) > 0$, then the following are equivalent.
    \begin{enumerate}[label = (\roman*)]
        \item $\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$.
        \item $\mathbb{P}(A | B) = \mathbb{P}(A)$.
        \item $\mathbb{P}(B | A) = \mathbb{P}(B)$.
    \end{enumerate}
    \begin{proof}
        We have just proved (i) $\implies$ (ii)    
        proof of (ii) $\implies$ (iii):
        \begin{equation}
        \mathbb{P}(B | A)\mathbb{P}(A) = \mathbb{P}(A \cap B) = \mathbb{P}(A | B)\mathbb{P}(B).
        \end{equation}
        From (2) we get
        \[
        \mathbb{P}(B | A)\mathbb{P}(A) = \mathbb{P}(A | B)\mathbb{P}(B)
        \]
        Since we assume (ii), we have $\mathbb{P}(A | B) = \mathbb{P}(A)$ so,  we get $\mathbb{P}(B | A)\mathbb{P}(A) = \mathbb{P}(A)\mathbb{P}(B)$ or, $\mathbb{P}(B | A) = \mathbb{P}(B)$.
    \end{proof}
\end{theorem}

\begin{theorem}
    Let $A,\, B,\, C$ be $3$ events, such that $\mathbb{P}(A \cap B \cap C) > 0$, then the following are equivalent\footnote{This tells us $A$ and $B$ are conditionally independent of $C$.}
    \begin{enumerate}[label = (\roman*)]
        \item $\mathbb{P}(A \cap B | C) = \mathbb{P}(A | C)\mathbb{P}(B | C)$,
        \item $\mathbb{P}(A | B \cap C) = \mathbb{P}(A | C)$,
        \item $\mathbb{P}(B | A \cap C) = \mathbb{P}(B | C)$.
    \end{enumerate}
\end{theorem}

\textbf{Independence of multiple events}
\begin{definition}
    Let $A_1, A_2, \dotsc, A_n$ be events, then they are mutually independent, if for every finite subcollection\footnote{Effectively equivalent to finite subsets.} $\mathcal{C}$ of indices $\{1, 2, \dotsc, n\}$,
    \[
    \mathbb{P}\left(\bigcap_{i \in \mathcal{C}}A_i\right) = \prod_{i \in \mathcal{C}}\mathbb{P}(A_i).
    \]
\end{definition}

\newpage

\section{Applications}

\subsection{Reliability of networks}
Reliability network/current flow.

Series: current flows from left to right. 

Parallel: current flows from left to right.

Assumption: current flows/system works from left to right,
system works if we can draw arrows from left to right.

Taking a series circuit (with two components, $1$ and $2$).
The system is functional if both components ($1$) and ($2$) are working

Let $w_i := \text{event that $i$th component is working}$

Let $S := \text{event system is working}$

$S = W_1 \cap W_2$.

\begin{definition}
    A reliability network is a diagram of arcs and nodes.
\end{definition}

Let $W_i$ be the event that the $i^{\text{th}}$ component is working.

We say that the system is working if we can go from left to right using working components only.

Let $S$ denote the event that the system is working.

Let $p_i$ be the probability that the $i^{\text{th}}$ component is working,
that is,
$\mathbb{P}(W_i) = p_i$.

(A) Series case:
System is working if both (1) and (2) are working.
$S = W_1 \cap W_2$. \\

(B) Parallel case:
System is working if either (1) or (2) (or both) is working.
$S = W_1 \cup W_2$. \\

(C) Combination case:
System is working if both (1) and (2) are working or (3) and (4) are working or both.
$S = (W_1 \cap W_2) \cup (W_3 \cap W_4)$.

Assumption: The components work independent of each other.

We want to find
$\P(\text{System is working)}) = \P(S).$

Recall: $\P(A \cap B) = \P(A)\P(B)$ if $A$ and $B$ are independent.

\begin{align*}
    \P(S) &= \P(W_1 \cup W_2) \\
    &= \P(W_1) + \P(W_2) - \P(W_1 \cap W_2) \\
    &= p_1 + p_2 - p_1p_2\footnotemark
\end{align*}
\footnotetext{By C6: $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$.}

(C)
\begin{align*}
    \P(S) &= \P((W_1 \cap W_2) \cup (W_3 \cap W_4)) \\
    &= \P(W_1 \cap W_2) + \P(W_3 \cap W_4) - \P((W_1 \cap W_2) \cap (W_3 \cap W_4)) \\
    &= p_1p_2 + p_3p_4 - p_1p_2p_3p_4\footnotemark
\end{align*}
\footnotetext{Because they are independent.}

\begin{example}
    Find $\P(S)$ for the following diagram
    
    \textbf{Gr12}
\end{example}


\subsection{Genetics}
\begin{example}
    A gene $X$ can have either allele $A$ or $a$.
\end{example}

Female: $XX$.

Male: $XY$.

Usually $A$ means dominant gene,
and $a$ means recessive gene.

A male child gets $X$ from mother and $Y$ from father.

So the allele on the male child's $X$ chromosome is from the mother.
So equal chance to inherit either of the alleles.


\begin{example}
    $aa$ is unhealthy (female).
    $a$ is unhealthy (male).
    
    Jane is healthy. Maternal aunt of Jane has an unhealthy son. ($a$ - allele). Jane's maternal grandparent's and father are healthy.
    \begin{enumerate}[label = (\roman*)]
        \item Find $\P(\text{Jane is } Aa)$.
    \end{enumerate}

    \textbf{Gr13}
    
    Let $J := \{\text{Jane is } Aa\}$.
    
    Let $M_1 := \{\text{Jane's mother is } AA\}$.
    
    Let $M_2 := \{\text{Jane's mother is } Aa\}$.

    Let $J := \{\text{Jane is } Aa\}$

    \[
    \P(M_2) = \P(\text{Jane's mother got $a$ from her mother}) = \frac{1}{2}.
    \]
    
    \begin{align*}
    \P(J) &= \underbrace{\P(J | M_1)}_{= 0}\P(M_1) + \P(J | M_2)\P(M_2) \\
    &= \P(J | M_2)\P(M_2) \\
    &= \frac{1}{2} \cdot \frac{1}{2}.
    \end{align*}
\end{example}

\newpage

\section{Random variables}

\subsection{Definition and notation}

Notation:

$X, Y$ is a general random variable.

\begin{example}[label = exmp1]
    Toss a coin three times. Let $X$ denote the number of heads.

    $H$ - Heads
    
    $T$ - Tails

    $\Omega = \{(H, H, H), (H, H, T), (H, T, H), (H, T, T), (T, H, H), (T, H, T), (T, T, H), (T, T, T)\}$.

    \begin{table}[h!]
        \centering
        \begin{tabular}{c|cccccccc}
             $\omega$ & $HHH$ & $HHT$ & $HTH$ & $HTT$ & $THT$ & $TTH$ & $THH$ & $TTT$ \\
             \hline
             $X(\omega)$ & 3 & 2 & 2 & 1 & 1 & 1 & 2 & 0 
        \end{tabular}
    \end{table}
\end{example}

$X : \Omega \rightarrow \R$
$X$ is a map from $\omega \mapsto X(\omega) \in \R$

\begin{definition}[Random variable]
    A random variable on $\Omega$ is a mapping from $\Omega$ to some set of possible values,
    which we denote by $X(\Omega) = \{X(\omega) : \omega \in \Omega\}$
    \footnote{We will most be concerned with $X(\Omega) \subseteq \R$ or $X(\Omega) \subseteq \R ^ d$, $X$ is real-values or vector valued.}.
\end{definition}

$X : \Omega \rightarrow \chi$.
For our examples
$\chi = \R \text{ or } \R ^ d$.

The image of $X$ is denoted by $X(\Omega) = \{X(\omega) : \omega \in \Omega\}$.
\begin{example}
    Throw $2$ "fair" dice and let $X$ be the random variable denoting sum of the scores.
    \[
    \Omega = \{(1, 1), (1, 2), \dotsc, (1, 6), (2, 1), (2, 2), \dotsc\} = \{(i, j) : 1 \leq i \leq 6,\, 1 \leq j \leq 6\}.
    \]
    $|\Omega| = 6 ^ 2$
    \begin{align*}
        X : \Omega &\rightarrow \R\qquad (i, j) \mapsto X(i, j) = i + j \\
        \omega &\mapsto X(\omega)
    \end{align*}
    where $i, j$ are the scores for the first and second dice respectively.
\end{example}

Let $f : \Delta \rightarrow\R$, then the pre-image for a set $B \subseteq \R$,
is defined as $f ^ {-1}(B) := \{x \in \Delta : f(x) \in B\}$.

\begin{example}[continues = exmp1]
    Let $A_1 = \{X = 2\}$
    
    $A_2 = \{X \in [0, 1.5]\}$
    
    $A_3 = \{2.5 \leq X \leq 4.5\}$.

    $A_1 = \{\omega : X(\omega) = 2\} = X ^ {-1} = \{\text{HHT, HTH, THH}\} \subseteq \Omega$.
    \[
    \P(A_1) = \frac{|A_1|}{|\Omega|} = \frac{3}{2 ^ 3} = \frac{3}{8}
    \]

    $A_2 = \{\omega : 0 \leq X(\omega) \leq 1.5\} = X ^ {-1}([0, 1.5]) = \{\omega : X(\omega) = 0\text{ or }X(\omega) = 1\} = \{TTT, HTT, THT, TTH\}$
    \[
    \P(A_2) = \frac{|A_2|}{|\Omega|} = \frac{4}{2 ^ 3} = \frac{1}{2}
    \]
\end{example}
\[
\P_X(B) := \P(X \in B) = \P\left(X ^ {-1}(B)\right)
\]
\textit{Note: For us $\chi$ will be mostly in $\R$ or $\R ^ d$, or subsets of $\R$ and $\R ^ d$}

\begin{definition}
    The function $P_X : X(\Omega) \rightarrow [0, 1]$ defined by $\P_X(B) := \P(X \in B) = \P(X^{-1}(B) = \P(\{\omega : X(\omega) \in B\})$
    for any set $B \subseteq X(\Omega)$.
    $\P_X$ is called the distribution\footnote{Or cumulative density/distribution function.} of $X$.
\end{definition}

\begin{theorem}
    Let $X: \Omega \rightarrow \R$ be a random variable.
    Then $\P_X$ is a probability on $\R$.
    \begin{proof}
        A1, A2, A3 (prove A4)

        \begin{enumerate}[label = (A\arabic*)]
            \item
            \item $\P_X(\R) = 1$
            $P_X(\R) := \P(X ^ {-1}(\R)) = \P(\Omega) = 1$

            $X ^ {-1}(\R) := \{\omega : X(\omega) \in \R\} = \Omega$.
            \item Let $A, B \subseteq \R$, such that $A \cap B = \emptyset$, then
            \[
            P_X(A \cup B) = \P_X(A) + \P_X(B).
            \]
            \[
            X^{-1}(A \cup B) = X^{-1}(A) \cup X^{-1}(B)?
            \]
            Since $A \cap B = \emptyset$ is it true that $X^{-1}(A)\cap X^{-1}(B) = \emptyset$
        \end{enumerate}
    \end{proof}
\end{theorem}
\begin{align*}
P_X(A \cup B) &= \P(X ^ {-1}(X \cup B)) \\
&= \P(X ^ {-1}(A) \cup X ^ {-1}(B)) \\
&= \P(X ^ {-1}(A) + \P(X ^ {-1}(B))) \\
&= \P_X(A) + \P_X(B).
\end{align*}
Let $A$ and $B$ be disjoint, i.e. $A \cap B = \emptyset$,
then
\[
X ^ {-1}(A) \cap X ^ {-1}(B) = \emptyset
\]
\begin{proof}
    Let us prove by contradiction,
    that is, let us assume that $X ^ {-1}(A) \cap X ^ {-1}(B) \neq \emptyset$,
    and then we will arrive at a contradiction,
    that is our assumption was not right.

    If the assumption holds,
    then $\exists \omega \in X ^ {-1}(A) \cap X ^ {-1}(B)$.
    For this $\omega$,
    we have $X(\omega) \in A$ and $X(\omega) \in B$.

    i.e. $\omega \in X ^ {-1}(A)$ and $\omega \in X ^ {-1}(B)$.
    From $X(\omega) \in A$ and $X(\omega) \in B$ implies that $A \cap B \neq \emptyset$,
    which contradicts $A \cap B = \emptyset$.
    Hence our first assumption cannot hold.
\end{proof}

\begin{definition}[Indicator random variable]
    Let $A \subseteq \Omega$,
    then the indicator of $A$,
    denoted by $\mathbb{1}_A$,
    is defined as follows,
    \begin{align*}
    \mathbb{1}_A(\omega) &= 1,\text{ if } \omega \in A \\
    &= 0,\text{ if } \omega \notin A.
    \end{align*}
\end{definition}

$\P(A) = \frac{|A|}{|\Omega|}$
\[
|A| = \sum_{\omega \in A}\mathbb{1}_A(\omega).
\]

\begin{example}
    Let $\Omega = \{\omega_1, \omega_2, \dotsc, \omega_5\}$,
    $A = \{\omega_1, \omega_2, \omega_4\}$
    \[
    \mathbb{1}_A(\omega_1) = \mathbb{1}_A(\omega_2) = \mathbb{1}_A(\omega_4) = 1.
    \]
    \[
    \mathbb{1}_A(\omega_3) = \mathbb{1}_A(\omega_5) = 0.
    \]
    $\P_A(\{1\}) = \P(\{\omega\,:\,\mathbb{1}_A(\omega) = 1\}) = \P(A)$
    \[
    \P_A(\{0\}) = \P(\{\omega\,:\,\mathbb{1}_A(\omega) = 0\}) = \P(A ^ c)
    \]
\end{example}

\textit{We will write $\P_X(\{x\}) = \P_X(x) = \P(\{X = x\}) = \P(X = x)$.}

\subsection{Discrete random variables}
\begin{definition}[Discrete random variable]
    Let $X$ be a random variable $X$ is discrete if there exists $\chi \subseteq X(\Omega)$,
    such that $\chi$ is either finite or countable.

    We will define $p : \chi \rightarrow [0, 1]$,
    given by
    \[
    p(x) = \P(X = x),\text{ for every } x \in \chi.
    \]
    This is called the probability mass function (pmf).
\end{definition}

\begin{example}[continues = exmp1]
    Toss a fair coin three times.
    Let $X$ denote the number of heads.

    $\chi = \{0, 1, 2, 3\}$.

    pmf of $X$.
    \begin{table}[h!]
        \begin{tabular}{c|cccc}
            $x$ & $0$ & $1$ & $2$ & $3$ \\
            \hline
            $p(x)$ & $\frac{1}{8}$ & $\frac{3}{8}$ & $\frac{3}{8}$ & $\frac{1}{8}$ \\
        \end{tabular}
    \end{table}

    $|\Omega| = 2 ^ 3$

    $\{\omega\,:\, X(\omega) = 0\} = \{\text{TTT}\}$
    
    $\{\omega\,:\, X(\omega) = 1\} = \{\text{HTT, THT, TTH}\}$
    
    $\{\omega\,:\, X(\omega) = 2\} = \{\text{HHT, HTH, THH}\}$
    
    $\{\omega\,:\, X(\omega) = 3\} = \{\text{HHH}\}$
\end{example}

\begin{theorem}
    Let $X$ be a discrete random variable,
    with probability mass function $p$.
    Then for any $A \subseteq \chi$,
    \begin{equation}
    \P(X \in A) = \sum_{x \in A}p(x).
    \end{equation}
    Furthermore,
    \begin{equation}
    \sum_{x \in \chi}p(x) = 1.
    \end{equation}
    \begin{proof}
        $A \subseteq \chi$,
        where $\chi$ is countable or finite (can enlist/enumerate the elements of $A$).
        Let $A = \{a_1, a_2, \dotsc\} = \bigcup_{i = 1}^{\infty}\{a_i\}$
        \[
        \P(X \in A) = \P\left(\bigcup_{i = 1}^\infty\{X = a_i\}\right)
        \]
        observe that $\{X = a_i\} \cap \{X = a_j\} = \emptyset$ for $i \neq j$.
        So
        \begin{align*}
        \P(X \in A) &= \P\left(\bigcup_{i = 1}^\infty\{X = a_i\}\right) \\
        &= \sum_{i = 1}^{\infty}\P(X = a_i) &\text{by (A4)} \\
        &= \sum_{i = 1}^{\infty}p(a_i) \\
        &= \sum_{x \in A}p(x).
        \end{align*}
        Since we have established ($1$),
        we know that
        \begin{equation}
        \P(x \in \chi) = \sum_{x \in \chi}p(x)
        \end{equation}
        and we already know that
        \begin{equation}
            \P(\chi) = 1
        \end{equation}
        ($3$) and ($4$) together complete the proof of ($2$)
        
    \end{proof}
\end{theorem}

$\chi$ is the collection of all values that the random variable $X$ can take.

A set $A$ is countable if and only if the elements of $A$ can be written as a potentially infinite sequence or list.

(A3) $\P(A \cup B) = \P(A) + \P(B)$, when $A \cap B = \emptyset$.

(A4) is an extension of (A3) for countable collections.
Let $A_1, A_2, \dotsc$ be a countable collection of pairwise disjoint sets,
i.e. $A_i \cap A_j - \emptyset$, for all $i \neq j$.
Then
\[
\P\left(\bigcup_{i = 1}^{\infty}A_i\right) = \sum_{i = 1}^{\infty}\P(A_i)
\]

\subsubsection{Summary}
$X$ is a discrete random variable,
the probability mass function is
\[
A \subseteq \chi, \P(A \in A) = \sum_{x \in A}p(x)
\]
and
\[
\sum_{x \in \chi}p(x) = 1.
\]

$\P_X(A)$ is a distribution
\[
\P_X(A) = \P(X \in A) = \sum_{x \in A}p(x)
\]
\[
\P_X(\chi) = \P(X \in \chi) = \sum_{x \in \chi}p(x).
\]

\subsection{Binomial and geometric random variable}
$p$ is the probability of success.
Repeat the trials $n$-times given and fixed.
Each trial is independent.
The outcomes are success and failure.
Let $X$ denote the number of successes in $n$-trials.

Let $\Omega = \{\omega:\, \omega = (\omega_1\omega_2\dotsi\omega_n) \text{ where } \omega_i = \text{S/F for } 1 \leq i \leq n\}$.

$X(\omega) = $ number of successes in the sequence $(\omega_1\omega_2\dotsi\omega_n)$.
Let us assign the value $1$ to success and $0$ to failure i.e. $\omega_i = 1$ if success $\omega_i = 0$ if failure.

For a given $\omega_1$
$X(\omega) = $ number of $i$, for which $\omega_i = 1$, $X(\omega) = \sum_{i = 1}^{n}\omega_i$.

Let $\omega = (\omega_1\dotsc \omega_n) \in \Omega$,
with $x$-successes and $(n - x)$ failures.
\[
\P(\{\omega\}) = p ^ x (1 - p) ^ {n - x}
\]
therefore
\[
\P(X = x) = \P\left(\bigcup_{w : \sum_{i = 1}^n\omega_i = x}\{\omega\}\right) = \sum_{w : \sum_{i = 1}^n\omega_i = x}\P(\{\omega\}) = \binom{n}{x}p ^ x (1 - p) ^ {n - x}.
\]
\[
\{X = x\} = \{\omega : X(\omega) = x\} = \{\omega : \omega = (\omega_1\dotsc \omega_n,\text{ then } \sum_{i = 1}^n\omega_i = x\}
\]
\begin{definition}[Binomial random variable]
    A discrete random variable $X$ is binomially distributed with parameters $n$ and $p \in [0, 1]$ if $\chi = \{0, 1, \dotsc, n\}$,
    and the probability mass function,
    is given by
    \[
    p(x) = \binom{n}{x}p ^ x(1 - p) ^ {n - x},\text{ if } x \in \{0, 1, \dotsc, n\},
    \]
    and $= 0$ elsewhere.

    We denote this as
    $X \sim \Bin(n, p)$.
\end{definition}

For $n = 1,
\Bin(1, p)$ is referred to as the Bernoulli random variable
(with success parameter $p$),
written as
\[
\mathrm{Ber}(p).
\]

\textbf{Geometric random variable}

Let us toss a coin until we get heads.
Let $X$ denote the number of tosses that we require
\[
\P(X = x) = \P(\text{first $(x - 1)$ tosses were $T$, and $x$-th toss is $H$}) = (1 - p) ^ {x - 1}p.
\]

\begin{definition}
    $X$ is geometrically distributed with parameter $p \in (0, 1]$,
    if $\chi = \N = \{1, 2, \dotsc\}$ with the probability mass function given by
    \begin{align*}
        p(x) &= (1 - p) ^ {x - 1}p,&x \in \N \\
        &= 0,&\text{elsewhere}.
    \end{align*}
    This is denoted $X \sim \Geo(p)$.
\end{definition}

\begin{example}
    $105$ people who bought a flight ticket,
    $\P(\text{that a person misses a flight}) = 0.04$.
    Each individual misses the flight independent of the other.
    Find
    \begin{enumerate}[label = (\alph*)]
        \item $\P(\text{nobody misses the flight})$.
        \item $\P(\text{$3$ or more people miss the flight})$.
    \end{enumerate}

    Let $X := \text{number of people who miss the flight}$, $p := \P(\text{of success}) = 0.04$.
    $n = 105$.
    $X \sim \Bin(105, 0.04)$.
    \begin{enumerate}[label = (\alph*)]
        \item
        \begin{align*}
        \P(\text{nobody misses the flight}) &= \P(X = 0) \\
        &= \binom{105}{0}p ^ 0(1 - p) ^ {105 - 0} \\
        &= (0.96) ^ {105} \approx 0.014.
        \end{align*}
        \item
        \begin{align*}
            \P(\text{$3$ or more people miss the flight}) &= \P(X \geq 3) \\
            &= 1 - \P(X < 3) \\
            &= 1 - p(0) - p(1) - p(2) \\
            &= \dotsc
        \end{align*}
    \end{enumerate}
\end{example}

Let us come back to $\Geo(p)$,
where $p > 0$.
Then observe that
\[
\sum_{x = 1}^{\infty}(1 - p) ^ {x - 1}p = p\sum_{x = 1}^{\infty}(1 - p) ^ {x - 1}
\]
write $x - 1 = t$,
then we get
\[
\sum_{x = 1}^{\infty}(1 - p) ^ {x - 1}p = p\left(\sum_{t = 0}^{\infty}(1 - p) ^ t\right)\text{Geometric series with common ratio $(1 - p)$.}
\]
which results in
\begin{align*}
    \P(x \in \N) &= \sum_{x = 1}^{\infty}(1 - p) ^ {x - 1}p \\
    &= p\left(\sum_{t = 0}^{\infty}(1 - p) ^ t\right) \\
    &= p \cdot \frac{1}{1 - (1 - p)} \\
    &= p \cdot \frac{1}{p} = 1.
\end{align*}

\subsection{Poisson random variable}
\begin{definition}
    A random variable $X$ with $\chi = \Z_+ := \{0, 1, 2, \dotsc\}$ is Poisson with parameter $\lambda (> 0)$,
    if the probability mass function
    \begin{align*}
        p(x) &= \P(X = x) = \frac{e ^ {-\lambda}\lambda ^ x}{x!},& x \in \Z_+ \\
        &= 0,&\text{elsewhere}.
    \end{align*}
    This is denoted $\Po(x)$, with $X \sim \Po(\lambda)$.
\end{definition}
$\lambda$ is taken as the average number of changes in a small amount of time.

\begin{example}
    Some airline has $32$ crashes in a $25$ year period.
    Where a year is $365$ days,
    there are $30$ days in a month,
    and $7$ days in a week.

    Let $Y, M, W$ denote the number of crashes in the next year,
    month
    and week.
    \begin{enumerate}[label = (\alph*)]
        \item Find the distributions for $Y, M$ and $W$.
        \item Find $\P(\text{there are no crashes in the next week})$.
    \end{enumerate}
    \begin{enumerate}[label = (\alph*)]
        \item 
        The daily rate of accidents is $\frac{32}{365 \cdot 25} = \frac{32}{9125}$.
        $\lambda_Y$ is the average number of crashes per year which is $\frac{32}{9125} \cdot 365$,
        $\lambda_M$ is the average number of crashes per month which is $\frac{32}{9125} \cdot 30$,
        $\lambda_W$ is the average number of crashes per week which is $\frac{32}{9125} \cdot 7$.
        $Y \sim \Po(\lambda_Y), M \sim \Po(\lambda_M), W \sim \Po(\lambda_W)$
        \item
        \begin{align*}
            \P(\text{there are no crashes in the next week}) &= \P(W = 0) \\
            &= e ^ {\lambda w}\frac{\lambda_W ^ 0}{0!} \\
            &- e ^ {-\lambda_W}.
        \end{align*}
    \end{enumerate}
\end{example}

\subsection{Continuous random variables}
\begin{definition}
    Let $X : \Omega \rightarrow \R$ be a random variable.
    We say $X$ is a continuous random variable or $X$ has a continuous probability distribution,
    if there exists a non-negative function $f$,
    $f : \R \rightarrow \R$,
    such that
    \[
    \P(X \in [a, b]) = \int_a^bf(t)\,dt.
    \]
    $f$ is called the probability density function of $X$ (pdf).
    The probability density function of $X$ is denoted $f_X$.
\end{definition}
Let $x \in \R$,
choose $a = b = x$
\[
\P(X = x) = \int_x^xf(t)\,dx = 0.
\]
Given this is a continuous random variable,
this shows us that the continuous random variable has no mass at a single point
\[
\P(X \in (a, b)) = \P(X \in [a, b)) = \P(X \in (a, b]) = \P(X \in [a, b]) = \int_a^bf(t)\,dt.
\]
Comment: $\P(X = x) \neq f(x)$














\end{document}