\documentclass[10pt, a4paper]{article}
\usepackage{preamble}
\usepackage{bbm}

\declaretheorem[style = avgstyle, name = Counting Principle]{countprinc}
\declaretheorem[style = avgstyle, name = Property, numberwithin = section]{property}
\counterwithin*{equation}{subsection}

\title{Probability I}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}

Probability is based on axioms


Events are based on sets

\begin{definition}
    A set $S$ is a\footnote{Unordered} collection of elements or objects.    
\end{definition}
\begin{example}
    Let $S$ be the set of all even integers from $1$ to $9$.
    \[
    S =  \{2, 4, 6, 8\} = \{4, 6, 2, 8\}.
    \]
\end{example}

$\Z$ is the set of all integers.

$\N$ is the set of all natural numbers.\footnote{Positive integers}

$\N$ can also be defined as follows
\[
\N = \{x \in \Z^+: x > 0\}.
\]

$\varnothing$ denotes the empty set.

\begin{definition}
    Let $A, B$ be sets. $A \subseteq B \iff x \in A \implies x \in B$.

    A strict subset $A \subset B$ means that there is an $x \in B$ such that $x \notin A$ 
\end{definition}

\begin{definition}
    Let $A, B$ be sets. $A = B \iff A\subseteq B \And B \subseteq A$
\end{definition}

\subsection{Basic set operations}

\begin{definition}
    Let $A, B$ be sets. The union of $A, B$ is denoted by $A \cup B$. The union of two sets is the set of elements in $A$ or $B$ or both.
    \[
    A \cup B = \{x: x \in A \text{ or } x \in B \text{ or } x \in A\And B\}.
    \]
\end{definition}

\begin{definition}
    Let $A, B$ be sets. The intersection of $A, B$ is denoted by $A \cap B$. The intersection of two sets is the collection of elements that are in both $A$ and $B$.
    \[
    A \cap B = \{x: x \in A \And x \in B\}.
    \]
\end{definition}

\begin{definition}
    Let $A$ be a set. The complement of a set $A$ is denoted by $A ^ c$. A complement is the collection of elements not in $A$.
    \[
    A ^ c = \{x: x \notin A\}.
    \]
\end{definition}

\begin{example}
    Let $S :=$ collection of integers from $1$ to $9$ 
    $S = \{x \in \N: 1 \leq x \leq 9\}$
    
    Let $A :=$ collection of even integers from $1$ to $9$ 
    $A = \{x \in \N, x \text{ is even}: 1 \leq x \leq 9\}$

    $A \subseteq S\quad A \subseteq \N\quad S\subseteq\N$

    Let $B :=$ collection of odd integers from $1$ to $9$ 
    $B = \{x \in \N, x \text{ is odd}: 1 \leq x \leq 9\}$

    $A \cap B = \varnothing$,\qquad $A ^ c = \{x \in \N: 1\leq x \leq 9, x \notin A\}$

    \begin{align*}
    D &:= \text{All integers in } S \text{ that are divisible by 3} \\
    &= \{3, 6, 9\}
    \end{align*}
    $D \cap B = \{3, 9\}\quad D\cap A = \{6\}$.
\end{example}

\subsection{Cardinality}
\begin{definition}[Cardinality]
    Let $A$ be a finite set. Cardinality or the size of $A$ denoted by
    \[
    |A|
    \]
    is the number of elements it contains.
\end{definition}

If $|A| = m$, $A = \{x_1, x_2, \dots, x_n\}$\footnote{List or sequence or length $m$.}.

\textbf{Types of infinity}:

There are two types of infinity. Countable infinities implies that we can write it as a sequence of possibly infinite length. Uncountable infinities, for example $\R$ cannot be written as a sequence.

$\N = \{1, 2, 3, \dots\}$ can be written as a list.

$[a, b] = \{x \in \R : a \leq x \leq b\}$ for example take $a = 1$ and $b = 2$ and you can keep taking the midpoint between any two numbers infinitely and not find all numbers.

When you have a finite number of sets
\[
A_1, \dots A_n, \quad\text{abbreviation: } \bigcup_{i = 1}^{n}A_i = A_1 \cup A_2 \cup \dotsi \cup A_n\footnote{Finite union.}
\]
Similarly
\[
\bigcap_{i = 1}^{n}A_i = A_1 \cap A_2 \cap \dotsi \cap A_n\footnote{Finite intersection.}
\]

\begin{definition}[Power set]
    For a set $A$, its power set denoted by $2 ^ A$\footnote{or $\mathcal{P}(A)$} is the set containing all subsets of $A$.
    \[
    2 ^ A = \{B : B \subseteq A\}.
    \]
    $\emptyset,\,A \in 2 ^ A$.
\end{definition}


\subsection{Axiomatic Probability}
\begin{enumerate}[label = (\roman*)]
    \item Sample, space \& events

    Throw a  six-sided die, and note the score, that can be a number from $1$ to $6$.

    Outcomes are the score, which we can write as $1, 2, 3, 4, 5, 6$.

    The outcomes are collected in the sample space
    \begin{definition}[Sample space]
        The collection of all outcomes is called the sample space, denoted by $\Omega$.
        \[
        \Omega := \text{ collection of all possible outcomes}.
        \]
        and $\omega \in \Omega$, we say $\omega$\footnote{Also known as a sample point.} is an outcome.
    \end{definition}
    \begin{example}\phantom{}
    
        Throw a die, $\Omega = \{1, 2, 3, 4, 5, 6\}$.\footnote{$|\Omega| = 6$.}

        $6$ is an outcome.
    \end{example}

    In many examples, $\Omega$ will be finite or countably infinite

    \begin{definition}[Event]
        An event $A$ is a subset of the sample space $\Omega$.
    \end{definition}
    \begin{example} \phantom{}
    
        $A := \text{Even score} = \{2, 4, 6\} \subseteq \Omega$.

        $\emptyset := \text{Impossible event}.$ 
    \end{example}

    \begin{definition}[Probability]
        Associated to $\Omega$, there is a collection denoted by $\mathcal{F}$, which is the collection of all possible events.
        \[
        \mathcal{F} = \{A : A \subseteq \Omega\}.
        \]
        Discrete (for finite $\Omega$) we will take $\mathcal{F} = 2 ^ \Omega$.

        Probability is a measure of randomness that is associated that we assign to events $A \in \mathcal{F}$.
    \end{definition}
\end{enumerate}

Set operation is the same as event calculus.

\newpage

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Notations & Set theory language & Prob analogue & Meaning as events \\ [0.1em]
        \hline
        $A \cup B$ & $A$ union $B$ & $A$ or $B$ & $A$ occurs or $B$ occurs or both occur \\
        \hline
        $A \cap B$ & $A$ intersect $B$ & $A$ and $B$ & both $A$ and $B$ occur \\
        \hline
        $A^c$ & $A$ complement & not $A$ & $A$ does not occur \\
        $A \setminus B$ & $A$ set minus $B$ & $A$ but not $B$ & $A$ occurs but $B$ does not \\
        \hline
        $A \subseteq B$ & $A$ subset $B$ & $A$ implies $B$ & if $A$ occurs then $B$ also occurs \\
        \hline
        $\emptyset$ & null set & impossible event & \phantom{1} \\
        \hline
    \end{tabular}
    \caption{Set operations in event calculus}
\end{table}

$A \cap B = \emptyset$, $A$ and $B$ are mutually exclusive.

In order of the event calculus table here are the event notations for each thing. 

\begin{itemize}
    \item $A \cup B = \{\omega : \omega \in A \text{ or } \omega \in B\}$
    \item $A \cap B = \{\omega : \omega \in A \text{ and } \omega \in B\}$
    \item $A \setminus B = \{\omega : \omega \in A \text{ but } \omega \notin B\}$
    \item $A \subseteq B \text{ if } \omega \in A \implies \omega \in B$
\end{itemize}

\newpage

\textbf{De-Morgan's Laws}

\begin{enumerate}[label = (\roman*)]
    \item
    \[
    \left(\bigcup_{i = 1}^{n} A_i\right) ^ c = \bigcap_{i = 1}^{n} A_i ^ c
    \]
    \item
    \[
    \left(\bigcap_{i = 1}^{n} A_i\right) ^ c = \bigcup_{i = 1}^{n} A_i ^ c
    \]
\end{enumerate}

Non-examined
\begin{enumerate}[label = (\roman*)]
    \item
    \[
    \left(\bigcup_{n = 1}^{\infty} A_n\right) ^ c = \bigcap_{n = 1}^{\infty} A_n ^ c
    \]
    \item
    \[
    \left(\bigcap_{n = 1}^{\infty} A_n\right) ^ c = \bigcup_{n = 1}^{\infty} A_n ^ c
    \]
\end{enumerate}

\begin{definition}
    A probability "$\mathbb{P}$" on a sample space $\Omega$, with $\mathcal{F}$ (collection of events) is a mapping such that for every $A$ in $\mathcal{F}$, $\mathbb{P}(A)$ is a real number satisfying.
    \begin{enumerate}[label = A\arabic*]
        \item $\mathbb{P}(A) \geq 0$ (Prob is always non-negative)
        \item $\mathbb{P}(\Omega) = 1$ (Sample space has full probability)
        \item If $A, B \in \mathcal{F}$, and $A$ and $B$ are disjoint\footnote{$A$ and $B$ are mutually exclusive.}, that is $A \cap B = \emptyset$, then \[\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B)\]
        (finite additivity)
        \item (Countable additivity). $A_1, A_2, \dots$ are pairwise disjoint, i.e. $A_i \cap A_j = \emptyset$, for all $i \neq j$, then
        \[
        \mathbb{P}\left(\bigcup_{n = 1}^{\infty}A_n\right) = \sum_{n = 1}^{\infty}\mathbb{P}(A_n).
        \]
    \end{enumerate}
\end{definition}

$(\Omega, \mathcal{F}, \mathbb{P})$, this triplet is known as the probability space.

\begin{example}
    $\Omega = \{\omega_1, \omega_2, \dots, \omega_m\}$, let $\omega_i$'s $1 \leq i \leq m$ be distinct. $A\subseteq \Omega,\ \mathbb{P}(A)$. $A = \{\omega_1, \omega_2, \omega_3\}$.
    \[
    \{\omega_1\} \subseteq \Omega, \{\omega_2\} \subseteq \Omega, \{\omega_3\} \subseteq \Omega.
    \]
    Therefore by (A3)
    \[
    \mathbb{P}(A) = \mathbb{P}(\{\omega_1\} \cup \{\omega_2\} \cup \{\omega_3\}) = \sum_{i = 1}^{3}\mathbb{P}(\{\omega_i\}).
    \]
\end{example}

\textbf{Equally likely outcomes}

\[
\mathbb{P}(\{\omega_i\}) = p_i \geq 0,\quad 1 \leq i \leq m,
\]
such that $\sum_{i = 1}^{m}p_i = 1$. $\Omega = \bigcup_{i = 1}^{m}\{\omega_i\}$, therefore $\mathbb{P}(\Omega) =\footnote{From A3.} \sum_{i = 1}^{m}\mathbb{P}(\{\omega_i\}) = 1\footnote{From A2.}$

Then $A \subseteq \Omega$,
\[
\mathbb{P}(A) = \sum_{i : \omega_i \in A}p_i
\]

Equally likely outcomes it means that $p_i$ are all equal and in this example $p_1 = p_2 = \dotsi = p_m = \frac{1}{m}$

\begin{example}
    Throw a fair die

    $\Omega = \{1, 2, \dots, 6\}$

    Fair = equally likely
    \[
    \mathbb{P}(\{1\}) = \mathbb{P}(\{2\}) = \dotsi = \mathbb{P}(\{6\}) = \frac{1}{6}
    \]
    $A = \{\text{even score}\} = \{2, 4, 6\}$
    \[
    \mathbb{P}(A) = \sum_{i : \omega_i \in A}p_i = \frac{1}{6}|A| = \frac{3}{6} = \frac{1}{2}.
    \]
\end{example}


\begin{definition}[Partition]
    $E_1, E_2,\dotsc,E_k \in \mathcal{F}$ for a finite partition of the sample space $\Omega$, if
    \begin{enumerate}[label = (\roman*)]
        \item $\mathbb{P}(E_i) > 0$ for all $1 \leq i \leq k$,
        \item $E_i\,\&\,E_j$ are pairwise disjoint, i.e. $\forall i \neq j,\,E_i \cap E_j = \emptyset$,
        \item $\displaystyle\Omega = \bigcup_{i = 1}^{k}E_i$.
    \end{enumerate}
\end{definition}
Easily, we can see that
\[
\sum_{i = 1}^{k} \mathbb{P}(E_i) = 1.
\]

\[
\mathbb{P}(\Omega) = \bigcup_{i = 1}^{k}(E_i) \implies 1 = \sum_{i = 1}^{k}\mathbb{P}(E_i)\qquad\text{by A3}
\]


\subsubsection{Sigma algebra}
\begin{definition}
    $\mathcal{F}$, a collection of subsets of $\Omega$ is called a $\sigma$-algebra, if it satisfies
    \begin{enumerate}[label = (S\arabic*)]
        \item $\Omega \in \mathcal{F}$
        \item $A \in \mathcal{F}$, then $A ^ c \in \mathcal{F}$.
        \item if $\underbrace{A_1,\,A_2,\,\dotsc}_{\text{countably infinite}} \in \mathcal{F}$, then $\bigcup_{n = 1}^{\infty}A_n \in \mathcal{F}$.
    \end{enumerate}
\end{definition}

\subsection{Consequences of (A1-A3)}

\begin{proposition} \phantom{} \\
    \begin{enumerate}[label = (C\arabic*)]
        \item $A, B \in \mathcal{F}$, then $\mathbb{P}(B \setminus A) = \mathbb{P}(B) - \mathbb{P}(A \cap B)$.
        \item $A \in \mathcal{F}$, $\mathbb{P}(A ^ c) = 1 - \mathbb{P}(A)$ (A2 + A3)
        \item $\mathbb{P}(\emptyset) = 0$
        \item $\mathbb{P}(A) \leq 1,\quad\text{for } A \in \mathcal{F}$ (Hint: use C2)
        \item Monotonicity: $A \subseteq B$
        \[
        \mathbb{P}(A) \leq \mathbb{P}(B)
        \]
        \item $\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)$
        \item Finite additivity: $A_1, A_2,\dotsc, A_k$ pairwise mutually exclusive
        \[
        \mathbb{P}\left(\bigcup_{i = 1}^{k}\right) = \sum_{i = 1}^{k}\mathbb{P}(A)
        \]
        \item Boole's inequality (if they are not mutually exclusive):
        \[
        \mathbb{P}\left(\bigcup_{i = 1}^{n}A_i\right) \leq \sum_{i = 1}^{n}\mathbb{P}(A_i)
        \]
    \end{enumerate}
    \begin{proof}
        C2:
        $A \cup A ^ c = \Omega$\quad$A \cap A ^ C = \emptyset$

        $\mathbb{P}(A \cup A ^ c) = 1$ (from A2)
        (from A3)
        $\mathbb{P}(A) + \mathbb{P}(A ^ c) = 1$

        C3:
        $\mathbb{P}(\emptyset) = 1 - \mathbb{P}(\Omega) = 1 - 1 = 0$ (by A3)

        C4:
        From 1, $\mathbb{P}(A) + \mathbb{P}(A ^ c) = 1$
        
        $\mathbb{P}(A) \geq 0,\,\mathbb{P}(A ^ c) \geq 0$, hence $\mathbb{P}(A) \leq 1$
    \end{proof}
\end{proposition}

\section{Counting}

\subsection{Equally likely outcomes}
Finite sample space $\Omega = \{\omega_1,\omega_2,\dotsc, \omega_m\}$, $\mathbb{P}(\omega_1) = \mathbb{P}(\omega_2) = \mathbb{P}(\omega_m) = \frac{1}{m}$\footnote{$\mathbb{P}(\{\omega_i\}) = \mathbb{P}(\omega_i)$}, $A$, $\mathbb{P}(A) = \frac{|A|}{m} = \frac{|A|}{|\Omega|}$

\begin{countprinc}[Multiplication principle]
    $k$ choices in succession, where
    \begin{enumerate}[label = (\roman*)]
        \item $m_1$, possibility of first choice
        \item $m_2$, possibility of second choice \\ \vdots
        \item $m_k$, possibility of $k$th choice
    \end{enumerate}
    and at each stage, our choices are not affected by the past.

    The total number of distinct choices = $\underbrace{m_1 \times m_2 \times \dotsi \times m_k}_{k \text{ times}} = \prod_{i = 1}^{k}m_i$.
\end{countprinc}

\begin{example}
    $3$ choices for breakfast, $4$ for lunch and $5$ for dinner. The total number of choices is
    \[
    3 \times 4 \times 5.
    \]
\end{example}

\begin{example}
    Debit Card Pins
    \begin{enumerate}[label = (\roman*)]
        \item Cannot be the same digit at all $4$ places.
        \item Cannot be an increasing or decreasing sequence of consecutive.
    \end{enumerate}

    $A := \text{ Number of possible PINs}$
    
    $A^c := \text{ Number of not possible PINs}$

    The number of sequences satisfying (i) is $10$.

    The number of (ii) increasing consecutive sequences can start with $0,\, 1,\, 2,\,\dotsc,\, 6$ implies $7$ options. Similarly, decreasing consecutive sequences can start with $9,\,8,\,7,\,\dotsc,\,3$.

    The total number of choices satisfying (ii) is $14$.

    $|A ^ c| = 10 + 14 = 24$.

    The number of unrestricted choices $= 10 ^ 4$.

    $|A| = 10 ^ 4 - 24$.
\end{example}

\begin{countprinc}[Ordered choice of distinct objects with replacement]
    $m$ distinct objects, choose $r$ of them with replacement, then the number of different ordered lists (i.e. $r$-tuples) is $\underbrace{m \times m \times \dotsi \times m}_{r \text{ choices}} = m ^ r$.
\end{countprinc}

\begin{countprinc}[Ordered choices of distinct objects without replacement]
    $m$ distinct objects, choose $r$ ($r \leq m$) of them without replacement, then the number of such choices is denoted by $(m)_r$, where $(m)_r = m \times (m - 1) \times \dotsc \times (m - r + 1) = \frac{m!}{(m - r)!}$.
\end{countprinc}

\begin{example}[Birthday Problem]
    There are $n$ people, a year is $365$ days,
    \[
    B = \{\text{At least two people have the same birthday}\}
    \]
    Find $\mathbb{P}(B)$.

    $B ^ c = \{\text{No two people have the same birth day}\}$

    Birthdays of the $n$ people $(D_1, D_2, D_3, \dotsc, D_n)$

    $\Omega$ is the collection of all such tuples

    $|\Omega| = 365 ^ n$

    Let $B_i$ denote the event that exactly $i$ people have the same birthday. Then $B = \displaystyle \bigcup_{i = 1}^{n - 1}B_i$.

    \[
    B ^ c = \bigcap_{i = 1}^{n - 1}B_i ^ c
    \]
    $|B ^ c| = (365)_n$
    \[
    \mathbb{P}(B ^ c) = \frac{(365)_n}{(365) ^ n}, \text{ hence } \mathbb{P}(B) = 1 - \frac{(365)_n}{(365) ^ n}
    \]
\end{example}

\begin{countprinc}
    [Unordered choices of distinct objects without replacement]
    $m$ objects, select $r \leq m$ of them without replacement, the number of distinct subsets that we can form.
    \[
    \binom{m}{r}\footnote{Or $C_r ^ m$} = \frac{m(m - 1)\dotsi(m - r + 1)}{r!} = \frac{m!}{r!(m - r)!}
    \]
    this is read as "$m$ choose $r$".
\end{countprinc}

\[
\binom{m}{r} = \binom{m}{m - r}.
\]

\begin{countprinc}[Ordered choices of two types of objects]
    $m$ objects, $r$ objects of type one, $(m - r)$ of type two. The number of distinct ordered choices is $\binom{m}{r}$.
\end{countprinc}

\begin{example}
    Toss a "fair" coin $7$ times.

    $E := \{\text{There are a total of $3$ heads}\}$.

    Find $\mathbb{P}(E)$

    The number of all possible choices is $|\Omega| = 2 ^ 7$.

    The number of possible outcomes in $E$ are $\binom{7}{3}$
    \[
    \mathbb{P}(E) = \frac{|E|}{|\Omega|} = \frac{\binom{7}{3}}{2 ^ 7}
    \]
\end{example}

\begin{countprinc}[Ordered grouping of indistinguishable objects]
    Suppose we have $m$ indistinguishable objects and $k$ groups. The number of all possible choices is
    \[
    \binom{m + k - 1}{m} = \binom{m + k - 1}{k - 1}.
    \]
\end{countprinc}

\newpage

\section{Conditional probability and independence}

\subsection{Conditional probability}

\begin{definition}
    Let $A,\, B \subseteq \Omega$ be $2$ events. We defined conditional probability of $A$ given $B$, and denote it by $\mathbb{P}(A | B)$, as follows
    \[
    \mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)},\text{ when } \mathbb{P}(B) > 0.
    \]
\end{definition}
The probability of $A$ when we have observed $B$.

\begin{example}
    Roll a fair die.

    $A = \{\text{Score is odd}\} = \{1, 3, 5\}$
    
    $A = \{\text{Score is at most $3$}\} = \{1, 2, 3\}$
    \[
    \mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
    \]
    \[
    A \cap B = \{1, 3\},\quad\mathbb{P}(A \cap B) = \frac{2}{6} = \frac{1}{3} 
    \]
    $\mathbb{P}(B) = \frac{3}{6} = \frac{1}{2}$
    \[
    \mathbb{P}(A | B) = \frac{\frac{1}{3}}{\frac{1}{2}} = \frac{2}{3}
    \]
\end{example}

\begin{example}
    A family has $2$ children, both sexes are equally likely.
    \[
    \Omega = \{\text{BB, BG, GB, GG}\}
    \]
    $A_1 = \{\text{Both girls}\} = \{\text{GG}\} = \frac{1}{4}$
    
    $A_2 = \{\text{At least one girl}\} = \{\text{BG, GB, GG}\} = \frac{3}{4}$

    $A_3 = \{\text{Eldest is a girl}\} = \{\text{GB, GG}\} = \frac{2}{4} = \frac{1}{2}$
    \[
    \mathbb{P}(A) = \frac{|A|}{|\Omega|}
    \]
    \[
    \mathbb{P}(A_1 | A_2) = \frac{\mathbb{P}(A_1 \cap A_2)}{\mathbb{P}(A_2)}
    \]
    $A_1 \subseteq A_2 \implies A_1 \cap A_2 = A_1$. Applying this, we get
    \[
    \mathbb{P}(A_1 | A_2) = \frac{\mathbb{P}(A_1)}{\mathbb{P}(A_2)} = \frac{\frac{1}{4}}{\frac{3}{4}} = \frac{1}{3}.
    \]
    \[
    \mathbb{P}(A_2 | A_1) = \frac{\mathbb{P}(A_1 \cap A_2)}{\mathbb{P}(A_1)} = \frac{\mathbb{P}(A_1)}{\mathbb{P}(A_1)} = 1.
    \]
\end{example}

\subsection{Properties of conditional probability}
\begin{enumerate}[label = P\arabic*]
    \item Let $B$ be an event, such that $\mathbb{P}(B) > 0$. Then $\mathbb{P}(\cdot | B)$ satisfies axioms (A1) - (A3)
\end{enumerate}

We have $(\Omega, \mathcal{F}, \mathbb{P})$

Given the event $B$ with $\mathbb{P}(B) > 0$, let us define

$\mathbb{P}_B : \mathcal{F} \rightarrow \R$, such that for every $A \in \mathcal{F},\,\mathbb{P}_B(A) := \mathbb{P}(A | B)$

$\mathbb{P}_B$ is a probability on $(\Omega, \mathcal{F})$ i.e. $\mathbb{P}_B$ satisfies A1, A2 and A3.

(A1) $\mathcal{P}_B(A) \geq 0$, for any $A \in \mathcal{F}$.
(A1) $\mathcal{P}_B(A) = 1$.

$\mathbb{P}(\cdot | B) \iff \mathbb{P}_B(A) := \mathbb{P}(A | B)$

\begin{enumerate}[label = A\arabic*]
    \item $\mathbb{P}_B(A) \geq 0$
    \item $\mathbb{P}_B(\Omega) = 1$
    \item Let $A_1, A_2$ be $2$ events in $\mathcal{F}$, such that, $A_1 \cap A_2 = \emptyset$, then
    \[
    \mathbb{P}_B(A_1 \cup A_2) = \mathbb{P}_B(A_1) + \mathbb{P}_B(A_2).
    \]
    \begin{proof}
        A3

        \begin{gather}
        \mathbb{P}_B(A_1 \cup A_2) = \frac{\mathbb{P}((A_1 \cup A_2) \cap B}{\mathbb{P}(B)}          
        \end{gather}
        Since $A_1 \cap A_2 = \emptyset$, $(A_1 \cap B) \cap (A_2 \cap B) = \emptyset$, therefore we have
        \begin{align*}
            \mathbb{P}((A_1 \cap A_2) \cap B) &= \mathbb{P}((A_1 \cap B) \cup (A_2 \cap B)) \\
            &= \mathbb{P}(A_1 \cap B) + \mathbb{P}(A_2 \cap B)
        \end{align*}
        $(A_1 \cap B)$ and $(A_2 \cap B)$ are mutually exclusive. Putting this into (1), we get
        \begin{align*}
        \mathbb{P}(A_1 \cup A_2) &= \frac{\mathbb{P}(A_1 \cap B) + \mathbb{P}(A_2 \cap B)}{\mathbb{P}(b)} \\
        &= \frac{\mathbb{P}(A_1 \cap B)}{\mathbb{P}(B)} + \frac{\mathbb{P}(A_2 \cap B)}{\mathbb{P}(B)} \\
        &= \mathbb{P}_B(A_1) + \mathbb{P}_B(A_2).
        \end{align*}
    \end{proof}
\end{enumerate}

Now that we have established that $\mathbb{P}(\cdot | B)$ is a probability, it should satisfy all the consequences.
\begin{enumerate}[label = C\arabic*]
    \item \phantom{}
    \item $\mathbb{P}(A ^ c \mid B) = 1 - \mathbb{P}(A \mid B)$

    Recall C2, $A \in \mathcal{F}, \mathbb{P}(A ^ c) = 1 - \mathbb{P}(A)$
    \item \phantom{}
    \item \phantom{}
    \item If $A_1 \subseteq A_2,\, \mathbb{P}_B(A_1) \leq \mathbb{P}_B(A_2)$ or $\mathbb{P}(A_1 \mid B) \leq \mathbb{P}(A_2 \mid B)$.
    \item $\mathbb{P}(A_1 \cup A_2 \mid B) = \mathbb{P}(A_1 \mid B) + \mathbb{P}(A_2 \mid B) - \mathbb{P}(A_1 \cup A_2 \mid B)$
    \item If $A_1, A_2,\dotsc, A_n$ are pairwise disjoint, then
    \[
    \mathbb{P}\left(\bigcup_{i = 1}^{n}A_i\mmid B\right) = \sum_{i = 1}^{n}\mathbb{P}(A_i \mid B)
    \]
\end{enumerate}

P2: The multiplication rule

For any $2$ events $A, B \in \mathcal{F}$, with $\mathbb{P}(A) \neq 0$, $\mathbb{P}(B) \neq 0$, we have
\[
\mathbb{P}(A \cap B) = \mathbb{P}(A | B)\mathbb{P}(B) = \mathbb{P}(B | A)\mathbb{P}(A)
\]
\begin{proof}
    of the first equality.
    
    Since $\mathbb{P}(A) \neq 0$ and $\mathbb{P} \neq 0$, we can define $\mathbb{P}(B | A)$ and $\mathbb{P}(A | B)$.

    By definition
    \[
    \mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
    \]
    \[
    \mathbb{P}(A \cap B) = \mathbb{P}(A | B)\mathbb{P}(B)
    \]
\end{proof}

More general statement, $\mathbb{P}(A \cap B) | C = \mathbb{P}(B | C)\mathbb{P}(A | B \cap C)$, if $\mathbb{P}(B \cap C) > 0$,.

In the statement, it is enough to assume $\mathbb{P}(B \cap C) > 0$. Because $B \cap C \leq C$, $\mathbb{P}(B \cap C) \leq \mathbb{P}(C)$ follows from monotonicity (C5). Therefore $\mathbb{P}(C) > 0$, and hence we can define conditional probability of events, given $C$.

P3, general multiplication rule. Let $A_0, A_1, \dotsc, A_k$ be events such that
\[
\mathbb{P}\left(\bigcap_{i = 1}^{k}A_i\right) > 0\text{, then}
\]
\[
\mathbb{P}\left(\bigcap_{i = 1}^{k}A_i \mmid A_0\right) = \mathbb{P}(A_1 | A_0)\mathbb{P}(A_2 | A_1 \cap A_0)\mathbb{P}(A_3 | A_2 \cap A_1 \cap A_0) \dotsi \mathbb{P}\left(A_k \mmid \bigcap_{i = 0}^{k - 1}A_i\right)
\]

Recall (what does it mean for $E_i$ to be a partition:
\[
\bigcup_{i = 1}^{k}E_i = \Omega,\text{ where } E_i \cap E_j = \emptyset,\,\forall i, j (i \neq j)
\]
and $\mathbb{P}(E_i) > 0$, for each $1 \leq i \leq k$.

P4:
\begin{theorem}[Partition theorem (Law of total probability]
    Let $E_1, E_2, \dotsc, E_k$ be a partition of $\Omega$. Then for any event $A$, we have
    \[
    \mathbb{P}(A) = \sum_{i = 1}^{k}\mathbb{P}(E_i)\mathbb{P}(A | E_i).
    \]
    \begin{proof}
        \[
        A = \bigcup_{i = 1}^{k}(A \cap E_i)
        \]
        Since $E_i$'s form a partition,
        \[
        (A \cap E_i) \cap (A \cap E_j) = \emptyset, \text{ for all } i \neq j.
        \]
        Hence, we can use A3, to get
        \[
        \mathbb{P}(A) = \sum_{i = 1}^{k}\mathbb{P}(A \cap E_i).
        \]
        Now use (P2) on the previous to get
        \[
        \mathbb{P}(A) = \sum_{i = 1}^{k}\mathbb{P}(E_i)\mathbb{P}(A | E_i).
        \]
    \end{proof}
\end{theorem}

\begin{example}
    There are $3$ machines $A,\, B$ and $C$. $10\%$ of components produced by $A$ are faulty, $20\%$ of $B$ are faulty and $30\%$ of $C$ are faulty.
    Equal number of components are collected from each machine. One component is selected at random. Find $\mathbb{P}(\text{the component is faulty}$.

    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        Let $F := \{\text{the component is faulty}\}$

        Let $M_A := \{\text{component comes from $A$}\}$

        Similarly define $M_B$ and $M_C$.
        \[
        \mathbb{P}(M_A) = \mathbb{P}(M_B) = \mathbb{P}(M_C) = \frac{1}{3}.
        \]
        \[
        \mathbb{P}(F | M_A) = \frac{10}{100},\,\mathbb{P}(F | M_B) = \frac{20}{200},\,\mathbb{P}(F | M_C) = \frac{30}{100}.
        \]
        \begin{align*}
        \mathbb{P}(F) &= \mathbb{P}(M_A)\mathbb{P}(F | M_A) + \mathbb{P}(M_B)\mathbb{P}(F | M_B) + \mathbb{P}(M_C)\mathbb{P}(F | M_C) \\
        &= \frac{1}{3}\times \frac{10 + 20 + 30}{100} = \frac{2}{10} = 0.2.
        \end{align*}
    \end{proof}
\end{example}

P5:
\begin{theorem}[Bayes' Theorem]
    Let $A$ and $B$ be two events, such that $\mathbb{P}(A) > 0,\, \mathbb{P}(A) > 0$, then
    \[
    \mathbb{P}(A | B) = \frac{\mathbb{P}(B | A)\mathbb{P}(A)}{\mathbb{P}(B)}.
    \]

    Bayes' Theorem (general)
    Let $A,\, B$ and $C$ be three events, such that $\mathbb{P}(A | C) > 0,\, \mathbb{P}(B | C) > 0$, then
    \[
    \mathbb{P}(A | B \cap C) = \frac{\mathbb{P}(A | C)\mathbb{P}(B | A \cap C)}{\mathbb{P}(B | C)}.
    \]
\end{theorem}

\begin{example}
    Find $\mathbb{P}(M_A | F)$
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
    \[
    \mathbb{P}(M_A | F) = \frac{\mathbb{P}(F | M_A)\mathbb{P}(M_A)}{\mathbb{P}(F)} = \frac{\frac{10}{100} \times \frac{1}{3}}{\frac{2}{10}} = \frac{1}{6}.
    \]
    \end{proof}
\end{example}

P6:
\begin{theorem}[Bayes' theorem (for partitions)]
   Let $A_1, \dotsc, A_k$ be a partition and let $B$ be an event, with $\mathbb{P}(B) > 0$. Then
   \[
   \mathbb{P}(A_j | B) = \frac{\mathbb{P}(A_j)\mathbb{P}(B | A_j)}{\sum_{i = 1}^{k}\mathbb{P}(B | A_i)\mathbb{P}(A_i)}
   \]
   \begin{proof}
   \[
   \mathbb{P}(A_j | B) = \frac{\mathbb{P}(A_j)\mathbb{P}(B | A_j)}{\mathbb{P}(B)}
   \]
   now use partition theorem to get
   \[
   \mathbb{P}(B) = \sum_{i = 1}^{k}\mathbb{P}(B | A_i)\mathbb{P}(A_i).
   \]
   \end{proof}
\end{theorem}


\begin{example}
    $\mathbb{P}(\text{it rains}) = \frac{1}{2}$ if it rains, Charlie the cat will go out with probability $\frac{1}{10}$.
    If it is day with probability $\frac{3}{5}$.
    If Charlie goes out, find the probability that it rained.
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
       Let $R := \{\text{it rains}\}$

       $C := \{\text{Charlie goes out}\}$

       \[
       \mathbb{P}(R) = \frac{1}{2},
       \]
       we want to find $\mathbb{P}(R | C)$.
       \[
       \mathbb{P}(R | C) = \frac{\mathbb{P}(R)\mathbb{P}(C | R)}{\mathbb{P}(C)} = \frac{\frac{1}{2} \times \frac{1}{10}}{\mathbb{P}(C)}.
       \]
       \begin{align*}
           \mathbb{P}(C) &= \mathbb{P}(C | R)\mathbb{P}( R) + \mathbb{P}(C | R ^ c)\mathbb{P}(R ^ c) \\
           &= \frac{1}{10} \times \frac{1}{2} + \frac{3}{5} \times \frac{1}{2} \\
           &= \frac{1}{2} \times \frac{7}{10}
       \end{align*}
       \[
       \mathbb{P}(R | C) = \frac{1}{7}.
       \]
    \end{proof}
\end{example}

\subsection{Independence of events}

\begin{definition}[Independence of two events]
    Let $A, B$ be $2$ events, then $A$ and $B$ are said to be independent if
    \[
    \mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B).
    \]
\end{definition}
Recall: $\mathbb{P}(A \cap B) = \mathbb{P}(A | B)\mathbb{P}(B)$. Then what the above formula is telling us is,
\[
\mathbb{P}(A | B) = \mathbb{P}(A).
\]

\begin{example}
    Roll a fair die.
    \[
    A_1 = \{2, 4, 6\}(\text{even score}\}\quad A_2 = \{3, 6\}(\text{score divisible by $3$})
    \]
    \[
    \mathbb{P}(A_1 \cap A_2) = \mathbb{P}(\{6\}) = \frac{1}{6}.
    \]
    \[
    \mathbb{P}(A_2)\mathbb{P}(A_2) = \frac{3}{6} \times \frac{2}{6} = \frac{1}{6} = \mathbb{P}(A_1 \cap A_2).
    \]
     Hence, $A_1$ and $A_2$ are independent.

     $A_3 = \{4, 5, 6\}$. Show that $A_1$ and $A_3$ are not independent.
\end{example}

Independence cannot be observed in a Venn diagram.

\begin{theorem}
    Let $A$ and $B$ be two events, with $\mathbb{P}(A) > 0,\,\mathbb{P}(B) > 0$, then the following are equivalent.
    \begin{enumerate}[label = (\roman*)]
        \item $\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$.
        \item $\mathbb{P}(A | B) = \mathbb{P}(A)$.
        \item $\mathbb{P}(B | A) = \mathbb{P}(B)$.
    \end{enumerate}
    \begin{proof}
        We have just proved (i) $\implies$ (ii)    
        proof of (ii) $\implies$ (iii):
        \begin{equation}
        \mathbb{P}(B | A)\mathbb{P}(A) = \mathbb{P}(A \cap B) = \mathbb{P}(A | B)\mathbb{P}(B).
        \end{equation}
        From (2) we get
        \[
        \mathbb{P}(B | A)\mathbb{P}(A) = \mathbb{P}(A | B)\mathbb{P}(B)
        \]
        Since we assume (ii), we have $\mathbb{P}(A | B) = \mathbb{P}(A)$ so,  we get $\mathbb{P}(B | A)\mathbb{P}(A) = \mathbb{P}(A)\mathbb{P}(B)$ or, $\mathbb{P}(B | A) = \mathbb{P}(B)$.
    \end{proof}
\end{theorem}

\begin{theorem}
    Let $A,\, B,\, C$ be $3$ events, such that $\mathbb{P}(A \cap B \cap C) > 0$, then the following are equivalent\footnote{This tells us $A$ and $B$ are conditionally independent of $C$.}
    \begin{enumerate}[label = (\roman*)]
        \item $\mathbb{P}(A \cap B | C) = \mathbb{P}(A | C)\mathbb{P}(B | C)$,
        \item $\mathbb{P}(A | B \cap C) = \mathbb{P}(A | C)$,
        \item $\mathbb{P}(B | A \cap C) = \mathbb{P}(B | C)$.
    \end{enumerate}
\end{theorem}

\textbf{Independence of multiple events}
\begin{definition}
    Let $A_1, A_2, \dotsc, A_n$ be events, then they are mutually independent, if for every finite subcollection\footnote{Effectively equivalent to finite subsets.} $\mathcal{C}$ of indices $\{1, 2, \dotsc, n\}$,
    \[
    \mathbb{P}\left(\bigcap_{i \in \mathcal{C}}A_i\right) = \prod_{i \in \mathcal{C}}\mathbb{P}(A_i).
    \]
\end{definition}

\newpage

\section{Applications}

\subsection{Reliability of networks}
Reliability network/current flow.

Series: current flows from left to right. 

Parallel: current flows from left to right.

Assumption: current flows/system works from left to right,
system works if we can draw arrows from left to right.

Taking a series circuit (with two components, $1$ and $2$).
The system is functional if both components ($1$) and ($2$) are working

Let $w_i := \text{event that $i$th component is working}$

Let $S := \text{event system is working}$

$S = W_1 \cap W_2$.

\begin{definition}
    A reliability network is a diagram of arcs and nodes.
\end{definition}

Let $W_i$ be the event that the $i^{\text{th}}$ component is working.

We say that the system is working if we can go from left to right using working components only.

Let $S$ denote the event that the system is working.

Let $p_i$ be the probability that the $i^{\text{th}}$ component is working,
that is,
$\mathbb{P}(W_i) = p_i$.

(A) Series case:
System is working if both (1) and (2) are working.
$S = W_1 \cap W_2$. \\

(B) Parallel case:
System is working if either (1) or (2) (or both) is working.
$S = W_1 \cup W_2$. \\

(C) Combination case:
System is working if both (1) and (2) are working or (3) and (4) are working or both.
$S = (W_1 \cap W_2) \cup (W_3 \cap W_4)$.

Assumption: The components work independent of each other.

We want to find
$\P(\text{System is working)}) = \P(S).$

Recall: $\P(A \cap B) = \P(A)\P(B)$ if $A$ and $B$ are independent.

\begin{align*}
    \P(S) &= \P(W_1 \cup W_2) \\
    &= \P(W_1) + \P(W_2) - \P(W_1 \cap W_2) \\
    &= p_1 + p_2 - p_1p_2\footnotemark
\end{align*}
\footnotetext{By C6: $\P(A \cup B) = \P(A) + \P(B) - \P(A \cap B)$.}

(C)
\begin{align*}
    \P(S) &= \P((W_1 \cap W_2) \cup (W_3 \cap W_4)) \\
    &= \P(W_1 \cap W_2) + \P(W_3 \cap W_4) - \P((W_1 \cap W_2) \cap (W_3 \cap W_4)) \\
    &= p_1p_2 + p_3p_4 - p_1p_2p_3p_4\footnotemark
\end{align*}
\footnotetext{Because they are independent.}

\begin{example}
    Find $\P(S)$ for the following diagram
    
    \textbf{Gr12}
\end{example}


\subsection{Genetics}
\begin{example}
    A gene $X$ can have either allele $A$ or $a$.
\end{example}

Female: $XX$.

Male: $XY$.

Usually $A$ means dominant gene,
and $a$ means recessive gene.

A male child gets $X$ from mother and $Y$ from father.

So the allele on the male child's $X$ chromosome is from the mother.
So equal chance to inherit either of the alleles.


\begin{example}
    $aa$ is unhealthy (female).
    $a$ is unhealthy (male).
    
    Jane is healthy. Maternal aunt of Jane has an unhealthy son. ($a$ - allele). Jane's maternal grandparent's and father are healthy.
    \begin{enumerate}[label = (\roman*)]
        \item Find $\P(\text{Jane is } Aa)$.
    \end{enumerate}

    \textbf{Gr13}
    
    Let $J := \{\text{Jane is } Aa\}$.
    
    Let $M_1 := \{\text{Jane's mother is } AA\}$.
    
    Let $M_2 := \{\text{Jane's mother is } Aa\}$.

    Let $J := \{\text{Jane is } Aa\}$

    \[
    \P(M_2) = \P(\text{Jane's mother got $a$ from her mother}) = \frac{1}{2}.
    \]
    
    \begin{align*}
    \P(J) &= \underbrace{\P(J | M_1)}_{= 0}\P(M_1) + \P(J | M_2)\P(M_2) \\
    &= \P(J | M_2)\P(M_2) \\
    &= \frac{1}{2} \cdot \frac{1}{2}.
    \end{align*}
\end{example}

\newpage

\section{Random variables}

\subsection{Definition and notation}

Notation:

$X, Y$ is a general random variable.

\begin{example}[label = exmp1]
    Toss a coin three times. Let $X$ denote the number of heads.

    $H$ - Heads
    
    $T$ - Tails

    $\Omega = \{(H, H, H), (H, H, T), (H, T, H), (H, T, T), (T, H, H), (T, H, T), (T, T, H), (T, T, T)\}$.

    \begin{table}[h!]
        \centering
        \begin{tabular}{c|cccccccc}
             $\omega$ & $HHH$ & $HHT$ & $HTH$ & $HTT$ & $THT$ & $TTH$ & $THH$ & $TTT$ \\
             \hline
             $X(\omega)$ & 3 & 2 & 2 & 1 & 1 & 1 & 2 & 0 
        \end{tabular}
    \end{table}
\end{example}

$X : \Omega \rightarrow \R$
$X$ is a map from $\omega \mapsto X(\omega) \in \R$

\begin{definition}[Random variable]
    A random variable on $\Omega$ is a mapping from $\Omega$ to some set of possible values,
    which we denote by $X(\Omega) = \{X(\omega) : \omega \in \Omega\}$
    \footnote{We will most be concerned with $X(\Omega) \subseteq \R$ or $X(\Omega) \subseteq \R ^ d$, $X$ is real-values or vector valued.}.
\end{definition}

$X : \Omega \rightarrow \chi$.
For our examples
$\chi = \R \text{ or } \R ^ d$.

The image of $X$ is denoted by $X(\Omega) = \{X(\omega) : \omega \in \Omega\}$.
\begin{example}
    Throw $2$ "fair" dice and let $X$ be the random variable denoting sum of the scores.
    \[
    \Omega = \{(1, 1), (1, 2), \dotsc, (1, 6), (2, 1), (2, 2), \dotsc\} = \{(i, j) : 1 \leq i \leq 6,\, 1 \leq j \leq 6\}.
    \]
    $|\Omega| = 6 ^ 2$
    \begin{align*}
        X : \Omega &\rightarrow \R\qquad (i, j) \mapsto X(i, j) = i + j \\
        \omega &\mapsto X(\omega)
    \end{align*}
    where $i, j$ are the scores for the first and second dice respectively.
\end{example}

Let $f : \Delta \rightarrow\R$, then the pre-image for a set $B \subseteq \R$,
is defined as $f ^ {-1}(B) := \{x \in \Delta : f(x) \in B\}$.

\begin{example}[continues = exmp1]
    Let $A_1 = \{X = 2\}$
    
    $A_2 = \{X \in [0, 1.5]\}$
    
    $A_3 = \{2.5 \leq X \leq 4.5\}$.

    $A_1 = \{\omega : X(\omega) = 2\} = X ^ {-1} = \{\text{HHT, HTH, THH}\} \subseteq \Omega$.
    \[
    \P(A_1) = \frac{|A_1|}{|\Omega|} = \frac{3}{2 ^ 3} = \frac{3}{8}
    \]

    $A_2 = \{\omega : 0 \leq X(\omega) \leq 1.5\} = X ^ {-1}([0, 1.5]) = \{\omega : X(\omega) = 0\text{ or }X(\omega) = 1\} = \{TTT, HTT, THT, TTH\}$
    \[
    \P(A_2) = \frac{|A_2|}{|\Omega|} = \frac{4}{2 ^ 3} = \frac{1}{2}
    \]
\end{example}
\[
\P_X(B) := \P(X \in B) = \P\left(X ^ {-1}(B)\right)
\]
\textit{Note: For us $\chi$ will be mostly in $\R$ or $\R ^ d$, or subsets of $\R$ and $\R ^ d$}

\begin{definition}
    The function $P_X : X(\Omega) \rightarrow [0, 1]$ defined by $\P_X(B) := \P(X \in B) = \P(X^{-1}(B) = \P(\{\omega : X(\omega) \in B\})$
    for any set $B \subseteq X(\Omega)$.
    $\P_X$ is called the distribution\footnote{Or cumulative density/distribution function.} of $X$.
\end{definition}

\begin{theorem}
    Let $X: \Omega \rightarrow \R$ be a random variable.
    Then $\P_X$ is a probability on $\R$.
    \begin{proof}
        A1, A2, A3 (prove A4)

        \begin{enumerate}[label = (A\arabic*)]
            \item
            \item $\P_X(\R) = 1$
            $P_X(\R) := \P(X ^ {-1}(\R)) = \P(\Omega) = 1$

            $X ^ {-1}(\R) := \{\omega : X(\omega) \in \R\} = \Omega$.
            \item Let $A, B \subseteq \R$, such that $A \cap B = \emptyset$, then
            \[
            P_X(A \cup B) = \P_X(A) + \P_X(B).
            \]
            \[
            X^{-1}(A \cup B) = X^{-1}(A) \cup X^{-1}(B)?
            \]
            Since $A \cap B = \emptyset$ is it true that $X^{-1}(A)\cap X^{-1}(B) = \emptyset$
        \end{enumerate}
    \end{proof}
\end{theorem}
\begin{align*}
P_X(A \cup B) &= \P(X ^ {-1}(X \cup B)) \\
&= \P(X ^ {-1}(A) \cup X ^ {-1}(B)) \\
&= \P(X ^ {-1}(A) + \P(X ^ {-1}(B))) \\
&= \P_X(A) + \P_X(B).
\end{align*}
Let $A$ and $B$ be disjoint, i.e. $A \cap B = \emptyset$,
then
\[
X ^ {-1}(A) \cap X ^ {-1}(B) = \emptyset
\]
\begin{proof}
    Let us prove by contradiction,
    that is, let us assume that $X ^ {-1}(A) \cap X ^ {-1}(B) \neq \emptyset$,
    and then we will arrive at a contradiction,
    that is our assumption was not right.

    If the assumption holds,
    then $\exists \omega \in X ^ {-1}(A) \cap X ^ {-1}(B)$.
    For this $\omega$,
    we have $X(\omega) \in A$ and $X(\omega) \in B$.

    i.e. $\omega \in X ^ {-1}(A)$ and $\omega \in X ^ {-1}(B)$.
    From $X(\omega) \in A$ and $X(\omega) \in B$ implies that $A \cap B \neq \emptyset$,
    which contradicts $A \cap B = \emptyset$.
    Hence our first assumption cannot hold.
\end{proof}

\begin{definition}[Indicator random variable]
    Let $A \subseteq \Omega$,
    then the indicator of $A$,
    denoted by $\mathbb{1}_A$,
    is defined as follows,
    \begin{align*}
    \mathbb{1}_A(\omega) &= 1,\text{ if } \omega \in A \\
    &= 0,\text{ if } \omega \notin A.
    \end{align*}
\end{definition}

$\P(A) = \frac{|A|}{|\Omega|}$
\[
|A| = \sum_{\omega \in A}\mathbb{1}_A(\omega).
\]

\begin{example}
    Let $\Omega = \{\omega_1, \omega_2, \dotsc, \omega_5\}$,
    $A = \{\omega_1, \omega_2, \omega_4\}$
    \[
    \mathbb{1}_A(\omega_1) = \mathbb{1}_A(\omega_2) = \mathbb{1}_A(\omega_4) = 1.
    \]
    \[
    \mathbb{1}_A(\omega_3) = \mathbb{1}_A(\omega_5) = 0.
    \]
    $\P_A(\{1\}) = \P(\{\omega\,:\,\mathbb{1}_A(\omega) = 1\}) = \P(A)$
    \[
    \P_A(\{0\}) = \P(\{\omega\,:\,\mathbb{1}_A(\omega) = 0\}) = \P(A ^ c)
    \]
\end{example}

\textit{We will write $\P_X(\{x\}) = \P_X(x) = \P(\{X = x\}) = \P(X = x)$.}

\subsection{Discrete random variables}
\begin{definition}[Discrete random variable]
    Let $X$ be a random variable $X$ is discrete if there exists $\chi \subseteq X(\Omega)$,
    such that $\chi$ is either finite or countable.

    We will define $p : \chi \rightarrow [0, 1]$,
    given by
    \[
    p(x) = \P(X = x),\text{ for every } x \in \chi.
    \]
    This is called the probability mass function (pmf).
\end{definition}

\begin{example}[continues = exmp1]
    Toss a fair coin three times.
    Let $X$ denote the number of heads.

    $\chi = \{0, 1, 2, 3\}$.

    pmf of $X$.
    \begin{table}[h!]
        \begin{tabular}{c|cccc}
            $x$ & $0$ & $1$ & $2$ & $3$ \\
            \hline
            $p(x)$ & $\frac{1}{8}$ & $\frac{3}{8}$ & $\frac{3}{8}$ & $\frac{1}{8}$ \\
        \end{tabular}
    \end{table}

    $|\Omega| = 2 ^ 3$

    $\{\omega\,:\, X(\omega) = 0\} = \{\text{TTT}\}$
    
    $\{\omega\,:\, X(\omega) = 1\} = \{\text{HTT, THT, TTH}\}$
    
    $\{\omega\,:\, X(\omega) = 2\} = \{\text{HHT, HTH, THH}\}$
    
    $\{\omega\,:\, X(\omega) = 3\} = \{\text{HHH}\}$
\end{example}

\begin{theorem}
    Let $X$ be a discrete random variable,
    with probability mass function $p$.
    Then for any $A \subseteq \chi$,
    \begin{equation}
    \P(X \in A) = \sum_{x \in A}p(x).
    \end{equation}
    Furthermore,
    \begin{equation}
    \sum_{x \in \chi}p(x) = 1.
    \end{equation}
    \begin{proof}
        $A \subseteq \chi$,
        where $\chi$ is countable or finite (can enlist/enumerate the elements of $A$).
        Let $A = \{a_1, a_2, \dotsc\} = \bigcup_{i = 1}^{\infty}\{a_i\}$
        \[
        \P(X \in A) = \P\left(\bigcup_{i = 1}^\infty\{X = a_i\}\right)
        \]
        observe that $\{X = a_i\} \cap \{X = a_j\} = \emptyset$ for $i \neq j$.
        So
        \begin{align*}
        \P(X \in A) &= \P\left(\bigcup_{i = 1}^\infty\{X = a_i\}\right) \\
        &= \sum_{i = 1}^{\infty}\P(X = a_i) &\text{by (A4)} \\
        &= \sum_{i = 1}^{\infty}p(a_i) \\
        &= \sum_{x \in A}p(x).
        \end{align*}
        Since we have established ($1$),
        we know that
        \begin{equation}
        \P(x \in \chi) = \sum_{x \in \chi}p(x)
        \end{equation}
        and we already know that
        \begin{equation}
            \P(\chi) = 1
        \end{equation}
        ($3$) and ($4$) together complete the proof of ($2$)
        
    \end{proof}
\end{theorem}

$\chi$ is the collection of all values that the random variable $X$ can take.

A set $A$ is countable if and only if the elements of $A$ can be written as a potentially infinite sequence or list.

(A3) $\P(A \cup B) = \P(A) + \P(B)$, when $A \cap B = \emptyset$.

(A4) is an extension of (A3) for countable collections.
Let $A_1, A_2, \dotsc$ be a countable collection of pairwise disjoint sets,
i.e. $A_i \cap A_j - \emptyset$, for all $i \neq j$.
Then
\[
\P\left(\bigcup_{i = 1}^{\infty}A_i\right) = \sum_{i = 1}^{\infty}\P(A_i)
\]

\subsubsection{Summary}
$X$ is a discrete random variable,
the probability mass function is
\[
A \subseteq \chi, \P(A \in A) = \sum_{x \in A}p(x)
\]
and
\[
\sum_{x \in \chi}p(x) = 1.
\]

$\P_X(A)$ is a distribution
\[
\P_X(A) = \P(X \in A) = \sum_{x \in A}p(x)
\]
\[
\P_X(\chi) = \P(X \in \chi) = \sum_{x \in \chi}p(x).
\]

\subsection{Binomial and geometric random variable}
$p$ is the probability of success.
Repeat the trials $n$-times given and fixed.
Each trial is independent.
The outcomes are success and failure.
Let $X$ denote the number of successes in $n$-trials.

Let $\Omega = \{\omega:\, \omega = (\omega_1\omega_2\dotsi\omega_n) \text{ where } \omega_i = \text{S/F for } 1 \leq i \leq n\}$.

$X(\omega) = $ number of successes in the sequence $(\omega_1\omega_2\dotsi\omega_n)$.
Let us assign the value $1$ to success and $0$ to failure i.e. $\omega_i = 1$ if success $\omega_i = 0$ if failure.

For a given $\omega_1$
$X(\omega) = $ number of $i$, for which $\omega_i = 1$, $X(\omega) = \sum_{i = 1}^{n}\omega_i$.

Let $\omega = (\omega_1\dotsc \omega_n) \in \Omega$,
with $x$-successes and $(n - x)$ failures.
\[
\P(\{\omega\}) = p ^ x (1 - p) ^ {n - x}
\]
therefore
\[
\P(X = x) = \P\left(\bigcup_{w : \sum_{i = 1}^n\omega_i = x}\{\omega\}\right) = \sum_{w : \sum_{i = 1}^n\omega_i = x}\P(\{\omega\}) = \binom{n}{x}p ^ x (1 - p) ^ {n - x}.
\]
\[
\{X = x\} = \{\omega : X(\omega) = x\} = \{\omega : \omega = (\omega_1\dotsc \omega_n,\text{ then } \sum_{i = 1}^n\omega_i = x\}
\]
\begin{definition}[Binomial random variable]
    A discrete random variable $X$ is binomially distributed with parameters $n$ and $p \in [0, 1]$ if $\chi = \{0, 1, \dotsc, n\}$,
    and the probability mass function,
    is given by
    \[
    p(x) = \binom{n}{x}p ^ x(1 - p) ^ {n - x},\text{ if } x \in \{0, 1, \dotsc, n\},
    \]
    and $= 0$ elsewhere.

    We denote this as
    $X \sim \Bin(n, p)$.
\end{definition}

For $n = 1,
\Bin(1, p)$ is referred to as the Bernoulli random variable
(with success parameter $p$),
written as
\[
\mathrm{Ber}(p).
\]

\textbf{Geometric random variable}

Let us toss a coin until we get heads.
Let $X$ denote the number of tosses that we require
\[
\P(X = x) = \P(\text{first $(x - 1)$ tosses were $T$, and $x$-th toss is $H$}) = (1 - p) ^ {x - 1}p.
\]

\begin{definition}
    $X$ is geometrically distributed with parameter $p \in (0, 1]$,
    if $\chi = \N = \{1, 2, \dotsc\}$ with the probability mass function given by
    \begin{align*}
        p(x) &= (1 - p) ^ {x - 1}p,&x \in \N \\
        &= 0,&\text{elsewhere}.
    \end{align*}
    This is denoted $X \sim \Geo(p)$.
\end{definition}

\begin{example}
    $105$ people who bought a flight ticket,
    $\P(\text{that a person misses a flight}) = 0.04$.
    Each individual misses the flight independent of the other.
    Find
    \begin{enumerate}[label = (\alph*)]
        \item $\P(\text{nobody misses the flight})$.
        \item $\P(\text{$3$ or more people miss the flight})$.
    \end{enumerate}

    Let $X := \text{number of people who miss the flight}$, $p := \P(\text{of success}) = 0.04$.
    $n = 105$.
    $X \sim \Bin(105, 0.04)$.
    \begin{enumerate}[label = (\alph*)]
        \item
        \begin{align*}
        \P(\text{nobody misses the flight}) &= \P(X = 0) \\
        &= \binom{105}{0}p ^ 0(1 - p) ^ {105 - 0} \\
        &= (0.96) ^ {105} \approx 0.014.
        \end{align*}
        \item
        \begin{align*}
            \P(\text{$3$ or more people miss the flight}) &= \P(X \geq 3) \\
            &= 1 - \P(X < 3) \\
            &= 1 - p(0) - p(1) - p(2) \\
            &= \dotsc
        \end{align*}
    \end{enumerate}
\end{example}

Let us come back to $\Geo(p)$,
where $p > 0$.
Then observe that
\[
\sum_{x = 1}^{\infty}(1 - p) ^ {x - 1}p = p\sum_{x = 1}^{\infty}(1 - p) ^ {x - 1}
\]
write $x - 1 = t$,
then we get
\[
\sum_{x = 1}^{\infty}(1 - p) ^ {x - 1}p = p\left(\sum_{t = 0}^{\infty}(1 - p) ^ t\right)\text{Geometric series with common ratio $(1 - p)$.}
\]
which results in
\begin{align*}
    \P(x \in \N) &= \sum_{x = 1}^{\infty}(1 - p) ^ {x - 1}p \\
    &= p\left(\sum_{t = 0}^{\infty}(1 - p) ^ t\right) \\
    &= p \cdot \frac{1}{1 - (1 - p)} \\
    &= p \cdot \frac{1}{p} = 1.
\end{align*}

\subsection{Poisson random variable}
\begin{definition}
    A random variable $X$ with $\chi = \Z_+ := \{0, 1, 2, \dotsc\}$ is Poisson with parameter $\lambda (> 0)$,
    if the probability mass function
    \begin{align*}
        p(x) &= \P(X = x) = \frac{e ^ {-\lambda}\lambda ^ x}{x!},& x \in \Z_+ \\
        &= 0,&\text{elsewhere}.
    \end{align*}
    This is denoted $\Po(x)$, with $X \sim \Po(\lambda)$.
\end{definition}
$\lambda$ is taken as the average number of changes in a small amount of time.

\begin{example}
    Some airline has $32$ crashes in a $25$ year period.
    Where a year is $365$ days,
    there are $30$ days in a month,
    and $7$ days in a week.

    Let $Y, M, W$ denote the number of crashes in the next year,
    month
    and week.
    \begin{enumerate}[label = (\alph*)]
        \item Find the distributions for $Y, M$ and $W$.
        \item Find $\P(\text{there are no crashes in the next week})$.
    \end{enumerate}
    \begin{enumerate}[label = (\alph*)]
        \item 
        The daily rate of accidents is $\frac{32}{365 \cdot 25} = \frac{32}{9125}$.
        $\lambda_Y$ is the average number of crashes per year which is $\frac{32}{9125} \cdot 365$,
        $\lambda_M$ is the average number of crashes per month which is $\frac{32}{9125} \cdot 30$,
        $\lambda_W$ is the average number of crashes per week which is $\frac{32}{9125} \cdot 7$.
        $Y \sim \Po(\lambda_Y), M \sim \Po(\lambda_M), W \sim \Po(\lambda_W)$
        \item
        \begin{align*}
            \P(\text{there are no crashes in the next week}) &= \P(W = 0) \\
            &= e ^ {\lambda w}\frac{\lambda_W ^ 0}{0!} \\
            &- e ^ {-\lambda_W}.
        \end{align*}
    \end{enumerate}
\end{example}

\subsection{Continuous random variables}
\begin{definition}
    Let $X : \Omega \rightarrow \R$ be a random variable.
    We say $X$ is a continuous random variable or $X$ has a continuous probability distribution,
    if there exists a non-negative function $f$,
    $f : \R \rightarrow \R$,
    such that
    \[
    \P(X \in [a, b]) = \int_a^bf(t)\,dt,\qquad a \leq b, a, b \in \R.
    \]
    $f$ is called the probability density function of $X$ (pdf).
    The probability density function of $X$ is denoted $f_X$.
\end{definition}
Let $x \in \R$,
choose $a = b = x$
\[
\P(X = x) = \int_x^xf(t)\,dx = 0.
\]
Given this is a continuous random variable,
this shows us that the continuous random variable has no mass at a single point
\[
\P(X \in (a, b)) = \P(X \in [a, b)) = \P(X \in (a, b]) = \P(X \in [a, b]) = \int_a^bf(t)\,dt.
\]
Comment: $\P(X = x) \neq f(x)$

\begin{example}
    $f_1(x) = 1, x \in [0, 1]$ and is $0$ elsewhere.
    $x_1$ is the random variable with the pdf $f_1$
    
    $f_2(x) = 1, x \in [0, 1)$ and is $0$ elsewhere.
    $x_2$ is the random variable with pdf $p_2$.

    For any interval $[a, b]$, $a \leq b$.
    \[
    \P(X_1 \in [a, b]) = \P(x_2 \in [a, b])
    \]
    \[
    \P(x_1 \in [0, 1]) = \int_0^1f_1(t)\,dt = 1 = \P(x_2 \in [0, 1]) = \int_0^1f_2(t)\,dt.
    \]

    (For this course),
    we will assume that pdf is unique except for a finitely many points,
    that is,
    if $X$ is a random variable.
    Let $f$ and $g$ be two possible candidates for pdfs of $X$,
    then there exists a finite $k \geq 1$,
    and a sequence $a_1, a_2, \dotsc, a_k$,
    such that $f(x) = g(x)$,
    for $x \notin \{a_1, a_2, \dotsc, a_k\}$.

    The pdf is unique up to a set of measure zero.
\end{example}

\begin{theorem}\label{prob_thm_622}
    Let $B \subseteq \R$,
    such that $\displaystyle B = \bigcup_{i = 1}^{m}[a_i, b_i]$
    ($B$ is a finite union of intervals,
    the intervals may be open or closed or a mixture).
    Then
    \[
    \P(X \in B) = \int_Bf(x)\,dx.
    \]
\end{theorem}

\begin{corollary}
    If the intervals are disjoint then
    \[
    \P(X \in B) = \sum_{i = 1}^{m}\int_{a_i}^{b_i}f(x)\,dx.
    \]
\end{corollary}

\begin{itemize}
    \item 
    \[
    \P(X \in (-\infty, b]) = \int_{-\infty}^bf(t)\,dt.
    \]
    \item 
    \[
    \P(X \in [a, \infty)) = \int_a^{\infty}f(t)\,dt.
    \]
    \item 
    \[
    \P(-\infty < X < \infty).
    \]
\end{itemize}

\begin{corollary}\label{prob_col_623}
    \[
    \P(X \in \R) = \P(-\infty < X < \infty) = \int_{-\infty}^{\infty}f(t)\,dt = 1.
    \]
\end{corollary}

\begin{example}
    Let $X$ be a random variable,
    with pdf
    \[
    f(x) = \begin{cases}
        k(1 + x), & -1 \leq x < 0, \\
        k(2 - x), & 0 \leq x \leq 2, \\
        0, & \text{elsewhere}.
    \end{cases}
    \]
    So $f$ is piecewise continuous between $(-\infty, -1), [-1, 0], [0, 2]$ and $(2, \infty)$.

    \textbf{Exercise: } check whether it is continuous everywhere.
\end{example}

From \autoref{prob_col_623} we know that
\begin{equation}
    \int_{-\infty}^{\infty}f(x) = 1,\text{ if } f \text{ is a pdf}.
\end{equation}
Then from \autoref{prob_thm_622},
$(-\infty, \infty) = \underbrace{(-\infty, -1) \cup [1, 0) \cup [0, 2] \cup (2, \infty)}_{\text{disjoint}}$.

So from \autoref{prob_thm_622},
we know that
\[
\int_{-\infty}^{\infty}f(x)\,dx = \int_{-\infty}^{-1}f(x)\,dx + \int_{-1}^0f(x)\,dx + \int_0^2f(x)\,dx + \int_2^{\infty}f(x)\,dx.
\]
\begin{align*}
    \int_{-\infty}^{\infty}f(x)\,dx &= \int_{-1}^{0}f(x)\,dx + \int_0^2f(x)\,dx \\
    &= k\int_{-1}^0(1 + x)\,dx + k\int_0^2(2 - x)\,dx \\
    &= \frac{k}{2}(1 + x) ^ 2\mid^0_{x = -1} - \frac{k}{2}(2 - x) ^ 2\mid^2_{x = 0} \\
    &= \frac{k}{2} + 2k \\
    &= \frac{5}{2}k.
\end{align*}
Therefore,
putting in ($1$),
we get $\frac{5}{2}k = 1 \implies k = \frac{2}{5}$.
Find $\P(X \in [0, 1])$
\[
\P(X \in [0, 1]) = \int_0^1f(x)\,dx = k\int_0^1(2 - x)\,dx = \frac{k}{2}(2 - x) ^ 2\mid_{x = 0}^1 = \frac{3}{2}l = \frac{3}{5}
\]
Recall $k = \frac{2}{5}$.

\subsection{Uniform distribution}
\begin{definition}
    Let $X$ be uniformly distribution $[a, b]$,
    $X \sim U[a, b]$,
    if the pdf is given by
    \[
    f(x) = \begin{cases}
        \frac{1}{b - a}, & x \in [a, b] \\
        0, & \text{elsewhere}.
    \end{cases}
    \]
\end{definition}

\subsection{Exponential distribution}
\begin{definition}
    Let $\beta > 0$.
    We say a random variable $X$ is distributed exponentially with parameter $\beta$,
    $X \sim \Exp(\beta)$,
    if the pdf $f$ is given by
    \[
    f(x) = \begin{cases}
        \beta e ^ {-\beta x}, & x \geq 0, \\
        0, & \text{elsewhere}.
    \end{cases}
    \]
\end{definition}
\textbf{Exercise:} find $\P(X = 0)$,
when $X \sim \Exp(\beta)$.

\subsection{The normal distribution}
\begin{definition}
    A random variable $X$ is normally distributed with parameters $\mu$,
    $\sigma ^ 2$, $(\mu, \sigma \in \R, \sigma > 0)$.
    If the pdf is given by
    \[
    f(x) = \frac{1}{\sigma\sqrt{2\pi}}e ^ {-\frac{1}{2}(\frac{x - \mu}{\sigma}) ^ 2}
    \]
    where $x \in \R$.
\end{definition}
Later on we will see $\mu$ is the mean,
and $\sigma$ is the standard deviation,
$\sigma ^ 2$ is the variance.
We will later see that it is enough to understand the normal distribution for $\mu = 0$ and $\sigma = 1$.

\subsection{Cumulative distribution functions}\footnote{cdf.}
Recall,
that for any random variable $X$
\[
\P_X(B) = \P(X \in B),\quad B \subseteq \R
\]
$B = (-\infty, x]$,
\[
\P_X((-\infty, x]) = \P(X \in (-\infty, x]) = \P(X \leq x) = F(x)
\]
\begin{definition}
    A cumulative distribution function $F : \R \rightarrow [0, 1]$ is the cumulative distribution function of $x$,
    if
    \[
    F(x) = \P(X \leq x)
    \]
\end{definition}

\[
\underset{a < b}{\P(X \in [a, b])} = F(b) - F(a).
\]
Recall C1,
\[
\P(A \setminus B) = \P(A) - \P(A \cap B)
\]
\[
\P(X \in B) = \int_Bf(t)\,dt
\]
\[
\{X \leq x\} = \{X \in (-\infty, x]\}
\]

Let $-\infty < a \leq b < \infty$.
\[
\P(X \in (a, b]) = F_X(b) - F_X(a)
\]
Using C1
\begin{align*}
    \P(X \in (a, b]) &= \P(X \in (-\infty, b) \setminus (-\infty, a]) \\
    &= \P(X \in (-\infty, b]) - \P(X \in (-\infty, b] \cap (-\infty, a]) \\
    &= \P(X \in (-\infty, b]) - \P(X \in (-\infty, a]) \\
    &= F(b) - F(a)
\end{align*}

\begin{theorem}
    Let $X$ be a continuous random variable with probability density function $f$.
    Then $F(x) = \P(X \leq x) = \int_{-\infty}^xf(t)\,dt$.
    Moreover,
    $\frac{dF}{dx}(x) = f(x)$.
\end{theorem}

\begin{theorem}
    Let $X$ be a discrete random variable with probability mass function $p$.
    Then $F(x) = \sum_{t : t \leq x}p(t)$.
    $F(x)$ is piecewise continuous,
    and the discontinuities are jump-discontinuities.
    I.e.,
    at the points of discontinuity
    \[
    \lim_{y \rightarrow x^{+}}F(y) \neq F(x),
    \]
    but
    \[
    \lim_{y \rightarrow x ^ {-}}F(y) = F(x).
    \]
\end{theorem}
$F$ is always right (both for discrete and continuous) continuous.

For a continuous random variable,
$F$ is continuous everywhere,
except at countably many points,
that is,
\[
\lim_{y \rightarrow x}F(y) = F(x),
\]
for all but countably many $x$.

So for a discrete random variable $\P(X = x) = \lim_{y \rightarrow x ^ {+}}F(y) = p(x)$.

\begin{theorem}
    Let $F$ be a cumulative distribution function.
    Then it has the following properties.
    \begin{enumerate}[label = (\roman*)]
        \item
        \[
        \lim_{x \rightarrow -\infty}F(x) = 0
        \quad\text{and}\quad
        \lim_{x \rightarrow \infty}F(x) = 1.
        \]
        \item $F$ is always right continuous.
        \item Monotonicity: $F$ is monotonically increasing,
        that is for any $s \leq t$,
        $F(s) \leq F(t)$.
    \end{enumerate}
    \begin{proof}\phantom{}
    \begin{enumerate}[label = (\roman*)]
        \item $F(x) = \P(X \in (-\infty, x])$
        \[
        \lim_{x \rightarrow \infty}(-\infty, x] = \R
        \]
        \[
        \lim_{x \rightarrow \infty}F(x) = \P(X \in \R) = 1.
        \]
        \[
        \lim_{x \rightarrow -\infty}(-\infty, x] = \emptyset
        \]
        \[
        \lim_{x \rightarrow -\infty}F(x) = \P(X \in \emptyset) = 0.
        \]
        \item \phantom{}
        \item 
        \[
        F(s) = \P(X \leq s) = \P(X \in (-\infty, s]).
        \]
        \[
        F(t) = \P(X \leq t) = \P(X \in (-\infty, t]).
        \]
        If $s \leq t$,
        $(-\infty, s] \subseteq (-\infty, t]$
        $\P(A) \leq \P(B)$,
        if $A \subseteq B$,
        using this we get
        \[
        F(s) \leq F(t).
        \]
    \end{enumerate}
    \end{proof}
\end{theorem}

\begin{theorem}
    Cumulative distribution function determines a random variable completely.
\end{theorem}

\subsection{Standard Normal distributions}
$N(0, 1)$ is the standard normal variable.
\begin{definition}[Standard Normal distributions]
    A random variable $Z$ is said to be standard Normal,
    if it has the following probability distribution function.
    \[
    p(z) = \frac{1}{\sqrt{2\pi}}e ^ {-\frac{z ^ 2}{2}}, Z \in \R,
    \]
    the cumulative distribution function is
    \[
    \Phi(z) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^ze ^ {-\frac{t ^ 2}{2}}\,dt.
    \]
    $Z$ is used to denote $N(0, 1)$.
\end{definition}

\subsection{Functions of random variables}
$f \circ X(\omega) = f(X(\omega))$.
Then $f \circ X$ is also a random variable from $\Omega$ to $\R$.
$\P(g(X) \leq x) = \P(\{\omega : g(X(\omega)) \leq x\})$
\begin{example}
    $g(x) = x ^ 2, x ^ 3$,
    $g(x) = ax + b$
    $a$ is the change of scale,
    $b$ is the shift of origin.
\end{example}

\begin{theorem}
    Let $X \sim N(\mu, \sigma ^ 2)$,
    and $Z \sim N(0, 1)$,
    then
    $\frac{X - \mu}{\sigma} \sim N(0, 1)$,
    and $\mu + \sigma Z \sim N(\mu, \sigma ^ 2)$.
    Therefore,
    it is enough to know the distribution of $Z$.
    \begin{proof}
        Let $Y = \frac{X - \mu}{\sigma}$,
        where $X \sim N(\mu, \sigma ^ 2)$
        \begin{align*}
            \P(Y \leq y) &= \P\left(\frac{X - \mu}{\sigma} \leq y\right) \\
            &= \P(X \leq \mu + \sigma Y) \\
            &= \frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\mu + \sigma y}e ^ {-\frac{(t - \mu) ^ 2}{2\sigma ^ 2}}\,dt.
        \end{align*}
        Let $Z = \frac{t - \mu}{\sigma}$,
        then $t \rightarrow -\infty,\, z \rightarrow -\infty$,
        $t = \mu + \sigma y,\, z = y$.
        $\sigma dz = dt$.
        Putting all of this into the integral,
        we get
        \[
        = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{y}e ^ {-\frac{z ^ 2}{2}}\,dt = \Phi(y),
        \]
        where $\Phi$ is the cumulative density function of a $N(0, 1)$.
    \end{proof}
\end{theorem}

\subsection{Properties of the standard normal}

The pdf of $N(0, 1)$ is symmetric around the origin,

$\phi(z) = \phi(-z)$
\[
\phi(z) = \frac{1}{\sqrt{2\pi}}e ^ {-\frac{z ^ 2}{2}} = \phi(-z).
\]
The pdf of $N(\mu, \sigma ^ 2)$ is symmetric around $\mu$.

\begin{theorem}
    \[
    \Phi(z) = 1 - \Phi(-z)
    \]
    \begin{proof}
        \begin{align*}
            \Phi(z) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{z}e ^ {-\frac{t ^ 2}{2}}\,dt.
        \end{align*}
        Put $t \mapsto -u$,
        then $dt = -du$,
        $t \rightarrow -\infty,\, -u \rightarrow +\infty$,
        $t = z,\, u = -z$.
        Putting all this back in the integral,
        \begin{align*}
            \Phi(z) &= -\frac{1}{\sqrt{2\pi}}\int_{\infty}^{-z}e ^ {-\frac{u ^ 2}{2}}\,du \\
            &= \frac{1}{\sqrt{2\pi}}\int_{z}^{\infty}e ^ {-\frac{u ^ 2}{2}}\,du \\
            &= \P(Z \geq -z) \\
            &= 1 - \Phi(-z).
        \end{align*}
    \end{proof}
\end{theorem}

\begin{example}
    Consider the following,
    \begin{table}[H]
        \centering
        \begin{tabular}{c|ccccc}
             $z$ & $0$ & $1.28$ & $1.64$ & $1.96$ & $2.58$  \\
             \hline
             $\Phi(z)$ & $0.5$ & $0.9$ & $0.95$ & $0.975$ & $0.995$
        \end{tabular}
    \end{table}
    Find $\P(-1.28 \leq Z \leq 1.64$,
    when $Z \sim N(0, 1)$.
    \begin{align*}
        \P(-1.28 \leq z \leq 1.64) &= \P(Z \leq 1.64) - \P(Z \leq -1.28) \\
        &= \Phi(1.64) - \Phi(-1.28) \\
        &= \Phi(1.64) - (1 - \Phi(1.28)) \\
        &= \Phi(1.64) + \Phi(1.28) - 1 \\
        &= 0.95 + 0.9 - 1 \\
        &= 0.85.
    \end{align*}
\end{example}

\newpage

\section{Multiple random variables}

\subsection{Joint probability distributions}
$X : \Omega \rightarrow \chi$.
Let us have $2$ or multiple random variables.
$Y : \Omega \rightarrow \mathcal{Y}$.
\[
(X, Y) : \Omega \rightarrow \chi \times \mathcal{Y}
\]
defined by
\[
(X, Y)(\omega) = (X(\omega), Y(\omega)).
\]
Let $A \in \chi \times \mathcal{Y}$,
How do we find $\P((X, Y) \in A)$?
$\{(X, Y) \in A\} = \{\omega : (X(\omega), Y(\omega)) \in A\}$.
\begin{example}
    Suppose $\mathcal{X} = \R = \mathcal{Y}$
    \[
    A = [0, 1] \times [0, 1]
    \]
    \[
    \P((X, Y) \in [0, 1] \times [0, 1]).
    \]
\end{example}
\begin{align*}
    \{X = x, Y = y\} &= \{\omega : X(\omega) = x, Y(\omega) = y\} \\
    &= \{\omega : X(\omega) = x\} \cap \{\omega : Y(\omega) = y\} \\
    &= \{X = x\} \cap \{Y = y\}.
\end{align*}

Independence:
$A$ and $B$ are independent if and only if
\[
\P(A \cap B) = \P(A)\P(B).
\]
Let us take two sets $C \subseteq \mathcal{X}, D \subseteq \mathcal{Y}$.
\[
\{X \in C\} \subseteq \Omega\quad \{Y \in D\} \subseteq \Omega.
\]
\begin{definition}
    Two random variables $X$ and $Y$ are independent
    (on the same sample space $\Omega$)
    if and only if
    \[
    \P(X \in C, Y \in D) = \P(X \in C)\P(Y \in D),
    \]
    for all sets $C, D$ such that $C \subseteq \mathcal{X}, D \subseteq \mathcal{Y}$.
\end{definition}

\subsection{Joint distributions for discrete random variables}
\begin{definition}[Joint probability mass function]
    Let $(X, Y)$ be bivariate
    (a vector of size $2$),
    such that $\P((X, Y) \in \mathcal{Z}) = 1$,
    then the join probability mass function
    (pmf)
    is given by $p(x, y) = \P(X = x, Y = y)$, for all $(x, y) \in \mathcal{Z}$.
\end{definition}

For $A \subseteq \mathcal{X}, B \subseteq \mathcal{Y}$,
\[
\P(\mathcal{X} \in A, \mathcal{Y} \in B) = \sum_{(x, y)\,:\,x \in A, y \in B}p(x, y).
\].

\begin{theorem}
    $p_X(x) = \sum_{y \in Y}p(x, y)$, $p_Y(y) = \sum_{x \in X}p(x, y)$.
\end{theorem}


\subsection{Joint distributions for continuous random variables}
\begin{definition}[Joint probability density function]
    Let $X, Y$ be real-valued random variables,
    then the joint probability density function
    (pdf)
    is a function $f : \R ^ 2 \rightarrow \R$,
    $f \geq 0$,
    such that
    \[
    \P(X \in [a, b], Y \in [c, d]) = \int_{a}^{b}\left(\int_{c}^{d}f(x, y)\,dy\right)\,dx = \int_{c}^{d}\left(\int_{a}^{b}f(x, y)\,dx\right)\,dy.
    \]
\end{definition}

Recall that for a continuous random variable
\[
\P(X \in [a, b]) = \int_{a}^{b}f(x)\,dx.
\]
\[
\P(X \in [a, b], Y \in [a, b]) = \int_{a}^{b}\left(\int_{c}^{d}f(x, y)\,dy\right)\,dx
\]

\textbf{Marginal}:

The marginal for $X$ is given by
\[
p_X(x) = \P(X = x).
\]
Similarly the marginal for $Y$ is given by
\[
p_Y(y) = \P(Y = y).
\]

\begin{theorem}
    Let $X$ and $Y$ be two random variables on the same probability space $\Omega$.
    Then the bivariate random variable $(X, Y)$ is discrete if and only if both $X$ and $Y$ are discrete.
    Moreover,
    if $\P(X \in \mathcal{X}) = \P(Y \in \mathcal{Y}) = 1$,
    the marginal of $x$ is given by
    \[
    p_X(x) := \P(X = x) = \sum_{y \in Y} p(x, y)
    \]
    and the marginal for $y$ is given by
    \[
    p_Y(y) := \P(Y = y) = \sum_{x \in X} p(x, y).
    \]
\end{theorem}

Marginals,
\[
p_X(x) = \sum_{y \in Y}p(x, y)
\]
\[
p_Y(y) = \sum_{x \in X}p(x, y)
\]


\begin{align*}
    \P(X = x) &= \P(X = x, Y \in \mathcal{Y}) \\
    &= \P\left(\bigcup_{y \in Y}\{X = x, Y = y\}\right) \\
    &= \sum_{y \in \mathcal{Y}}\P(X = x, Y = y) & \text{(By partition theorem)} \\
    &= \sum_{y \in \mathcal{Y}}p(x, y).
\end{align*}

\begin{theorem}
    If $X$ and $Y$ re jointly discrete,
    then $A \subseteq \mathcal{Z}$,
    where $\P((X, Y) \in \mathcal{Z}) = 1$,
    then for any $A \in \mathcal{Z}$,
    \[
    \P((X, Y) \in A = \sum_{(x, y) \in A}p(x, y)
    \]
\end{theorem}

\textbf{Independence of jointly discrete random variables}


Recall:
For any two sets $A, B$,
are independent if and only if $\P(A \cap B) = \P(A)\P(B)$.

\begin{definition}
    Let $(X, Y) : \Omega \rightarrow (X \times Y)$.
    $X$ and $Y$ are independent if and only if for all $A \subseteq X$,
    $B \subseteq Y$
    \[
    \P(X \in A, Y \in B) = \underbrace{\P(X \in A)\P(Y \in B)}_{\text{product of the marginals}}
    \]
\end{definition}

\begin{definition}
    Let $X$ and $Y$ be discrete random variables.
    $X$ and $Y$ are independent if and only if
    \[
    p_{X, Y}(x, y) = p_X(x)p_Y(y)
    \]
\end{definition}

Conditional probability is defined as
\[
\P(A | B) = \frac{\P(A \cap B)}{\P(B)}\text{ when } \P(B) > 0.
\]
This motivates the definition of conditional probability for random variables:
\[
\P(X \in A\,|\,Y \in B) = \frac{\P(X \in A, Y \in B)}{\P(Y \in B)}.
\]

\begin{definition}
    For two random variables $X$ and $Y$,
    the conditional probability of $X$ given $Y = y$ is defined as
    \[
    p_{X|Y}(x | y) := \frac{p_{X, Y}(x, y)}{p_Y(y)},\text{ where } p_Y(y) > 0.
    \]
\end{definition}

Similarly,
\[
p_{Y|X}(y|x)\footnotemark = \frac{p(x, y)}{p_X(x)},\text{ when }p_X(x) > 0
\]
\footnotetext{read as probability of $Y = y$, given $X = x$ which is equal to $\P(Y = y | X = x)$.}

\begin{theorem}[Partition theorem]
    Let $X$ and $Y$ be two discrete random variables.
    \[
    \P(X = x) = p_X(x) = \sum_{y \in Y}p_{X | Y}(x | y)p_Y(y)
    \]
    and similarly
    \[
    \P(Y = y) = p_Y(y) = \sum_{x \in X}p_{Y | X}(y | x)p_X(x)
    \]
\end{theorem}

\subsection{Jointly continuous random variables}
\begin{definition}
    Two real-valued random variables $X$ and $Y$ are jointly continuously distributed if they have a joint probability density function,
    i.e. a function $f : \R ^ 2 \rightarrow \R$,
    such that $f \geq 0$.
    And
    \begin{align*}
        \P(X \in [a, b], Y \in [c, d]) &= \int_{a}^{b}\left(\int_{c}^{d}f(x, y)\,dy\right)\,dx
        &= \int_{c}^{c}\left(\int_{a}^{b}f(x, y)\,dx\right)\,dy.
    \end{align*}
\end{definition}

For $A \subseteq \R ^ 2$
\[
\P((X, Y) \in A) = \iint\limits_{A}f(x, y)\,dydx
\]
is not necessarily true.
\begin{theorem}
    For a set $A \subseteq \R ^ 2$,
    \[
    \P((X, Y) \in A) = \iint\limits_{A}f(x, y)\,dydx,
    \]
    when $A$ is a finite union of sets of the following type.
    \[
    \{x \in [a, b], \phi_1(x) \leq y \leq \phi_2(x)\}, \{\psi_1(y) \leq x \leq \psi_2(y), y \in [c, d]\}
    \]
    for continuous function $\phi_1, \phi_2, \psi_1, \psi_2$.
\end{theorem}

For $A$ of special type,
we have $\P((X, Y) \in A) = \iint\limits_{A}f(x, y)\,dxdy$.

Recall:
\[
\P(X = 0) = 0
\]
\[
\P(X \in [x, x + dx]) \approx f(x)dx
\]
\[
\P(X \in [a, x + dx], Y \in [y, y + dy]) \approx f(x, y)dxdy.
\]

\begin{corollary}
    \[
    \iint\limits_{\R ^ 2}f(x, y)\,dxdy = 1.
    \]
\end{corollary}
\[
\R ^ 2 = (-\infty, \infty) \times (-\infty, \infty) = A := \{-\infty < x < \infty, -\infty < y < \infty\}
\]
\[
\P((X, Y) \in \R ^ 2) = \P(X \in (-\infty, \infty), Y \in (-\infty, \infty)).
\]
\begin{corollary}
    The marginal densities
    (of $X$ and $Y$)
    are given
    (respectively)
    by
    \[
    f_X(x) = \int_{-\infty}^{\infty}f(x, y)\,dy,\quad f_Y(y) = \int_{-\infty}^{\infty}f(x, y)\,dx.
    \]
\end{corollary}
The converse is not true,
i.e. if $X$ is continuous,
$f_X$ (pdf) and $Y$ is continuous with (pdf) $f_Y$,
then $(X, Y)$ is not necessarily continuously distributed.

\begin{example}
    Let $X$ be a continuous random variable with pdf $f_X$.
    Define $Y = 2X$.
    Then $(X, Y)$ is not jointly continuously distributed.
    \begin{proof}
        Suppose not.
        Then there exists a joint pdf,
        let's say $f(x, y)$
        \[
        \P(2X = Y) = \iint\limits_{\{(x, y)\,|\,y = 2x\}}f(x, y)\,dydx = \int_{-\infty}^{\infty}\left(\int_{2x}^{2x}f(x, y)\,dy\right)\,dx = 0.
        \]
        However,
        we know that
        \[
        \P(Y = 2X) = 1.
        \]
    \end{proof}
\end{example}

\begin{theorem}[Partition theorem]
    If $(X, Y)$ are jointly continuous,
    then $X$,
    is continuously distributed and $Y$ is continuously distributed,
    with marginal pdf's,
    \[
    f_X(x) = \int_{-\infty}^{\infty}f(x, y)\,dy,
    \]
    \[
    f_Y(y) = \int_{-\infty}^{\infty}f(x, y)\,dx,
    \]
\end{theorem}

\begin{lemma}
    Two (jointly) continuous random variables $X$ and $Y$ are independent if and only if
    \[
    f(x, y) = f_X(x)f_Y(y).
    \]
\end{lemma}

The conditional pdf of $X$ given $Y = y$,
\[
f_{X | Y}(x | y) = \frac{f(x, y)}{f_Y(y)},\text{ when } f_Y(y) > 0
\]
similarly,
\[
f_{Y | X}(y | x) = \frac{f(x, y)}{f_X(x)},\text{ when } f_X(x) > 0.
\]

\begin{align*}
    \P(X \in [x, x + dx]\,|\, Y \in [y, y + dy]) &= \frac{\P(X \in [x, x + dx], Y \in [y, y + dy])}{\P(Y \in [y, y + dy])} \\
    &\approx \frac{f(x, y)dxdy}{f_Y(y)dy} \\
    &\approx \frac{f(x, y)}{f_Y(y)}dx.
\end{align*}

\begin{example}
    Throw two fair dice,
    let $X$ denote the number of sixes,
    and $Y$ denote the number of ones and twos.

    Find the joint pmf.
    \begin{table}[H]
        \centering
        \begin{tabular}{c|ccc|c}
             $p(x, y)$ & $x = 0$ & $x = 1$ & $x = 2$ & $p_Y(y)$ \\
             \hline
             $y = 0$ & $\frac{9}{36}$ & $\frac{6}{36}$ & $\dfrac{1}{36}$ & $\frac{16}{36}$ \\
             $y = 1$ & $\frac{12}{36}$ & $\frac{4}{36}$ & $0$ & $\frac{16}{36}$ \\
             $y = 2$ & $\frac{4}{36}$ & $0$ & $0$ & $\frac{4}{36}$ \\
             \hline
             $p_X(x)$ & $\frac{25}{36}$ & $\frac{10}{36}$ & $\frac{1}{36}$ & $1$
        \end{tabular}
    \end{table}
    \[
    p(2, 1) = \P(X = 2, Y = 1) = \P(\emptyset) = 0
    \]
    \[
    p(0, 0) = \P(X = 0, Y - 0) = \P(\text{See $3, 4$ or $5$}) = \frac{3 ^ 2}{36} = \frac{9}{36}
    \]
    \begin{align*}
        p(1, 0) &= \P(X = 1, Y = 0) \\
        &= \P(\text{Exactly one $6$ and $3, 4$ or $5$ for the second throw}) \\
        &= \frac{3 \times 2}{6 ^ 2} \\
        &= \frac{6}{36}.
    \end{align*}

    Find $p_X(x)$ and $p_Y(y)$.
    \[
    p_X(0) = \P(X = 0) = \frac{9}{36} + \frac{12}{36} + \frac{4}{36} = \frac{25}{36}
    \]
    
    Find $\P(X \geq 1, Y \leq 1)$
    \begin{align*}
        \P(X \geq 1, Y \leq 1) &= \P((X, Y) \in \{(1, 0), (1, 1), (2, 0), (2, 1)\} \\
        &= \frac{6}{36} + \frac{4}{36} + \frac{1}{36} + 0 \\
        &= \frac{11}{36}.
    \end{align*}

    Are $X$ and $Y$ independent?
    \[
    \P(X = 0) = p_X(0) = \frac{25}{36}
    \]
    \[
    \P(Y = 2) = p_Y(2) = \frac{4}{36}
    \]
    \[
    \P(X = 0, Y = 2) = \frac{4}{36} \neq p_X(0)p_Y(0),
    \]
    which shows $X$ and $Y$ aren't independent.
\end{example}

\begin{example}
    Let $(X, Y)$ be jointly continuous with pdf
    \[
    f(x, y) = \begin{cases}
        c(x + y), &\text{ if } 0 \leq x \leq 1,\ 0 \leq y \leq 1 \\
        0, &\text{otherwise}.
    \end{cases}
    \]
    Find $c$?
    \begin{align*}
        \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x, y)\,dxdy &= 1 \\
        &= c\int_{0}^{1}\int_{0}^{1}(x + y)\,dxdy \\
        &= c\int_{0}^{1}\left.\left(xy + \frac{y ^ 2}{2}\right)\right|_{y = 0}^{1}\,dx \\
        &= c\int_{0}^{1}\left(x + \frac{1}{2}\right)\,dx \\
        &= c\left[\frac{x ^ 2}{2} + \frac{1}{2}x\right]_{x = 0}^{1} = c.
    \end{align*}
    Equating this to $1$ we get $c = 1$.

    Find $\P\left(\frac{1}{4} < X \leq \frac{3}{4}, 0 < Y < \frac{1}{2}\right)$.

    \begin{align*}
        \int_{\frac{1}{4}}^{\frac{3}{4}}\int_{0}^{\frac{1}{2}}(x + y)\,dydx &= \int_{\frac{1}{4}}^{\frac{3}{4}}\left(xy + \frac{1}{2}y ^ 2\right)|_{y = 0}^{\frac{1}{2}} \\
        &= \frac{3}{16}.
    \end{align*}

    Find $\P(X ^ 2 < Y < X)$.
    \begin{align*}
        \P(X ^ 2 < Y < X) &= \int_{0}^{1}\int_{x ^ 2}^{x}(x + y)\,dydx \\
        &= \int_{0}^{1}\left(xy + \frac{y ^ 2}{2}\right)|_{y = x ^ 2}^{x}\,dx \\
        &= \int_{0}^{1}\left(\frac{3x ^ 2}{2} - x ^ 3 - \frac{x ^ 4}{2}\right)\,dx \\
        &= \frac{3}{20}.
    \end{align*}

    \begin{align*}
        f_X(x) &= \int_{-\infty}^{\infty}f(x, y)\,dy \\
        &= \int_{0}^{1}(x + y)\,dy \\
        &= \left(x + \frac{1}{2}\right).
    \end{align*}
    \begin{align*}
        f_Y(x) &= \int_{0}^{1}f(x, y)\,dx \\
        &= \int_{0}^{1}(x + y)\,dx \\
        &= \left(y + \frac{1}{2}\right).
    \end{align*}

    Are $X$ and $Y$ independent?
    \[
    f(x, y) \neq f_X(x)f_Y(y).
    \]
    \[
    f(x, y) = (x + y), f_X(x) = \left(x + \frac{1}{2}\right), f_Y(y) = \left(y + \frac{1}{2}\right)
    \]
    \[
    f(x, y) = (x + y) \neq \left(x + \frac{1}{2}\right)\left(y + \frac{1}{2}\right)
    \]
\end{example}

\newpage

\section{Expectation}
Collect the height of a sample of $n$ people $x_1, x_2, \dotsc, x_n$.
The sample mean/sample average $\bar{x}_n = \frac{1}{n}\sum_{i = 1}^{n}x_i$.
Height is distributed $N(\mu, \sigma ^ 2)$,
$\mu$ is the population mean, population average.

The sample mean converges to the population mean.

The population mean is what we call expectation
(average over space, ($\Omega$)).

Assume,
our random variables are real-valued,
and they either have a pmf or a pdf.

\begin{definition}
    The expectation of $X$ is defined as
    \[
    \E[X] = \sum_{x \in \R}xp(x)\text{ discrete}
    \]
    \[
    \E[X] = \int_{-\infty}^{\infty}xp(x)\,dx\text{ continuous}.
    \]
\end{definition}

Suppose $p(x) = \frac{1}{n}$, $x \in \{1, 2, \dotsc, n\}$
\[
\E(X) = \sum_{x = 1}^{n}n \frac{1}{n} = \frac{1}{n}\sum_{x = 1}^{n}x = \frac{n + 1}{2}.
\]

Existence of expectation:
$\E(X)$ does not always exist.
\[
\sum xp(x)
\]
\[
\int xp(x)\,dx.
\]
may not be finite.

Cauchy distribution
\[
f(x) = \frac{1}{\pi(1 + x ^ 2)}, x \in \R.
\]

Expectation exists finitely,
we mean that,
$\sum_{x}|x|p(x) < \infty \rightarrow$ discrete and for continuous,
$\int_{-\infty}^{\infty}|x|f(x)\,dx < \infty$

$X(\Omega)$ is countably infinite,
then $\sum_{\mathcal{X} \in X(\Omega)}|x|p(x) = +\infty$ can be
$\int|x|f(x)\,dx = +\infty$

\begin{example}
    Discrete.
    Let $X$ be a random variable such that
    \[
    p(x) = \frac{c}{x ^ 2}, x \in \{1, 2, \dotsc\}
    \]
    $\E(X) = \sum_{x = 1}^{\infty}xp(x) = c\sum){x = 1}^{\infty} \frac{x}{x ^ 2} = c\sum_{x = 1}^{\infty} \sim c\log x$.

    Continuous.
    Cauchy distribution.
    $X \sim \mathrm{Cauchy}$,
    that is the pdf of $X$ is given by
    \[
    f(x) = \frac{1}{\pi(1 + x ^ 2)}, x \in \R.
    \]
    Exercise: $\E(|X|) = +\infty$.

    Let $Z \sim N(0, 1)$, find $\E(Z)$.
    \begin{align*}
    \E(Z) &= \int_{-\infty}^{\infty}z\varphi(z)\,dz \\
    &= \int_{-\infty}^{0}z\varphi(z) + \int_{0}^{\infty}z\varphi(z)\,dz \\
    &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{0}ze ^ {-\frac{z ^ 2}{2}}\,dz + \frac{1}{\sqrt{2\pi}}\int_{0}^{\infty}ze ^ {-\frac{z ^ 2}{2}}\,dz \\
    \end{align*}
    $z \rightarrow -t_1$, $dz = -dt$ $z \rightarrow -\infty$, $t \rightarrow \infty$, $z = 0$, $t = 0$.
    Then
    \begin{align*}
        I_1 &= \frac{1}{\sqrt{2\pi}}\int_{\infty}^{0}(-t)e ^ {-\frac{t ^ 2}{2}}(-dt) \\
        &= \frac{1}{\sqrt{2\pi}}\int_{\infty}^{0}te ^ {-\frac{t ^ 2}{2}}\,dt \\
        &= -I_2.
    \end{align*}
    Therefore,
    $\E(Z) = I_1 - I_1 = 0$.
\end{example}

\subsection{Expectation of functions of random variables}
Let $Y = g(x)$.
Let us assume that $X$ is discrete with pmf $p$.
Then by definition,
we know that
\[
\E(g(X)) = \E(Y) = \sum_{y}y\P(Y = y).
\]
\begin{align*}
    \P(Y = y) &= \P(g(X) = y) \\
    &= \sum_{x}\P(g(X) = y\mid X = x)\P(X = x) \\
    &= \sum_{x : g(x) = y}1\P(X = x) \\
    &= \sum_{x : g(x) = y}p(x)
\end{align*}
Note that $\P(g(X) = y\mid X = x) = 1$,
if $y = g(x)$ and $0$ otherwise.

Putting this in ($1$),
$\E(g(X)) = \sum_{y}y\left(\sum_{x : g(x) = y}p(x)\right)$.
Interchange the order of summation.
\begin{align*}
    \E(g(X)) &= \sum_{x}\left(\sum_{y : g(\R)}yp(x)\right) \\
    &= \sum_{x \in \R}\left(\sum_{y : y = g(x)}g(x)\right)p(x) \\
    &= \sum_{x \in \R}g(x)p(x).
\end{align*}
\begin{theorem}[Law of the unconscious statistician]\footnote{LOTUS}
    \[
    \E(g(X)) = \sum_{x}g(x)p(x), \text{$X$ is discrete with pmf $p(\cdot)$}.
    \]
    \[
    \E(g(X)) = \int_{-\infty}^{\infty}g(x)f(x)\,dx,\text{ $X$ is continuous with pdf $f(\cdot)$}.
    \]
\end{theorem}

\begin{example}
    Let $X \sim U(-1, 1)$.
    $\E(X ^ 2)$.

    $f(x) = \begin{cases}
        \frac{1}{2} & x \in [-1, 1], \\
        0, &\text{elsewhere}.
    \end{cases}$
    \[
    \E(X ^ 2) = \int_{-\infty}^{\infty}x ^ 2f(x)\,dx = \int_{-1}^{1}\frac{x ^ 2}{2}\,dx = \frac{1}{2}\left[\frac{x ^ 3}{3}\right]_{-1}^{1} = \frac{1}{3}
    \]
\end{example}
\begin{remark}
    If $\E(X)$ exists finitely,
    it is not always true that $\E(g(X))$ exists finitely.
\end{remark}

\begin{example}
    $X \sim U(-1, 1)$.
    $\E(X)$ exists.

    Let $g(x) = \frac{1}{x}$.
    Does $\E(g(X))$ exist finitely?

    Answer is no,
    (why?) - \textbf{Exercise}.
    [\textit{Hint: integrating $\frac{1}{x}$ around zero is a problem.}]
\end{example}

\begin{theorem}[Multiple random variable of LOTUS]
    $(X, Y) : \Omega \rightarrow \R ^ 2$, $g L \R ^ 2 \rightarrow \R$,
    then
    \[
    \E(g(X, Y)) = \sum_{x}\sum_{y}g(x, y)p(x, y)
    \]
    where $(X, Y)$ is jointly discrete with joint pmf $p(\cdot)$.
    \[
    \E(g(X, Y)) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x, y)f(x, y)\,dxdy,
    \]
    $(X, Y)$ is jointly continuous with pdf $f(\cdot)$.
\end{theorem}

\begin{example}
    Let $(X, Y)$ have joint pdf
    \[
    f(x, y) = \begin{cases}
        1, &\text{ if $(x, y) \in [0, 1] ^ 2$} \\
        0, &\text{elsewhere}.
    \end{cases}
    \]
    Find $\E(XY)$.

    Remark:
    $(X, Y)$ is jointly uniformly distributed over the square $[0, 1] \times [0, 1]$.
    \begin{align*}
        \E(XY) &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyf(x, y)\,dxdy \\
        &= \int_{0}^{1}\int){0}^{1}xy\,dxdy \\
        &= \int_{0}^{1}x\,dx\int_{0}^{1}y\,dy \\
        &= \left[\frac{x ^ 2}{2}\right]_{0}^{1} \times \left[\frac{y ^ 2}{2}\right]_{0}^{1} \\
        &= \frac{1}{4}.
    \end{align*}
\end{example}

\subsection{Linearity of Expectation}
Integrals and summations are linear operations.
\[
\sum_{i}(\alpha f(xi) + \beta g(xi)) = \alpha\sum f(xi) + \beta\sum g(xi)
\]
$\alpha, \beta \in \R$
\[
\int(\alpha f(x) + \beta g(x)\,dx = \alpha\int f(x)\,dx + \beta\int g(x)\,dx.
\]
Integral of the sum becomes a sum of the integrals.

\begin{theorem}[Linearity]
    Let $X : \Omega \rightarrow \R$ be a random variable.
    Then for any $\alpha, \beta \in \R$,
    \[
    \E(\alpha X + \beta) = \alpha\E(X) + \beta.
    \]
\end{theorem}

\begin{remark}
    Let $\P(Y = \beta) = 1$,
    \[
    \E(Y) = \sum yp(y) = \beta \P(Y = \beta) = \beta
    \]
    $\E(\text{constant}) = \text{Constant itself}$.
\end{remark}

\begin{theorem}[Linearity version II]
    Let $X, Y : \Omega \rightarrow \R$ be two random variables.
    Then
    \[
    \E(X + Y) = \E(X) + \E(Y).
    \]
    Let $X_1, X_2, \dotsc, X_n$ be a collection of random variables,
    and $\alpha_1, \alpha_2, \dotsc, \alpha_n$ be some real-numbers,
    then
    \[
    \E\left(\sum_{i = 1}^{n}\alpha_iX_i\right) = \sum_{i = 1}^{n}\alpha_i\E(X_i)
    \]
    \begin{proof}
        $g : \R ^ 2 \rightarrow \R$,
        $g(x, y) := (x + y)$.

        Let us assume that $(X, Y)$ is jointly discrete with pmf $p(\cdot)$.
        By LOTUS
        \begin{align*}
            \E(g(x, y)) &= \sum_{x}\sum_{y}(x + y)p(x, y) \\
            &= \sum_{x}x\left(\sum_{y}p(x, y)\right) + \sum_{y}y\left(\sum_{x}p(x, y)\right) \\
            &=\sum_{x}xp_X(x) + \sum_{y}yp_Y(y) \\
            &= \E(X) + \E(Y).
        \end{align*}
    \end{proof}
\end{theorem}

\begin{example}
    Let $X \sim \Bin(n, p)$.
    Find $\E(X)$.
    Look at the definition of the probability mass function to see that
    \[
    \E(X) = \sum_{x = 0}^{n} xp(x) = \sum_{x = 0}^{n}x\binom{n}{x}p ^ x(1 - p) ^ {n - x}.
    \]
    Alternative method
    
    Let $Y_i = 1$,
    if the $i$th trial is a success and $0$ if it is a failure.
    \[
    X = \sum_{i = 1}^{n}Y_i
    \]
    \[
    \E(X) \overset{Linearity}{=} \sum_{i = 1}^{n}\E(Y_i)
    \]
    \begin{align*}
        \E(Y_i) &= 1 \times \P(Y_i\text{ is success}) + 0 \times \P(Y_i \text{ is failure}) \\
        &= \P(Y_i\text{ is success}) = p \text{ for } 1 \leq i \leq n.
    \end{align*}
    Therefore,
    \[
    \E(X) = np.
    \]
\end{example}

\subsection{Covariance and Variance}
\begin{definition}
    $X : \Omega \rightarrow \R$.
    The variance of $X$,
    denoted by $\Var(X)$ is defined as
    \[
    \Var(X) := \E\left((x - \E(X)) ^ 2\right).
    \]
    Standard deviation\footnote{SD.}
    \[
    SD(X) = \sqrt{\Var(X)}.
    \]
\end{definition}

By LOTUS,
taking $g(x) = (X - \E(X)) ^ 2$,
we have
\[
\Var(X) = \sum_{x}(x - \E(X)) ^ 2p(x), \text{ discrete case}
\]
\[
\Var(X) = \int_{-\infty}^{\infty}(x - \E(X)) ^ 2 f(x)\,dx\text{ continuous case}.
\]
\begin{example}
    $X$ is discrete with pmf $p(0) = p(10) = p(20) = \frac{1}{3}$
    \[
    \E(X) = 0 \times \frac{1}{3} + 10 \times \frac{1}{3} + 20 \times \frac{1}{3} = 10
    \]
    \[
    \Var(X) = \frac{(0 - 10) ^ 2}{3} + \frac{(10 - 10) ^ 2}{3} + \frac{(20 - 10) ^ 2}{3} = \frac{200}{3}
    \]
\end{example}

\begin{definition}
    $X : \Omega \rightarrow \R, Y : \Omega \rightarrow \R$.
    Covariance of $X$ and $Y$ is defined as
    \[
    \Cov(X, Y) := \E((X - \E(X))(Y - \E(Y))).
    \]
\end{definition}

$\Cov(X, Y) > 0$ random variables $X$ and $Y$ are positively correlated.

$\Cov(X, Y) < 0$ random variables $X$ and $Y$ are negatively correlated.

Intuition:
Suppose $\E(X) = \E(Y) = 0$.
Then $\Cov(X, Y) = \E(XY)$

Correlation coefficient $\rho(X, Y) := \frac{\Cov(X, Y)}{\sqrt{\Var(X)}\sqrt{\Var(Y)}}$.

By LOTUS,
\[
\Cov(X, Y) = \sum_{x}\sum_{y}(x - \E(X)(y - \E(Y))p(x, y)\text{ discrete case}.
\]
\[
\Cov(X, Y) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x - \E(X)(y - \E(Y))f(x, y)\,dxdy\text{ continuous case}.
\]

\begin{example}
    \begin{table}[H]
        \centering
        \begin{tabular}{c|cc|c}
             $p(x, y)$ & $x = 1$ & $x = 2$ & $p_Y(y)$ \\
             \hline
             $y = 1$ & $\frac{1}{4}$ & $0$ & $\frac{1}{4}$ \\
             $y = 4$ & $0$ & $\frac{3}{4}$ & $\frac{3}{4}$ \\
             \hline
             $p_X(x)$ & $\frac{1}{4}$ & $\frac{3}{4}$ & $1$
        \end{tabular}
    \end{table}
    Find $\Cov(X, Y)$
    \[
    \E(X) = 1 \times \frac{1}{4} + 2 \times \frac{3}{4} = \frac{7}{4},
    \]
    \[
    \E(Y) = 1 \times \frac{1}{4} + 4 \times \frac{3}{4} = \frac{13}{4}.
    \]
    \begin{align*}
        \Cov(X, Y) &= \E\left(\left(X - \frac{7}{4}\right)\left(Y - \frac{13}{4}\right)\right) \\
        &= \sum_{x \in \{1, 2\}}\sum_{y \in \{1, 4\}}\left(x - \frac{7}{4}\right)\left(y - \frac{13}{4}\right)p(x, y) \\
        &= \frac{9}{16}.
    \end{align*}
\end{example}

\begin{property}
    $\Var(X) = \Cov(X, X)$.
\end{property}

\begin{property}
    Covariance is symmetric,
    that is
    \[
    \Cov(X, Y) = \Cov(Y, X)\footnotemark.
    \]
    \footnotetext{Follows from definition.}
\end{property}

\begin{corollary}
    Let $X, Y, Z$ be random variables and $\alpha, \beta, \gamma \in \R$.
    \[
    \Var(\alpha + \beta X) = \beta ^ 2\Var(X).
    \]
    \[
    \Cov(\alpha + \beta X, \delta + \delta Y) = \beta\delta\Cov(X, Y).
    \]
    \[
    \Cov(X + Y, Z) = \Cov(X, Z) + \Cov(Y, Z).
    \]
    \[
    \Cov(X, Y + Z) = \Cov(X, Y) + \Cov(X, Z).
    \]
    \begin{proof}
        By linearity,
        we know that $\E(\alpha + \beta X) = \alpha + \beta\E(X)$.
        \begin{align*}
            \Var(\alpha + \beta X) &= \E((\alpha + \beta X - \E(\alpha + \beta X) ^ 2) \\
            &= \E((\alpha + \beta X - \alpha - \beta\E(X)) ^ 2) \\
            &= \E(\beta ^ 2(X - \E(X)) ^ 2) \\
            &= \beta ^ 2\E((X - \E(X)) ^ 2) \\
            &= \beta ^ 2\Var(X).
        \end{align*}
    \end{proof}
\end{corollary}

\begin{example}
    Let $X \sim N(\mu, \sigma ^ 2)$.
    Find $\Var(X)$.
    Recall,
    $\E(Z) = 0$,
    if $Z \sim N(0, 1)$.
    \[
    \Var(Z) = 1.
    \]
    From chapter $6$,
    \[
    Z := \frac{X - \mu}{\sigma} \sim N(0, 1).
    \]
    So then $X = \mu + \sigma Z$.
    By linearity,
    $\E(X) = \mu + \sigma\E(Z) = \mu$.

    $\Var(X) = \Var(\mu + \sigma Z) = \sigma ^ 2 \Var(Z) = \sigma ^ 2$.
\end{example}

\begin{corollary}
    \[
    \Var(X) = \E(X ^ 2) - \E(X) ^ 2.
    \]
    \begin{proof}
        \begin{align*}
            \Var(X) &= \E((X - \E(X)) ^ 2) \\
            &= \E(X ^ 2 - 2X\E(X) + \E(X) ^ 2) \\
            &= \E(X ^ 2) - 2\E(X)\E(X) + \E(X) ^ 2 \\
            &= \E(X ^ 2) - \E(X) ^ 2
        \end{align*}
    \end{proof}
\end{corollary}

\begin{corollary}
    \[
    \Cov(X, Y) = \E(XY) - \E(X)\E(Y)
    \]
    \begin{proof}
        \begin{align*}
            \Cov(X, Y) &= \E((X - \E(X))(Y - \E(Y)) \\
            &= \E(XY - X\E(Y) - Y\E(X) + \E(X)\E(Y)) \\
            &= \E(XY) - \E(X)\E(Y) - \E(X)\E(Y) + \E(X)\E(Y) \\
            &= \E(XY) - \E(X)\E(Y).
        \end{align*}
    \end{proof}
\end{corollary}

\begin{theorem}
    Let $X$ and $Y$ be two random variables,
    then
    \[
    \Var(X + Y) = \Var(X) + \Var(Y) + 2\Cov(X, Y).
    \]
    Let $X_1, \dotsc, X_n$ be random variables,
    then
    \[
    \Var\left(\sum_{i = 1}^{n}X_i\right) = \sum_{i = 1}^{n}\Var(X_i) + 2\sum_{\begin{subarray}{c}
        i < j \\
        i = 1, \dotsc, n
    \end{subarray}}^{n}\Cov(X_i, X_j)
    \]
    \begin{proof}
        \begin{align*}
            \Var(X + Y) &= \Cov(X + Y, X + Y) \\
            &= \Cov(X, X) + \Cov(X, Y) + \Cov(Y, X) + \Cov(Y, Y) \\
            &= \Var(X) + \Var(Y) + 2\Cov(X, Y).
        \end{align*}
        Since $\Cov(X, Y) = \Cov(Y, X)$.
    \end{proof}
\end{theorem}

\subsection{Conditional expectations}
\[
\ind : \Omega \rightarrow \{0, 1\}, \text{ for } A \subseteq \Omega.
\]
\[
\ind(\omega) = \begin{cases}
    1, & \text{if } \omega \in A, \\
    0, & \text{elsewhere}.
\end{cases}
\]
$\E(\ind) = 1 \times \P(A) = \P(A)$.

\begin{definition}[Conditional expectation of $X$ given $A$]
    \[
    \E(X\mid A) := \frac{\E(X\ind)}{\P(A)},\text{ where $\P(A) > 0$}.
    \]
\end{definition}
$\E(X \mid A)$ should be interpreted as the expectation of $X$ with respect to the probability $\P(\cdot\mid A)$.

\begin{example}
    $A, B \subseteq \Omega$,
    \[
    \ind[A \cap B] = \ind\ind[B].
    \]
    \[
    \ind(\omega) = \begin{dcases*}
        1, & if $\omega \in A$, \\
        0, & elsewhere.
    \end{dcases*}
    \]
   \[
   \ind[A \cap B] = \begin{dcases*}
       1, & if and only if $\omega \in A$ and $\omega \in B$, \\
       0, & elsewhere.
   \end{dcases*}
   \]
   $\ind[A \cap B] = 1 \implies \ind = 1, \ind[B] = 1$.
   If $\omega \notin A$ or $\omega \notin B$,
   then $\ind[A]\ind[B] = 0$
   as
   $\omega \notin A \implies \ind = 0$
   $\omega \notin B \implies \ind[B] = 0$.

   \[
   \ind[A \cap B] = \ind\ind[B].
   \]
   Let us assume that $\P(B) > 0$.
   \[
   \E(\ind\mid B) = \frac{\E(\ind\ind[B]}{\P(B)} = \frac{\P(A \cap B)}{\P(B)} = \P(A \mid B).
   \]
\end{example}

\begin{theorem}
    Let $X$ be a discrete random variable,
    \[
    \E(X \mid A) = \sum x\P(X = x\mid A).
    \]
    \begin{proof}
        Define $Y = X\ind$
        \begin{equation}
            \E(X\ind) = \E(Y) = \sum_{x}x\P(Y = x)
        \end{equation}
        \begin{align*}
            \sum_{x}x\P(Y = x) &= \P(X\ind = x) \\
            &= \P(\{X = x\} \cap A) \\
            &= \P(X = x\mid A)\P(A).
        \end{align*}
        Putting in ($1$) we get
        \begin{align*}
            E(X\ind) &= \sum_{x}x\P(X = x\mid A)\P(A) \\
            &= \P(A)\sum_{x}x\P(X = x\mid A)
        \end{align*}
        which shows that 
        \[
        \frac{\E(X\ind)}{\P(A)} = \sum x\P(X = x\mid A).
        \]
    \end{proof}
\end{theorem}

\begin{example}
    $X, Y$ are two random variables.
    Let $B \subseteq \R$ and $(X, Y)$ has a joint pmf $p(\cdot, \cdot)$,
    Let $g : \R \rightarrow \R$
    \[
    \E(g(X)\mid Y \in B).
    \]
    $B = \{y\}$.
    \begin{align*}
        \E(g(X)\mid Y = y) &= \sum_{z}z\P(g(X) = z \mid Y = y) \\
        &= \frac{\sum_{x}g(x)p(x, y)}{p_Y(y)} \\
        &= \sum_{x}g(x)p_{X\mid Y}(x \mid y).
    \end{align*}
    Similarly,
    for any set $B$,
    we can write.
    \[
    \E(g(X) \mid Y \in B) = \sum_{x}g(x)\left(\frac{\sum_{y \in B}p(x, y)}{\sum_{y \in B}p_Y(y)}\right).
    \]
\end{example}

Continuous case:
joint,
pdf,
we have a similar formula.
\[
\E(g(X) \mid Y \in B) = \frac{\int_{\R}g(x)\int_{y \in B}f(x, y)\,dydx}{\int_{y \in B}f(y)\,dy}
\]

\begin{example}
    Raffle game,
    one of the prizes is worth $\pounds 500$,
    and five of the prizes are worth $\pounds 100$.
    There are $2000$ raffle tickets.
    Let $X$ be our winning,
    and $A$ be the event that we get the top prize.
    We can choose only one ticket.
    Find $\E(X \mid A)$.

    \[
    \P(X = 500\mid A) = 1 = \frac{\P((X = 500) \cap A)}{\P(A)} = \frac{\P(A)}{\P(A)} = 1
    \]
    \[
    \E(X\mid A) = \frac{\E(X\ind)}{\P(A)} = \frac{500\P((X = 500) \cap A)}{\P(A)} = 500.
    \]
\end{example}

\begin{theorem}[Partition Theorem]
    Let $E_1, E_2, \dotsc, E_k$ be a finite partition.
    Then
    \begin{equation}
        \E(X) = \sum_{i = 1}^{k}\E(X \mid E_i)\P(E_i).
    \end{equation}
    A countable infinite partition,
    $E_1, E_2, \dotsc$,
    then
    \[
    \E(X) = \sum_{i = 1}^{\infty}\E(X \mid E_i)\P(E_1).
    \]
    \begin{proof}[Proof of $(2)$]
        $\Omega = \bigcup_{i = 1}^{k}E_i$,
        where $E_i$'s are disjoint
        $\ind[\Omega] = \sum_{i = 1}^{k}\ind[E_i]$ only for disjoint sets.
        \begin{align*}
            E(X) &= \E(X\ind[\Omega]) \\
            &= \E\left(X\left(\sum_{i = 1}^{k}\ind[E_i]\right)\right) \\
            &= \E\left(\sum_{i = 1}^{k}x\ind[E_i]\right) \\
            &= \sum_{i = 1}^{k}\E(X\ind[E_i]) \\
            &= \sum_{i = 1}^{k}\E(X \mid E_i)\P(E_i).
        \end{align*}
    \end{proof}
\end{theorem}

\begin{definition}
    $X, Y : \Omega \rightarrow \R$,
    let us define $g : Y(\Omega) \rightarrow \R$ by
    \[
    g(y) = \E(X\mid Y = y).
    \]
    Conditional expectation of $X$ given $Y = y$,
    is a random variable denoted by $\E(X \mid Y)$ and defined by
    \[
    \E(X \mid Y) := g(Y)
    \]
    i.e.,
    $\E(X \mid Y)(\omega) = \E(X \mid Y = Y(\omega))$.
\end{definition}

Say $Y$ is discrete,
and $\P(Y = y)$.

$\E(X \mid Y)$ is a random variable which takes the value $\E(X \mid Y = y)$ with probability $\P(Y = y)$.

\begin{theorem}[Partition Theorem]
    Let $X$ and $Y$ be two random variables.
    Then
    \[
    \E(X) = \E(\E(X \mid Y)).
    \]
    \begin{proof}
        Let us assume that $Y$ is discrete.
        \[
        \E(X) = \sum_{y}\E(X \mid Y = y)\P(Y = y),
        \]
        (our partition is $\{Y = y\}$)
        by the previous theorem.

        \begin{align*}
            \sum_{y}\E(X \mid Y = y)\P(Y = y) &= \sum_{y}g(y)\P(Y = y) \\
            &= \E(g(Y)) &(\text{By LOTUS}) \\
            &= \E(\E(X \mid Y)).
        \end{align*}
    \end{proof}
\end{theorem}

\begin{example}
    Toss a coin $3$ times and let $H$ denote the number of heads.
    Then roll a fair die $H$ times.
    Let $T := \text{total score}$.
    Find $\E(T)$.

    Let $X_1, X_2, \dotsc$ denote the score of the $i$th throw of the die.
    \[
    T = \sum_{i = 1}^{H}X_i.
    \]
    We know that $H \sim \Bin\left(3, \frac{1}{2}\right)$.
    So $\E(T) = \E(\E(T \mid H))$.
    Given $\{H = h\}$,
    observe that
    \[
    \E(T \mid H = h) = \E\left(\sum_{i = 1}^{h}X_i\right) = h\E(X_1) = \frac{7}{2}h.
    \]
    Putting it back in we get
    \[
    \E(T) = \E\left(\frac{7}{2}H\right) = \frac{7}{2}\E(H).
    \]
    \[
    \E(H) = 3 \times \frac{3}{2} = \frac{3}{2}\quad\text{(As it follows binomially)}.
    \]
    Putting it back in,
    we get
    \[
    \E(T) = \frac{7}{2} \times \frac{3}{2} = \frac{21}{4}
    \]
\end{example}

\subsection{Independence: multiplication rule for expectation}
Recall,
that if $(X, Y)$ has joint pmf/pdf,
then independence is characterised as follows.
\[
\text{join pmf } \leftarrow p(x, y) = p_X(x)p_Y(y)
\]
\[
\text{join cdf } \leftarrow f(x, y) = f_X(x)f_Y(y).
\]

\begin{theorem}
    If $X$ and $Y$ are independent,
    then
    \begin{align*}
        \E(XY) &= \E(X)\E(Y) \\
        \E(g(X)h(Y)) &= \E(g(X))\E(h(Y))
    \end{align*}
    for functions $g, h : \R \rightarrow \R$.
    \begin{proof}
        Let us assume that $(X, Y)$ is jointly discrete with pmf $p(x, y)$.
        \begin{equation}
            \E(g(X)h(Y)) = \sum_{x}\sum_{y}g(x)h(y)p(x, y)
        \end{equation}
        by LOTUS.

        Since $X$ and $Y$ are independent.
        $p(x, y) = p_X(x)p_Y(y)$,
        put in ($1$),
        we get
        \begin{align*}
            \E(g(X)h(Y)) &= \sum_{x}\sum_{y}g(x)g(y)p_X(x)p_Y(y) \\
            &= \left(\sum_{x}g(x)p_X(x)\right)\left(\sum_{y}h(y)p_Y(y)\right) \\
            &= \E(g(X))\E(g(Y)).
        \end{align*}
    \end{proof}
\end{theorem}

\begin{corollary}
    If $X$ and $Y$ are independent,
    then
    \[
    \Cov(X, Y) = 0.
    \]
    \begin{proof}
        \begin{align*}
            \Cov(X, Y) &= \E(XY) - \E(X)\E(Y) \\
            \intertext{By the previous theorem,
            $\E(XY) = \E(X)\E(Y)$,
            and thus we get}
            &= \E(X)\E(Y) - \E(X)\E(Y) \\
            &= 0.
        \end{align*}
    \end{proof}
\end{corollary}

The converse is not true
\begin{example}
    \begin{align*}
        \P((X, Y) = (1, 0)) &= \P((X, Y) = (0, 1)) \\
        &= \P((X, Y) = (-1, 0)) \\
        &= \P((X, Y) = (0, -1)) \\
        &= \frac{1}{4}.
    \end{align*}
    $\P(XY = 0) = 1$.
    Therefore $\E(XY) = 0$,
    $\E(X) = \E(Y) = 0$.

    $\P(X = 1, Y = 1) = 0$,
    $\P(X = 1) = \P(X = 1, Y = 0) = \frac{1}{4}$.

    Similarly,
    $\P(Y = 1) = \frac{1}{4}$.

    $\P(X = 1, Y = 1) = 0 \neq \P(X = 1)\P(Y = 1)$.
    Hence $X$ and $Y$ are not independent.
\end{example}

\begin{corollary}
    Let $X_1, \dotsc, X_n$ be random variables.
    If they are pairwise independent,
    then
    \[
    \Var\left(\sum_{i = 1}^{n}X_i\right) = \sum_{i = 1}^{n}\Var(X_i).
    \]
    \begin{proof}
        Recall that
        \begin{align*}
            \Var\left(\sum_{i = 1}^{n}X_i\right) &= \sum_{i = 1}^{n}\Var(X_i) + 2\sum_{i < j}\Cov(X_i, X_j) & \text{(Since $X_i, X_j$ are independent)} \\
            &= \sum_{i = 1}^{n}\Var(X_i).
        \end{align*}
    \end{proof}
\end{corollary}

\subsection{Inequalities}
$f, g : \R \rightarrow \R$,
such that
$f(x) \geq g(x)$ for all $x$,
then
\[
\int_{\R}f(x)\,dx \geq \int_{\R}g(x)\,dx.
\]
\begin{theorem}[Monotonicity of expectation]\label{prob_thm_monotonicityofexpectation}
    Let $X : \Omega \rightarrow \R$,
    $a \in \R$,
    such that $\P(X \geq a) = 1$.
    Then $\E(X) \geq a$.
    \begin{proof}
        Define $Y : \Omega \rightarrow \R$,
        as $Y = (X - a).$
        Since $\P(X \geq a) = 1$,
        we have $\P(Y \geq 0) = 1$.
        Therefore $\E(Y) \geq 0$.
        Let us assume $X$
        (with pdf $f$)
        is continuous.
        Then
        \begin{align*}
            \E(Y) &= \int_{-\infty}^{\infty}xf(x)\,dx - \int_{-\infty}^{\infty}af(x)\,dx \\
            &= \E(X) - a\int_{-\infty}^{\infty}f(x)\,dx \\
            &= \E(X) - a.
        \end{align*}
        From $\E(Y) \geq 0$,
        we know that,
        $\E(Y) \geq 0$,
        which implies,
        $\E(X) - a \geq 0$ or,
        $\E(X) \geq a$.
    \end{proof}
\end{theorem}

\begin{example}
    $\P(X \geq Y) = 1$,
    then $\E(X) \geq \E(Y)$.
    \begin{proof}
        $Z := (X - Y)$ then $\P(Z \geq 0) = 1$,
        since $\P(X \geq Y) = 1$.
        From \autoref{prob_thm_monotonicityofexpectation} we have $\E(Z) \geq 0$.
        By linearity,
        $\E(Z) = \E(X) - \E(Y)$,
        hence,
        $\E(X) \geq \E(Y)$.
    \end{proof}
\end{example}

\begin{corollary}
    \[
    \E(X ^ 2) \geq \E(X) ^ 2.
    \]
    \begin{proof}
        \[
        \Var(X) = \E((X - \E(X)) ^ 2).
        \]
        Observe that $Y := (X - \E(X)) ^ 2 \geq 0$.
        From \autoref{prob_thm_monotonicityofexpectation},
        we have $\E(Y) \geq 0$.
        Putting this in the following,
        \[
        \Var(X) = \E((X - \E(X)) ^ 2) = \E(X ^ 2) - \E(X) ^ 2
        \]
        we have,
        $\E(X ^ 2) - \E(X) ^ 2 \geq 0$ or,
        $\E(X ^ 2) \geq \E(X) ^ 2$
    \end{proof}
\end{corollary}

\begin{corollary}[Markov inequality]
    If $x \geq 0$,
    then for any $a > 0$,
    \[
    \P(X > a) \leq \frac{\E(X)}{a}.
    \]
    \begin{proof}
        Observe that $X \geq a\ind[\{X \geq a\}]$
        \[
        a\ind[\{X \geq a\}](\omega) = \begin{cases}
            a, & \text{if } X(\omega) \geq a, \\
            0, & \text{otherwise}.
        \end{cases}
        \]
        Hence
        \[
        \E(X) \geq \E(a\ind[\{X \geq a\}]) = a\E(\ind[\{X \geq a\}]) = a\P(X \geq a).
        \]
        \[
        a\E(\ind[\{X \geq a\}]) = a\P(X \geq a)
        \]
        because $\E(\ind) = \P(A)$.
    \end{proof}
\end{corollary}

\begin{corollary}[Chebyshev's inequality]\label{prob_col_chebyshevineq}
    Let $X : \Omega \rightarrow \R$ be a random variable.
    Let $0 < a \in \R$.
    Then
    \[
    \P(|X - \E(X)| > a) \leq \frac{1}{a ^ 2}\Var(X).
    \]
    \begin{proof}
        \begin{align*}
            \P(|X - \E(X)| > a) &= \P((X - \E(X)) ^ 2 > a ^ 2) \\
            &\leq \frac{1}{a ^ 2}\E((X - \E(X)) ^ 2) & \text{(By Markov inequality)} \\
            &= \frac{1}{a ^ 2}\Var(X).
        \end{align*}
    \end{proof}
\end{corollary}

\begin{example}
    Let $X \sim \Bin(10, 0.1)$
    \[
    \P(X \geq 6).
    \]

    Recall $\E(X) = 10 \times 0.1 = 1$.
    By Markov inequality
    \[
    \P(X \geq 6) \leq \frac{1}{6}\E(X) = \frac{1}{6}.
    \]
\end{example}

\newpage

\section{Limit Theorems}

\subsection{Weak law of large numbers}\footnote{WLLN.}
Recall:
$X_1, X_2, \dotsc, X_n$
$\bar{X_n} := \frac{1}{n}\sum_{i = 1}^{n}X_i$

$\bar{X_n}$ converges to $\E(X)$.
The sample mean converges to the population mean/expectation.

Let $X$ denote the number of heads in $n$ tosses of a coin with probability of success as $p$.
\[
X \sim \Bin(n, p)
\]
$\E(X) = np$,
$\Var(X) = np(1 - p)$.
Define $B_n = \frac{1}{n}X$.
Therefore,
\[
\E(B_n) = \frac{1}{n}\E(X) = p
\]
\[
\Var(B_n) = \frac{1}{n ^ 2}\Var(X) = \frac{p(1 - p)}{n}
\]
Apply \autoref{prob_col_chebyshevineq},
for any $\varepsilon > 0$,
\[
\P(|B_n - \E(B_n)| > \varepsilon) = \P(|B_n - p| > \varepsilon) \leq \frac{1}{\varepsilon ^ 2}\Var(B_n) = \frac{p(1 - p)}{\varepsilon ^ 2 \cdot n} \rightarrow 0
\]
$B_n$ will be very close to $p$,
with very high probability.A

Recall
\[
X = \sum_{i = 1}^{n}X_i,
\]
where $X_i \sim \Bin(p)$.
\[
X_i = \begin{dcases*}
    1, & if the $i$th trial is $H$, \\
    0, & otherwise,
\end{dcases*}
\]
\[
B_n = \frac{1}{n}\sum_{i = 1}^{n}X_i
\]
$\E(X_i) = p$.
\begin{theorem}[Weak law of large numbers]
    Let $X_1, X_2, \dotsc$ be an independent sequence of random variables,
    with $\E(X_i) = \mu$,
    and $\Var(X_i) = \sigma ^ 2$,
    for all $i \geq 1$.
    Let $\bar{X_n} = \frac{1}{n}\sum_{i = 1}^{n}X_i$.
    Then for any $\varepsilon > 0$,
    \[
    \lim_{n \rightarrow \infty}\P(|\bar{X_n} - \mu| > \varepsilon) = 0.
    \]
    [$\bar{X_1}, \bar{X_1}, \dotsc$ in probability to the random variable $Y$,
    where $\P(Y = \mu) = 1$].

    \begin{proof}
        \[
        \E(\bar{X_n}) = \frac{1}{n}\sum_{i = 1}^{n}\E(X_i) = \frac{1}{n}n\mu = \mu.
        \]
        \begin{align*}
            \Var(\bar{X_n}) &= \frac{1}{n ^ 2}\Var\left(\sum_{i = 1}^{n}X_i\right) \\
            &= \frac{1}{n ^ 2}\left[\sum_{i = 1}^{n}\Var(X_i) + 2 \sum_{i < j}\Cov(X_i, X_j)\right] \\
            &= \frac{1}{n ^ 2}\sum_{i = 1}^{n}\Var(X_i) \\
            &= \frac{1}{n ^ 2} \cdot n \sigma ^ 2 \\
            &= \frac{\sigma ^ 2}{n}.
        \end{align*}
        Apply Chebyshev's inequality,
        to get $\forall \varepsilon > 0$,
        \[
        \P(|\bar{X_n} - \E(\bar{X_n})| > \varepsilon) = \P(|\bar{X_n} - \mu| > \varepsilon) \leq \frac{1}{\varepsilon ^ 2}\Var(\bar{X_n}) = \frac{\sigma ^ 2}{\varepsilon ^ 2 n} \rightarrow 0 \text{ as } n \rightarrow \infty.
        \]
        \[
        0 \leq \lim_{n \rightarrow \infty}\P(|X_n - \mu| > \varepsilon) \leq 0.
        \]
        Uses sandwich theorem,
        then we get
        \[
        \lim_{n \rightarrow \infty}\P(|X_n - \mu| > \varepsilon) = 0.
        \]
        An equivalent statement is the following
        \[
        \lim_{n \rightarrow \infty}\P(|\bar{X_n} - \mu| \leq \varepsilon) = 1.
        \]
        since
        \[
        \P(|X_n - \mu| > \varepsilon) = 1 - \P(|X_n - \mu| \leq \varepsilon).
        \]
    \end{proof}
\end{theorem}

\begin{definition}[Convergence in probability]
    A sequence of random variables $X_1, X_2, \dotsc$ is said to converge in probability to a random variable $X$,
    if $\forall \varepsilon > 0$
    \[
    \lim_{n \rightarrow \infty}\P(|X_n - X| > \varepsilon) = 0.
    \]
\end{definition}

\begin{example}
    Let us collect a sample of size $n$ of heights of individuals.
    Let $H_i$ denote the height of the $i$th individual.

    Suppose we know that $\E(H_i) = \mu$,
    and $\Var(H_i) = \sigma ^ 2$.
    Then if $\overline{X}_n = \frac{1}{n}\sum_{i = 1}^{n}H_i$,
    then,
    \[
    \forall \varepsilon > 0, \P(|\overline{X}_n - \mu| > \varepsilon) \rightarrow 0\text{ as } n \rightarrow \infty,
    \]
    that is with high probability $\overline{X}_n$ is very close to $\mu$.
\end{example}

Comment:
In the most general case,
we need not know $\sigma ^ 2$.
Here in our case,
we wanted to Chebyshev's inequality and hence we needed $\sigma ^ 2$.

IID:
A sequence of random variables $X_1, X_2, X_3, \dotsc$ are called independent and identically distributed
(I.I.D)
if
\begin{enumerate}[label = (\roman*)]
    \item the random variables are independent,
    \item identically distributed,
    i.e.,
    all $X_i$'s have the same distribution,
    that is,
    $F_{X_1}(\cdot) = F_{X_2}(\cdot) = F_{X_3}(\cdot) = \dotsi$
\end{enumerate}

One can assume,
if the random variables are discrete,
then they have pmf $p(\cdot)$.
If the random variables are continuous then they have pdf $f(\cdot)$.

If a sequence of random variables $X_1, X_2, \dotsc$ is IID,
then $\E(X_1) = \E(X_2) = \dotsi$
and same variance $\Var(X_1) = \Var(X_2) = \dotsi$.

\begin{proof}
    \[
    \E(X_1) = \int_{-\infty}^{\infty}xf(x)\,dx
    \]
    \[
    \E(X_2) = \int_{-\infty}^{\infty}xf(x)\,dx
    \]
    When $X_1$ and $X_2$ have the same distribution.
    Hence $\E(X_1) = \E(X_2)$.

    If $X_1$ and $X_2$ have the same distribution,
    then let us assume they have the same pdf $f$.
    \[
    \E(X_1 ^ 2) = \E(X_2 ^ 2) = \int_{-\infty}^{\infty}x ^ 2f(x)\,dx
    \]
    and $\Var(X_1) = \E(X_1 ^ 2) - \E ^ 2(X_1) = \Var(X_2)$.
\end{proof}

\subsection{The central limit theorem}\footnote{CLT.}

\begin{theorem}[Central limit theorem]
    Let $X_1, X_2, \dotsc$ be an IID sequence.
    Let $\mu = \E(X_i)$ and $\sigma ^ 2 = \Var(X_i) > 0$.
    Define $\overline{X}_n = \frac{1}{n}\sum_{i = 1}^{n}X_i$,
    and  $S_n = \sum_{i = 1}^{n}X_i$.

    Let
    \[
    Z_n := \frac{S_n - n\mu}{\sigma\sqrt{n}} = \frac{\overline{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}
    \]
    $\overline{X}_n = \frac{1}{n}S_n$.
    Let $F_{Z_n}(\cdot)$ denote the cdf of $Z_n$.
    Then for any $z \in \R$,
    \[
    \lim_{n \rightarrow \infty}F_{Z_n}(z) = \lim_{n \rightarrow \infty}\P(Z_n \leq z) = \phi(z),
    \]
    where $\phi(z)$ is the cdf of the standard normal distribution.
\end{theorem}

We say that,
$Z_n$ converges in distribution to a standard normal random variable.

\begin{definition}[Convergence in distribution]
    A sequence of random variables $X_1, X_2, \dotsc$ with cdf $F_{X_1}(\cdot), F_{X_2}(\cdot), \dotsc$ is said to converge to a random variable $X$
    (with cdf $F_X(\cdot)$)
    in distribution,
    if
    \[
    \lim_{n \rightarrow \infty}F_{X_n}(x) = F_{X}(x),
    \]
    for all $x \in \R$,
    where $F_X(\cdot)$ is continuous.
    
    This is denoted as
    \[
    X_n \xrightarrow{d} X.
    \]
\end{definition}

Interpretation:
$Z_n \approx \mathcal{N}(0, 1)$.
\begin{align*}
    \E(Z_n) &= \frac{\E(\overline{X}_n) - \mu}{\frac{\sigma}{n}} = \frac{\mu - \mu}{\frac{\sigma}{\sqrt{n}}} = 0. \\
    \Var(Z_n) &= \frac{1}{\sigma ^ 2 n}\Var(S_n) = \frac{1}{\sigma ^ 2n}\sum_{i = 1}^{n}\Var(X_i) = \frac{1}{\sigma ^ 2 n}\cdot n\sigma ^ 2 = 1.
\end{align*}

CLT roughly tells us that
\begin{align*}
    S_n &\approx \mathcal{N}(n\mu, n \sigma ^ 2) \\
    \overline{X}_n &\approx \mathcal{N}\left(\mu, \frac{\sigma ^ 2}{n}\right), \\
    \P(S_n \leq x) &\approx \Phi\left(\frac{x - n\mu}{\sqrt{n}\sigma}\right) \\
    \P(\overline{X}_n \leq x) &\approx \Phi\left(\frac{x - \mu}{\frac{\sigma}{\sqrt{n}}}\right).
\end{align*}

\textit{Comment:
A stronger version of CLT is the Lindeberg-Feller
(for triangular array)
CLT and is a topic of advanced study.}

\begin{example}
    Average weight is $100$g,
    and the standard deviation is $40$g for a potato.
    Packed in a bag to contain at least $2500$g.
    What is the chance that we need more than $30$ potatoes?

    Given $\Phi(2.282) = 0.989$.

    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        $N := \text{ Number of potatoes needed to exceed $2500$g}$.
        Let $X_i$ be the weight of the $i$th potato.
        $W := \text{ Total weight of $30$ potatoes}$.
        $W := \sum_{i = 1}^{30}X_i$
        \[
        \E(W) = 30 \cdot \E(X_I) = 30 \cdot 100 = 3000.
        \]
        \[
        \Var(W) = 30 \cdot \Var(X_i) = 30 \cdot (40) ^ 2 = 48000.
        \]
        From CLT,
        $\frac{W - \E(W)}{\frac{\sigma}{\sqrt{n}}} = \frac{W - 3000}{\sqrt{48000}} \approx Z$,
        where $Z \sim \mathcal{N}(0, 1)$.
        \begin{align*}
            \P(N > 30) &= \P(W < 2500) \\
            &= \P\left(\frac{W - 3000}{\sqrt{48000}} < \frac{2500 - 3000}{\sqrt{48000}}\right) \\
            &= \P\left(\frac{W - 3000}{\sqrt{48000}} < -2.282\right) \\
            &\approx \P(Z < -2.282) \\
            &= \P(Z > 2.282) \\
            &= 1 - \Phi(2.282) \\
            &= 0.011.
        \end{align*}
    \end{proof}
\end{example}

\subsection{Moment generating functions}
























\end{document}