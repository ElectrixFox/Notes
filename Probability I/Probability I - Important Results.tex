\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\title{Probability I \\
    \large Important Results}
\author{Luke Phillips}
\date{April 2025}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Probability mass functions and probability density functions}

\subsection{Discrete}

\begin{definition}[Binomial distribution]
    A discrete random variable $X$ is binomially distributed with parameters $n \in \N$ and $p \in [0, 1]$,
    and we write $X \sim \Bin(n, p)$,
    when $\mathcal{X} = \{0, 1, \dotsc, n\}$ and
    \[
    p(x) = \binom{n}{x}p ^ x(1 - p) ^ {n - x}\text{ for all } x \in \{0, 1, 2, \dotsc, n\}.
    \]
\end{definition}

\begin{definition}[Geometric distribution]
    A discrete random variable $X$ is geometrically distributed with parameter $p \in (0, 1]$,
    and we write $X \sim \Geo(p)$,
    when $\mathcal{X} = \N \coloneqq \{1, 2, 3, \dotsc\}$ and
    \[
    p(x) = (1 - p) ^ {x - 1}p,\text{ for all } x \in \{1, 2, 3, \dotsc\}.
    \]
\end{definition}

\begin{definition}[Poisson distribution]
    A discrete random variable $X$ is Poisson distributed with parameter $\lambda$,
    and we write $X \sim \Po(\lambda)$,
    when $\mathcal{X} = \Z_{+} \coloneqq \{0, 1, 2, \dotsc\}$ and
    \[
    p(x) = \frac{e ^ {-\lambda}\lambda ^ x}{x!}\quad\text{for } x \in \Z_{+}.
    \]
\end{definition}

\subsubsection{Binomial-Poisson approximation}

\begin{theorem}
    Consider any $\lambda > 0$.
    Let $X_n \sim \Bin(n, p_n)$ where $\liminfty np_n = \lambda$,
    and let $Y \sim \Po(\lambda)$.
    Then for all $x \in \Z_{+}$,
    \[
    \liminfty p_{X_n}(x) = p_Y(x).
    \]
\end{theorem}

\subsection{Continuous}

\begin{definition}[Uniform distribution]
    Let $a, b \in \R$ with $a < b$.
    We say a continuous random variable $X$ is uniformly distributed on $[a, b]$,
    and write $X \sim U(a, b)$,
    when
    \[
    f(x) = \begin{cases}
        1 / (b - a) &\text{for all } x \in [a, b], \\
        0 &\text{elsewhere}.
    \end{cases}
    \]
\end{definition}

\begin{definition}[Exponential distribution]
    Let $\beta > 0$.
    A continuous random variable $X$ is exponentially distributed with parameter $\beta$,
    and write $X \sim \Exp(\beta)$,
    when
    \[
    f(x) = \begin{cases}
        \beta e ^ {-\beta x} &\text{for all } x \geq 0, \\
        0 &\text{elsewhere}.
    \end{cases}
    \]
\end{definition}

\begin{definition}[Normal distribution]
    Let $\mu, \sigma \in \R$ with $\sigma > 0$.
    A continuous random variable $X$ is normally distributed with parameters $\mu$ and $\sigma ^ 2$,
    write $X \sim \mathcal{N}(\mu, \sigma ^ 2)$,
    when
    \[
    f(x) = \frac{1}{\sigma\sqrt{2\pi}}e ^ {-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right) ^ 2}\text{ for all } x \in \R.
    \]
\end{definition}

\textbf{Remember we can standardise the normal distribution}
\begin{theorem}[Standardising the normal distribution]
    Suppose $\mu \in \R$ and $\sigma > 0$.
    If $X \sim \mathcal{N}(\mu, \sigma ^ 2)$ and $Z \sim \mathcal{N}(0, 1)$,
    then
    \[
    \frac{X - \mu}{\sigma} \sim \mathcal{N}(0, 1),\qquad\text{and}\qquad\sigma Z + \mu \sim \mathcal{N}(\mu, \sigma ^ 2).
    \]
\end{theorem}

\newpage

\section{Multiple random variables}

\begin{definition}[Independence of two random variables]
    Two random variables $X, Y$ are independent if
    \[
    \P(X \in A, Y \in B) = \P(X \in A)\P(Y \in B)\quad\text{for all } A \subseteq X(\Omega)\text{ and }B \subseteq Y(\Omega).
    \]
\end{definition}

\begin{definition}[Joint probability mass function]
    Let $(X, Y)$ be a bivariate discrete random variable with $\P((X, Y) \in \mathcal{Z}) = 1$ for a finite or countable $\mathcal{Z} \subseteq (X, Y)(\Omega)$.
    The joint probability mass function $p$ of $X$ and $Y$ is defined by
    \[
    p(x, y) \coloneqq \P(X = x, Y = y)\qquad\text{for all } (x, y) \in \mathcal{Z}.
    \]
\end{definition}

\begin{theorem}
    If $X, Y$ are both discrete random variables with $\P(X \in \mathcal{X}) = \P(Y \in \mathcal{Y}) = 1$,
    then their marginal probability mass functions are given in terms of the joint probability mass function
    \[
    p_X(x) = \sum_{y \in \mathcal{Y}}p(x, y)\text{ for all } x \in \mathcal{X} \qquad p_Y(y) = \sum_{x \in \mathcal{X}}p(x, y)\text{ for all } y \in \mathcal{Y}.
    \]
\end{theorem}

\begin{definition}[Conditional probability mass function]
    Let $X, Y$ be discrete random variables.
    For $y \in \mathcal{Y}$,
    the conditional probability mass function of $X$ given $Y = y$ is defined by
    \begin{align*}
        p_{X \mid Y}\left(x\mmid y\right) &\coloneqq \cP{X = x}{Y = y} = \frac{p_{X, Y}(x, y)}{p_Y(y)} \\
        &\text{for all $x \in \mathcal{X}$ and $y \in \mathcal{Y}$ such that $p_Y(y) > 0$.}
        \intertext{and similarly,
        the conditional probability mass function of $Y$ given $X = x$ is}
        p_{Y \mid X}\left(y\mmid x\right) &\coloneqq \cP{Y = y}{X = x} = \frac{p_{X, Y}(x, y)}{p_X(x)} \\
        &\text{for all $y \in \mathcal{Y}$ and $x \in \mathcal{X}$ such that $p_X(x) > 0$.}
    \end{align*}
\end{definition}

\begin{theorem}[Partition theorem for discrete random variables]
    Let $X, Y$ be discrete random variables.
    Then,
    \[
    p_X(x) = \sum_{y \in \mathcal{Y}}p_{X\mid Y}\left(x\mmid y\right)p_Y(y).
    \]
\end{theorem}

\begin{lemma}[Independence of discrete random variables]
    Two discrete random variables $X, Y$ are independent if and only if
    \[
    p_{X, Y}(x, y) = p_X(x)p_Y(y)\text{ for all $x \in \mathcal{X}$ and $y \in \mathcal{Y}$.}
    \]
\end{lemma}

\begin{definition}[Joint probability density function]
    Two real-valued random variables $X : \Omega \to \R$ and $Y : \Omega \to \R$.
    $X, Y$ are jointly continuously distributed when there is a non-negative piecewise continuous function $f : \R ^ 2 \to \R$,
    called the joint probability density function,
    such that
    \[
    \P(X \in [a, b], Y \in [c, d]) = \int_{a}^{b}\left(\int_{c}^{d}f(x, y)\,dy\right)\,dx
    \]
    for all $[a, b] \times [c, d] \subseteq \R ^ 2$.
\end{definition}

\begin{corollary}[Marginal probability density function]
    Let $X, Y$ be jointly continuously distributed random variables.
    Then $X$ and $Y$ are continuously distributed,
    with
    \[
    f_X(x) = \int_{-\infty}^{\infty}f(x, y)\,dy,\text{ for all $x \in \R$},\quad f_Y(y) = \int_{-\infty}^{\infty}f(x, y)\,dx,\text{ for all $y \in \R$}.
    \]
\end{corollary}

\newpage

\section{Expectation identities}

We define expectation of a random variable $X$ as
\[
\E(X) = \sum_{x \in \mathcal{X}}x\P(X = x)
\]
discrete $X$,
\[
\E(X) = \int_{-\infty}^{\infty}x\P(X = x)
\]
continuous $X$.

We also define variance of a random variable $X$ as
\[
\Var(X) = \E((X - \E(X)) ^ 2).
\]

We define the covariance of two random variables $X, Y$ by
\[
\Cov(X, Y) = \E((X - \E(X))(Y - \E(Y)))
\]
or similarly
\[
\Cov(X, Y) = \E(XY) - \E(X)\E(Y).
\]

Additionally there is the conditional expectation
\[
\E(X\mid A) = \sum_{x \in \mathcal{X}}x\cP{X = x}{A}.
\]

\newpage

\section{Probability inequalities}

\begin{theorem}[Monotonicity of expectation]
    For any random variable $X$,
    and any $a \in \R$,
    if $\P(X \geq a) = 1$ then $\E(X) \geq a$.
\end{theorem}

\begin{theorem}[Markov's inequality]
    If $X \geq 0$ then,
    for any $a > 0$,
    \[
    \P(X \geq a) \leq \frac{\E(X)}{a}.
    \]
\end{theorem}

\begin{corollary}[Chebyshev's inequality]
    For any random variable $X$ and any $a > 0$,
    we have
    \[
    \P(|X - \E(X)| \geq a) \leq \frac{\Var(X)}{a ^ 2}.
    \]
\end{corollary}

\newpage

\section{Methods}

\subsection{Central limit theorem}
Given a distribution if we can find $\sigma$ and $\mu$ then we are able to use the central limit theorem to show that
\[
X \sim \sigma ^ 2 Z + \mu
\]
which means that for any
\[
\P(X \leq x) = \P\left(\sigma ^ 2 Z + \mu \leq x\right) \iff \P(X \leq x) = \P\left(Z \leq \frac{x - \mu}{\sigma ^ 2}\right) = \Phi\left(\frac{x - \mu}{\sigma}\right).
\]

However,
if we are looking at a situation with $n$ "parts" then we will need to account for this.
Let $S_n \coloneqq \sum_{i = 1}^{n}X_i$,
so $\bar{X}_n \coloneqq S_n / n$ thus
\[
\P(\bar{X} \leq x) = \P\left(\sigma ^ 2\frac{S_n}{n} + \mu \leq x\right) \iff \P(\bar{X} \leq x) = \P\left(Z \leq \frac{x - \mu}{\sigma ^ 2 / n}\right) = \Phi\left(\frac{x - \mu}{\sigma / \sqrt{n}}\right).
\]

\subsection{Joint distribution tables}
To find a joint distribution table from variables $X, Y$,
label the columns with $y = 0, 1, 2, \dotsc$ and the rows $x = 0, 1, 2, \dotsc$,
then in the box where $x = i, y = j$ put $\P(x = i, y = j)$,
with an extra row on the bottom being the sum of the column for each $y$,
likewise for the rows to get $p(y)$ along the bottom and $p(x)$ down the side.

\begin{example}
    \begin{table}[H]
        \centering
        \begin{tabular}{c|ccc|c}
             $p(x, y)$ & $y = 0$ & $y = 1$ & $y = 2$ & $p(x)$  \\
             \hline
             $x = 0$ & $p(0, 0)$ & $p(0, 1)$ & $p(0, 1)$ & $\sum_{i = 0}^{2}p(0, i)$ \\
             $x = 1$ & $p(1, 0)$ & $p(1, 1)$ & $p(1, 1)$ & $\sum_{i = 0}^{2}p(1, i)$ \\
             $x = 2$ & $p(2, 0)$ & $p(2, 1)$ & $p(2, 1)$ & $\sum_{i = 0}^{2}p(2, i)$ \\
             \hline
             $p(y)$ & $\sum_{i = 0}^{2}p(i, 0)$ & $\sum_{i = 0}^{2}p(i, 1)$ & $\sum_{i = 0}^{2}p(i, 2)$ &
        \end{tabular}
    \end{table}
\end{example}

\subsection{Finding \texorpdfstring{$\E(X ^ k)$}{} using moment generating functions}
If we can find a moment generating function $M_X(t)$ for a random variable $X$ then we can find $\E(X ^ k)$ by the following
\[
\E(X ^ k) = \frac{d ^ k}{dt ^ k}M_X(0) = M_X ^ {(k)}(0).
\]
We can use this together with the fact that
\[
M_{aX + b}(t) = e ^ {bt}M_X(at)
\]
and for $X_1, \dotsc, X_n$ independent random variables with $Y = \sum_{i = 1}^{n}X_i$ then
\[
M_Y(t) = \prod_{i = 1}^{n}M_{X_i}(t)
\]
to get expectations for distributions such as the Binomial distribution
($Y_n = X_1 + X_2 + \dotsc X_n$ Bernoulli random variables).


\end{document}