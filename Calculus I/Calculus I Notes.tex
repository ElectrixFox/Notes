\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\title{Calculus I}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Functions}

\subsection{Functions, domain \& range}

\textbf{Gr10}

\begin{definition}
    A function $f : D \rightarrow C$ is a map ($f$) from a set $D$ called the domain ($\Dom f$) to a set $C$ called the codomain ($\mathrm{Codom}\ f$).
    
    For any $x \in D$, there is a unique $f(x) \in C$ called the image of $x$ under $f$.
\end{definition}

Sometimes  write $f : D \rightarrow C$ $x \mapsto f(x)$.

\begin{definition}
    The range of a function $f$ is the set of all images of $f$. This is a subset of the codomain.
    
    We can write $\Ran f = \{f(x)\,|\,x \in \Dom f\} \subseteq \mathrm{Codom}\ f$
\end{definition}

Real-valued functions\footnote{Always assume codomain is $\R$.} (i.e. the codomain is $\R$, or a subset) of a single real variable (i.e. the domain is $\R$, or a subset)

\begin{example}
    $f : \R \rightarrow \R$ given by $f(x) = x ^ 2\ \forall x \in \R$

    So $\Dom f = \R$ and $\mathrm{Codom}\ f = \R$ \& $\Ran f = [0, \infty)$.
\end{example}

If domain of $f$ isn't explicitly given, we take it to be the maximal subset of $\R$ for which the function is defined.

\subsection{The graph of a function}
\begin{definition}[Graph]
    The graph of $f$ is the set of all points $(x, y) \in \R ^ 2$ such that $x \in \Dom f$ $y = f(x)$.
\end{definition}
To sketch $f$, we often just restrict to an interval of the domain, $[a, b]$.

We often pick a region such that the graph shows significant features like turning points \& intercepts.

When necessary we use a closed shaded circle to indicate an included point, and an open unshaded circle to denote an excluded point.

\textbf{The vertical line test}

If any vertical line intersects the curve more than once then the curve is not the graph of a function.

\subsection{Even and odd functions}
\begin{definition}
    A function is even if $f(x) = f(-x)$ $\forall x \in \Dom f$
    
    A function is odd if $f(x) = -f(-x)$ $\forall x \in \Dom f$
\end{definition}
\begin{example}
    $f(x) = x ^ 2$ is even

    $f(x) = x ^ 3$ is odd
\end{example}

All functions can be written as the sum of an odd part and an even part.
\[
f(x) = f_{even}(x) + f_{odd}(x)
\]
where $f_{even}(x) = \frac{1}{2}\left(f(x) + f(-x)\right)$,
$f_{odd}(x) = \frac{1}{2}\left(f(x) - f(-x)\right)$

\begin{example}
    $f(x) = e ^ x$ with
    
    $f_{even}(x) = \frac{1}{2}(e ^ x + e ^ {-x}) =: \cosh(x)$
    
    $f_{odd}(x) = \frac{1}{2}(e ^ x - e ^ {-x}) =: \sinh(x)$
\end{example}

\begin{example}
    $f(x) = (\cos x + x)\sin x$ with

    $f_{even}(x) = x\sin x$
    
    $f_{odd}(x) = \cos x \sin x$
\end{example}

Product of two odd functions is an even function, product of an odd function and an even function is an odd function.

\subsection{Piecewise functions}
Some functions may be defined piecewise, i.e. with different expressions for different intervals in the domain.

\begin{example}
    \begin{align*}
        f(x) &= |x| \\
        &= \begin{cases}
            \phantom{-}x\quad \text{if} &x \geq 0 \\
            -x\quad \text{if} &x < 0
        \end{cases}
    \end{align*}
\end{example}

A step function is a piecewise function which is constant on each piece.
\begin{example}
    Heaviside step function
    \[
    H(x) = \begin{cases}
        0 & x < 0 \\
        1 & x > 0
    \end{cases}
    \]
    Here $\Dom H = \R \setminus \{0\}$ though it is sometimes useful to extend the domain to $\R$ by defining $H(0): (0, 1, \frac{1}{2})$
\end{example}

\subsection{Operations with functions}
Given two functions $f, g\  \&\  c \in \R$, we can define the following functions:
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Name & Definition & Domain \\
        \hline
        Sum $(f + g)$ & $(f + g)(x) = f(x) + g(x)$ & $\Dom (f + g) = \Dom f \cap \Dom g$ \\
        Difference $(f - g)$ & $(f - g)(x) = f(x) - g(x)$ & $\Dom (f - g) = \Dom f \cap \Dom g$ \\
        Product $(fg)$ & $(fg)(x) = f(x)g(x)$ & $\Dom (fg) = \Dom f \cap \Dom g$ \\
        Ratio $\left(\frac{f}{g}\right)$ & $\left(\frac{f}{g}\right)(x) = \frac{f(x)}{g(x)}$ & $\Dom \left(\frac{f}{g}\right) = (\Dom f \cap \Dom g) \setminus \{x\,|\,g(x) = 0\}$ \\
        Composition $(f\circ g)$ & $(f \circ g)(x) = f(g(x))$ & $\Dom (f \circ g) = \{x \in \Dom g\,|\, g(x) \in \Dom f\}$ \\
        Scalar Multiplication $(cf)$ & $(cf)(x) = cf(x)$ & $\Dom (cf) = \Dom f$ \\
        \hline
    \end{tabular}
\end{table}

Unlike multiplication, composing functions in opposite order changes the function (generally).

Using the above we can also form more general linear combinations, such as $(af + bg)(x)$ with $(af + bg)(x) = af(x) + bg(x)$ \& $\Dom (af + bg) = \Dom f \cap \Dom g$.

\subsection{Inverse Functions}
\begin{definition}[Surjective function]
    A function $f : D \rightarrow C$ is surjective if $\Ran f = \mathrm{Codom}\, f = C$, i.e. if $\forall\, y \in C\, \exists\, x \in D$ such that $f(x) = y$
\end{definition}
\begin{example}
    $f : \R \rightarrow \R$, $x \mapsto 2x + 1$ is surjective.

    $f : \R \rightarrow \R$, $x \mapsto x ^ 2$ is not surjective since for example $(-2)$ is not the image of any $x \in \R$ (in fact true $\forall\,u < 0,\, u \in \R$).
\end{example}

\begin{definition}[Injective function]
    $f : D \rightarrow C$ is injective if $\forall\, x_1, x_2 \in \R$ with $x_1 \neq x_2,\, f(x_1) \neq f(x_2)$.
\end{definition}

\begin{example}
    $f(x) = 2x + 1$ is injective since $f(x_1) = f(x_2)$ implies that $x_1 = x_2$.

    $f(x) = x ^ 2$ is not injective since $f(2) = f(-2)$ (in fact $f(x) = f(-x)$ but $x \neq -x\, \forall\, x\neq 0$).
\end{example}

\textbf{Horizontal line test}

If no horizontal line intersects the graph of $f$ more than once then $f$ is injective, otherwise it is not.

\begin{definition}[Bijective function]
    A function $f : D \rightarrow C$ is bijective if it is both injective and surjective.
\end{definition}

\begin{example}
    We have showed that $f(x) = 2x + 1$ is both injective and surjective, so it is bijective.
\end{example}

A bijective function $f : D \rightarrow C$ has a unique inverse $f ^ {-1}$ such 
\[
(f \circ f ^ {-1})(x) = x = (f ^ {-1} \circ f)(x).
\]
We can see that $\Ran f ^ {-1} = \Dom f$, $\Dom f ^ {-1} = \Ran f$. Equivalently, $y = f ^ {-1} (x) \iff f(y) = x$.

\begin{example}
    Since $f : \R \rightarrow \R$, $x \mapsto 2x + 1$ is bijective, we can find its inverse.

    Let $y = f^{-1}(x)$ then $f(y) = x = 2y + 1$ so $y = \frac{x - 1}{2}$.
\end{example}

\begin{example}
    $f : \R \rightarrow \R$, $x \mapsto x ^ 2$ is neither injective nor surjective, but if we reduce the domain to $[0, \infty)$ then it is injective with range $[0, \infty)$. So $g : [0, \infty) \rightarrow [0, \infty)$ $x \mapsto x ^ 2$ is bijective, and so we can find an inverse.

    Let $y = f^{-1}(x)$, then $f(y) = x = y ^ 2$ so $y = f^{-1}(x) = \sqrt{x}$.
\end{example}

\newpage

\section{Limits and continuity}

Rough idea: A function $f(x)$ has a limit $L$ at $x = a$ if $f(x)$ is close to $L$ whenever $x$ is close to $a$.

More precise idea: If we have an acceptable 'error' ($\epsilon,\ \epsilon > 0$) between $f$ and $L$, then I would need
\[
|f(x) - L| < \epsilon\quad\forall x \in \text{some interval around } a.
\]
If for any $\epsilon > 0$ can find some distance ($\delta,\ \delta > 0$) such that
\[
|f(x) - L| < \epsilon\quad\forall\quad 0 < |x - a| < \delta
\]
then we say that $f(x)$ has a limit $L$ as $x$ tends to $a$.

\begin{definition}
    $f(x)$ has a limit $L$ as $x$ tends to $a$ if
    
    $\forall \epsilon > 0,\,\exists \delta > 0$ such that
    \[
    |f(x) - L| < \epsilon
    \]
    $\forall x$ such that
    \[
    0 < |x - a| < \delta
    \]
\end{definition}
We write $\displaystyle\lim_{x \rightarrow a}f(x) = L$ or $f(x) \rightarrow L \text{ as } x \rightarrow a$.

If there is no such $L$ then the limit doesn't exist.

Note: we do not require $f(a) = L$ or for $f(a)$ to even exist, as we only consider the interval $0 < |x - a| < \delta$.

\subsection{Continuity}

\begin{definition}
    A function is continuous at the point $a$ if
    \begin{itemize}
        \item $f(a)$ exists,
        \item $\lim_{x \rightarrow a}f(x)$ exists,
        \item $\lim_{x \rightarrow a}f(x) = f(a)$.
    \end{itemize}
\end{definition}

\begin{definition}
    $f(x)$ is continuous on a subset $S$ of its domain if it is continuous at all points in $S$.
\end{definition}

\begin{definition}
    $f(x)$ is continuous if it is continuous at every point in its domain.
\end{definition}

\begin{example}
    Let $f(x) = \begin{cases}
        x ^ 2 &\text{if}\  -1\leq x < 0 \\
        1 &\text{if}\  x = 0 \\
        x ^ 2 &\text{if}\  0 < x \leq 1 
    \end{cases}$
    Then $f(x)$ is continuous on $[-1,\, 1] \setminus \{0\}$, but not continuous at $x = 0$ as $\displaystyle\lim_{x \rightarrow 0}f(x) = 0 \neq f(0)$.
\end{example}

\begin{example}
    Let $f(x) = \begin{cases}
        1 &\text{if}\  x \leq 0 \\
        x ^ 2 &\text{if}\  x > 0
    \end{cases}$
    is not continuous at $x = 0$ as $\displaystyle\lim_{x \rightarrow 0}f(x)$ does not exist.
\end{example}

\subsection{Classification of discontinuities}
\begin{definition}
    $f(x)$ has a right-sided limit $\displaystyle L ^ + = \lim_{x \rightarrow a+}f(x)$ as $x$ tends to $a$ from above (or the right) if
    $\forall\epsilon >0,\ \exists\delta > 0$ such that
    \[
    |f(x) - L^+| < \epsilon
    \]
    $\forall x$ such that $0 < x - a < \delta$.
\end{definition}

\begin{definition}
    $f(x)$ has a left-sided limit $\displaystyle L ^ - = \lim_{x \rightarrow a-}f(x)$ as $x$ tends to $a$ from below (or the left) if
    $\forall\epsilon >0,\ \exists\delta > 0$ such that
    \[
    |f(x) - L^-| < \epsilon
    \]
    $\forall x$ such that $0 < a - x < \delta$.
\end{definition}

$\displaystyle L = \lim_{x \rightarrow a}f(x)$ exists if and only if $L ^ +,\ L ^ -$ both exist and $L ^ + = L ^ -$. Then $L = L ^ + = L ^ -$.

Using the definitions of right and left sided limits, there are three types of discontinuity.
\begin{enumerate}[label = \roman*)]
    \item Removable discontinuity. $L$ exists but $f(a) \neq L$, we can always 'remove' the discontinuity to make a continuous function
    \[
    g(x) = \begin{cases}
        f(x) &\text{if}\ x \neq a \\
        L &\text{if}\ x = a
    \end{cases}
    \]

    \begin{example}
        \[
        f(x) = \begin{cases}
            x ^ 2 &\text{if}\ x \neq 0 \\
            1 &\text{if}\ x = 0
        \end{cases}
        \]
        has a removable discontinuity at $x = 0$ removing this gives the continuous function $g(x) = x ^ 2$
    \end{example}
    
    \item Jump discontinuity. $L ^ +$ and $L ^ -$ both exist, but $L ^ + \neq L ^ -$.
    \begin{example}
        \[
        f(x) = \begin{cases}
            1 &\text{if}\ x \leq 0 \\
            x ^ 2 &\text{if}\ x > 0
        \end{cases}
        \]
        Here $L ^ + = 0,\ L ^ - = 1$, sp $L ^ + \neq L ^ -$ and no limit exists
    \end{example}

    \item Infinite (Essential) discontinuity. At least one of $L ^ +$ and $L ^ -$ does not exist.
    \begin{example}
        $f(x) = \frac{1}{x}$ has an infinite discontinuity at $x = 0$. Here neither $L ^ +$ nor $L ^ -$ exists.
    \end{example}
    \begin{example}
        $f(x) = \sin\left(\frac{1}{x}\right)$ has an infinite discontinuity at $x = 0$. Neither $L ^ +$ nor $L ^ -$ exists.
    \end{example}
\end{enumerate}

\subsection{Facts about limits and continuity}
Facts about limits
\begin{itemize}
    \item The limit is unique.
    \item If $f(x) = g(x)$ (except possibly at $x = a$) in some open interval containing $a$, then
    \[
    \lim_{x \rightarrow a}f(x) = \lim_{x \rightarrow a}g(x).
    \]
    \item If $f(x) \geq K$ on either some interval $(a, b)$ or $(c, a)$ and if $\lim_{x \rightarrow a}f(x) = L$ then $L \geq K$ (Similarly replacing $\geq$ with $\leq$).
    \item Calculus of Limits Theorem (COLT)
    
    If $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M$, then
    \begin{enumerate}[label = \roman*)]
        \item \[
        \lim_{x \rightarrow a}(f(x) + g(x)) = L + M
        \]
        \item \[
        \lim_{x \rightarrow a}(f(x) \cdot g(x)) = L \cdot M
        \]
        \item If $M \neq 0$ then \[
            \lim_{x \rightarrow a}\left(\frac{f(x)}{g(x)}\right) = \frac{L}{M}.
        \]
    \end{enumerate}
\end{itemize}

The following are facts about continuity
\begin{itemize}
    \item If $f$ and $g$ are continuous, then so are $(f + g),\ (fg), \left(\frac{f}{g}\right)$ and $|f|$.
    \item All polynomial, rational, trigonometric and hyperbolic functions are continuous.
    \item If $\lim_{x \rightarrow a}g(x) = L$ and $f(x)$ is continuous at $x = L$, then
    \[
    \lim_{x \rightarrow a}(f \circ g)(x) = f(L).
    \]
\end{itemize}

\begin{example}
    The following are continuous
    \[
    f(x) = 2x ^ 3 + x + 7,\quad g(x) = \frac{3x}{x - 1},\quad h(x) = \left|\frac{1 + x ^ 2}{\sin x}\right|
    \]
\end{example}


\begin{example}
    \[
    \lim_{x \rightarrow 0}\,\frac{1}{x}\ \text{does not exist.}
    \]
\end{example}

\begin{example}
    \begin{align*}
    \lim_{x \rightarrow \frac{\pi}{2}}\,x ^ 2 \sin x &=\footnotemark \left(\lim_{x \rightarrow \frac{\pi}{2}} x^ 2 \right)\left(\lim_{x \rightarrow \frac{\pi}{2}} \sin x\right) \\
    &= \left(\frac{\pi}{2}\right) ^ 2 \cdot 1 = \frac{\pi ^ 2}{4}.
    \end{align*}
    \footnotetext{COLT}
\end{example}

\begin{example}
    \begin{align*}
    \lim_{x \rightarrow 3}\,\frac{2x ^ 2 - 18}{x - 3} &= \lim_{x \rightarrow 3}\, \frac{2(x + 3)(x - 3)}{x - 3} \\
    &= \lim_{x \rightarrow 3}\,2(x + 3)\quad\text{Since the value at $x = 3$ is irrelevant for the limit.} \\
    &= 2(6) = 12.
    \end{align*}
\end{example}

\subsection{The pinching (squeezing) theorem}
\begin{theorem}[Squeezing theorem]\label{calc_thm_squeze}
    If $g(x) \leq f(x) \leq h(x)$ for all $x \neq a$ in some open interval containing $a$, and $\displaystyle \lim_{x \rightarrow a}g(x) = \lim_{x \rightarrow a} h(x) = L$ then
    \[
    \lim_{x \rightarrow a}f(x) = L.
    \]
\end{theorem}

\begin{example}
    Calculate
    \[
    \lim_{x \rightarrow 0} x ^ 2 \sin \left(\frac{1}{x}\right).
    \]
    As $-1 \leq \sin \left(\frac{1}{x}\right) \leq 1$ then $-x ^ 2 \leq x ^ 2 \sin \left(\frac{1}{x}\right) \leq x ^ 2$.

    We have $\lim_{x \rightarrow 0}(-x ^ 2) = 0$, $\lim_{x \rightarrow 0}(x ^ 2) = 0$ so by \autoref{calc_thm_squeze}
    \[
    \lim_{x \rightarrow 0} x ^ 2 \sin \left(\frac{1}{x}\right) = 0.
    \]
\end{example}

\subsection{Two trigonometric limits}
Two important limits are the following
\[
\lim_{x \rightarrow 0}\frac{\sin x}{x} = 1
\]
\[
\lim_{x \rightarrow 0}\frac{1 - \cos x}{x} = 0
\]

We can prove the first limit using \autoref{calc_thm_squeze}.

\textbf{Gr11}

Let $T_1$ be the area of the triangle OAB, $T_2$ be the area of the triangle OAC, $S$ be the area of sector OAB. Clearly $T_1 < S < T_2$. The length of BD is $\sin x$, the length of AC is $\tan x$
\[
T_1 = \frac{1}{2}\sin x\quad T_2 = \frac{1}{2}\tan x\quad S = \left(\frac{x}{2\pi}\right) \cdot \pi = \frac{x}{2}.
\]
Therefore $\sin x < x < \tan x$. Now for $x \in \left(0, \frac{\pi}{2}\right)$
\[
1 = \frac{\sin x}{\sin x} < \frac{x}{\sin x} < \frac{\tan x}{\sin x} = \frac{1}{\cos x}
\]
and since all these functions are even, the same holds on $\left(-\frac{\pi}{2}, 0\right)$.

We also have
\[
\lim_{x \rightarrow 0} 1 = 1\quad\lim_{x \rightarrow 0}\frac{1}{\cos x} = 1,
\]
so by \autoref{calc_thm_squeze}
\[
\lim_{x \rightarrow 0}\frac{x}{\sin x} = 1\text{ and therefore } \lim_{x \rightarrow 0}\frac{\sin x}{x} = 1
\]

We can use this to prove the second limit
\begin{align*}
\lim_{x \rightarrow 0}\frac{1 - \cos x}{x} &= \lim_{x \rightarrow 0}\frac{(1 - \cos x)(1 + \cos x)}{x(1 + \cos x)} \\ 
&= \lim_{x \rightarrow 0}\frac{\sin ^ 2 x}{x(1 + \cos x)} \\
&= \lim_{x \rightarrow 0}\left(\frac{\sin x}{x}\right)\left(\frac{\sin x}{1 + \cos x}\right) \\
&= 1 \cdot 0 = 0.
\end{align*}

\begin{example}
    \[
    \lim_{x \rightarrow 0}\frac{\tan x}{x}
    \]

    \[
    \lim_{x \rightarrow 0}\frac{\tan x}{x} = \lim_{x \rightarrow 0} \left(\frac{\sin x}{x}\right) \left(\frac{1}{\cos x}\right) = 1 \cdot 1 = 1.
    \]
\end{example}


\subsection{Limits as \texorpdfstring{$x \rightarrow \infty$}{x tends to infinity}}

A function has a limit $L$ as $x \rightarrow \infty$ if $f(x)$ can be kept arbitrarily close to $L$ as long as $x$ is sufficiently large.

\begin{definition}
    $f(x)$ has a limit $L$ as $x \rightarrow \infty$ if $\forall \epsilon > 0,\, \exists S > 0$ such that $\forall x > S$,
    \[
    |f(x) - L| < \epsilon.
    \]
\end{definition}

\begin{example}
    \[
    \lim_{x \rightarrow \infty}\frac{1}{x} = 0
    \]
    as $\frac{1}{x}$ can be made as close to zero as wanted by taking $x$ sufficiently large.
\end{example}

An easy way to calculate limits as $x \rightarrow \infty$ it to make the substitution $x = \frac{1}{u}$. Then 
\[
\lim_{x \rightarrow \infty} f(x) = \lim_{x \rightarrow 0^+}f\left(\frac{1}{u}\right).
\]
\begin{example}
    \[
    \lim_{x \rightarrow \infty}\frac{1}{x} = \lim_{u \rightarrow 0^+}u = 0 
    \]
\end{example}

\begin{example}
    \begin{align*}
    \lim_{x \rightarrow \infty}\frac{x\cos \left(\frac{1}{x}\right) + 2}{x} &= \lim_{u \rightarrow 0^+}(\cos u + 2u) \\
    &= 1 + 0 = 1.
    \end{align*}
\end{example}

\begin{example}
    \begin{align*}
    \lim_{x \rightarrow \infty}\frac{2x + 3}{x + 5} &= \lim_{x \rightarrow \infty}\frac{2 + \frac{3}{x}}{1 + \frac{5}{x}} \\
    &= \lim_{u \rightarrow 0^+}\frac{2 + 3u}{1 + 5u} \\
    &= 2
    \end{align*}
\end{example}

Note the graph of $f(x)$ has a horizontal asymptote to the right (or left) at $y = 1$ if
\[
\lim_{x \rightarrow \infty}f(x) = L\quad\text{(or} \lim_{x \rightarrow -\infty}f(x) = L\text{ respectively})
\]


\subsection{The Intermediate Value Theorem}
If $f(x)$ is continuous on $[a, b]$, and it is any number between $f(a)$ and $f(b)$ then $\exists c \in (a, b)$ such that $f(c) = u$.

\begin{example}
    $f(x) = \sin x$ is continuous on $\left[0, \frac{\pi}{2}\right]$ and
    \[
    f(0) = 0 < \frac{1}{2} < 1 = f\left(\frac{\pi}{2}\right) 
    \]
    so by the intermediate value theorem there exists at least one $x \in \left(0, \frac{\pi}{2}\right)$ such that $\sin x = \frac{1}{2}$.
\end{example}

Note: we do require continuity for the intermediate value theorem to apply
\begin{example}
    Let \[
    \mathrm{sgn}(x) = \begin{cases}
        1 & x > 0 \\    
        0 & x = 0 \\ 
        -1 & x < 0    
    \end{cases}
    \]
    then $f(x) = \frac{\mathrm{sgn}(x)}{1 + x ^ 2}$ has $f(-1) = -\frac{1}{2}$ and $f(1) = \frac{1}{2}$ but $\nexists x \in (-1, 1)$ such that $f(x) = \frac{1}{5}$.

    The intermediate value theorem does not apply, as $f$ has a jump discontinuity at $x = 0$
    $\displaystyle L ^ - = \lim_{x \rightarrow 0^-}f(x) = -1\ L ^ + = \lim_{x \rightarrow 0^+}f(x) = 1$, so $L^+ \neq L^-$
\end{example}

One important application of the intermediate value theorem is finding the zeros (roots) of a function. If $f$ is continuous on $[a, b]$ and $f(a) < 0 < f(b)$ or $f(a) > 0 > f(b)$, then by the intermediate value theorem, there is at least one root such that $f(x) = 0$ between $a$ and $b$.

\begin{example}
    $f(x) = x ^ 2 - 2$ is continuous on $[1, 2]$ with $f(1) = -1 < 0$ and $f(2) = 2 > 0$, so there is at least one root of $x ^ 2 - 2 = 0$ between $1$ and $2$ (given by $x = \sqrt{2}$).
\end{example}

Iterating this leads to the bisection method, which can be used to find roots to arbitrary accuracy.

\newpage

\section{Differentiation}

\subsection{Derivative as a limit}
Geometrically, the derivative $f'(a)$ (or $\frac{df}{dx}(a)$ or $\frac{df}{dx}\left|_{x = a}\right)$)
of a function $f(x)$ at $x = a$ is equal to the slope of the (unique, non-vertical) tangent to the graph of $f$ at $a$ (if it exists).

This is naturally defined as the limit of the slope of a line segment joining two nearby points on the graph of $f$.
\[
f'(a) = \lim_{h \rightarrow 0}\frac{f(a + h) - f(a)}{h}
\]
if the limit exists.

If the limit exists we say $f$ is differentiable at $x = a$.

\begin{example}
    With $f(x) = x ^ 2$ calculate $f'(a)$ using the limit definition.
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        \begin{align*}
        f'(a) &= \lim_{h \rightarrow 0}\frac{f(a + h) - f(a)}{h} \\
        &= \lim_{h \rightarrow 0}\frac{(a + h) ^ 2 - a ^ 2}{h} \\
        &= \lim_{h \rightarrow 0}\frac{2ah + h ^ 2}{h} \\
        &= \lim_{h \rightarrow 0}(2a + h) \\
        &= 2a.
        \end{align*}
    \end{proof}
\end{example}
If $f'(a)$ exists $\forall a \in \Dom f$, we say that $f$ is differentiable and the function $f'(x)$ is called the derivative.

\begin{example}
    Calculate the derivative of $f(x) = \sin(x)$
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        \begin{align*}
            f'(x) &= \lim_{h \rightarrow 0}\frac{f(x + h) - f(x)}{h} \\
            &= \lim_{h \rightarrow 0}\frac{\sin(x + h) - \sin(x)}{h} \\
            &= \lim_{h \rightarrow 0}\frac{\sin(x)\cos(h) + \cos(x)\sin(h) - \sin(x)}{h} \\
            &= \lim_{h \rightarrow 0}\left(\sin(x)\frac{\cos(h) - 1)}{h} + \cos(x)\frac{\sin(h)}{h}\right) \\
            &= \cos(x)
        \end{align*}
    \end{proof}
\end{example}

\textbf{Notes on differentiability}

\begin{itemize}
    \item The equation of the tangent to $f$ at a point $(a, f(a))$ is given in terms of the derivative as $y = f(a) + f'(a)(x - a)$.
    \item A necessary condition for $f'(a)$ to exist is for $f$ to be continuous at $a$, but this is not a sufficient condition.
    \item There are two ways a function can fail to be differentiable at a point where it is continuous:
        \begin{itemize}
            \item The tangent line is vertical at the point e.g. $f(x) = x ^ {\frac{1}{3}}$ which is continuous but not differentiable at $x = 0$.
            \item There is no tangent line at the point e.g. $f(x) = |x|$ is continuous everywhere but not differentiable at $x = 0$.
        \end{itemize}
\end{itemize}

\subsection{The Leibniz and chain rules}
Smooth functions are functions which are infinitely differentiable.

If $f(x)$ is differentiable, then $f'(x)$ may also be differentiable we say $f$ is twice differentiable. We write $(f'(x))'$ as $f''(x)$, the $2 ^ {\text{nd}}$ derivative. More generally $f^{(n)}(x)$ is the $n^{\text{th}}$ derivative.

Other common notation includes $\frac{d^nf}{dx^n}$ or $Df,\,D^nf$.

If we have a function with time as the variable e.g. $r(t)$ (position), then we usually write $r'(t) = \dot{r}$ (velocity) and $\ddot{r}(t)$ (acceleration).

\textbf{Leibniz rule}: If $f(x)$ and $g(x)$ are both differentiable $n$ times, then so is $(fg)(x)$. We have
\[
D(fg) = (Df)g + f(Dg).
\]
\begin{align*}
    D^2(fg) &= (D^2f)g + (Df)(Dg) + (Df)(Dg) + fD ^ 2g \\
    &= (D^2f)g + 2(Df)(Dg) + fD^2g.
\end{align*}

General Leibniz rule
\[
D^n(fg) = \sum_{k = 0}^{n}\binom{n}{k}(D ^ kf)(D ^ {n - k}g)
\]

\begin{example}
    Calculate $D ^ 3(x ^ 2 \sin x)$.

    We know $D ^ 3(fg) = (D ^ 3 f)g + 3(D ^ 2f)g + 3(Df)(D ^ 2 g) + f D ^ 3g$
    
    so with
    \[
    f(x) = x ^ 2,\,Df = 2x,\,D^2f = 2,\,D^3f = 0
    \]
    \[
    g(x) = \sin x,\,Dg = \cos x,\,D^2g = -\sin x,\,D ^ 3g = -\cos x
    \]
    So
    \begin{align*}
        D ^ 3(x ^ 2 \sin x) &= 3(2)\cos x + 3(2x)(-\sin x) + x ^ 2(-\cos x) \\
        &= (6 - x ^ 2)\cos x - 6x\sin x.
    \end{align*}
\end{example}

For differentiating the composition of functions $(f \circ g)(x) = f(g(x))$
\[
(f \circ g)'(x) = f'(g(x))g'(x)
\]
or $\frac{d}{dx}f(g(x)) = \frac{df}{dg}\frac{dg}{dx}$

\begin{example}
    Calculate $\frac{d}{dx}\left((x ^ 2 + 3x) ^ 4\right)$.

    Let $f(x) = x ^ 4,\,g(x) = x ^ 2 + 3x$ then $\left((x ^ 2 + 3x) ^ 4\right) = f(g(x))$. Then $f'(x) = 4x ^ 3,\quad g'(x) = 2x + 3$
    \[
    \frac{d}{dx}\left((x ^ 2 + 3x) ^ 4\right) = 4(x ^ 2 + 3x) ^ 3 \cdot (2x + 3)
    \]
\end{example}

\subsection{L' H\^opital's rule}
Recall, if $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M \neq 0$, then
\[
\lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \frac{L}{M}.
\]
If $L \neq 0$ and $M = 0$ then $\lim_{x \rightarrow a}\frac{f(x)}{g(x)}$ does not exist.

If $L = M = 0$ then we say the limit is of in-determinant form. This means the limit might or might not have a well defined value, but we cannot tell simply by knowing that $L$ and $M$ are both $0$.

Limits of the following form are all in-determinant
\begin{align*}
    \frac{0}{0} & \frac{\infty}{\infty} & 0 \cdot \infty & \infty - \infty \\
    0 ^ 0 & 1 ^ \infty & \infty^0
\end{align*}

\textbf{L' H\^opital's rule} applies to the $1^{\text{st}}$ (and $2^{\text{nd}}$) case.

If $f(x)$ and $g(x)$ are differentiable on
\[
I = (a - h,\,a) \cup (a,\,a + h)\quad\text{for some }h > 0
\]
with $\lim_{x \rightarrow a}f(x) = \lim_{x \rightarrow a}g(x) = 0$ and if $\lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}$ exists and $g'(x) \neq 0\ \forall x \in I$, then
\[
\lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}.
\]
\begin{proof}
    For the case when $f'(a)$ and $g'(a)$ both exist and continuous and $g'(a) \neq 0$.

    \begin{align*}
        \lim_{x \rightarrow a}\frac{f(x)}{g(x)} &= \lim_{x \rightarrow a}\frac{f(x) - f(a)}{g(x) - g(a)}\footnotemark \\
        &= \lim_{x \rightarrow a}\frac{\frac{f(x) - f(a)}{x - a}}{\frac{g(x) - g(a)}{x - a}}\footnotemark \\
        &= \frac{\lim_{x \rightarrow a}\frac{f(x) - f(a)}{x - a}}{\lim_{x \rightarrow a}\frac{g(x) - g(a)}{x - a}} \\
        &= \frac{f'(a)}{g'(a)} \\
        &= \lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}.
    \end{align*}
        \footnotetext{Adding $0$ by subtracting $f(a)$ as it approaches $0$.}
        \footnotetext{Dividing by $1$ as $x - a \rightarrow 0 \text{ as } x \rightarrow a$.}
\end{proof}

\begin{example}
    Calculate
    \[
    \lim_{x \rightarrow 0}\frac{\sin x}{x}.
    \]
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        This satisfies all the needed properties,
        with $f(x) = \sin x$ and $g(x) = x$, $f'(x) = \cos x$, $g'(x) = 1$
        and so by L'H\^opital's rule
        $\lim_{x \rightarrow 0}\frac{\sin x}{x} = \lim_{x \rightarrow 0}\frac{\cos x}{1} = 1$.
    \end{proof}
\end{example}

If applying L'H\^opital's rule leaves another indetermininat form,
then we can apply L'H\^opital's rule again, and so on.

\begin{example}
    \begin{align*}
    \lim_{x \rightarrow 0}\frac{2\sin x - \sin (2x)}{x - \sin x} &= \lim_{x \rightarrow 0}\frac{2\cos x - 2\cos (2x)}{1 - \cos x} \\
    &= \lim_{x \rightarrow 0}\frac{-2\sin x + 4\sin(2x)}{\sin x} \\
    &= \lim_{x \rightarrow 0}\frac{-2\cos x + 8\cos (2x)}{\cos x} \\
    &= \frac{-2 + 8}{1} = 6.
    \end{align*}
\end{example}

\subsection{Boundedness and Monotonicity}
Let $f(x)$ be a function defined on some interval $I$.
\begin{definition}
    $f(x)$ is bounded (from) above in $I$ by an upper bound $k_1$ if $f(x) \leq k_1\ \forall x \in I$.
\end{definition}

$f(x)$ need not take the value $k_1$ in $I$.
If $\exists x_1 \in I$ such that $f(x_1) = k_1$ then
we say the upper bound $k_1$ is attained,
and $k_1$ is the global (in some interval) maximum value of $f(x)$ in $I$.

Similarly
\begin{definition}
    If $f(x)$ is bounded (from) below in $I$ by a lower bound $k_2$ if $f(x) \geq k_2,\ \forall x \in I$.
\end{definition}

If $\exists x_2 \in I$ such that $f(x_2) = k_2$,
then the lower bound is attained,
and $k_2$ is the global (in some interval) minimum.

\begin{definition}
    $f(x)$ is bounded in $I$ if it is bounded both above and below.
    That is if $\exists k$ such that
    \[
    |f(x)| \leq k\quad\forall x \in I
    \]
\end{definition}

As for other properties if we don't specify an interval,
we take it to be the domain.

\begin{example}
    $\cos x$ is bounded (in $\R$) as $|\cos x| \leq 1\quad\forall x \in \R$.
    Both the upper and lower bounds ($1$, $-1$ respectively) are attained (e.g. $0$ and $\pi$),
    and so these are the global maximum and minimum values.
\end{example}

\begin{example}
    \[
    f(x) = \frac{\mathrm{sgn}(x)}{1 + x ^ 2}\quad\text{for } x \in [-1,\,1].
    \]
    $f(x)$ is bounded on $[-1,\, 1]$,
    as $|f(x)| \leq 1\ \forall x \in [-1,\,1]$,
    but neither bound is attained (as $f(0) = 0$) and $f(x)$ has no global maximum or minimum on $[-1,\,1]$.
\end{example}

\begin{example}
    $\tan x$ is bounded from below on $\left[0,\,\frac{\pi}{2}\right)$ but not above.
\end{example}

\begin{theorem}[Extreme Value Theorem]\label{calc_thm_extvalthm}
    If $f(x)$ is continuous on a closed interval $[a,\,b]$,
    then it is bounded on that interval,
    and has upper and lower bounds which are attained,
    i.e. $\exists x_1, x_2 \in [a, b]$ such that $f(x_2) \leq f(x) \leq f(x_1)\ \forall x \in [a,\,b]$.
\end{theorem}

\begin{definition}
    $f(x)$ is monotonic increasing in $[a,\,b]$ if
    $f(x_1) \leq f(x_2)\ \forall x_1, x_2$ such that
    $a \leq x_1 < x_2 \leq b$.
\end{definition}

\begin{definition}
    $f(x)$ is strictly monotonic increasing in $[a, b]$
    if $f(x_1) < f(x_2)\, \forall x_1, x_2$ such that
    $a \leq x_1 < x_2 \leq b$.
\end{definition}

Similarly for decreasing.

\begin{example}
    $\mathrm{sgn}(x)$ is monotonic increasing in $[-1,\,1]$
    but not strictly monotonic increasing.
\end{example}

\begin{example}
    $x ^ 2$ is strictly monotonic increasing on $[0,\,b]$ for any $b > 0$.
\end{example}

\subsection{Critical points}

\begin{definition}
    We say that $f(x)$ has a local maximum (or local minimum) at $x = a$ if $\exists h > 0$ such that $f(a) \geq f(x)$ (or $f(a) \leq f(x)$ respectively) $\forall x \in (a - h, a + h)$.
\end{definition}

A local extremum is either a local maximum (max) or a local minimum (min).

If $f(x)$ has a local maximum (or minimum) at $x = a$ and is differentiable at $x = a$,
then $f'(a) = 0$.
\begin{proof}
    Since $f(x)$ is differentiable at $x = a$
    \[
    \lim_{x \rightarrow a ^ {-}}\frac{f(x) - f(a)}{x - a} = 
    \lim_{x \rightarrow a ^ {+}}\frac{f(x) - f(a)}{x - a} =
    f'(a).
    \]
    If $f(x)$ has a local maximum at $x = a$,
    then
    \[
    \frac{f(x) - f(a)}{x - a} \leq 0\quad\text{for}\quad x \in (a, a + h)
    \]
    so $\displaystyle \lim_{x \rightarrow a^{+}}\frac{f(x) - f(a)}{x - a} = f'(a) \leq 0$
    and
    \[
    \frac{f(x) - f(a)}{x - a} \geq 0\quad\text{for}\quad x \in (a - h, a)
    \]
    so $\displaystyle \lim_{x \rightarrow a^{-}}\frac{f(x) - f(a)}{x - a} = f'(a) \geq 0$.
    Since $0 \leq f'(a) \leq 0$,
    $f'(a) = 0$.
\end{proof}

\begin{definition}
    $f(x)$ has a stationary point at $x = a$ if it is differentiable at $x = a$ and $f'(a) = 0$.
\end{definition}

\begin{definition}
    An interior point $x = a$ of the domain of $f(x)$ is a critical point if either $f'(a) = 0$ or $f'(a)$ doesn't exist.
\end{definition}

\begin{example}
    $f(x) = x ^ 3$ has $f'(0) = 0$ so $x = 0$ is a stationary point (and critical point\footnote{As all stationary points are critical.}) but not a local extremum, as $f(x) > f(0)\quad\forall x \in (0, h)$ but $f(x) < f(0)\quad \forall x \in (-h, 0)$.
\end{example}

\begin{definition}
    If $f(x)$ is twice differentiable in an open interval around $x = a$ with $f''(a) = 0$ and if $f''(x)$ changes sign at $x = a$,
    then $x = a$ is a point of inflection.
\end{definition}

\begin{example}
    If $f(x) = x ^ 3,\ f''(x) = 6x$, so $f''(0) = 0$.
    Since $f''(x) < 0$ for $x < 0$ and $f''(x) > 0$ for $x > 0$,
    then $x = 0$ is a point of inflection.
\end{example}

\textbf{The first derivative test}

Suppose $f(x)$ is continuous at a critical point $x = a$.
\begin{enumerate}[label = \roman*)]
    \item If $\exists h > 0$ such that $f'(x) < 0\ \forall x \in (a - h, a)$ and $f'(x) > 0\ \forall x \in (a, a + h)$,
    then $x = a$ is a local minimum.
    \item If $\exists h > 0$ such that $f'(x) > 0\ \forall x \in (a - h, a)$ and $f'(x) < 0\ \forall x \in (a, a + h)$,
    then $x = a$ is a local maximum.
    \item If $\exists h > 0$ such that $f'(x)$ has constant sign $\forall x \neq a$ in $(a - h, a + h)$,
    then $x = a$ is not a local extremum.
\end{enumerate}

\begin{example}
    $f(x) = |x|$ is a continuous function,
    but $f'(x) \neq 0$ so there doesn't exist any stationary point,
    $x = 0$ is a critical point
    (as the $1^{\text{st}}$ derivative doesn't exist here)
    $f'(x) < 0\ \forall x \in (-1, 0)$ and $f'(x) > 0 \ \forall x \in (0, 1)$
    so $x = 0$ is a local minimum.
\end{example}

\begin{example}
    $f(x) = x ^ 4 - 2x ^ 3$ has $f'(x) = 4x ^ 3 - 6x ^ 2 = 2x ^ 2(2x - 3)$
    we have critical points (all stationary) at $x = 0, \frac{3}{2}$.
    Since $f'(x) < 0$ for $x < \frac{3}{2}$ near $\frac{3}{2}$ and
    $f'(x) > 0\ \forall x > \frac{3}{2}$,
    $x = \frac{3}{2}$ is a local minimum.
    Since $f'(x) < 0$ in a small interval around $x = 0$,
    $x = 0$ is not a local extremum.
\end{example}


\textbf{Second derivative test}

Suppose $f(x)$ is twice differentiable at $x = a$,
with $f'(a) = 0$.
\begin{enumerate}[label = \roman*)]
    \item If $f''(a) > 0$ then $x = a$ is a local minimum.
    \item If $f''(a) < 0$ then $x = a$ is a local maximum.
\end{enumerate}
If $f''(x) = 0$ the $2^{\text{nd}}$ derivative test doesn't tell us anything.

\begin{example}
    $f(x) = 2x ^ 3 - 9x ^ 2 + 12x$ has $f'(x) = 6x ^ 2 - 18x + 12 = 6(x ^ 2 - 3x + 2) = 6(x - 1)(x - 2)$.
    So $x = 1$ and $x = 2$ are stationary.
    $f''(x) = 12x - 18 = 6(2x - 3)$
    so $f''(1) < 0$ and $x = 1$ is a local maximum.
    $f''(2) > 0$ and $x = 2$ is a local minimum.
\end{example}

\textbf{Endpoints}
\begin{definition}
    If $c \in \Dom f$ and if $\exists h > 0$ such that $f(x)$ is defined on $[c, c + h)$ but not $(c - h, c]$ (or vice-versa)
    then $x = c$ is an endpoint of $f(x)$.
\end{definition}

\begin{example}
    If $\Dom f = [a, b]$ then $x = a$ and $x = b$ are endpoints.
\end{example}

\begin{definition}
    If $x = c$ is an endpoint of $f(x)$,
    then $f(x)$ has an endpoint maximum (or minimum) at $x = c$
    if $f(x) \leq f(c)$ (or $f(x) \geq f(c)$ respectively)
    for points in the domain close to $c$.
\end{definition}

Note: If $f(x)$ is continuous and differentiable close to an endpoint,
then the sign of the derivative can determine the nature of the endpoint.

If $f(x)$ is continuous on $[a, b]$,
then all global extrema in this interval are attained either at critical points or endpoints.

\begin{example}
    Find the global extrema of $f(x) = x ^ 2 - 2|x| + 2$ for $x \in \left[-\frac{1}{2}, 2\right]$.
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        First, note
        \[
        f(x) = \begin{cases}
            x ^ 2 - 2x + 2 & x \in [0, 2] \\
            x ^ 2 + 2x + 2 & x \in \left[-\frac{1}{2}, 0\right]
        \end{cases}
        \]
        On $(0, 2)$ $f'(x) = 2x - 2$,
        so critical point at $x = 1$, $f(1) = 1$.

        On $\left(-\frac{1}{2}, 0\right)$ $f'(x) = 2x + 2$ so no critical point in this interval.
        \[
        \lim_{x \rightarrow 0^{+}}f'(x) = -2 \neq 2 = \lim_{x \rightarrow 0^{-}}f'(x)
        \]
        so $f'(x)$ does not exist at $x = 0$.
        So $x = 0$ is a critical point with $f(0) = 2$.
        The endpoint values are $f\left(-\frac{1}{2}\right) = \frac{5}{4}$ and $f(2) = 2$.
        So the global minimum is $1$ (at $x = 1$) and the global maximum is $2$ (at $x = 0$ and $x = 2$).
    \end{proof}
\end{example}

\subsection{Rolle's theorem}

\begin{theorem}[Rolle's theorem]\label{calc_thm_rolle}
    If $f(x)$ is continuous on $[a, b]$, differentiable on $(a, b)$ and $f(a) = f(b)$, then $\exists c \in (a, b)$ such that $f'(c) = 0$.
    \begin{proof}
        By \autoref{calc_thm_extvalthm} $\exists x_1, x_2 \in [a, b]$ such that
        \[
        f(x_1) \leq f(x) \leq f(x_2)\ \forall x \in [a, b]
        \]
        If $x_2 \in (a, b)$ then $x_2$ is a local maximum and so $f'(x_2) = 0$.

        If $x_1 \in (a, b)$ then $x_1$ is a local minimum and so $f'(x_1) = 0$.

        Otherwise, $x_1$ and $x_2$ are endpoints $a, b$.
        But since $f(a) = f(b),\quad f(x_1) = f(x_2) = f(a)$.
        So $f(a) \leq f(x) \leq f(a)\ \forall x \in [a, b]$.
        So $f(x)$ is constant on $[a, b]$ and $f'(x) = 0\ \forall x \in (a, b)$.
    \end{proof}
\end{theorem}

\begin{corollary}\label{calc_corl_rolle_1}
    If $f(x)$ is differentiable on an open interval $I$,
    then each pair of zeros of $f$ is separated by at least one zero of $f'(x)$.
    \begin{proof}
        This follows from \autoref{calc_thm_rolle} with $a$ and $b$ zeros of $f(x)$ such that $f(a) = f(b) = 0$.
    \end{proof}
\end{corollary}
We can use this to find a bound on the number of distinct zeros of a function
\begin{example}
    Show $f(x) = \frac{1}{7}x ^ 7 - \frac{1}{5}x ^ 6 + x ^ 3 - 3x ^ 2 - x$ has no more than $3$ distinct real roots.
    \begin{proof}
        We prove this by contradiction.
        Assume there are at least $4$ real roots of $f(x)$.
        Then by \autoref{calc_corl_rolle_1} there are at least $4$ distinct real roots of $f'(x)$.
        \[
        f'(x) = x ^ 6 - \frac{6}{5}x ^ 5 + 3x ^ 2 - 6x - 1.
        \]
        Then applying \autoref{calc_corl_rolle_1} to $f'(x)$,
        there are at least $2$ distinct real roots of $f''(x)$.
        \begin{align*}
            f''(x) &= 6x ^ 5 - 6x ^ 4 + 6x - 6 \\
            &= 6(x ^ 5 - x ^ 4 + x - 1) \\
            &= 6(x - 1)(x ^ 4 + 1).
        \end{align*}
        But $x ^ 4 \geq 0$, so $x ^ 4 \neq -1$ in $\R$.
        So $f''(x)$ has only $1$ real root.
    \end{proof}
\end{example}

\subsection{The mean value theorem}

\begin{theorem}[The mean value theorem]\label{calc_thm_meanval}
    If $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$,
    then $\exists c \in (a, b)$ such that
    \[
    f'(c) = \frac{f(b) - f(a)}{b - a}.
    \]
    \begin{proof}
        Let $g(x) = (b - a)(f(x) - f(a)) - (x - a)(f(b) - f(a))$.
        Then $g(a) = 0 = g(b)$ and $g(x)$ satisfies the requirements of \autoref{calc_thm_rolle}.
        So $\exists c \in (a, b)$ such that $g'(c) = 0$.
        Bur $g'(x) = (b - a)f'(x) - (f(b) - f(a))$ so $g'(c) = (b - a)f'(c) - (f(b) - f(a)) = 0$
        implies $f'(c) = \frac{f(b) - f(a)}{b - a}$.
    \end{proof}
\end{theorem}

We can use \autoref{calc_thm_meanval} to prove results relating monotonicity to the sign of the derivative.
    
\begin{example}
    Suppose $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$ with $f'(x) \geq 0\ \forall x \in (a, b)$, then $f(x)$ is monotonic increasing.
    \begin{proof}
        If $a \leq x_1 < x_2 \leq b$,
        then by \autoref{calc_thm_meanval},
        $\exists c \in (x_1, x_2)$ such that $f'(c) = \frac{f(_x) - f(x_1)}{x_2 - x_1}$.
        Since $f'(x) \geq 0\ \forall x \in (x_1, x_2)$,
        then we must have $f(x_2) \geq f(x_1)$, i.e. $f(x)$ is monotonic increasing.
    \end{proof}
\end{example}

We can show similar results for monotonic decreasing,
and strictly increasing/decreasing.

\subsection{The inverse function rule}
The inverse function rule: if $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$ with $f'(x) > 0\ \forall  x \in (a, b)$,
then its inverse exists $g(y)$
(such that $g(f(x)) = x$)
is differentiable $\forall y$ such that
\[
f(a) < y < f(b)
\]
with
\[
g'(y) = \frac{1}{f'(g(y))}.
\]
Similarly if $f'(x) < 0$ then $g(y)$ is differentiable then true $\forall y$ such that
\[
f(b) < y < f(a).
\]
Assuming $g(y)$ is differentiable, this follows from the chain rule:
\[
\frac{d}{dx}g(f(x)) = g'(f(x))f'(x) = 1
\]
and so with $y = f(x)\quad g'(y) = \frac{1}{f'(g(y))}$.

\begin{example}
    $f(x) = \sin x$ has $f'(x) = \cos x$ so $f'(x) > 0\ \forall x \in \left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$.
    The inverse $g(y) = \sin ^ {-1}(y)$ is therefore differentiable $(-1, 1)$ with
    \begin{align*}
        g'(y) &= \frac{1}{f'(g(y))} \\
        &= \frac{1}{\cos(g(y))} \\
        &= \frac{1}{\sqrt{1 - \sin ^ 2 (g(y))}}
    \end{align*}
    But $\sin(g(y)) = f(g(y)) = y$ so
    $g'(y) = \frac{1}{\sqrt{1 - y ^ 2}}$
    So $\frac{d}{dx}\sin ^ {-1}(x) = \frac{1}{\sqrt{1 - x ^ 2}}$.
\end{example}

\subsection{Partial derivatives}
We can also consider functions of more than one variables
where $f(x, y)$ can be thought of as the height of a function over the $xy$-plane.
We can now think of the rate of change of $f(x, y)$ as we vary $x$ or $y$,
keeping the other fixed.

\begin{definition}
    The partial derivative of $f(x, y)$ with respect to $x$ is the following
    \[
    \frac{\partial f}{\partial x}(x, y) = \lim_{h \rightarrow 0}\frac{f(x + h, y) - f(x, y)}{h}
    \]
    assuming this limit exists.

    Similarly
    \[
    \frac{\partial f}{\partial y}(x, y) = \lim_{h \rightarrow 0}\frac{f(x, y + h) - f(x, y)}{h}
    \]
    is the partial derivative of $f$ with respect to $y$.
\end{definition}

In practice we can often simply differentiate with respect to one variable while treating the other as constant.

\begin{example}
    Let $f(x, y) = x ^ 2 y ^ 3 - \sin(x)\cos(y) - y$.
    Then
    \[
    \frac{\partial f}{\partial x} = 2xy ^ 3 - \cos(x)\cos(y).
    \]
    \[
    \frac{\partial f}{\partial y} = 3x ^ 2y ^ 2 + \sin(x)\sin(y) - 1.
    \]
\end{example}

We can also use the notation $f_x = \frac{\partial f}{\partial x}$ and $f_y = \frac{\partial f}{\partial y}$.

We can take partial derivatives of partial derivatives,
to get second partial derivatives, such as
\[
\frac{\partial ^ 2 f}{\partial x ^ 2} = \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}\right),
\quad
\frac{\partial ^ 2 f}{\partial y ^ 2} = \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}\right),
\quad
\frac{\partial ^ 2 f}{\partial x \partial y} = \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}\right),
\quad
\frac{\partial ^ 2 f}{\partial y \partial x} = \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right).
\]
Or $f_{xx} = \frac{\partial ^ 2 f}{\partial x ^ 2}$ etc.
$f_{xy}$ is used as $(f_x)_y) = \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right) = \frac{\partial ^ 2 f}{\partial y \partial x}$.

Some people use this to mean $\frac{\partial ^ 2 f}{\partial x \partial y}$.

\begin{example}
    Let $f(x, y) = x ^ 2 y ^ 3 - \sin(x)\cos(y) - y$.
    We have 
    \[
    \frac{\partial ^ 2 f}{\partial x ^ 2} = 2y ^ 3 + \sin(x)\cos(y).
    \]
    \[
    \frac{\partial ^ 2 f}{\partial y ^ 2} = 6x ^ 2 y + \sin(x)\cos(y).
    \]
    \[
    \frac{\partial ^ 2 f}{\partial x \partial y} = 6xy ^ 2 + \cos(x)\sin(y).
    \]
    \[
    \frac{\partial ^ 2 f}{\partial y \partial x} = 6xy ^ 2 + \cos(x)\sin(x).
    \]
\end{example}

Note: we have $\frac{\partial ^ 2 f}{\partial x \partial y} = \frac{\partial ^ 2 f}{\partial y \partial x}$.
This is true in general if $f$ and all first and second order partial derivatives are continuous (Clairaut's theorem).

\newpage

\section{Integration}

\subsection{Indefinite and Definite integrals}
\begin{definition}
    A function $F(x)$ is called an indefinite integral (or antiderivative) of a function $f(x)$ in the interval $(a, b)$ if $F(x)$ is differentiable
    $F'(x) = f(x) \forall x \in (a, b)$.
    We write
    \[
    F(x) = \int f(x)\,dx
    \]
\end{definition}

Note
\begin{itemize}
    \item If $F(x)$ is an indefinite integral,
    then so is $F(x) + c$,
    for any constant $c$,
    $\frac{d}{dx}(F(x) + c) = \frac{d}{dx}(F(x))$.
    It is important to include this arbitrary constant when writing indefinite integrals.
    \item If $F_1(x)$ and $F_2(x)$ are both indefinite integrals of $f(x)$,
    then $F_1(x) - F_2(x) = c$.
    \begin{proof}
        $\frac{d}{dx}(F_1(x) - F_2(x)) = f(x) - f(x) = 0$.
        Therefore $F_1(x) - F_2(x)$ is constant.
    \end{proof}
\end{itemize}

\begin{example}\phantom{}
    \begin{itemize}
        \item $\int\cos(x)\,dx = \sin(x) + c$ in $\R$.
        \item $\int\frac{1}{x ^ 2} = -\frac{1}{x} + c$ in $\R \setminus\{0\}$.
        \item $\int\mathrm{sgn}(x)\,dx = |x| + c$ in $\R \setminus \{0\}$.
    \end{itemize}
\end{example}

\begin{definition}
    We say $f(x)$ is integrable in $(a, b)$ if it has an indefinite integral on $(a, b)$ which is continuous on $[a, b]$.
\end{definition}

\begin{example}\phantom{}
    \begin{itemize}
        \item $\cos(x)$ is integrable in any finite interval $(a, b)$ as $\sin(x)$ is continuous on $\R$.
        \item $\frac{1}{x ^ 2}$ is not integrable on $(0, 1)$ because the indefinite integral $-\frac{1}{x}$ is not continuous at $x = 0$.
        \item $\mathrm{sgn}(x)$ is integrable on $(0, 1)$ because its indefinite integral $|x|$ is continuous on $[0, 1]$.
    \end{itemize}
\end{example}

Definite integrals

A definite integral of a function $f(x)$ gives the (signed) area under the graph of $f(x)$.

We define this as the limit of a Riemann sum,
by splitting the area into rectangles.
There are many ways we could do this.
If $f(x)$ is continuous and the rectangles has small width,
we expect this to be a good approximation and the error to be reduced to zero by taking the limit of the width of the rectangles to zero.

We say this limit exists if it is independent of how we construct the rectangles.

\begin{definition}
    A subdivision of $[a, b]$ is a partition of the interval into a finite number of sub-intervals (i.e. breaking the interval up)
    \[
    [x_0, x_1], [x_1, x_2], \dotsc, [x_{n - 1}, x_n]
    \]
    with $a = x_0 < x_1 < x_2 < \dotsi < x_n - b$
\end{definition}
The norm of the subdivision $|S|$ is the maximum of the subinterval lengths.

The numbers $z_1, z_2, \dotsc z_n$ for a set of sample points from $S$ if $z_j \in [x_{j - 1}, x_j]$
for $j = 1, \dotsc, n$.
\begin{definition}
    Let $f(x)$ be a function defined on $[a, b]$.
    The Riemann sum $\mathcal{R}$ (which really depends on a subdivision and set of sample points,
    $\mathcal{R}(S, \{z_i\}_{i = 1}^{n})$ is
    \[
    \mathcal{R} = \sum_{j = 1}^{n}(x_j - x_{j - 1})f(z_j).
    \]
\end{definition}

The definite integral is then given in terms of a Riemann sum by taking a limit as $|S|$ goes to zero.
We say this limit exists if it is independent of subdivision and set of sample points.
\begin{definition}
    Let $f(x)$ be defined $\forall x \in [a, b]$.
    The definite integral of $f(x)$ from $a$ to $b$ is then
    \[
    \int_{a}^{b}f(x)\,dx = \lim_{|S| \rightarrow 0}\mathcal{R}.
    \]
\end{definition}
It can be shown that this limit exists if $f(x)$ is continuous on the interval $[a, b]$.

The definite integral satisfies the following properties.
Let $f(x)$ and $g(x)$ be integrable in $(a, b)$.
\begin{enumerate}[label = \roman*)]
    \item Linearity. If $\lambda, \mu \in \R$,
    then $(\lambda f + \mu g)(x)$ is integrable in $(a, b)$, with 
    \[
    \int_a^b(\lambda f + \mu g)(x)\,dx = \lambda \int_a^bf(x)\,dx + \mu\int_a^bg(x)\,dx.
    \]
    \item If $c \in [a, b]$, then
    \[
    \int_a^bf(x)\,dx = \int_a^cf(x)\,dx + \int_c^bf(x)\,dx.
    \]
    \item If $f(x) \geq g(x)\ \forall x \in (a, b)$,
    then
    \[
    \int_a^bf(x)\,dx \geq \int_a^bg(x)\,dx.
    \]
    \item If $m \geq f(x) \leq M\ \forall x \in [a, b]$ then,
    \[
    m(b - a) \leq \int_a^bf(x)\,dx \leq M(b - a).
    \]
\end{enumerate}

\subsection{The fundamental theorem of calculus}
The following theorem connects definite and indefinite integrals.
\begin{theorem}[The fundamental theorem of calculus (FTOC)]
    If $f(x)$ is continuous on $[a, b]$,
    then the function
    \[
    F(x) = \int_a^xf(t)\,dt
    \]
    defined for $x \in [a, b]$ it is continuous on $[a, b]$,
    differentiable on $(a, b)$ and is an indefinite integral of $f(x)$ on $(a, b)$,
    i.e. $F'(x) = \frac{d}{dx}\int_a^xf(t)\,dt = f(x)\ \forall x \in (a, b)$.

    Furthermore if $\Tilde{F}(x)$ is any indefinite integral of $f(x)$ on $(a, b)$,
    then
    \[
    \int_a^bf(t)\,dt = \Tilde{F}(b) - \Tilde{F}(a) = \left[\Tilde{F}(x)\right]^b_a
    \]
    \begin{proof}[Proof Sketch]\renewcommand{\qedsymbol}{$\triangle$}
        For $a \leq x < x + h < b$,
        we have
        \begin{align*}
            F(x + h) - F(x) &= \int_a^{x + h}f(t)\,dt - \int_a^xf(t)\,dt \\
            &= \int_x^af(t)\,dt + \int_a^{x + h}f(t)\,dt \\
            &= \int_x^{x + h}f(t)\,dt&\text{using property (ii)}.
        \end{align*}
        Let $m(h)$ and $M(h)$ denote the minimum and maximum values of $f(x)$ on $[x, x + h]$.
        Then by (iv)
        \[
        h \cdot m(h) \leq \int_x^{x + h}f(t)\,dt \leq h \cdot M(h)
        \]
        and so $m(h) \leq \frac{F(x + h) - F(x)}{h} \leq M(h)$.
        Since $f(x)$ is continuous on $[x, x + h]$ we have $\displaystyle\lim_{h \rightarrow 0 ^ +}m(h) = f(x) = \lim_{h \rightarrow 0 ^ +}M(h)$
        and so
        \[
        \lim_{h \rightarrow 0 ^ +}\frac{F(x + h) - F(x)}{h} = f(x)
        \]
        by \autoref{calc_thm_squeze}.

        We can give a similar argument for $\lim_{h \rightarrow 0 ^ -}$,
        and putting these together
        \[
        \lim_{h \rightarrow 0}\frac{F(x + h) - F(x)}{h} = f(x).
        \]
        Showing $F(x)$ is an indefinite integral of $f(x)$.

        For the final part,
        if $\Tilde{F}(x)$ is an indefinite integral of $f(x)$ in $(a, b)$,
        then $\Tilde{F}(x) = F(x) + c$ for some $c \in \R$,
        and so
        \begin{align*}
        \left[\Tilde{F}(x)\right]_a^b &= \Tilde{F}(b) - \Tilde{F}(a) = (F(b) + c) - (F(a) + c) \\
        &= F(b) - F(a) \\
        &= \int_a^bf(b)\,dt - \int_a^af(t)\,dt \\
        &= \int_a^bf(b)\,dt.
        \end{align*}
    \end{proof}
\end{theorem}

The fundamental theorem of calculus also gives a simple rule for differentiating a definite integral with respect to its limits.
\begin{example}
    \[
    \frac{d}{dx}\int_0^x\frac{1}{1 + \sin ^ 2 (t)}\,dt = \frac{1}{1 + \sin ^ 2 (x)}.
    \]
\end{example}
This can be combined with the chain rule for more complicated expressions.
\begin{example}
    \[
    \frac{d}{dx}\int_0^{x ^ 2}\frac{1}{1 + e ^ t}\,dt
    \]
    let $u = x ^ 2$
    \begin{align*}
    \frac{d}{dx}\int_0^u\frac{1}{1 + e ^ t}\,dt &= \left(\frac{d}{du}\int_0^u\frac{1}{1 + e ^ t}\,dt\right)\left(\frac{du}{dx}\right) \\
    &= \frac{1}{1 + e ^ u} \cdot 2x \\
    &= \frac{2x}{1 + e ^ {x ^ 2}}
    \end{align*}
\end{example}

\subsection{Limits with logarithms powers and exponentials}
\begin{lemma}\label{calc_lem_lemma1}
    $\forall x \geq 0\quad e ^ x \geq 1 + x$
    \begin{proof}
        Consider $f(x) = e ^ x - (1 + x)$.
        We have $f(0) = 0$ and $f'(x) = e ^ x - 1 \geq 0\ \forall x \geq 0$,
        so $f(x)$ is monotonic increasing on $[0, \infty]$.
        So $f(x) \geq 0$ on $[0, \infty)$.
    \end{proof}
\end{lemma}
\begin{lemma}\label{calc_lem_lemma2}
    $\forall x \geq 0$ and positive integer $n$
    \[
    e ^ x \geq \sum_{j = 0}^{n}\frac{x ^ j}{j!}.
    \]
    \begin{proof}
        Note that when $n = 1$,
        this is \autoref{calc_lem_lemma1}.
        Now let $f_n(x) = e ^ x - \sum_{j = 0}^n\frac{x ^ j}{j!} = e ^ x - \left(1 + x + \frac{x ^ 2}{2} + \dotsc + \frac{x ^ n}{n!}\right)$.
        Note $f_n(0) = 0\ \forall n \in \N$
        \[
        f_n'(x) = e ^ x - \left(1 + x + \frac{x ^ 2}{2} + \dotsc + \frac{x ^ {n - 1}}{(n - 1)!}\right) = e ^ x - \sum_{j = 0}^{n = 1}\frac{x ^ j}{j!} = f_{n - 1}(x).
        \]
        So since $f_1(x) \geq 0$ by \autoref{calc_lem_lemma1},
        $f_2'(x) = f_{n - 1}(x) \geq 0$,
        so
        $f_3(x) \geq 0\ \forall x \geq 0$.
        Then $f_3'(x) \geq 0$ and so $f_3(x) \geq 0$ and so on (induction).
    \end{proof}
\end{lemma}

Result 1: Powers beat logs

For any $a > 0$, $\displaystyle\lim_{x \rightarrow \infty}\frac{\log x}{x ^ a} = 0$
\footnote{$\log x$ generally means $\ln x$.}
\begin{proof}
    Let $x = e ^ y$ then $\displaystyle\lim_{x \rightarrow \infty}\frac{\log(x)}{x ^ a} = \lim_{y \rightarrow \infty}\frac{y}{e ^ {ay}}$
    For $y > 0$,
    then using \autoref{calc_lem_lemma2} with $n = 2$
    \[
    0 \leq \frac{y}{e ^ {ay}} \leq \frac{y}{1 + ay + \frac{(ay) ^ 2}{2}} \leq \frac{y}{\frac{1}{2}a ^ 2 y ^ 2} = \frac{2}{a ^ 2 y}
    \]
    since $\lim_{y \rightarrow \infty}\frac{2}{a ^ 2y} = 0$ then by \autoref{calc_thm_squeze}
    \[
    \lim_{y \rightarrow \infty}\frac{y}{e ^ {ay}} = 0 = \lim_{x \rightarrow \infty}\frac{\log(x)}{x ^ a}.
    \]
\end{proof}

Result 2: Exponentials beat powers

For any $a > 0$
\[
\lim_{x \rightarrow \infty}\frac{x ^ a}{e ^ x} = 0
\]
\begin{proof}
    Let $n$ be the smallest integer such that $n > a$.
    By \autoref{calc_lem_lemma2} (for $x > 0$)
    \[
    0 \leq \frac{x ^ a}{e ^ x} \leq \frac{x ^ a}{1 + x + \frac{x ^ 2}{2} + \dotsc + \frac{x ^ n}{n!}} = \frac{x ^ {a - n}}{x ^ {-n} + x ^ {1 - n} + \dotsc + \frac{1}{n!}}
    \]
    As $a - n < 0$,
    $\displaystyle\lim_{x \rightarrow \infty}\frac{x ^ {a - n}}{x ^ {-n} + \dotsc + \frac{1}{n!}} = 0$
    and so by \autoref{calc_thm_squeze}
    \[
    \lim_{x \rightarrow \infty}\frac{x ^ a}{e ^ x} = 0.
    \]
\end{proof}

Result 3: Exponential as a limit

For any $a \in \R$ $\displaystyle\lim_{x \rightarrow \infty}\left(1 + \frac{a}{x}\right) ^ x = e ^ a$
\begin{proof}
    Let $f(x) = \log(x)$ then $f'(x) = \frac{1}{x}$ and
    \[
    1 = f'(1) = \lim_{h \rightarrow 0}\frac{\log(1 + h) - \log(1)}{h} = \lim_{h \rightarrow 0}\frac{\log(1 + h)}{h}.
    \]
    Now letting $h = \frac{a}{x}$
    \[
    1 = \lim_{x \rightarrow \infty}\frac{\log\left(1 + \frac{a}{x}\right)}{\frac{a}{x}}.
    \]
    So
    \[
    a = \lim_{x \rightarrow \infty}x\log\left(1 + \frac{a}{x}\right) = \lim_{x \rightarrow \infty}\left(1 + \frac{a}{x}\right) ^ x.
    \]
    Now exponentiating
    \[
    e ^ a = e ^ {\lim_{x \rightarrow \infty}\log\left(1 + \frac{a}{x}\right) ^ x} = \lim_{x \rightarrow \infty}\left(1 + \frac{a}{x}\right) ^ x
    \]
    using that $e ^ x$ is continuous.
\end{proof}

\subsection{Integration using a recurrence relation}
One common way we find integrals related by recurrence relations is when integrating by parts.
\[
(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)\text{ by Leibniz rule}.
\]
So integrating over $[a, b]$ and using the fundamental theorem of calculus
\[
\left[f(x)g(x)\right]_a^b = \int_a^bf'(x)g(x)\,dx + \int_a^bf(x)g'(x)\,dx,
\]
which is usually written as
\[
\int_a^bf'(x)g(x)\,dx = \left[f(x)g(x)\right]_a^b - \int_a^bf(x)g'(x)\,dx.
\]
\begin{example}
    Calculate
    \[
    \int_0^1x ^ 3 e ^ x\,dx
    \]
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        Let $I_n = \int_0^1x ^ n e ^ x\,dx$ for all integers $n \geq 0$ and consider
        \begin{align*}
        I_{n + 1} &= \int_0^1 x ^ {n + 1}e ^ x\,dx \\
        &= \left[x ^ {n + 1} e ^ x\right]_0^1 - \int_0^1(n + 1)x ^ n e ^ x\,dx &\text{Integrating by parts ($f'(x) = e ^ x,\ g(x) = x ^ {n + 1}$)} \\
        &= e - (n + 1)I_n.
        \end{align*}
        Since we can easily calculate $I_0 = \int_0^1e ^ x\,dx = \left[e ^ x\right]_0^1 = (e - 1)$.
        The recurrence relation $I_{n + 1} = e - (n + 1)I_n$ can be used to calculate $I_n$ for any $n > 0$.
        \begin{align*}
        I_1 &= e - I_0 = e - (e - 1) = 1 \\
        I_2 &= e - 2I_1 = e - 2 \\
        I_3 &= e - 3I_2 = e - 3(e - 2) = 6 - 2e
        \end{align*}
        
    \end{proof}
\end{example}

Note that one can also obtain recurrence relations without integrating by parts (e.g. using trig identities).
\begin{example}
    Calculate
    \[
    \int\tan ^ 4 x\,dx.
    \]
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        \[
        I_n = \int\tan ^ n x\,dx\text{ for } n \geq 0\, (n \neq 1)
        \]
        we want to compute $I_4$.
        \begin{align*}
            I_n &= \int\tan ^ {n - 2}x(\sec ^ 2 x - 1)\,dx \\
            &= \int\tan ^ {n - 2} x\sec ^ 2 x\,dx - \int\tan ^ {n - 2}\,dx
        \end{align*}
        Then letting $u = \tan x\quad du = \sec ^ 2 x\,dx$
        \begin{align*}
            I_n &= \int u ^ {n - 2}\,du - I_{n - 2} \\
            &= \frac{u ^ {n - 1}}{n - 1} - I_{n - 2} \\
            &= \frac{\tan ^ {n - 1} x}{n - 1} - I_{n - 2}
        \end{align*}
        gives a recurrence relation.
        We have $I_0 = \int 1\,dx = x + c$,
        so $I_2 = \tan x - (x + c)$, $I_4 = \frac{1}{3}\tan ^ 3 x - \tan x + x + a$.
    \end{proof}
\end{example}

\subsection{Definite integrals using odd and even functions}
If $f_{\text{odd}}(x)$ is an integrable odd function on $[-a, a]$ then
\[
\int_{-a}^af_{\text{odd}}(x)\,dx = 0.
\]
\begin{proof}
    \begin{align*}
        \int_{-a}^af_{\text{odd}}(x)\,dx &= \int_{-a}^0f_{\text{odd}}(x)\,dx + \int_0^af_{\text{odd}}(x)\,dx \\
        &= -\int_a^0f_{\text{odd}}(-x)\,dx + \int_0^af_{\text{odd}}(x)\,dx \\
        &= \int_0^af_{\text{odd}}(x)\,dx + \int_0^af_{\text{odd}}(x)\,dx \\
        &= \int_0^a(f_{\text{odd}}(-x) + f_{\text{odd}}(x))\,dx \\
        &= 0
    \end{align*}
\end{proof}
Similarly if $f_{\text{even}}(x)$ is an integrable even function on $[-a, a]$ then
\[
\int_{-a}^af_{\text{even}}(x)\,dx = 2\int_0^af_{\text{even}}(x)\,dx.
\]
Recall that any function can be decomposed into the sum of an odd and even function,
$f(x) = f_{\text{even}}(x) + f_{\text{odd}}(x)$ then
\[
\int_{-a}^{a}f(x)\,dx = 2\int_0^af_{\text{even}}(x)\,dx
\]
\begin{example}
    Calculate
    \[
    \int_{-1}^{1}\frac{e ^ x x ^ 4}{\cosh x}
    \]
    with
    \[
    f(x) = \frac{e ^ x x ^ 4}{\cosh x}\,dx,
    \]
    recalling $\cosh(x) = \frac{e ^ x + e ^ {-x}}{2}$
    \begin{align*}
        f_{\text{even}}(x) &= \frac{1}{2}(f(x) + f(-x)) \\
        &= \frac{1}{2}\left(\frac{e ^ x x ^ 4}{\cosh(x)} + \frac{e ^ {-x} x ^ 4}{\cosh(x)}\right) \\
        &= \frac{x ^ 4}{\cosh(x)}\left(\frac{e ^ x + e ^ {-x}}{2}\right) \\
        &= x ^ 4.
    \end{align*}
    So $\int_{-1}^{1}f(x)\,dx = 2\int_0^1x ^ 4\,dx = \frac{2}{5}\left[x ^ 5\right]_0^1 = \frac{2}{5}$.
\end{example}

\begin{example}
    Calculate
    \[
    \int_{-\frac{\pi}{4}}^{\frac{\pi}{4}}\frac{(1 + 2x) ^ 3\cos(x)}{1 +  12x ^ 2}\,dx
    \]
    we have
    \begin{align*}
        f_{\text{even}}(x) &= \frac{1}{2}\left(f(x) + f(-x)\right) \\
        &= \frac{\cos(x)}{2(1 + 12x ^ 2)}((1 + 2x) ^ 3 + (1 - 2x) ^ 3) \\
        &= \frac{\cos(x)}{2(1 + 12x ^ 2)}(2 + 24x ^ 2) \\
        &= \cos(x)
    \end{align*}
    so $\displaystyle\int_{-\frac{\pi}{4}}^{\frac{\pi}{4}}f(x)\,dx = 2\int_0^{\frac{\pi}{4}}\cos(x)\,dx = 2\left[\sin(x)\right]_0^{\frac{\pi}{4}} = \frac{2}{\sqrt{2}} = \sqrt{2}$.
\end{example}

\newpage

\section{Double integrals}

\subsection{Rectangular regions}
Given a function $f(x, y)$ and a region $D$ in the $xy$-plane the double integral $\iint\limits_Df(x, y)\,dxdy$ is the signed volume of the region between the surface $Z = f(x, y)$ and the region $D$ in the $z = 0$ plane.

We define the double (definite) integral like the single definite integral,
as the limit of a Riemann sum.

For now,
let $D$ be rectangular,
$D = [a_0, a_1] \times [b_0, b_1]$
and construct a subdivision $S$ by specifying the points of a $2$D lattice,
with
\[
a_0 = x_0 < x_1 < x_2 < \dotsc < x_n = a_1\qquad b_0 = y_0 < y_1 < \dotsc < y_m = b_1.
\]
The edge lengths of these rectangles in the $xy$-plane are $dx_i = x_i - x_{i - 1},\ i = 1, \dotsc, n$ and $dy_j = y_j - y_{j - 1},\ j = 1, \dotsc, m$.

The norm $|S|$ is the maximum of the $dx_i$ and $dy_j$ we can take sample points $(p_i, q_j) \in [x_{i - 1}, x_i] \times [y_{j - 1}, y_j]$.

The area of this rectangle is $dx_idy_j$ and we can associate to each rectangle a cuboid of height $f(p_i, q_j)$.
The volume of all the cuboids is the Riemann sum
\[
R = \sum_{i = 1}^n\sum_{j = 1}^mf(p_i, q_j)dx_idy_j
\]
and the double integral is defined as the limit
\[
\iint\limits_D f(x, y)\,dxdy = \lim_{|S| \rightarrow 0}R
\]
Note that in the special case $f(x, y)$ is constant,
$f(x, y) = c$ then
\[
\iint\limits_Df(x, y)\,dxdy = C \cdot \mathrm{Area}(D)
\]

For a rectangular region $D = [a_0, a_1] \times [b_0, b_1]$,
a practical way to compute the double integral is as an iterated integral
(Fubinís theorem, valid if $f$ is continuous on $D$),
\[
\iint\limits_{D}f(x, y)\,dxdy = \int_{a_0}^{a_1}\left(\int_{b_0}^{b_1}f(x, y)\,dy\right)\,dx = \int_{b_0}^{b_1}\left(\int_{a_0}^{a_1}f(x, y)\,dx\right)\,dy.
\]
\begin{example}
    Let $D = [-2, 1] \times [0, 1]$.
    Calculate
    \[
    \iint\limits_D(x ^ 2 + y ^ 2)\,dxdy.
    \]
    \begin{align*}
        \iint\limits_D(x ^ 2 + y ^ 2)\,dxdy &= \int_{-2}^{1}\left(\int_0^1(x ^ 2 + y ^ 2)\,dy\right)\,dx \\
        &= \int_{-2}^{1}\left[yx ^ 2 + \frac{1}{3}y ^ 3\right]^1_0\,dx \\
        &= \int_{-2}^{1}\left(x ^ 2 + \frac{1}{3}\right)\,dx \\
        &= \left[\frac{1}{3}x ^ 3 + \frac{1}{3}x\right]_{-2}^1 \\
        &= \left(\frac{1}{3} + \frac{1}{3}\right) - \left(-\frac{8}{3} - \frac{2}{3}\right) \\
        &= 4.
    \end{align*}
\end{example}

\subsection{Beyond Rectangular Regions}
\begin{definition}
    A region $D$ is $y$-simple if every line parallel to the $y$-axis which intersects $D$ does so in a single line segment or point.
\end{definition}
A line must be able to be drawn such that a vertical line can go through it and not go back through it again.
The lower boundary and upper boundaries are given by the functions $y = \phi_1(x)$ and $y = \phi_2(x)$ respectively.

The region $D_1$ is $y$-simple and has upper and lower boundaries $y = \phi_1(x)$ and $y = \phi_2(x)$ respectively.
The region $D_0$ is not $y$-simple.
The double integral of $f(x, y)$ over $D_1$ can be calculated as an iterated integral,
integrating over $y$ first.
\[
\iint\limits_{D_1}f(x, y)\,dxdy = \int_{a_0}^{a_1}\left(\int_{y = \phi_1(x)}^{y = \phi_2(x)}f(x, y)\,dy\right)\,dx
\]
\begin{example}
    Integrate $f(x, y) = 6xy$ over the region $D$ between the curve $y = x$ and $y = x ^ 2$ for $0 \leq x \leq 1$.
    Since the region is $y$-simple
    \begin{align*}
        \iint\limits_{D}6xy\,dxdy &= \int_0^1\left(\int_{y = x ^ 2}^{y = x}6xy\,dy\right)\,dx \\
        &=\int_0^1\left[3xy^2\right]_{y = x ^ 2}^{y = x}\,dx \\
        &= \int_0^1\left(3x ^ 3 - 3x ^ 5\right)\,dx \\
        &= \left[\frac{3}{4}x ^ 4 - \frac{1}{2}x ^ 6\right]_0^1 \\
        &= \frac{3}{4} - \frac{1}{2} \\
        &= \frac{1}{4}.
    \end{align*}
\end{example}

We have a similar definition for $x$-simple.
\begin{definition}
    A region $D$ is $x$-simple if every line parallel to the $x$-axis which intersects $D$ does so in a single line segment or point.
\end{definition}

The region $D_2$ is $x$-simple
(but not $y$-simple)
and has upper and lower boundaries $x = \psi_1(y)$ and $x = \psi_2(y)$ respectively.
The double integral of $f(x, y)$ over this region can be written as an iterated integral by iterating over $x$ first
\[
\iint\limits_Df(x, y)\,dxdy = \int_{b_0}^{b_1}\left(\int_{\psi_1(y)}^{\psi_2(y)}f(x, y)\,dx\right)\,dy.
\]
If a region is both $x$-simple and $y$-simple,
then the double integral can be calculated by integrating over $x$ or $y$ first.

\begin{example}
    With $D$ the region between the curves $y = x$ and $y = x ^ 2$ for $0 \leq x \leq 1$ as before,
    use the fact that $D$ is $x$-simple to calculate
    \[
    \iint\limits_{D}6xy\,dxdy.
    \]
    The boundaries are given as functions of $y$ as $x = y$ and $x = \sqrt{y}$ for $0 \leq y \leq 1$.
    \begin{align*}
        \iint\limits_{D}6xy\,dxdy. &= \int_0^1\left(\int_{x = y}^{x = \sqrt{y}}6xy\,dx\right)\,dy \\
        &= \int_0^1\left[3x ^ 2y\right]_{y}^{\sqrt{y}}\,dy \\
        &= \int_0^1\left(3y ^ 2 - 3y ^ 3\right)\,dy \\
        &= \left[y ^ 3 - \frac{3}{4}y ^ 4\right]_0^1 \\
        &= 1 - \frac{3}{4} \\
        &= \frac{1}{4}.
    \end{align*}
\end{example}

Sometimes a region may be both $x$-simple and $y$-simple,
but we can only do the integral explicitly in one of the two orders.

\begin{example}
    Let $D$ be the region between $y = x$ and $y = \sqrt{x}$ for $0 \leq x \leq 1$.
    Calculate
    \[
    \iint\limits_D\frac{e ^ y}{y}\,dxdy.
    \]
    Using that $D$ is $y$-simple
    \[
    \iint\limits_D\frac{e ^ y}{y}\,dxdy = \int_0^1\left(\int_{x}^{\sqrt{x}}\frac{e ^ y}{y}\,dy\right)\,dx
    \]
    But we don't know how to integrate $\frac{e ^ y}{y}$ with respect to $y$.
    However,
    using that $D$ is $x$-simple,
    \begin{align*}
        \iint\limits_D\frac{e ^ y}{y}\,dxdy &= \int_0^1\left(\int_{y ^ 2}^{y}\frac{e ^ y}{y}\,dx\right)\,dy \\
        &= \int_0^1\left[\frac{e ^ y}{y}x\right]_{y ^ 2}^{y}\,dy \\
        &= \int_0^1\left(e ^ y - ye ^ y\right) \\
        &= \left[e ^ y\right]_0^1 - \left[ye ^ y\right]_0^1 + \int_0^1e ^ y\,dy \\
        &= (e - 1) - e + (e - 1) \\
        &= e - 2.
    \end{align*}
\end{example}

For a non $x$-simple and $y$-simple region $D$,
split it into multiple regions e.g. $D_1$ and $D_2$ which are either $x$-simple or $y$-simple or both,
then sum the double integrals of both regions.

\subsection{Integration using polar coordinates}
Sometimes,
such as when the region of integration is rotationally symmetric it may be useful to describe the region in terms of polar coordinates $r$ and $\theta$,
where
$x = r\cos\theta$, $y = r\sin\theta$ ($r > 0\ 0 \leq \theta < 2\pi$).
In order to convert $\iint\limits_Df(x, y)\,dxdy$ into polar coordinates,
we need to know what to do with the area element $dxdy = dA$.
This entered our definition of the integral when we divided $D$ into small rectangles with sides $dx$ and $dy$,
and area $dxdy = dA$.

The area of the small region formed by taking the point with polar coordinates $(r, \theta)$,
and extending $r$ by $dr$ and $\theta$ by $d\theta$ is $dA \approx rdrd\theta$.
This approximation improves as the area decreases.
In the infinitesimal limit which defines the integral,
we have $dA = rdrd\theta$.

If the region is either $r$-simple or $\theta$-simple,
we can then compute the double integral as an iterated integral.

\begin{example}
    Let $D$ be the region between the curves $x ^ 2 + y ^ 2 = 1$,
    and $x ^ 2 + y ^ 2 = 4$ with $x \geq 0, y \geq 0$.
    Calculate $\iint\limits_Dxy\,dxdy$.

    In polar coordinates,
    $D$ is the region $1 \leq r \leq 2$,
    $0 \leq \theta \leq \frac{\pi}{2}$.

    The area element is $dxdy = rdrd\theta$.
    The integrand $xy = r ^ 2 \cos(\theta)\sin(\theta)$.
    \begin{align*}
        \iint\limits_Dxy\,dxdy &= \iint\limits_Dr ^ 2 \cos(\theta)\sin(\theta)r\,drd\theta \\
        &= \int_1^2\left(\int_0^{\frac{\pi}{2}}r ^ 3 \cos(\theta)\sin(\theta)\,d\theta\right)\,dr \\
        &= \int_1^2r ^ 3 \left(\int_0^{\frac{\pi}{2}}\frac{1}{2}\sin(2\theta)\,d\theta\right)\,dr \\
        &= \int_1^2\frac{r ^ 3}{2}\left[-\frac{1}{2}\cos(2\theta)\right]_0^{\frac{\pi}{2}}\,dr \\
        &= \int_1^2\frac{r ^ 3}{2}\left(\frac{1}{2} + \frac{1}{2}\right)\,dr \\
        &= \left[\frac{r ^ 4}{8}\right]_1^2 \\
        &= 2 - \frac{1}{8} \\
        &= \frac{15}{8}
    \end{align*}
\end{example}


\subsection{Change of variables and the Jacobian}
As well as changing to polar variables,
$r, \theta$,
we might want to change variables to new variables $u$ and $v$,
given in terms of relations $x = g(u, v), y = h(u, v)$.
We need to consider the integration region $D$ in terms of our new variables,
as well as the area element $dA = dxdy$.

\begin{definition}
    The Jacobian of the transformation from $x, y$ to $u, v$ is
    \[
    J = \frac{\partial(x, y)}{\partial(u, v)} = \begin{vmatrix}
        \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
        \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
    \end{vmatrix}
    \]
\end{definition}
i.e. the determinant of the matrix of partial derivatives.
The area element is then given in terms of the Jacobian as
\[
dA = dxdy = |J|dudv,
\]
where $|J|$ is the absolute value of the Jacobian.

\begin{example}
    For polar coordinates we have $x = r\cos\theta, y = r\sin\theta$ and the new variables $r$ and $\theta$ play the role of $u$ and $v$ above.
    \begin{align*}
    J &= \frac{\partial(x, y)}{\partial(r, \theta)} = \begin{vmatrix}
        \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\
        \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta}
    \end{vmatrix} \\
    &=
    \begin{vmatrix}
        \cos\theta & -r\sin\theta \\ \sin\theta & r\cos\theta
    \end{vmatrix} \\
    &= r\cos ^ 2\theta + r\sin ^ 2\theta \\
    &= r.
    \end{align*}
    So $dA = dxdy = |J|drd\theta = rdrd\theta$ as before.
\end{example}

\begin{example}
    Let $D$ be a square in the $xy$-plane with vertices $(0, 0), (1, 1), (2, 0)$ and $(1, -1)$.
    Calculate $\iint_D(x + y)\,dxdy$ using the change of variables $x = u + v, y = u - v$.

    First we express $D$ in terms of $u$ and $v$.
    We have
    \[
    \frac{x + y}{2} = u,\quad\frac{x - y}{2} = v.
    \]
    So the vertices of $D$ map to the $[u, v]$ coordinates
    \[
    [0, 0], [1, 0], [1, 0], [0, 1]
    \]
    respectively.
    The edges of $D$ in the $xy$-plane lie on the lines $y = x$, $y = -x$, $y = x - 2$ and $y = -x + 2$.
    These map to the lines $v = 0$, $u = 0$, $v = 1$ and $u = 1$ respectively.
    So $D$ is a square in the $uv$-plane,
    with vertices as above.
    The Jacobian is
    \[
    J = \frac{\partial(x, y)}{\partial(u, v)} = \begin{vmatrix}
        \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
        \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
    \end{vmatrix} = \begin{vmatrix}
        1 & 1 \\ 1 & -1
    \end{vmatrix} = -2
    \]
    and so $dA = dxdy = |J|dudv = 2dudv$.
    Finally the integrand is $x + y = 2u$ so
    \begin{align*}
        \iint\limits_D(x + y)\,dxdy &= \int_0^1\left(\int_0^12u \cdot 2\,du\right)\,dv \\
        &= \int_0^1\left(\int_0^14u\,du\right)\,dv \\
        &= \int_0^1\left[2u ^ 2\right]_0^1\,dv \\
        &= \int_0^1 2\,dv \\
        &= \left[2v\right]^1_0 \\
        &= 2.
    \end{align*}
\end{example}

\subsection{The Gaussian Integral}
The integral $\displaystyle\int_{-\infty}^{\infty}e ^ {-ax ^ a}$ for $a > 0$ is known as the Gaussian integral and is important in a range of contexts.

We can derive this result with a double integral

Let $I = \int_{-\infty}^{\infty}e ^ {-ax ^ 2}\,dx$

then
\begin{align*}
I ^ 2 &= \left(\int_{-\infty}^{\infty}e ^ {-ax ^ 2}\,dx\right)\left(\int_{-\infty}^{\infty}e ^ {-ay ^ 2}\,dy\right) \\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e ^ {-a(x ^ 2 + y ^ 2)}\,dxdy \\
&= \iint\limits_{\R ^ 2}e ^ {-a(x ^ 2 + y ^ 2)}\,dxdy
\end{align*}
Now switching to polar
\begin{align*}
    I ^ 2 &= \iint\limits_{\R ^ 2}e ^ {-ar ^ 2}r\,drd\theta \\
    &= \int_0^{\infty}\int_{0}^{2\pi}re ^ {-ar ^ 2}\,d\theta dr \\
    &= 2\pi\int_{0}^{\infty}re ^ {-ar ^ 2}\,dr \\
    &= 2\pi\left[\frac{-e ^ {-ar ^ 2}}{2a}\right]_{0}^{\infty} \\
    &= 2\pi\left(0 + \frac{1}{2a}\right) \\
    &= \frac{\pi}{a}
\end{align*}
and therefore $I = \sqrt{\frac{\pi}{a}}$.

\newpage

\section{First Order Differential Equations}

\subsection{First order separable ODEs}
A differential equation is an equation which relates an unknown function to one or more of its derivatives.

The order of a differential equation is the order of the highest derivative which appears in the equation.

Solving a differential equation is the process of finding a function
(solution)
which satisfies the differential equation.
When the function appearing in the equation only depends on a single variable,
such as $y(x)$,
then the differential equation is ordinary,
or ordinary differential equation
(ODE).

We say $y(x)$ is a dependent variable in the ODE,
and the $x$ is the independent variable $x$.

Solving the ODE means finding $y(x)$.

The generic first order ODE can be written in the form
\begin{equation}
    \frac{dy}{dx} = f(x, y)\quad\text{or}\quad y'(x) = f(x, y).
\end{equation}
Generically,
this has a one parameter family of solutions,
i.e. the general solution contains an arbitrary constant.
Assigning a particular value to this constant gives a particular solution.
Requiring that the solution satisfies an initial condition $y(x_0) = y_0$ for fixed $x_0$ and $y_0$ will fix a specific solution to the ODE.
In this case the ODE is called an initial value problem
(IVP).
To solve an IVP one should first find the general solution to the ODE,
and then use the initial condition
(initial value)
to find the specific solution which satisfies the IVP.

The method used to solve the ODE depends on the form of $f(x, y)$.

The ODE ($1$) is separable if $f(x, y) = X(x)Y(y)$.
It can be solved by direct integration.
\[
\int\frac{1}{Y(y)}\,dy = \int X(x)\,dx
\]
then differentiating both sides with respect to $x$,
\begin{align*}
    \frac{d}{dx}\left(\int\frac{1}{Y(y)}\,dy\right) &= \frac{d}{dx}\int X(x)\,dx \\
    \frac{1}{Y(y)} \cdot \frac{dy}{dx} &= X(x)
\end{align*}
so $\frac{dy}{dx} = X(x)Y(y)$.

\begin{example}
    Solve the IVP $\frac{dy}{dx} = xe ^ {y - x}$ with $y(0) = 0$
    \[
    y' = xe ^ y e ^ {-x}
    \]
    \[
    \int e ^ {-y}\,dy = \int xe ^ {-x}\,dx
    \]
    \begin{align*}
    -e ^ {-y} &=  -xe ^ {-x} + \int e ^ {-x}\,dx \\
    -e ^ {-y} &= -xe ^ {-x} - e ^ {-x} + c \\
    e ^ {-y} &= e ^ {-x}(1 + x) - c \\
    y &= -\log(e ^ {-x}(1 + x) - c)
    \end{align*}
    is the general solution to the ODE.
    
    Now using the initial condition $y(0) = 0$
    \[
    y(0) = -\log(1 - c) = 0 \text{ so } c = 0
    \]
    and the specific solution to the IVP is
    \[
    y(x) = -\log(e ^ {-x}(1 + x)) = x - \log(1 + x).
    \]
\end{example}

\begin{example}
    Solve $y' = \frac{x ^ 2 y - y}{1 + y} = \frac{y}{1 + y}(x ^ 2 - 1)$
    \begin{align*}
        \int\frac{1 + y}{y}\,dy &= \int(x ^ 2 - 1)\,dx \\
        y + \log|y| &= \frac{x ^ 3}{3} - x + c
    \end{align*}
    is the general solution to the ODE
    (in implicit form).
\end{example}

\subsection{First order homogeneous ODEs}
The ODE ($1$) is homogeneous if $f(tx, ty) = f(x,  y)\\forall t \in \R$.
In this case,
the substitution $y = xv(x)$ gives a separable ODE for $v(x)$.

\begin{example}
    Solve $y' = \frac{y ^ 2 - x ^ 2}{xy}$
    with $f(x, y) = \frac{y ^ 2 - x ^ 2}{xy}$, $f(tx, ty) = \frac{t ^ 2y ^ 2 - t ^ 2 x ^ 2}{t ^ 2xy} = f(x, y)$.
    Let $y(x) = xv(x)$
    \[
    y' = v(x) + xv'(x) = \frac{x ^ 2 v ^ 2 - x ^ 2}{x ^ 2 v} = \frac{v ^ 2 - 1}{v} = v - \frac{1}{v}
    \]
    \begin{align*}
        v + xv' &= v - \frac{1}{v} \\
        v' &= -\frac{1}{xv} \\
        \int v\,dv &= \int -\frac{1}{x}\,dx \\
        \frac{v ^ 2}{2} &= -\log|x| + c \\
        &= -\log|x| + \log|a| \\
        &= -\log\left|\frac{a}{x}\right| \\
        v &= \pm\sqrt{2\log\left|\frac{a}{x}\right|}.
    \end{align*}
    Since $y(x) = xv(x) = \pm x\sqrt{2\log\left|\frac{a}{x}\right|}$
\end{example}

\subsection{First order linear ODEs}
The ODE ($1$) is linear if $f(x, y) = -p(x)y + q(x)$,
we can then write the ODE in the form
\[
y' + p(x)y = q(x).
\]
To solve this ODE we need to use the integrating factor $I(x) = e ^ {\int p(x)\,dx} = \exp\left(\int p(x)\,dx\right)$.
Note that $I'(x) = p(x)e ^ {\int p(x)\,dx} = p(x)I(x)$ which is separable for $I$.
Then
\[
(yI)' = y'I + yI' = y'I + ypI = (y' + yp)I = qI
\]
\[
yI = \int qI\,dx
\]
\[
y(x) = \frac{1}{I(x)}\int q(x)I(x)\,dx\text{ is the general solution to the linear ODE}.
\]
\begin{example}
    Solve the IVP $y' - \frac{2}{x}y = 3x ^ 3$ with $y(-1) = 2$.
    Here we have $p(x) = -\frac{2}{x}$ and $q(x) = 3x ^ 3$.
    \[
    I = \exp\left(\int p(x)\,dx\right) = \exp(-2\log(x)) = x ^ {-2}
    \]
    then $y(x) = x ^ 2\int 3x\,dx = x ^ 2 \left(\frac{3}{2}x ^ 2 + c\right) = \frac{3}{2}x ^ 4 + cx ^ 2$ is the general solution.
    We need $y(-1) = \frac{3}{2} + c = 2$ so $c = \frac{1}{2}$ and the particular solution to the IVP is then
    \[
    y(x) = \frac{3}{2}x ^ 4 + \frac{1}{2}x ^ 2 = \frac{1}{2}x ^ 2(3x ^ 2 + 1).
    \]
\end{example}

\subsection{First order exact ODEs}
A first order ODE can also be written in the form
\begin{equation}
    M(x, y)\,dx + N(x, y)\,dy = 0
\end{equation}
This can be rearranged to
\[
N(x, y)\frac{dy}{dx} + M(x, y) = 0
\]
or
\[
\frac{dy}{dx} = -\frac{M(x, y)}{N(x, y)}
\]
in the form of ($1$).

\textit{Note that a given $f(x, y)$ does not correspond to a unique $M$ and $N$,
as multiplying $M$ and $N$ by the same arbitrary function would correspond to the same $f(x, y)$.}

For a function $g(x, y)$,
the total differential $dg$ is defined as $dg = \frac{\partial g}{\partial x}\,dx + \frac{\partial g}{\partial y}\,dy$.
We can think of $dg$ as the small change in $g$ that occurs when moving from the point $(x, y)$ to the point $(x + dx, y + dy)$ for small $dx$ and $dy$.

Then $dg$ being $0$ means that $g$ is constant.

The ODE in the form ($2$) is exact if there is a function $g(x, y)$ such that the left-hand side of ($2$) is the total differential $dg$,
i.e. such that
\[
\frac{\partial g}{\partial x} = M(x, y)\qquad\frac{\partial g}{\partial y} = N(x, y).
\]
If so the ODE is equivalent to $dg = 0$,
in which case $g$ must be constant,
and $g = c$ gives a solution to the ODE.

The equality of the mixed partial derivatives i.e.
\[
\frac{\partial ^ 2 g}{\partial x \partial y} = \frac{\partial ^ 2 g}{\partial y \partial x}
\]
requires that for an exact ODE $\frac{\partial N}{\partial x} = \frac{\partial M}{\partial y}$.

We therefore have the following test for exactness.
The ODE $M(x, y)\,dx + N(x, y)\,dy = 0$ is exact if and only if
\[
\frac{\partial N}{\partial x} = \frac{\partial M}{\partial y}.
\]
\begin{example}
    Show that $(3e ^ {3x}y + e ^ x)\,dx + (e ^ {3x} + 1)\,dy = 0$
    is exact and hence find the general solution.
    We have $M(x, y) = 3e ^ {3x}y + e ^ x$ and $N(x, y) = e ^ {3x} + 1$.
    So
    \[
    \frac{\partial N}{\partial x} = 3e ^ {3x} = \frac{\partial M}{\partial y},
    \]
    so the ODE is exact.
    From $M = \frac{\partial g}{\partial x}$, $\frac{\partial g}{\partial x} = 3e ^ {3x}y + e ^ x$,
    so we must have $g(x, y) = e ^ {3x}y + e ^ x + \phi(y)$ where we have some arbitrary function $\phi(y)$ in place of our usual constant of integration.
    Then $\frac{\partial g}{\partial y} = e ^ {3x} + \phi'(y) = N(x, y) = e ^ {3x} + 1$.
    So $\phi'(y) = 1$ and hence $\phi(y) = y + c$.
    The general solution is then
    \begin{align*}
        g(x, y) &= e ^ {3x}y + e ^ x + y + c = a \\
        &= e ^ {3x}y + e ^ x + y + c &\text{(absorbing the $a$ into the $c$)}
    \end{align*}
    this gives a solution in implicit form.
    In this case we can rearrange to find an explicit form
    \begin{align*}
        e ^ {3x}y + y &= c - e ^ x \\
        y(e ^ {3x} + 1) &= c - e ^ x \\
        y(x) &= \frac{c - e ^ x}{e ^ {3x} + 1}.
    \end{align*}
\end{example}

\textbf{Exercise: } By rewriting the ODE from this question in the form of ($1$),
check that this is indeed a solution.

Consider an ODE $Mdx + Ndy = 0$ which is not exact,
i.e. with $\frac{\partial M}{\partial y} \neq \frac{\partial N}{\partial x}$.
Multiplying the ODE by $I(x, y)$ gives an equivalent ODE
\[
mdx + ndy = 0,\text{ where}
\]
$m = MI$ and $n = NI$.
If this new ODE is exact,
i.e.
\[
\frac{\partial M}{\partial y} = \frac{\partial n}{\partial x}
\]
then $I(x, y)$ is an integrating factor for the original ODE.

\begin{example}
    Show that $x$ is an integrating factor for
    \[
    (3xy - y ^ 2)dx + x(x - y)dy = 0
    \]
    and hence find the general solution.
    $M = 3xy - y ^ 2$, $N = x ^ 2 - xy$,
    so
    \[
    \frac{\partial M}{\partial y} = 3x - 2y\quad \frac{\partial N}{\partial x} = 2x - y
    \]
    so not exact,
    with $I(x, y) = x$ we have $m = MI = 3x ^ 2y - xy ^ 2$, $n = NI = x ^ 3 - x ^ 2y$ and
    \[
    \frac{\partial m}{\partial y} = 3x ^ 2 - 2xy = \frac{\partial n}{\partial x}
    \]
    so $mdx + ndy = 0$ is exact.
    We can now solve with our previous method
    \[
    m = \frac{\partial g}{\partial x} = 3x ^ 2y - xy ^ 2
    \]
    so
    \[
    g(x, y) = x ^ 3y - \frac{1}{2}x ^ 2y ^ 2 + \phi(y).
    \]
    Then $\frac{\partial g}{\partial y} = x ^ 3 - x ^ 2y + \phi(y)$ and $n = x ^ 3 - x ^ 2y = \frac{\partial g}{\partial y}$,
    so $\phi'(y) = 0$ and we can take $\phi(y) = 0$.
    The final solution is then $g = c$,
    i.e.
    \[
    x ^ 3y - \frac{1}{2}x ^ 2y ^ 2 = c.
    \]
\end{example}

\subsection{Bernoulli Equations}

A Bernoulli equation is a non-linear ODE of the form
\[
y' + p(x)y = q(x)y ^ n\text{ with } n \neq 0, 1.
\]
Dividing through by $y ^ n$ gives
\[
y ^ {-n}y' + p(x)y ^ {1 - n} = q(x).
\]
Using the substitution $v(x) = y ^ {1 - n}(x)$.
We have $v' = (1 - n)y ^ {-n}y'$ and so we can rewrite the ODE in terms of $v$ as
\[
\frac{1}{1 - n}v' + p(x)v = q(x)
\]
or
\[
v' + (1 - n)p(x)v = (1 - n)q(x)\text{ which is linear for $v$}.
\]

\begin{example}
    Solve $y' - \frac{2y}{x} = -x ^ 2y ^ 2$.
    This is Bernoulli with $n = 2$,
    so we make the substitution $v = y ^ {1 - n} = y ^ {-1}$.
    Hence $v' = -y ^ {-2}y'$
    If we divide our original ODE by $y ^ 2$
    \[
    y ^ {-2}y' - \frac{2}{xy} = -x ^ 2.
    \]
    Then we can write this in terms of $v$ as
    \[
    -v' - 2\frac{v}{x} = -x ^ 2,
    \]
    so we have the linear ODE
    \[
    v' + 2\frac{v}{x} = x ^ 2.
    \]
    The integrating factor is $I(x) = \exp\left(\int\frac{2}{x}\,dx\right) = x ^ 2$ and hence the solution is
    \begin{align*}
    v(x) &= \frac{1}{I}\int qI\,dx \\
    &= x ^ {-2}\int x ^ 4\,dx \\
    &= x ^ {-2}\left(\frac{1}{5}x ^ 5 + \frac{c}{5}\right) \\
    &= \frac{x ^ 5 + c}{5x ^ 2}.
    \end{align*}
    Hence the solution to the original Bernoulli ODE is
    \[
    y(x) = \frac{1}{v} = \frac{5x ^ 2}{x ^ 5 + c}.
    \]
\end{example}

\newpage

\section{Second order ODEs}

\subsection{Linear constant coefficient homogeneous ODEs}
The general form for a second order linear ODE is
\begin{equation}\tag{$\dagger$}
    a_2y'' + a_1y' + a_0y = \phi(x)
\end{equation}
where $a_2 \neq 0$ and $\phi(x)$ is an arbitrary function,
and we take $a_2, a_1, a_0$ are constant.

If the constants $a_2, a_1$ and $a_0$ are replaced by function of $x$,
the ODE is still second order linear,
but harder to solve.

If the function $\phi(x) = 0$,
we call the ODE homogeneous
\begin{equation}\tag{$\ddagger$}
    a_2y'' + a_1y' + a_0y = 0.
\end{equation}

We consider the homogeneous case first.
First,
note that if $y_1$ and $y_2$ are two solutions to ($\ddagger$),
then so is the linear combination $y = Ay_1 + By_2$ for constants $A, B$.
\begin{proof}
    Let $y = Ay_1 + By_2$,
    then
    \[
    y' = Ay_1' + By_2'
    \]
    and
    \[
    y'' = Ay_1'' + By_2'',
    \]
    and so
    \begin{align*}
        a_2y'' + a_1y' + a_0y &= a_2(Ay_1'' + By_2'') + a_1(Ay_1' + By_2') + a_0(Ay_1 + By_2) \\
        &= A(a_2y_1'' + a_1y_1' + a_0y_1) + B(a_2y_2'' + a_1y_2' + a_0y_2) \\
        &= A\cdot 0 + B\cdot 0 \\
        &= 0.
    \end{align*}
\end{proof}
Hence $y$ is a solution to ($\ddagger$).
Note that the linearity of ($\ddagger$) is crucial.

The general solution to ($\ddagger$) contains two arbitrary constants
(as the ODE is second order).
So if $y_1$ and $y_2$ are any two solutions which are independent,
then the general solution is $y = Ay_1 + By_2$.
We therefore need to find two independent particular solutions.
We look for solutions in the form $y = e ^ {\lambda x}$.
Substituting this into ($\ddagger$) gives
\[
a_2\lambda ^ 2e ^ {\lambda x} + a_1\lambda e ^ {\lambda x} + a_0e ^ {\lambda x} = 0
\]
and hence
\begin{equation}\tag{Char}
    a_2\lambda ^ 2 + a_1\lambda + a_0 = 0
\end{equation}
which is known as the characteristic
(or auxiliary)
equation.
There are now three cases,
depending on the roots of the (char) equation.
\begin{enumerate}[label = \roman*)]
    \item Distinct real roots

    If (char) has real roots $\lambda_1 \neq \lambda_2$ then
    \[
    y_1 = e ^ {\lambda_1x}\text{ and } y_2 = e ^ {\lambda_2x}
    \]
    are the required solutions and the general solution is
    \[
    y = Ae ^ {\lambda_1x} + Be ^ {\lambda_2x}.
    \]
    \item One repeated real root:
    The characteristic equation has a repeated real root if
    \[
    a_1 ^ 2 - 4a_0a_2 = 0,
    \]
    i.e. when $a_0 = \frac{a_1 ^ 2}{4a_2}$
    In this case,
    the characteristic equation factorises as
    \[
    a_2\left(\lambda + \frac{a_1}{2a_2}\right) ^ 2 = 0
    \]
    and the double root is
    \[
    \lambda = -\frac{a_1}{2a_2}.
    \]
    We therefore find one real solution $y = e ^ {\lambda_1x}$.
    However,
    $y_x = xe ^ {\lambda_1x}$ is also a solution:
    ($\ddagger$) has the form
    \begin{align*}
        0 &= a_2y'' + a_1y' + \frac{a_1 ^ 2}{4a_2}y \\
        &= a_2(y'' - 2\lambda_1y' + \lambda_1 ^ 2y)
    \end{align*}
    we have $y'_2 = e ^ {\lambda_1x} + \lambda_1xe ^ {\lambda_1x}$,
    $y''_2 = \lambda_1e ^ {\lambda_1x} + \lambda_1e ^ {\lambda_1x} + \lambda_1 ^ 2xe ^ {\lambda_1x} = 2\lambda_1e ^ {\lambda_1x} + \lambda_1 ^ 2xe ^ {\lambda_1x}$.
    So
    \[
    y''_2 - 2\lambda_1y'_2 + \lambda_1 ^ 2y_2 = (2\lambda_1e ^ {\lambda_1x} + \lambda_1 ^ 2e ^ {\lambda_1x}) - 2\lambda_1(e ^ {\lambda_1x} + \lambda_1xe ^ {\lambda_1x} + \lambda_1 ^ 2xe ^ {\lambda_1x}) = 0
    \]
    so $y_2$ is a second independent solution to ($\ddagger$),
    and the general solution is $y(x) = Ae ^ {\lambda_1x} + Bxe ^ {\lambda_1x}$.
    \item Two complex roots.
    If the characteristic equation has complex roots,
    they appear as a conjugate pair,
    i.e. $\lambda_1 = \alpha + i\beta$, $\lambda_2 = \alpha - i\beta$.
    We therefore have two solutions
    \begin{align*}
        y_1 &= e ^ {\lambda_1x} = e ^ {\alpha x}e ^ {i\beta x} \\
        y_2 &= e ^ {\lambda_2x} = e ^ {\alpha x}e ^ {-i\beta x}.
    \end{align*}
    However,
    these are complex functions and we would like real functions.
    Since any linear combination of these solutions will give a solution,
    we can consider
    \begin{align*}
        Y_1 &=
        \begin{aligned}
            \frac{1}{2}(y_1 + y_2) &= \frac{1}{2}e ^ {\alpha x}(e ^ {i\beta x} + e ^ {-i\beta x}) \\
            &= e ^ {\alpha x}\cos(\beta x) 
        \end{aligned} \\
        Y_2 &=
        \begin{aligned}
            \frac{1}{2}(y_1 - y_2) &= \frac{1}{2}e ^ {\alpha x}(e ^ {i\beta x} - e ^ {-i\beta x}) \\
            &= e ^ {\alpha x}\sin(\beta x) 
        \end{aligned}.
    \end{align*}
    These ate two independent real solutions,
    and therefore the general solution is
    \begin{align*}
        y(x) &= Ae ^ {\alpha x}\cos(\beta x) + Be ^ {\alpha x}\sin(\beta x) \\
        &= e ^ {\alpha x}(A\cos(\beta x) + B\sin(\beta x))
    \end{align*}
    with $A$ and $B$ real constants.
    Note that $\alpha$ and $\beta$ are real numbers;
    the real and imaginary parts of the complex roots of the characteristic equation.
\end{enumerate}

\begin{example}
    Solve $y'' - 2y' - 3y = 0$.
    This ODE has characteristic equation
    \[
    \lambda ^ 2 - 2\lambda - 3 = 0 = (\lambda - 3)(\lambda + 1)\quad(\text{char})
    \]
    with roots $\lambda = -1$ and $\lambda = 3$.
    The general solution is therefore
    \[
    y = Ae ^ {-x} + Be ^ {3x}.
    \]
\end{example}

\begin{example}
    Solve $3y'' + 12y' + 12y = 0$.

    This ODE has characteristic equation
    \[
    3\lambda ^ 2 + 12\lambda + 12 = 0 = 3(\lambda + 2) ^ 2
    \]
    with double root $\lambda = -2$.
    The general solution is
    \[
    y(x) = Ae ^ {-2x} + Bxe ^ {-2x}.
    \]
\end{example}

\begin{example}
    Solve $y'' + 2y' + 5y = 0$.
    This has the characteristic equation $\lambda ^ 2 + 2\lambda + 5 = -0$.
    \[
    \lambda = \frac{-2 \pm \sqrt{-16}}{2} = -1 \pm 2i.
    \]
    We have $\alpha = 1$ and $\beta = 2$,
    so the general solution is
    \[
    y(x) = e ^ {-\alpha}(A\cos(2x) + B\sin(2x)).
    \]
\end{example}

\subsection{The method of undetermined coefficients}
We now return to the general inhomogeneous ODE ($\dagger$) with $\phi(x)$ a function not identically zero.

We want a general solution to this,
which would contained two arbitrary constants.
The general solution takes the form $y = y_{CF} + y_{PI}$,
where $y_{CF}$ is the complementary function and is the general solution to the homogeneous ODE
\[
a_2y'' + a_1y' + a_0y = 0
\]
and $y_{PI}$ is known as the particular integral,
and is any one particular solution to the inhomogeneous ODE ($\dagger$).

We know a general solution to ($\ddagger$) contains two arbitrary constants,
and so does $y$,
our new solution.
We check that $y = y_{CF} + y_{PI}$ is a solution to ($\dagger$)
\begin{align*}
    a_2y'' + a_1y' + a_0y &= a_2(y''_{CF} + y''_{PI}) + a_1(y'_{CF} + y'_{PI}) + a_0(y_{CF} + y_{PI}) \\
    &= (a_2y''_{CF} + a_1y'_{CF} + a_0y_{CF}) + (a_2y''_{PI} + a_1y'_{PI} + a_0y_{PI}) \\
    &= 0 + \phi \\
    &= \phi.
\end{align*}
We therefore need to find a particular integral.
When $\phi(x)$ has the form of a polynomial,
exponential or $\sin/\cos$ function,
or a sum or product of these,
we can apply the method of undetermined coefficients.
To apply this method,
we try $y_{PI}$ of a particular form based on the form of $\phi$,
containing some unknown constant coefficients.
We then determine these coefficients such that $y_{PI}$ solves to ($\dagger$).
The following table lists terms that can appear in $\phi$ in order to use this method,
and the table also gives the corresponding form to try for $y_{PI}$.
\begin{table}[H]
    \begin{tabular}{c|c}
         Term in $\phi(x)$ & Form to try for $y_{PI}$ \\
         \hline
         $e ^ {\gamma x}$ & $a_1e ^ {\gamma x}$ \\
         $x ^ n$ & $a_0 + a_1x + a_2x ^ 2 + \dotsc + a_nx ^ n$ \\
         $\cos(\gamma x)$ & $a_1\cos(\gamma x) + a_2\sin(\gamma x)$ \\
         $\sin(\gamma x)$ & $a_1\cos(\gamma x) + a_2\sin(\gamma x)$
    \end{tabular}
\end{table}
If $\phi(x)$ contains sums/products of the above terms,
we try the corresponding sum/product of the corresponding forms.

\textbf{Special case rule}:
If the listed form to try is already a term in $y_{CF}$,
then putting this into ($\dagger$) will give $0$ rather than $\phi(x)$.
We then try a $y_{PI}$ of the suggested form multiplied by $x$.
We can apply this twice
(or more times)
if the first application of the rule also gives a term in $y_{CF}$.
\begin{example}
    Solve $y'' - 2y' - 3y = -3x ^ 2$.
    From an earlier example $y_{CF} = Ae ^ {-x} + Be ^ {3x}$.
    For a $y_{PI}$ we try $y_{PI} = a_0 + a_1x + a_2x ^ 2$.
    Then $y'_{PI} = a_1 + 2a_2x$.
    $y''_{PI} = 2a_2$.
    Therefore $y''_{PI} - 2y'_{PI} - 3y_{PI} = 2a_2 - 2a_1 - 4a_2x - 3(a_0 + a_1x + a_2x ^ 2) = -3x ^ 2$.
    Comparing coefficients,
    we find $a_2 = 1$, $a_1 = -\frac{4}{3}$, $a_0 = \frac{14}{9}$.
    So the general solution to this inhomogeneous ODE is
    \[
    y = y_{CF} + y_{PI} = Ae ^ {-x} + Be ^ {3x} + \left(\frac{14}{9} - \frac{4}{3}x + x ^ 2\right).
    \]
\end{example}

\begin{example}
    Solve $y'' - y' - 2y = 6e ^ {-x}$.
    We first find $y_{CF}$,
    the solution to the associated homogeneous ODE with characteristic equation
    \[
    \lambda ^ 2 - \lambda - 2 = 0 = (\lambda + 1)(\lambda - 2).
    \]
    This has roots $\lambda = -1$ and $\lambda = 2$,
    so $y_{CF} = Ae ^ {-x} + Be ^ {2x}$.
    To find $y_{PI}$,
    the table suggests we should try a multiple of $a_1e ^ {-x}$,
    but this appears in $y_{CF}$, so instead we try $y_{PI} = a_1xe ^ {-x}$.
    Then $y'_{PI} = a_1e ^ {-x} - a_1xe ^ {-x}$, $y''_{PI} = -a_1e ^ {-x} - a_1e ^ {-x} + a_1xe ^ {-x} = a_1xe ^ {-x} - 2a_1e ^ {-x}$.
    Putting this into the ODE gives
    \[
    (a_1xe ^ {-x} - 2a_1e ^ {-x}) - (a_1e ^ {-x} - a_1xe ^ {-x}) - 2(a_1xe ^ {-x}) = -3a_1e^{-x} = 6e ^ {-x}
    \]
    so we get a solution when $a_1 = -2$,
    $y_{PI} = -2xe ^ {-x}$.
    The general solution is therefore $y(x) = Ae ^ {-x} + Be ^ {2x} - 2xe ^ {-x} = (A - 2x)e ^ {-x} + Be ^ {2x}$.
\end{example}

\begin{example}
    Solve $y'' - 2y' + y = e ^ x\cos(x)$.
    The associated homogeneous ODE has characteristic equation
    \[
    \lambda ^ 2 - 2\lambda + 1 = 0 = (\lambda - 1) ^ 2
    \]
    with a repeated root $\lambda = 1$.
    We therefore have $y_{CF} = Ae ^ {x} + Bxe ^ {x}$.

    For $y_{PI}$,
    try $y_{PI} = a_1e ^ x(b_1\cos(x) + b_2\sin(x)) = e ^ x(c_1\cos(x) + c_2\sin(x))$.
    
    
    \textbf{Check - exercise} $c_1 = -1$ and $c_2 = 0$.

    So $y_{PI} = -e ^ x\cos(x)$ and so the general solution is $y(x) = Ae ^ {x} + Bxe ^ {x} - e ^ {x}\cos(x) = e ^ {x}(A + Bx - \cos(x))$.
\end{example}

\subsection{Initial and Boundary value problems}
The two arbitrary constants in the general solution of a second order ODE may be fixed by specifying two extra requirements on the solution and/or the derivative.

An initial value problem (IVP) is when we require $y(x_0) = y_0$ and $y'(x_0) = \delta$ for given constants $x_0, y_0$ and $\delta$.
In this case the two constants are for the same value of the independent variable.

A boundary value problem (BVP) is when we require $y(x_0) = y_0$ and $y(x_1) = y_1$ for given constants $x_0, x_1, y_0, y_1$.
In this case the constants are given at two different values of the independent variables.

To solve either an IVP or BVP,
we first find the general solution to the ODE and then find the value of the constants such that the conditions are satisfied.

\begin{example}
    Solve the IVP $y'' -y' - 2y = 6e ^ {-x}$,
    $y(0) = 6$,
    $y'(0) = 7$.

    In a previous example we showed that the general solution to the ODE is $y(x) = (A - 2x)e ^ {-x} + Be ^ {2x}$.

    Since $y(0) = A + B = 6$ we have $A + B = 6$.
    Then $y'(x) = -Ae ^ {-x} - 2e ^ {-x} + 2xe ^ {-x} + 2Be ^ {2x}$ and so $y'(0) = -A - 2 + 2B = 7$,
    therefore $-A + 2B = 9$.
    
    Solving simultaneously gives $B = 5$ and $A = 1$,
    so the solution to the IVP is $y(x) = (1 - 2x)e ^ {-x} + 5e ^ {2x}$
\end{example}

\subsection{The method of variation of parameters}
Our method of undetermined coefficients allows us to solve an inhomogeneous ODE,
but only when the inhomogeneous term $\phi$ takes particular forms.

The following method allows us to solve more general inhomogeneous ODEs.
\begin{definition}
    Given two differentiable functions $y_1$ and $y_2$,
    we define the Wronskian as
    \[
    W(y_1, y_2) = \begin{vmatrix}
        y_1 & y_2 \\ y'_1 & y'_2
    \end{vmatrix} = y_1y'_2 - y'_1y_2.
    \]
\end{definition}
If $y_1$ and $y_2$ are linearly dependent $y_1(x) = \lambda y_2(x)$ for all $x$.
Then $y'_1(x) = \lambda y'_2(x)$ and $W(y_1, y_2) = \lambda y_2y'_2 - \lambda y'_2y_2$ which is identically zero,
i.e. $W(y_1, y_2) = 0\  \forall x$.
Therefore if $W(y_1, y_2)$ is not identically zero,
$y_1$ and $y_2$ are linearly independent.

We want to find a particular integral for the inhomogeneous ODE ($\dagger$)
\[
a_2y'' + a_1y' + a_0 = \phi(x).
\]
We start by considering the two linearly independent solutions $y_1, y_2$ of the associated homogeneous ODE ($\ddagger$)
\[
a_2y'' + a_1y' + a_0 = 0
\]
i.e. $y_{CF} = Ay_1 + By_2$.

We now replace the constants $A$ and $B$ in $y_{CF}$ by functions $u_1(x)$ and $u_2(x)$ respectively.
That is,
we look for solutions to ($\dagger$) of the form
\[
y_{PI} = u_1(x)y_1(x) + u_2(x)y_2(x).
\]
We wish to find $u_1(x)$ and $u_2(x)$ such that $y_{PI}$ is a solution to ($\dagger$).
This gives one constraint on $u_1$ and $u_2$ so to find unique functions we should impose a second constraint on $u_1$ and $u_2$.
We choose
\begin{equation}\tag{$1$}
    u'_1y_1 + u'_2y_2 = 0.
\end{equation}
Then $y'_{PI} = u'_1y_1 + u_1y'_1 + u'_2y_2 + u_2y'_2 = u_1y'_1 + u_2y'_2$ by ($1$) and $y''_{PI} = u'_1y'_1 + u_1y''_1 + u'_2y'_2 + u_2y''_2$.

Next,
we need to substitute this into ($\dagger$) in order to identify $u_1(x)$ and $u_2(x)$.

This gives the following
(after collecting terms)
\[
\alpha_2(u'_1y'_1 + u'_2y'_2 + u_1y''_1 + u_2y''_2) + \alpha_1(u_1y'_1 + u_2y'_2) + \alpha_0(u_1y_1 + u_2y_2) = \phi.
\]
Since $y_1$ and $y_2$ are solutions to the homogeneous equation ($\ddagger$),
the first two terms on the left-hand side vanish,
giving
\begin{equation}\tag{$2$}
    a_2(u'_1y'_1 + u'_2y'_2) = \phi.
\end{equation}
We now have two simultaneous equations for $u_1$ and $u_2$,
namely ($1$) and ($2$),
which we can solve
Multiplying ($2$) by $y_1$ and rearranging gives
\[
u'_1y'_1y_1 + u'_2y'_2y_1 = \frac{\phi y_1}{a_2}
\]
then using $u'_1y_1 = -u'_2y_2$ by ($1$)
\[
u'_2(y'_2y_1 - y'_1y_2) = \frac{\phi y_1}{a_2}.
\]
$u'_2 = \frac{\phi y_1}{a_2W(y_1, y_2)}$
and this is similar for $u'_1$
$u'_1 = -\frac{\phi y_2}{a_2W(y_1, y_2)}$.

We hence find $u_1$ and $u_2$ by integrating.
\[
u_1(x) = -\int\frac{\phi(x)y_2(x)}{a_2W(y_1, y_2)}\,dx,\quad u_2(x) = \int\frac{\phi(x)y_1(x)}{a_2W(y_1, y_2)}\,dx.
\]
\begin{example}
    Solve $y'' - y = e ^ {2x}$.
    We could solve this using the method of undetermined coefficients.
    Here we use the method of variation of parameters to check it gives the same answer.

    We have the complementary function
    \[
    y_{CF} = Ae ^ {x} + Be ^ {-x},
    \]
    therefore
    $y_1 = e ^ x$ and $y_2 = e ^ {-x}$.
    \[
    W(y_1, y_2) = \begin{vmatrix}
        y_1 & y_2 \\
        y'_1 & y'_2
    \end{vmatrix} = \begin{vmatrix}
        e ^ x & e ^ {-x} \\
        e ^ x & -e ^ {-x}
    \end{vmatrix} = -1 - 1 = -2.
    \]
    So
    \[
    u_1(x) = -\int{e ^ {2x}e ^ {-x}}{-2}\,dx = \frac{1}{2}\int e ^ {x}\,dx = \frac{1}{2}e ^ x
    \]
    \[
    u_2(x) = \int{\frac{e ^ {2x}e ^ {x}}{-2}}\,dx = -\frac{1}{2}\int e ^ {3x}\,dx = -\frac{1}{6}e ^ {3x}.
    \]
    Therefore
    \[
    y_{PI} = u_1y_1 + u_2y_2 = \frac{1}{2}e ^ xe ^ x + -\frac{1}{6}e ^ {3x}e ^ {-x} = \frac{1}{3}e ^ {2x}.
    \]
    The general solution to the ODE is then
    \begin{align*}
        y(x) &= y_{CF} + y_{PI} \\
        &= Ae ^ x + Be ^ {-x} + \frac{1}{3}e ^ {2x}.
    \end{align*}
\end{example}

\begin{example}
    Solve $y'' - 2y' + y = \frac{e ^ x}{x ^ 2 + 1}$.

    The characteristic equation is
    \[
    \lambda ^ 2 - 2\lambda + 1 = 0
    \]
    with repeated root $\lambda = 1$,
    so $y_{CF} = Ae ^ {x} + Bxe ^ {x}$.
    So $y_1 = e ^ x$,
    $y_2 = xe ^ x$
    \[
    W(y_1, y_2) = \begin{vmatrix}
        e ^ x & xe ^ x \\
        e ^ x & e ^ x + xe ^ x
    \end{vmatrix} = e ^ {2x} + xe ^ {2x} - xe ^ {2x} = e ^ {2x}
    \]
    and so
    \[
    u_1 = -\int\frac{e ^ x \cdot xe ^ x}{(x ^ 2 + 1)e ^ {2x}}\,dx = -\int\frac{x}{x ^ 2 + 1}\,dx = -\frac{1}{2}\log(x ^ 2 + 1).
    \]
    \[
    u_2 = \int\frac{e ^ x \cdot e ^ x}{(x ^ 2 + 1)e ^ {2x}}\,dx = \int\frac{1}{x ^ 2 + 1}\,dx = \arctan(x).
    \]
    So $y_{PI} = u_1y_1 + u_2y_2 = -\frac{1}{2}e ^ x\log(x ^ 2 + 1) + xe ^ x\arctan(x)$ and so the general solution to the ODE is
    \begin{align*}
        y(x) &= y_{CF} + y_{PI} \\
        &= e ^ x\left(A + Bx - \frac{1}{2}\log(x ^ 2 + 1) + x\arctan(x)\right).
    \end{align*}
\end{example}

\subsection{Systems of first order linear ODEs}
A system of $n$ coupled first order linear ODEs for $n$ dependent variables can be rewritten as a single $n$th order linear ODE for a single dependent variable,
by eliminating the other dependent variables.

An associated IVP is obtained if all the values of the dependent variables are specified at a single value of the independent variable.

\begin{example}
    Find the solution $y(x), z(x)$ of the pair of first order linear ODEs
    \[
    y' = -y + z + x ^ 2,
    \]
    \[
    z' = -8y + 5z + 8x ^ 2 - 7x + 1,
    \]
    satisfying the initial condition $y(0) = 2$,
    $z(0) = 0$.

    We rewrite the system of two coupled first order ODEs as a second order ODE for $y$.

    From the first equation $z = -\frac{y'}{2} + \frac{y}{2} + x$ and hence $z' = -\frac{y''}{2} + \frac{y'}{2} + 1$ substitute these expressions for $z$ and $z'$ into the second equation gives
    \[
    -\frac{y''}{2} + \frac{y'}{2} + 1 = 3y - 4\left(-\frac{y'}{2} + \frac{y}{2} + x\right) + 2x.
    \]
    Substituting these expressions into the second equation gives
    $y'' + y' - 2x = -8y + 5(y' + y - x ^ 2) + 8x ^ 2 - 7x + 1$

    Solving using our earlier methods.
    The characteristic equation is
    \[
    \lambda ^ 2 - 4\lambda + 3 = 0 = (\lambda - 3)(\lambda - 1)
    \]
    with roots $\lambda = 1, 3$.
    $y_{CF} = Ae ^ x + Be ^ {3x}$.
    Trying $y = a_0 + a_1x + a_2x ^ 2$ for $y_{PI}$ gives
    \[
    y'' - 4y' + 3y = 2a_2 - 4(a_1 + 2a_2x) + 3(a_0 + a_1x + a_2x ^ 2) = 3a_2x ^ 2 + (3a_1 - 8a_2)x + 2a_2 - 4a_1 + 3a_0 = 3x ^ 2 - 5x + 1.
    \]
    Comparing coefficients gives the solution $a_2 = 1, a_1 = 1, a_0 = 1$.
    Giving us the general solution for $y$
    \[
    y = Ae ^ x + Be ^ {3x} + x ^ 2 + x + 1.
    \]
    Obtaining $z$ is done through using the earlier relation $z = y' + y - x ^ 2 = 2Ae ^ x + 4Be ^ {3x} + 3x + 2$.
    
    Now we have a general solution for both $y$ and $z$.

    We can now solve the IVPs
    \[
    y(0) = A + B + 1 = 2
    \]
    \[
    z(0) = 2A + 4B + 2 = 0
    \]
    which have solutions $A = 3, B = -2$.

    Hence the solution of the IVP is
    \[
    y = 3e ^ x - 2e ^ {3x} + x ^ 2 + x + 1,
    \]
    \[
    z = 6e ^ x - 8e ^ {3x} + 3x + 2.
    \]
\end{example}

\newpage

\section{Taylor Series}

\subsection{Taylor's theorem}

\begin{theorem}
    If $f(x)$ has $(n + 1)$ continuous derivatives in an open interval $I$ containing the point $x = a$,
    then $\forall x \in I$
    \[
    f(x) = \sum_{k = 0}^{n}\frac{f ^ {(k)}(a)}{k!}(x - a) ^ k + R_n(x)
    \]
    where $R_n(x) = \frac{1}{n!}\int_{a}^{x}(x - t) ^ n f ^ {(n + 1)}(t)\,dt$ is called the remainder.
    \begin{proof}
        Fix $x \in I$,
        then
        \[
        \int_{a}^{x}f'(t)\,dt = f(x) - f(a).
        \]
        Therefore $f(x) = f(a) + \int_{a}^{x}f'(t)\,dt$.
        We now use the following form of integration by parts.
        Let $g(t), h(t)$ be integrable in an interval $(a, b)$,
        and let $G(t)$ and $H(t)$ be indefinite integrals of $g(t)$ and $h(t)$ respectively on $(a, b)$,
        then
        \[
        (G H)' = G'H + H'G = gH + hG
        \]
        and integrating both sides over $(a, b)$ and rearranging
        \[
        \int_{a}^{b}g(t)H(t)\,dt = [G(t)H(t)]_{a}^{b} - \int_{a}^{b}h(t)G(t)\,dt.
        \]
        Applying this over $(a, x)$ with $G(t) = t - x$ such that $g(t) = 1$ and $H(t) = f'(t)$ such that $h(t) = f''(t)$,
        then
        \begin{align*}
            \int_{a}^{x}f'(t)\,dt &= [(t - x)f'(t)]_{a}^{x} - \int_{a}^{x}f''(t)(t - x)\,dt \\
            &= (x - a)f'(a) + \int_{a}^{x}f''(t)(x - t)\,dt
        \end{align*}
        and so $f(x) = f(a) + f'(a)(x - a) + \int_{a}^{x}f''(t)(x - t)\,dt$ which is the claimed result when $n = 1$.
        For $n = 2$ we integrate by parts again
        \begin{align*}
            f(x) &= f(a) + f'(a)(x - a) + \left[-\frac{(x - t) ^ 2}{2}f''(t)\right]_{a}^{x} - \int_{a}^{x}f'''(t)\left(\frac{-(x - t) ^ 2}{2}\right) \\
            &= f(a) + f'(a)(x - a) + f''(a)\frac{(x - a) ^ 2}{2} + \int_{a}^{x}f'''(t)\frac{(x - t) ^ 2}{2}\,dt
        \end{align*}
        proving Taylor's theorem for $n = 2$.
        We continue in this way,
        integrating by parts a total of $n$ times to prove the result.

        This can also be proved inductively.
    \end{proof}
\end{theorem}

\subsection{Taylor Polynomials}
The combination $P_n(x) = f(x) - R_n(x)$ is a polynomial in $x$ of degree $n$ called the $n$th order Taylor polynomial of $f$ about $x = a$
\[
P_n(x) = f(a) + f'(a)(x - a) + \frac{f''(a)}{2}(x - a) ^ 2 + \dotsi + \frac{f ^ {(n)}(a)}{n!}(x - a) ^ n.
\]
If the term about $x = a$ is omitted,
then we take this to mean about $x = 0$,
as this is the most common case.
The Taylor polynomial $P_n(x)$ is an approximation to $f(x)$.
Generally it is a good approximation for $x$ close to $a$,
and the approximation improves as $n$ increases.
The remainder $R_n(x)$ gives an exact expression for the error.

\begin{example}
    Calculate the $n$th order Taylor polynomial of $e ^ x$.

    We have $f(x) = e ^ x = f ^ {(k)}(x)$,
    so $f ^ {(k)}(0) = 1$,
    and hence
    \[
    P_n(x) = 1 + x + \frac{x ^ 2}{2} + \dotsi + \frac{x ^ n}{n!} = \sum_{k = 0}^{n}\frac{x ^ k}{k!}
    \]
\end{example}

If $f(x)$ is infinitely differentiable
(smooth)
on an open interval $I$ that contains the point $x = a$,
and if in addition for each $x \in I$ we have $\lim_{n \rightarrow \infty}R_n(x) = 0$,
then we say $f(x)$ can be expanded as a Taylor series about $x = a$
\[
f(x) = \sum_{k = 0}^{\infty}\frac{f ^ {(k)}(x)}{k!}(x - a) ^ k
\]
Infinite series and there convergence are covered in Analysis I.

Note that the $n$th order Taylor polynomial $P_n(x)$ is obtained by taking the first $(n + 1)$
\footnote{$(n + 1)$ because of the constant terms.}
terms of the Taylor series
(counting terms even if they're zero).

\begin{example}
    The Taylor series of $e ^ x$ is
    \[
    e ^ x = \sum_{k = 1}^{\infty}\frac{x ^ k}{k!} = 1 + x + \frac{x ^ 2}{2} + \dotsc
    \]
    Setting $x = 1$ gives a formula for $e$ as
    \[
    e = \sum_{k = 1}^{\infty}\frac{1}{k!} = 1 + 1 + \frac{1}{2} + \frac{1}{3!} + \dotsi
    \]
\end{example}

\begin{example}
    Calculate the Taylor series of $\sin(x)$.

    $f(x) = \sin(x)$,
    $f(0) = 0$,
    
    $f'(x) = \cos(x)$,
    $f'(0) = 1$,

    $f''(x) = -\sin(x)$,
    $f''(0) = 0$,
    
    $f'''(x) = -\cos(x)$,
    $f'''(0) = -1$.

    Since $f ^ {(4)}(x) = f(x)$,
    $f ^ {(k + 4)}(x) = f ^ {(k)}(x)$

    so for $k \in \N$ and so $f ^ {(2k)}(0) = 0$ and
    $f ^ {(2k + 1)}(0) = (-1) ^ k$
    and so the Taylor series of $\sin$ is
    \[
    \sin(x) = \sum_{k = 0}^{\infty}\frac{(-1) ^ k}{(2k + 1)!}x ^ {2k + 1} = x - \frac{x ^ 3}{3!} + \dotsc
    \]
    Note that only odd powers of $x$ appear,
    which is what we should expect,
    since $\sin(x)$ is an odd function.

    Similarly,
    the Taylor series of $\cos(x)$ is
    \[
    \cos(x) = \infsum[k = 0]\frac{(-1) ^ {k}}{(2k)!}x ^ {2k} = 1 - \frac{x ^ 2}{2} + \frac{x ^ 4}{4!} + \dotsc
    \]
    Which only includes even powers of $x$ since $\cos$ is even.
\end{example}

We can calculate the Taylor series of $\sinh(x) = f(x)$ by noting that for any non-negative integer $k$
\[
f ^ {(k)}(x) = \begin{cases}
    \sinh(x), & \text{even } k, \\
    \cosh(x), & \text{odd } k.
\end{cases}
\]
and so $f ^ {(2k)}(0) = 0$ and $f ^ {(2k + 1)}(0) = 1$.
Therefore
\[
\sinh(x) = \infsum[k = 0]\frac{1}{(2k + 1)!}x ^ {2k + 1} = x + \frac{x ^ 3}{3!} + \frac{x ^ 5}{5!} + \dotsc
\]
and similarly
\[
\cosh(x) = \infsum[k = 0]\frac{1}{(2k)!}x ^ {2k} = 1 + \frac{x ^ 2}{2} + \frac{x ^ 4}{4!} + \dotsc
\]
This also follows directly from the Taylor series of $e ^ x$,
since $\cosh(x)$ is the even part of $e ^ x$ and $\sinh(x)$ is the odd part.

Since $\log(x)$ is not defined at $x = 0$,
we usually consider the Taylor series about $x = 1$.
With

$f(x) = \log(x)$,
$f(1) = 0$,

$f'(x) = \frac{1}{x}$,
$f'(1) = 1$,

$f''(x) = -\frac{1}{x ^ 2}$,
$f''(1) = -1$,

$f'''(x) = \frac{2}{x ^ 3}$,
$f'''(1) = 2$,


$f ^ {(4)}(x) = \frac{-3 \cdot 2}{x ^ 4}$,
$f ^ {(4)}(1) = -6 = -3!$.

We see that for $k > 0$,
$f ^ {(k)}(x) = (-1) ^ {k - 1}\frac{(k - 1)!}{x ^ k}$,
so $f ^ {(k)}(1) = (-1) ^ {k - 1}(k - 1)!$.
The Taylor series of $\log{x}$ about $x = 1$ is therefore
\[
\log{x} = \infsum[k = 1]\frac{(-1) ^ {k - 1}(k - 1)!}{k!}(x - 1) ^ k = \infsum[k = 1]\frac{(-1) ^ {k - 1}}{k}(x - 1) ^ k.
\]
It is common to change variables with $y = x - 1$ such that
\[
\log{(y + 1)} = \infsum[k = 1]\frac{(-1) ^ {k - 1}}{k}y ^ k
\]
or renaming $y$ as $x$
\[
\log{(x + 1)} = \infsum[k = 1]\frac{(-1) ^ {k - 1}}{k}x ^ k.
\]
One can show that in order for this $\lim_{n \rightarrow \infty}R_n(x) = 0$ we must have $-1 < x \leq 1$.
So this Taylor series for $\log(1 + x)$ is valid only for $-1 < x \leq 1$
(otherwise it does not converge).

\subsection{Lagrange form for the remainder}
There is a more convenient expansion for the remainder term in Taylor's theorem.
The Lagrange form of the remainder is
\[
R_n(x) = \frac{f ^ {(n + 1)}(c)}{(n + 1)!}(x - a) ^ {n + 1}
\]
for some $c$ in $(a, x)$.
To prove this we need the following lemma:
\begin{lemma}
    Let $h(t)$ be differentiable $(n + 1)$ times on $[a, x]$,
    with $h ^ {(k)}(a) = 0$ for $0 \leq k \leq n$ and $h(x) = 0$ then $\exists c \in (a, x)$ such that $h ^ {(n + 1)}(c) = 0$.
    \begin{proof}
        We use Rolle's theorem
        (\autoref{calc_thm_rolle}),
        since $h(a) = h(x) = 0$,
        $\exists c_1 \in (a, x)$ such that $h'(c_1) = 0$.

        Then since $h'(a) = h'(c_1) = 0$,
        $\exists c_2 \in (a, c_1)$ such that $h''(c_2) = 0$.

        Applying this argument a total of $n$ times we get
        \[
        h ^ {(n)}(a) = h ^ {(n)}(c_n) = 0
        \]
        so $\exists c_{n + 1} \in (a, c_n)$ such that $h ^ {(n + 1)}(c_n) = 0$.
    \end{proof}
\end{lemma}

\begin{proof}[Proof of Lagrange form of the remainder]
    We use a similar idea to our proof of the mean value theorem.
    
    Consider $h(t) = (f(t) - P_n(t))(x - a) ^ {n + 1} - (f(x) - P_n(x))(t - a) ^ {n + 1}$.

    We have $h(x) = 0$.
    Since $P_n(t) = f(a) + f'(a)(t - a) + \frac{f''(a)}{2}(t - a) ^ 2 + \dotsi + \frac{f ^ {(n)}(a)}{n!}(t - a) ^ n$.
    Then $P_n(a) = f(a)$,
    $P'_n(a) = f'(a)$ and more generally since
    \[
    \left.\frac{d ^ k}{(dt) ^ k}(t - a) ^ {n + 1}\right|_{t = a} = 0
    \]
    for $0 \leq k \leq n$ and
    \[
    \left.\frac{d ^ {n + 1}}{(dt) ^ {n + 1}}(t - a) ^ {n + 1}\right|_{t = a} = (n + 1)!,
    \]
    \[
    P_n ^ {(k)}(a) = f ^{(k)}(a).
    \]
    Then $h ^ {(k)}(a) = 0$ for $0 \leq k \leq a$ and hence by the previous lemma $\exists c \in (a, x)$ such that $h ^ {(n + 1)}(c) = 0$.
    \[
    P_n ^ {(n + 1)}(t) = 0
    \]
    since $P_n(t)$ is degree $n$,
    and $\frac{d ^ {n + 1}}{(dt) ^ {n + 1}}(t - a) ^ {n + 1} = (n + 1)!$ hence $0 = h ^ {(n + 1)}(c) = f ^ {(n + 1)}(c)(x - a) ^ {n + 1} - (f(x) - P_n(x))(n + 1)!$ and by rearranging $f(x) = P_n(x) + \frac{f ^ {(n + 1)}(c)}{(n + 1)!}(x - a) ^ {n + 1}$.
\end{proof}

The Lagrange form of the remainder is useful for putting a bound on the error of a Taylor polynomial approximation to a function.

Suppose
\[
|f ^ {(n + 1)}(t)| \leq M\ \forall t \in [a, x].
\]
Then
\[
|R_n(x)| \leq M\frac{|x - a| ^ {n + 1}}{(n + 1)!}
\]
provides a bound on the error.
\begin{example}
    Show that the error in approximating $e ^ x$ by its sixth order Taylor polynomial is less than $0.0006$ throughout the interval $[0, 1]$.
    
    We have $f(x) = e ^ x$ and $a = 0$,
    and we want a bound on $|R_6(t)|$ for $t \in [0, 1]$.
    Then $|f ^ {(7)}(t)| = |e ^ t|$ since $e ^ t$ is monotonic increasing and positive everywhere $|e ^ t| = e ^ t \leq e ^ 1 < 3$ so take
    $M = 3$.
    Then
    \[
    |R_6(x)| \leq \frac{3|x| ^ {7}}{7!} \leq \frac{3}{7!} \text{ for } x \in [0, 1].
    \]
    Since $\frac{3}{7!} < 0.0006$ then we have the required result.
\end{example}

\subsection{Calculating limits using Taylor series}
\begin{definition}
    Let $n$ be a positive integer.
    We say that
    \[
    f(x) = o(x ^ n)\qquad\text{(as $x \rightarrow 0$)}
    \]
    if
    \[
    \lim_{x \rightarrow 0}\frac{f(x)}{x ^ n} = 0.
    \]
\end{definition}
In particular for any constant $\alpha \neq 0$,
$\alpha x ^ n = o(x ^ n) \iff m > n$.
\begin{example}\phantom{}
    \begin{itemize}
        \item $3x ^ 4 = o(x ^ 3)$.
        \item $3x ^ 4 = o(x ^ 2)$.
        \item $3x ^ 8 - x ^ 6 = o(x ^ 5)$.
        \item $o(x ^ 8) + o(x ^ 5) = o(x ^ 5)$.
    \end{itemize}
\end{example}

If we know a Taylor series is valid on some suitable open interval,
then it may be useful for calculating certain limits.

One can show that the Taylor series for $\sin$ and $\cos$ are valid for all $x \in \R$.
You may assume this to be true and use this fact.
\begin{example}
    Use Taylor series to calculate
    \[
    \lim_{x \rightarrow 0}\frac{\sin{x}}{x}.
    \]
    From the Taylor series
    \[
    \sin(x) = x - \frac{x ^ 3}{3!} + \frac{x ^ 5}{5!} + \dotsc = x + o(x ^ 2).
    \]
    Hence
    \[
    \lim_{x \rightarrow 0}\frac{\sin(x)}{x} = \lim_{x \rightarrow 0}\left(\frac{x + o(x ^ 2)}{x}\right) = \lim_{x \rightarrow 0}(1 + o(x)) = 1.
    \]
\end{example}

\begin{example}
    Use Taylor series to calculate $\lim_{x \rightarrow 0}\frac{1 - \cos(x)}{x}$.
    From the Taylor series
    \[
    \cos(x) = 1 - \frac{x ^ 2}{2} + \frac{x ^ 4}{4!} + \dotsc = 1 - \frac{x ^ 2}{2} + o(x ^ 3)
    \]
    Hence
    \begin{align*}
    \lim_{x \rightarrow 0}\frac{1 - \cos(x)}{x} &= \lim_{x \rightarrow 0}\left(\frac{1 - \left(1 - \frac{x ^ 2}{2} + o(x ^ 3)\right)}{x}\right) \\
    &= \lim_{x \rightarrow 0}\left(\frac{x}{2} + o(x ^ 2)\right) \\
    &= 0.
    \end{align*}
\end{example}

\newpage

\section{Fourier Series}

\subsection{Fourier Coefficients}
We have seen how to write and approximate functions by polynomials.
If the functions we're interested in are periodic,
it is more appropriate to use trigonometric functions.

Consider a function $f(x)$ of period $2L$
($f(x + 2L) = f(x)$)
given on the interval $(-L, L)$.

The functions $\cos\left(\frac{n\pi x}{L}\right)$ and $\sin\left(\frac{n\pi x}{L}\right)$ for $n$ any positive integer are also periodic with period $2L$,
so we can try to write $f(x)$ in terms of these functions.
Since any constant is periodic
(for any period),
we can include a constant.
We therefore aim to express $f(x)$ as
\begin{equation}\tag{FS}
    f(x) = \frac{a_0}{2} + \infsum[n = 1]\left(a_n\cos\left(\frac{n\pi x}{L}\right) + b_n\sin\left(\frac{n\pi x}{L}\right)\right)
\end{equation}
where $a_0, a_n$ and $b_n$ are constants labelled by the positive integer $n$,
and these are called the Fourier coefficients of $f(x)$.

If the constants are such that the series converges,
then this is called the Fourier series of $f(x)$.

\begin{example}
    \[
    f(x) = 2\cos ^ 2\left(\frac{\pi x}{L}\right) + 3\sin\left(\frac{\pi x}{L}\right)
    \]
    has period $2L$.
    Since $\cos(2x) = 2\cos ^ 2(x) - 1$
    \[
    f(x) = 1 + \cos\left(\frac{2\pi x}{L}\right) + 3\sin\left(\frac{\pi x}{L}\right)
    \]
    is the Fourier series of $f(x)$ containing only a finite number of terms,
    with $a_0 = 2$,
    $a_2 = 1$,
    $b_1 = 3$,
    and all other Fourier coefficients are zero.
\end{example}

In general,
an infinite number of coefficients may be non-zero,
so we need a method to determine them from a given $f(x)$.

We first need to prove the following identities:
\begin{lemma}
    Let $m, n \in \N$,
    then
    \begin{enumerate}[label = \roman*)]
        \item
        \[
        \frac{1}{L}\int_{-L}^{L}\cos\left(\frac{n\pi x}{L}\right)\,dx = 0.
        \]
        \item
        \[
        \frac{1}{L}\int_{-L}^{L}\sin\left(\frac{n\pi x}{L}\right)\,dx = 0.
        \]
        \item
        \[
        \frac{1}{L}\int_{-L}^{L}\sin\left(\frac{n\pi x}{L}\right)\cos\left(\frac{n\pi x}{L}\right)\,dx = 0.
        \]
        \item
        \[
        \frac{1}{L}\int_{-L}^{L}\cos\left(\frac{n\pi x}{L}\right)\cos\left(\frac{m\pi x}{L}\right)\,dx = \begin{cases}
            0 & \text{if } m \neq n \\
            1 & \text{if } m = n.
        \end{cases}
        \]
        \item
        \[
        \frac{1}{L}\int_{-L}^{L}\sin\left(\frac{n\pi x}{L}\right)\sin\left(\frac{m\pi x}{L}\right)\,dx = \begin{cases}
            0 & \text{if } m \neq n \\
            1 & \text{if } m = n.
        \end{cases}
        \]
    \end{enumerate}
    \fbox{
    \begin{minipage}{\textwidth}
        \centering
        It is convenient to introduce the Kronecker delta function
        \[
        \delta_{mn} = \begin{cases}
                0 & \text{if } m \neq n \\
                1 & \text{if } m = n.
            \end{cases}
        \]
        then for example iv) can be written as
        \[
        \frac{1}{L}\int_{-L}^{L}\cos\left(\frac{n\pi x}{L}\right)\cos\left(\frac{m\pi x}{L}\right)\,dx = \delta_{mn}
        \]
    \end{minipage}
    }
    \begin{proof}\phantom{}
        \begin{enumerate}[label = \roman*)]
            \item Is simple
            \[
            \frac{1}{L}\int_{-L}^{L}\cos\left(\frac{n\pi x}{L}\right)\,dx = \frac{1}{L}\left[\frac{L}{n\pi}\sin\left(\frac{n\pi x}{L}\right)\right]_{-L}^{L} = 0.
            \]
            ii) and iii) are immediate as the integrals of odd functions over symmetric integrals.
            \item[iv)]
            Recall $\cos(x \pm y) = \cos(x)\cos(y) \mp \sin(x)\sin(y)$
            \[
            \cos(x + y) + \cos(x - y) = 2\cos(x)\cos(y).
            \]
            Then,
            first assume $m \neq n$
            \begin{align*}
                \frac{1}{L}\int_{-L}^{L}\cos\left(\frac{m\pi x}{L}\right)\cos\left(\frac{n\pi x}{L}\right)\,dx &= \frac{1}{L}\int_{-L}^{L}\left(\cos\left(\frac{(m + n)\pi x}{L}\right) + \cos\left(\frac{(m - n)\pi x}{L}\right)\right)\,dx \\
                &= \frac{1}{2L}\left[\frac{L}{(m + n)\pi}\sin\left(\frac{(m + n)\pi x}{L}\right) + \right. \\
                &\left.\frac{L}{(m - n)\pi}\sin\left(\frac{(m - n)\pi x}{L}\right)\right]_{-L}^{L} \\
                &= 0.
            \end{align*}
            If $m = n$
            \begin{align*}
                \frac{1}{L}\int_{-L}^{L}\cos\left(\frac{m\pi x}{L}\right)\,dx &= \frac{1}{L}\int_{-L}^{L}\cos ^ 2\left(\frac{n\pi x}{L}\right)\,dx \\
                &= \frac{1}{2L}\int_{-L}^{L}\left(1 + \cos\left(\frac{2n\pi x}{L}\right)\right)\,dx \\
                &= \frac{1}{2L}\left[x + \frac{L}{2n\pi}\sin\left(\frac{2n\pi x}{L}\right)\right]_{-L}^{L} \\
                &= 1
            \end{align*}
            \item [v)] Similar
            (\textbf{exercise})
        \end{enumerate}
    \end{proof}
\end{lemma}

We say the set of functions $\left\{1, \cos\left(\frac{n\pi x}{L}\right), \sin\left(\frac{n\pi x}{L}\right)\right\}$ form an orthogonal
(orthonormal)
set on $[-L, L]$,
because if $\phi, \psi$ are different elements of the set,
then $\frac{1}{L}\int_{-L}^{L}\phi\psi\,dx = 0$.

We can use these identities to determine the Fourier coefficients in (FS).
Firstly,
by integrating (FS)
\begin{align*}
    \frac{1}{L}\int_{-L}^{L}f(x)\,dx &= \frac{1}{L}\int_{-L}^{L}\left(\frac{a_0}{2} + \infsum[n = 1]\left[a_n\cos\left(\frac{n\pi x}{L}\right) + b_n\sin\left(\frac{n\pi x}{L}\right)\right]\right)\,dx \\
    &= \frac{1}{L}\left[\frac{a_0x}{2}\right]_{-L}^{L} = a_0
\end{align*}
by identities i) and ii).

Therefore
\[
a_0 = \frac{1}{L}\int_{-L}^{L}f(x)\,dx.
\]
or $a_0 = \langle 1, f(x) \rangle$ where
\[
\langle f, g \rangle = \frac{1}{L}\int_{-L}^{L}fg\,dx.
\]

Now,
letting $m$ be a positive integer,
multiplying (FS) by $\cos\left(\frac{m\pi x}{L}\right)$ and integrating
\begin{align*}
    \frac{1}{L}\int_{-L}^{L}f(x)\cos\left(\frac{m\pi x}{L}\right)\,dx &= \frac{1}{L}\int_{-L}^{L}\left(\frac{a_0}{2} + \infsum[n = 1]\left[a_n\cos\left(\frac{n\pi x}{L}\right) + b_n\sin\left(\frac{n\pi x}{L}\right)\right]\right)\cos\left(\frac{m\pi x}{L}\right)\,dx \\
    &= \infsum[n = 1]a_n\delta_{mn} = a_m
\end{align*}
using i),
iii) and iv).
Therefore 
\[
a_n = \frac{1}{L}\int_{-L}^{L}f(x)\cos\left(\frac{n\pi x}{L}\right)\,dx
\]
or $a_n = \langle a_n, \cos\left(\frac{n\pi x}{L}\right) \rangle$.

\textit{Note: that with $n = 0$ this also gives the correct expression for $a_0$.}

Similarly,
multiplying (FS) by $\sin\left(\frac{m\pi x}{L}\right)$ and integrating gives
\begin{align*}
    \frac{1}{L}\int_{-L}^{L}f(x)\sin\left(\frac{m\pi x}{L}\right)\,dx &= \frac{1}{L}\int_{-L}^{L}\left(\frac{a_0}{2} + \infsum[n = 1]\left[a_n\cos\left(\frac{n\pi x}{L}\right) + b_n\sin\left(\frac{n\pi x}{L}\right)\right]\right)\sin\left(\frac{m\pi x}{L}\right)\,dx \\
    &= \infsum[n = 1]b_n\delta_{mn} = b_m
\end{align*}
using ii),
iii) and v).
So
\[
b_n = \frac{1}{L}\int_{-L}^{L}f(x)\sin\left(\frac{n\pi x}{L}\right)\,dx
\]

\subsection{Examples of Fourier Series}
\begin{example}
    The function $f(x)$ has period $2$
    (i.e. $f(x + 2) = f(x)$)
    and is given by $f(x) = |x|$ for $-1 < x < 1$.

    \begin{solution}
        
        To calculate the Fourier series of $f(x)$,
        we compute the Fourier coefficients using our previous results with $L = 1$.
        \[
        a_0 = \int_{-1}^{1}f(x)\,dx = \int_{-1}^{1}|x|\,dx = 2\int_{0}^{1}x\,dx = \left[x ^ 2\right]_{0}^{1} = 1
        \]
        for $n > 0$
        \begin{align*}
            a_n &= \int_{-1}^{1}f(x)\cos(n\pi x)\,dx \\
            &= \int_{-1}^{1}|x|\cos(n\pi x)\,dx \\
            &= 2\int_{0}^{1}x\cos(n\pi x)\,dx \\
            &= 2\left(\left[\frac{1}{n\pi}\sin(n\pi x)\right]_{0}^{1} - \int_{0}^{1}\frac{1}{n\pi}\sin(n\pi x)\right)\,dx \\
            &= \frac{2}{n ^ 2\pi ^ 2}\left[\cos(n\pi x)\right]_{0}^{1} \\
            &= \frac{2}{n ^ 2\pi ^ 2}(\cos(n\pi) - 1) \\
            &= \frac{2}{n ^ 2\pi ^ 2}((-1) ^ n - 1).
        \end{align*}
        \[
        b_n = \int_{-1}^{1}|x|\sin(n\pi x)\,dx = 0
        \]
        since this is the integral of an odd function over a symmetric interval.
        
        In general if $f(x)$ is even on $(-L, L)$,
        then all $b_n = 0$ and the Fourier series contains only cosine terms
        (plus a constant term).
        Such a series is called a cosine series.
        
        Putting the above together,
        we have the Fourier
        (cosine)
        series for $f(x)$,
        \begin{align*}
            f(x) &= \frac{a_0}{2} + \infsum[n = 1]a_n\cos\left(\frac{n\pi x}{L}\right) + b_n\sin\left(\frac{n\pi x}{L}\right) \\
            &= \frac{1}{2} + \infsum[n = 1]\frac{2((-1) ^ n - 1)}{n ^ 2\pi ^ 2}\cos(n\pi x) \\
            &= \frac{1}{2} - \frac{4}{\pi ^ 2}\left(\cos(\pi x) + \frac{1}{9}\cos(3\pi x) + \frac{1}{25}\cos(5\pi x) + \dotsc\right).
        \end{align*}
        Note that in this example $a_{2n} = 0$,
        $a_{2n - 1} = \frac{-4}{(2n - 1) ^ 2\pi ^ 2}$.
        So we can also write the Fourier series as
        \[
        f(x) = \frac{1}{2} - \frac{4}{\pi ^ 2}\infsum[n = 1]\frac{\cos((2n - 1)\pi x)}{(2n - 1) ^ 2}.
        \]
    \end{solution}
\end{example}

\newpage

\section{Many-Variable Calculus}

\subsection{Plotting functions}
We are used to plotting functions of one variable $y = f(x)$ as a
(one-dimensional)
curve $C$ in the two-dimensional plane.

It is also possible to define functions 'implicitly' via equations such as $x ^ 2 + y ^ 2 = 1$.

We can plot a function of two variables $z = f(x, y)$ as a two dimensional surface $S$ embedded in three dimensions.

It is similarly possible to define functions of two variables $z = z(x, y)$ implicitly and plot them through an equation such as $x ^ 2 + y ^ 2 + z ^ 2 = 1$.

More generally we can extend this to a function of $n$ variables,
$x_{n + 1} = f(x_1, x_2, \dotsc, x_n)$ as defining an $n$-dimensional surface embedded in $\R ^ {n + 1}$ parametrically
\[
(x_1, x_2, \dotsc, x_n, x_{n + 1}) = (x_1, x_2, \dotsc, x_n, f(x_1, \dotsc, x_n)).
\]

\subsection{Continuity and Differentiability of functions of one and more variables}

\subsubsection{Continuity for \texorpdfstring{$f(x)$}{}}
A function $f(x)$ $f : D \rightarrow \R$,
$D \subseteq \R$ is continuous at $x_0 \in D$ if and only if

\textbf{In words:}
We can make a change in $f$,
$\delta f = |f(x) - f(x_0)|$ as small as we like by choosing $x$ close to $x_0$.

\textbf{Mathematically:}
\begin{definition}[Continuity]
    $f(x)$ is continuous at $x = x_0$ if $\forall \varepsilon > 0\,\exists \delta > 0$ such that if $x \in D$ and $|x - x_0| < \delta \implies \delta f = |f(x) - f(x_0)| < \varepsilon$
\end{definition}

\subsubsection{Continuity for \texorpdfstring{$f(x, y)$}{}}
Let $f(x, y)$ $f: D \rightarrow\R$ $D \subseteq \R ^ 2$.
$f(x, y)$ is continuous at $(x_0, y_0) = \mbf{x}_0$ if and only if

\textbf{In words:}
As before We can make a change in $f$,
$\delta f = |f(\mbf{x}) - f(\mbf{x}_0)|$ as small as we like by choosing $\mbf{x}$ close to $\mbf{x}_0$.

The subtlety is what is closes?
We take $|\mbf{x} - \mbf{x}_0| = |(x, y) - (x_0, y_0) = \sqrt{(x - x_0) ^ 2 + (y - y_0) ^ 2}$.

\begin{definition}[Continuity for functions of multiple variables]
    The function $f(x, y)$ $f : D \rightarrow \R$ $D \subseteq \R ^ 2$ is continuous at $\mbf{x}_0 = (x_0, y_0)$ if and only if $\forall \varepsilon > 0\,\exists \delta > 0$ such that if $\mbf{x} \in D$ and $|\mbf{x} - \mbf{x}_0| < \delta \implies \delta f = |f(\mbf{x}) - f(\mbf{x}_0)| < \varepsilon$.
\end{definition}

\begin{example}
    Show that $f(x, y) = x ^ 2 + y ^ 2$ is continuous at $\mbf{x}_0 = (0, 0)$.
    \begin{solution}
        \[
        \delta f = |f(\mbf{x}) - f(\mbf{x}_0)| = |x ^ 2 + y ^ 2 - 0| = |\mbf{x} ^ 2| = |\mbf{x} - \mbf{x}_0| ^ 2.
        \]
        If we choose $|\mbf{x} - \mbf{x}_0| < \delta \implies |f(\mbf{x}) - f(\mbf{x}_0)| < \delta ^ 2 < \varepsilon$.
        Choose $\delta ^ 2 < \varepsilon$.
    \end{solution}
\end{example}

\subsubsection{Differentiability for functions of one variable}
A function $f(x)$ $f : D \rightarrow \R$ $D \subseteq \R$ is differentiable at $x = x_0$ if.

\textbf{In words:}
All points close to $x_0$ are in $D$.
When we "zoom" into the function near $x = x_0$ it looks like a
(non-vertical)
straight line.

\begin{definition}[Differentiability]
    The function $f(x)$ $f : D \rightarrow \R$ $D \subseteq \R$ is differentiable at $x_0 \in D$ if and only if
    \begin{enumerate}[label = \arabic*)]
        \item $\exists \delta > 0\,:\,|x - x_0| < \delta \implies x \in D$.
        \item The function can be written as
        \[
        f(x) = f(x_0) + M(x - x_0) + R(x)
        \]
        where $M$ is a constant and $\lim_{x \rightarrow x_0}\frac{R(x)}{|x - x_0|} = 0$.
    \end{enumerate}
\end{definition}

$\delta f = f(x) - f(x_0) = M(x - x_0) + R(x)$
\begin{align*}
    \lim_{x \rightarrow x_0}\frac{f(x) - f(x_0)}{x - x_0} &= M + \lim_{x \rightarrow x_0}\frac{R(x)}{x - x_0} \\
    \intertext{If $f(x)$ differentiable at $x = x_0$} \\
    &= M + 0 = M.
\end{align*}
$x = x_0 + \delta x$,
this implies $M = \lim_{\delta x \rightarrow 0}\frac{f(x_0 + \delta x) - f(x_0)}{\delta x} = \frac{df}{dx}(x_0)$.
Substitute $M = \frac{df}{dx}(x_0)$
\[
f(x) = \underbrace{f(x_0) + \frac{df}{dx}(x_0)(x - x_0)}_{\text{$2$ terms of Taylor series}} + R(x)
\]

\subsubsection{Differentiability of functions of two variables \texorpdfstring{$f(x, y)$}{}}
\textbf{In words}:

A function $f(x, y)$ is differentiable at $(x_0, y_0) = \mbf{x}_0$ if when we zoom in it looks like its tangent plane.
\[
z = f(x_0, y_0) + M(x - x_0) + N(y - y_0)
\]

\begin{definition}
    The function $f(x, y) : D \rightarrow \R$
    $D \subseteq \R ^ 2$ is differentiable at $\mbf{x}_0 = (x_0, y_0)$ if and only if
    \begin{enumerate}[label = (\roman*)]
        \item $\exists \delta > 0$ if $|\mbf{x} - \mbf{x}_0| < \delta$ then $\mbf{x} \in D$
        
        \item $\underbrace{f(x, y) = f(x_0, y_0) + M(x - x_0) + N(y - y_0)}_{\text{tangent plane}} + R(x, y)$.
        Where $M, N$ are constant and $\lim_{\mbf{x} \rightarrow \mbf{x}_0}\frac{R(x, y)}{|\mbf{x} - \mbf{x}_0|} \rightarrow 0$.
    \end{enumerate}
\end{definition}

\begin{example}
    Show that $f(x, y) = x ^ 2 + y ^ 2$ is differentiable at $\mbf{x}_0 = (x_0, y_0) = (1, 1)$.
    \begin{solution}
        \begin{align*}
            f(x, y) &= ((x - 1) + 1) ^ 2 + ((y - 1) + 1) ^ 2 \\
            &= (x - 1) ^ 2 + 2(x - 1) + 1 + (y - 1) ^ 2 + 2(y - 1) + 1 \\
            &= 2 + 2(x - 1) + 2(y - 1) + (x - 1) ^ 2 + (y - 1) ^ 2 \\
            &= f(1, 1) + M(x - 1) + N(y - 1) + R(x)
        \end{align*}
        With $2 = f(x, y)$,
        $2 = M = N$,
        $(x - 1) ^ 2 + (y - 1) ^ 2 = R(n)$.
        \[
        R(x, y) = (x - 1) ^ 2 + (y - 1) ^ 2 = \left|(x, y) - (1, 1)\right| ^ 2 = \left|\mbf{X} -  \mbf{x}_0\right| ^ 2
        \]
        \[
        \lim_{\mbf{x} \rightarrow \mbf{x}_0}\frac{R(x, y)}{|\mbf{x} - \mbf{x}_0|} = 
        \lim_{\mbf{x} \rightarrow \mbf{x}_0}\frac{|\mbf{x} - \mbf{x}_0| ^ 2}{|\mbf{x} - \mbf{x}_0|} = |\mbf{x} - \mbf{x}_0| = 0.
        \]
    \end{solution}
\end{example}

\subsection{Partial Derivatives}
In $1$D the derivative $\frac{df}{dx}(x_0)$ is the gradient of the function at $x = x_0$.
\[
f(x) = f(x_0) + \frac{df}{dx}(x_0)(x - x_0) + R(x)
\]
\[
\delta f = f(x) - f(x_0) = \frac{df}{dx}(x_0)(x - x_0) + R(x)
\]
\[
x = x_0 + \delta x
\]
$\delta f = \frac{\delta f}{\delta x}(x_0)\delta x + R(x)$
if differentiable $R(x) \rightarrow 0$ faster than $\delta x$
\[
\delta f \approx \frac{df}{dx}\delta x.
\]

In $2$D
\[
\delta f = f(x, y) - f(x_0, y_0) = M(x - x_0) + N(y - y_0) + R(x, y)
\]
if differentiable at $(x_0, y_0)$.
$x = x_0 + \delta x$,
$y = y_0 + \delta y$.
\[
\delta f = M\delta x + N\delta y + R(x, y)
\]
$R(x, y) \rightarrow 0$ faster than $\delta x, \delta y$ as $\delta x, \delta y \rightarrow 0$.
\[
\delta f \approx M\delta x + N\delta y.
\]
In two or more dimensions the slope we experience depends on which direction we move in.
Suppose we move parallel to the $x$-axis $\mbf{x} = (x_0 + \delta x, y_0)$
$\delta y = 0$
\begin{align*}
    \delta f &= f(x_0 + \delta x, y_0) - f(x_0, y_0) \\
    &= M\delta x + N\delta y + R(x_0 + \delta x, y_0) \\
    &= M\delta x + R(x_0 + \delta x, y_0).
\end{align*}
\[
\lim_{\delta x \rightarrow 0}\frac{f(x_0 + \delta x, y_0) - f(x_0, y_0)}{\delta x} = M + \lim_{\delta x \rightarrow 0}R(x_0 + \delta x, y_0) = \frac{R(x, y)}{|\mbf{x} - \mbf{x}_0|} = 0
\]
since differentiable.
\begin{definition}[Partial derivative]
    \[
    \frac{\partial f}{\partial x}(x_0, y_0) = \lim_{\delta x \rightarrow 0}\frac{f(x_0 + \delta x, y_0) - f(x_0, y_0)}{\delta x} = f_x(x_0, y_0) = \partial_xf(x_0, y_0).
    \]
\end{definition}
This limit coincides with the ordinary derivative of $F(x) = f(x, y_0)$ rules still apply
\[
\frac{\partial}{\partial x}(fg) = \frac{\partial f}{\partial x}g + f\frac{\partial g}{\partial x}.
\]
Similarly we can define
\[
\frac{\partial y}{\partial y}(x_0, y_0) = \lim_{\delta y \rightarrow 0}\frac{f(x_0, y_0 + \delta y) - f(x_0, y_0)}{\delta y} = f_y = \delta_yf.
\]

\subsection{Higher Derivatives}
If $f(x, y)$ is differentiable then can find $f_x, f_y$,
and in turn $f_x, f_y$ may be differentiable,
in which case can differentiate again.
\[
\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}\right) = \frac{\partial ^ 2f}{\partial x ^ 2} = (f_x)_x = f_{xx}.
\]
\[
\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}\right) = \frac{\partial ^ 2f}{\partial y ^ 2} = (f_y)_y = f_{yy}.
\]
\[
\frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right) = \frac{\partial ^ 2f}{\partial y \partial x} = (f_x)_y = f_{xy}.
\]
\[
\frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}\right) = \frac{\partial ^ 2f}{\partial x \partial y} = (f_y)_x = f_{yx}.
\]

\begin{theorem}
    Let $f(x_1, x_2, \dotsc, x_n)$ be a function $f : \R ^ n \rightarrow \R$.
    If $\frac{\partial ^ 2f}{\partial x_i \partial y_j}$ $1 \leq i, j \leq n$ all exist and are continuous in some neighbourhood.
    \[
    \mbf{x} = \mbf{a}.
    \]
    Then
    \[
    \frac{\partial ^ 2f(\mbf{a})}{\partial x_i \partial x_j} = \frac{\partial ^ 2f(\mbf{a})}{\partial x_j \partial x_i}
    \]
\end{theorem}

\begin{example}
    \[
    f(x, y) = x ^ 2 + 3xy\ln(x + y).
    \]
    \begin{align*}
        f_x &= 2x + 3y\ln(x + y) + \frac{3xy}{x + y} \\
        f_y &= 3x\ln(x + y) + \frac{3xy}{x + y} \\
        \frac{\partial}{\partial y}(f_x) = f_{xy} &= 3\ln(x + y) + \frac{3y}{x + y} + \frac{3x}{x + y} - \frac{3xy}{(x + y) ^ 2} \\
        \frac{\partial}{\partial x}(f_y) = f_{yx} &= 3\ln(x + y) + \frac{3x}{x + y} + \frac{3y}{x + y} - \frac{3xy}{(x + y) ^ 2}
    \end{align*}
    \[
    f_{xy} = f_{yx}.
    \]
\end{example}

In general,
for suitably smooth functions order of differentiation doesn't matter
\[
f_{xyxyzx} = f_{xxyyzz}.
\]

\subsection{The Chain Rule}
In one dimension $(f(x))$
\[
\delta f = f(x + \delta x) - f(x) = \frac{df}{dx}\delta x + R(x)
\]
where $R(x)$ goes to zero faster than $\delta x$ as $\delta x \rightarrow 0$.
$\delta x$ small
\[
\delta f \approx \frac{df}{dx}\delta x
\]
\[
df = \frac{df}{dx}\,dx
\]
$df$ is the differential.

Suppose $y$ is a function of $x$ and $x$ in turn is a function of $t$ can think of $y$ as a function of $t$ $y(x(t))$.
\begin{equation}\label{calc:eq:1}
    dy = \frac{dy}{dt}\,dt.
\end{equation}
Can think of $y$ as a function of $x$
\begin{equation}\label{calc:eq:2}
    dy = \frac{dy}{dx}\,dx
\end{equation}
$x$ is a function of $t$
\begin{equation}\label{calc:eq:3}
    dx = \frac{dx}{dt}\,dt.
\end{equation}
Substitute \eqref{calc:eq:3} into \eqref{calc:eq:2}
\[
dy = \frac{dy}{dx}\left(\frac{dx}{dt}\,dt\right) = \left(\frac{dy}{dx}\frac{dx}{dt}\right)\,dt
\]
Comparing with \eqref{calc:eq:1}
\[
\frac{dy}{dt} = \frac{dy}{dx}\frac{dx}{dt}.
\]

\begin{example}
    \[
    y = x ^ 2\qquad x = \sin(t)
    \]
    \[
    \frac{dy}{dx} = 2x\qquad \frac{dx}{dt} = \cos(t)
    \]
    \[
    \frac{dy}{dt} = \frac{dy}{dx}\frac{dx}{dt} = 2x\cos(t) = 2\sin(t)\cos(t) = \sin(2t).
    \]
\end{example}

\begin{example}[2D polars]
    \begin{align*}
        x &= r\cos(\theta) & r = \sqrt{x ^ 2 + y ^ 2} \\
        y &= r\sin(\theta) & \theta = \arctan\left(\frac{y}{x}\right) \\
    \end{align*}
    \begin{align*}
        \frac{\partial x}{\partial r} &= \frac{\partial}{\partial r}(rcos(\theta)) = \cos(\theta) \\
        \frac{\partial r}{\partial x} &= \frac{\partial}{\partial x}(\sqrt{x ^ 2 + y ^ 2}) = \frac{1}{2\sqrt{x ^ 2 + y ^ 2}}2x = \frac{x}{\sqrt{x ^ 2 + y ^ 2}} = \frac{x}{r}.
    \end{align*}
    \[
    \frac{\delta r}{\delta x} \approx \cos(\theta)
    \]
\end{example}

\subsection{Chain rule for \texorpdfstring{$f(x, y)$}{}}
\[
\delta f = f(x + \delta xm y + \delta y) - f(x, y) = \frac{\partial f}{\partial x}\delta x +  \frac{\partial f}{\partial y}\delta y + R(x, y)
\]
where $R(x, y)$ goes to $0$ faster than $\delta x, \delta y$ as $\|\delta\mbf{x}\| \rightarrow 0$.
\[
\delta f \approx \frac{\partial f}{\partial x}\delta x + \frac{\partial f}{\partial y}\delta y
\]
\[
df = \frac{\partial f}{\partial x}\,dx + \frac{\partial f}{\partial y}\,dy
\]

\textbf{Situation $1$:}
Find $\frac{df}{dt}$ when $f(x, y)$ and $(x(t), y(t))$ i.e. move along a curve parametrically.
$f$ can be thought of as a function of $t$ $f(x(t), y(t))$
\[
df = \frac{df}{dt}dt
\]
\[
df = \frac{\partial f}{\partial x}\,dx + \frac{\partial f}{\partial y}\,dy
\]
$x, y$ functions of $t$
\[
dx = \frac{dx}{dt}\,dt\qquad dy = \frac{dy}{dt}\,dt.
\]
Using substitution
\[
df = \frac{\partial f}{\partial x}\left(\frac{dx}{dt}\,dt\right) + \frac{\partial f}{\partial y}\left(\frac{dy}{dt}\,dt\right) = \left(\frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt}\right)\,dt
\]
Compare with the first equation
\[
\frac{df}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt}.
\]

\begin{example}
    $H(x, y) = e ^ {-x ^ 2 - y ^ 2}$,
    $(x(t), y(t)) = (t, t ^ 2)$.
    What is $\frac{dH}{dt}$ at $t = 1$?
    \begin{solution}
        \textbf{Method 1 (direct substitution)}
        \begin{align*}
            H &= e ^ {-t ^ 2 - (t ^ 2) ^ 2} = e ^ {-t ^ 2 - t ^ 4} \\
            \frac{dH}{dt} &= (-2t - 4t ^ 3)e ^ {-t ^ 2 - t ^ 4} = -6e ^ {-2}.
        \end{align*}

        \textbf{Method 2 (Chain rule)}
        \begin{align*}
            \frac{dH}{dt} &= \frac{\partial H}{\partial x}\frac{dx}{dt} + \frac{\partial H}{\partial y}\frac{dy}{dt} \\
            &= (-2xe ^ {-x ^ 2 - y ^ 2}) + (-2ye ^ {-x ^ 2 - y ^ 2})2t \\
            \intertext{At $t = 1$} \\
            \frac{dH}{dt} &= (-2e ^ {-2}) + (-2e ^ {-2}) \cdot 2\\
            &= -6e ^ {-2}.
        \end{align*}
    \end{solution}
\end{example}

\textbf{Situation $2$:}
Change in function $f(x, y)$ when we move along a curve $y = y(x)$.

$\frac{df}{dx}$ is the total derivative.
$f$ can be thought of as a function of $f \times f(x, y(x))$ .
\begin{equation}\label{calc:eq:4}
    df = \frac{df}{dx}\,dx
\end{equation}
$f$ can also be thought of a function of $x$ and $y$.
\[
df = \pd[f]{x}\,dx + \pd[f]{y}\,dy.
\]
\[
dy = \frac{dy}{dx}\,dx.
\]
Substituting our expression for $dy$ into the previous equation we get
\begin{align*}
    df &= \pd[f]{x}\,dx + \pd[f]{y}\left(\frac{dy}{dx}\,dx\right) \\
    &= \left(\pd[f]{x} + \pd[f]{y}\frac{dy}{dx}\right)\,dx
\end{align*}
Comparing with \eqref{calc:eq:4}
\[
\frac{df}{dx} = \pd[f]{x} + \pd[f]{y}\frac{dy}{dx}.
\]
\begin{example}
    An ellipse,
    $C$,
    is given by the equation $f \equiv x ^ 2 + y ^ 2 + xy = 7$.
    Find the gradient $\frac{dy}{dx}$ of $C$ at the point $(1, 2)$.

    \begin{solution}
        $\frac{df}{dx} = 0$ along $C$ since $f = 7$ is constant
        \[
        0 = \frac{df}{dx} = \pd[f]{x} + \pd[f]{y}\frac{dy}{dx} \\
        \]
        \begin{align*}
            \frac{dy}{dx} &= -\frac{-\pd[f]{x}}{\pd[f]{y}} \\
            &= -\frac{2x + y}{2y + x}
        \end{align*}
        when at $(1, 2)$ the gradient is
        \[
        -\frac{4}{5}.
        \]
    \end{solution}
\end{example}

\subsection{Directional Derivatives}
Directional derivative tells me the slope of $f$ when I move in the direction of the unit vector $\mbf{\hat{n}} = (n_1, n_2)$ in the $x$-$y$ plane.
This is the same as the derivative with respect to $t$ along the curve $(x(t), y(t)) = (x_0 + n_1t, y_0 + n_2t)$.

From situation $1$
\[
\frac{df}{dt} = \pd[f]{x}\frac{dx}{dt} + \pd[f]{y}\frac{dy}{dt} = \pd[f]{x}n_1 + \pd[f]{y}n_2 = \left(\pd[f]{x}, \pd[f]{y}\right)\cdot (n_1, n_2) = \nabla f \cdot \mbf{\hat{n}}.
\]
\[
\nabla f = \left(\pd[f]{x}, \pd[f]{y}\right)
\]
is called the gradient of $f$.
If $f$ is a function of $n$ variables $f(x_1, x_2, \dotsc, x_n)$
\[
\nabla f = \left(\pd[f]{x_1}, \pd[f]{x_2}, \dotsc, \pd[f]{x_n}\right).
\]
If we want the slope in the direction of $\mbf{v}$ where $\mbf{v}$ is not a unit vector
\[
\nabla f \cdot \frac{\mbf{v}}{|\mbf{v}|}.
\]

\begin{example}
    Calculate the slope of the function $f(x, y) = e ^ {-x ^ 2 - y ^ 2}$ at the point $(1, 0)$ in the direction $(1, 1)$.

    \begin{solution}
        First work out the directional derivative
        \[
        \nabla f = \left(\pd[f]{x}, \pd[f]{y}\right) = \left(-2xe ^ {-x ^ 2 - y ^ 2}, -2ye ^ {-x ^ 2 - y ^ 2}\right)
        \]
        so $\nabla f$ at the point $(1, 0)$ is $(-2e ^ {-1}, 0)$.
        \begin{align*}
            \nabla f \cdot \frac{\mbf{v}}{|\mbf{v}|} &= \frac{(-2e ^ {-1}, 0) \cdot (1, 1)}{\sqrt{1 ^ 2 + 1 ^ 2}} \\
            &= -\sqrt{2}e ^ {-1}.
        \end{align*}
    \end{solution}
\end{example}

We can write
\[
\nabla f \cdot \mbf{\hat{n}} = |\nabla f||\mbf{\hat{n}}|\cos(\theta) = |\nabla f|\cos(\theta).
\]
$-|\nabla f| \leq \nabla f \cdot n \leq |\nabla f|$,
the directional derivative is a maximum when $\nabla f$ and $\mbf{\hat{n}}$ point in the same direction,
i.e. $\nabla f$ points in the direction you move in $(x, y)$ plane to experience maximum slope.

If $\mbf{hat{n}}$ is tangent to level set $f = c$ then $\nabla f \cdot \mbf{\hat{n}} = 0$
(as slope vanishes on level set)
therefore $\nabla f$ is normal to level set $f = c$.

\begin{example}
    Find the tangent plane to the $2$D surface $f(x, y, z) \equiv x ^ 4 + y ^ 2x ^ 2 + yz + z ^ 2 = 13$ at the point $(1, 2, 2)$.
    \begin{solution}
        $\nabla f$ is normal to surface $f = 13$ at $(1, 2, 2)$.
        \[
        \nabla f = \left(\pd[f]{x}, \pd[f]{y}, \pd[f]{z}\right) = \left(4x ^ 3 + 2xy ^ 2, 2yx ^ 2 + z, y + 2z\right)
        \]
        at the point $(1, 2, 2)$ $\nabla f = (12, 6, 6)$.
        $\nabla f$ is also normal to the tangent plane
        \[
        12x + 6y + 6z = c.
        \]
        $(x, y, z) = (1, 2, 2)$ lies on the tangent plane
        \[
        12 + 12 + 12 = c = 36
        \]
        hence
        \[
        12x + 6y + 6z = 36 \iff 2x + y + z = 6.
        \]
    \end{solution}
\end{example}

\subsection{Taylor's Theorem}

\begin{example}
    Find Taylor series of $f(x, y) = x + \sin(x + y)$ around the point $(x_0, y_0) = \left(\frac{\pi}{2}, 0\right)$ to quadratic order.

    \begin{solution}
        $f(x_0, y_0) = f\left(\frac{\pi}{2}, 0\right) = \frac{\pi}{2} + \sin\left(\frac{\pi}{2} + 0\right) = \frac{\pi}{2} + 1$.
        \begin{align*}
            f_x &= 1 + \cos(x + y) = 1 + 0 = 1 \\
            f_y &= \cos(x + y) = 0 \\
            f_{xx} &= -\sin(x + y) = -1 \\
            f_{xy} &= -\sin(x + y) = -1 \\
            f_{yy} &= -\sin(x + y) = -1
        \end{align*}

        \[
        f(x, y) = \frac{\pi}{2} + 1 + \left(x - \frac{\pi}{2}\right) + \frac{1}{2}\left(-\left(x - \frac{\pi}{2}\right) - 2\left(x - \frac{\pi}{2}\right)y - y ^ 2\right) + \dotsc.
        \]
    \end{solution}
\end{example}

\subsection{Stationary Points}

In $1$D for $f(x)$,
$f(x)$ has a maximum or minimum when $\frac{df}{dx} = 0$.
\begin{example}
    \[
    f(x) = 1 + x ^ 2.
    \]
    
    \begin{solution}
        $\frac{df}{dx} = 2x = 0$ when $x = 0$.
    \end{solution}
\end{example}

In $2$D for $f(x, y)$,
stationary points come in three categories.

\begin{example}
    $f(x, y) = 1 + x ^ 2 + y ^ 2$,
    this has a minimum.

    $f(x, y) = 1 - x ^ 2 - y ^ 2$,
    has a maximum.

    $f(x, y) = 1 + x ^ 2 - y ^ 2$,
    this has a saddle point.
\end{example}

At a stationary point the slope vanishes,
$\nabla f \cdot \hat{\mbf{n}} = 0 \implies \forall \hat{\mbf{n}} \nabla f = 0$.

\begin{definition}
    Let $f(x_1, \dotsc, x_n)$ be a function $f : D \rightarrow \R$,
    $D \subseteq \R ^ n$.
    The point $\mbf{a} = (a_1, \dotsc, a_n)$ is a stationary point of $f$ if and only if $f$ is differentiable at $\mbf{a}$ and $\nabla f = \left(\pd[f]{x_1}, \pd[f]{x_2}, \dotsc, \pd[f]{x_n}\right) = (0, 0, \dotsc, 0)$,
    i.e. $\pd[f]{x_i} = 0$ $1 \leq i \leq n$.
\end{definition}

\begin{example}
    Find the stationary points of $f(x, y) = xye ^ {-x ^ 2 - y ^ 2}$.

    \begin{solution}
        \[
        \pd[f]{x} = 0 = ye ^ {-x ^ 2 - y ^ 2} + x(-2x)ye ^ {-x ^ 2 - y ^ 2} = y(1 - 2x ^ 2)e ^ {-x ^ 2 - y ^ 2}
        \]
        \[
        \pd[f]{y} = 0 = x(1 - 2y ^ 2)e ^ {-x ^ 2 - y ^ 2}
        \]
        \begin{align*}
            y(1 - 2x ^ 2) &= 0 \\
            x(1 - 2y ^ 2) &= 0
        \end{align*}
        this implies either $y = 0$ or $2x ^ 2 = 1$.

        If $y = 0$ we have $x = 0$.
        If $x ^ 2 = \frac{1}{2}$ we have $2y ^ 2 = 1 \implies y ^ 2 = \frac{1}{2}$.

        Stationary points are $(0, 0), \left(\pm\frac{1}{\sqrt{2}}, \pm\frac{1}{\sqrt{2}}\right)$,
        with $(0, 0)$ a saddle point,
        $\left(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right), \left(-\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}\right)$ maxima,
        $\left(\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}\right), \left(-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right)$ minima.
    \end{solution}
\end{example}

How can we see if we have a maximum,
minimum or saddle?

In $1$D we decide between maximum and minimum as follows.
Suppose $f(x)$ has a stationary point at $x = x_0$ i.e. $f'(x_0) = 0$
\[
f(x_0 + \delta x) = f(x_0) + \delta xf'(x_0) + \frac{(\delta x) ^ 2}{2!}f''(x_0) + \dotsc
\]
\[
f(x_0 + \delta x) = f(x_0) + \frac{(\delta x) ^ 2}{2!}f''(x_0) + o(\delta x ^ 2).
\]
For small enough $\delta x$,
quadratic term dominates $+ \dotsc$
\[
\delta f = f(x_0 + \delta x) - f(x_0) \approx \frac{(\delta x) ^ 2}{2!}f''(x_0) > 0 \qquad\text{ if } f''(x_0) > 0
\]
must be at a minimum
$< 0$ if $f''(x_0) > 0$ maximum.

If $f''(x_0) = 0$ stationary point it is called degenerate.


We shall use three methods to determine the nature of stationary points for functions of more than one variable.

\textbf{Method $1$}

Suppose $f(x, y)$ has a stationary point at $(x_0, y_0)$
\[
\pd[f]{x}(x_0, y_0) = \pd[f]{y}(x_0, y_0) = 0.
\]
Near the stationary point by Taylor series.
\[
f(x_0 + \delta x, y_0 + \delta y) = f(x_0, y_0) + f_x\delta x + f_y\delta y + \frac{1}{2}\left(f_{xx}(\delta x) ^ 2 + 2f_{xy}(\delta x) + f_{yy}(\delta y) ^ 2\right) + \dotsc
\]
\[
\delta f = f(x_0 + \delta x, y_0 + \delta y) - f(x_0, y_0) = \frac{1}{2}\left(f_{xx}(\delta x) ^ 2 + 2f_{xy}(\delta x) + f_{yy}(\delta y) ^ 2\right) + \dotsc + o((\|\delta\mbf{x}\|) ^ 2).
\]

$\delta f = f(x_0 + \delta x, y_0 + \delta y) - f(x_0, y_0) \approx \frac{1}{2}Q(\delta x, \delta y)$
where
\[
Q(x, y) = f_{xx}x ^ 2 + 2f_{xy}xy + f_{yy}y ^ 2.
\]
$Q$ is an example of a quadratic form $Ax ^ 2 + Bxy + Cy ^ 2$.
$Q$ can be written
\[
\begin{pmatrix}
    x & y
\end{pmatrix}\overbrace{\begin{pmatrix}
    f_{xx} & f_{xy} \\
    f_{xy} & f_{yy}
\end{pmatrix}} ^ {= H}
\begin{pmatrix}
    x \\ y
\end{pmatrix} = \mbf{x} ^ TH\mbf{x}
\]
$H$ is called the Hessian matrix.
For a function of $n$ variables $f(x_1, x_2, \dotsc, x_n)$
\[
H_{ij} = \frac{\partial ^ 2f}{\partial x_i\partial x_j}.
\]
Because order of differentiation doesn't matter $H_{ij} = H_{ji}$ i.e. $H$ is symmetric.
Note that $Q(\lambda x, \lambda y) = (\lambda x) ^ 2 f_{xx}(\lambda x\lambda y)f_{xy} + (\lambda y) ^ 2f_{yy} = \lambda ^ 2Q(x, y)$.

\begin{example}
    $Q(x, y) = x ^ 2 - y ^ 2$ has a saddle point.
\end{example}

For a saddle point regions where $Q > 0$ and $Q < 0$ we must have two lines
(at least)
where $Q = 0$.

If $Q = 0$
\begin{align*}
    f_{xx}x ^ 2 + 2f_{xy}xy + f_{yy}y ^ 2 = 0 &\iff f_{xx} + 2f_{xy}\left(\frac{y}{x}\right) + f_{yy}\left(\frac{y}{x}\right) ^ 2 \\
    &\implies \frac{y}{x} = \frac{-2f_{xy} \pm \sqrt{(2f_{xy}) ^ 2 - 4f_{xx}f_{yy}}}{2f_{yy}}
\end{align*}
Need two real roots.
Therefore we must have $(f_{xy}) ^ 2 - f_{xx}f_{yy} > 0$ which implies $\det(H) < 0$.

If $\det(H) > 0$ then there are no lines for which $Q = 0 \implies Q > 0 \forall (x, y) \neq (0, 0) \text{ or } Q < 0 \forall (x, y) \neq (0, 0)$,
in the first case there is a minimum and the second would be a maximum.

$Q(x, y) = f_{xx}x ^ 2 + 2f_{xy}xy + f_{yy}y ^ 2$.
Consider $Q(x, 0) = f_{xx}x ^ 2$,
so $Q < 0 \forall (x, y) \neq (0, 0)$ if $f_{xx} > 0$ and $Q < 0 \forall (x, y) \neq (0, 0)$ if $f_{xx} < 0$.

Classification of stationary points using method $1$.
\begin{enumerate}[label = \arabic*.]
    \item $\det(H) < 0$ $f_{xx} > 0$ then minimum.
    
    \item $\det(H) > 0$ $f_{xx} < 0$ then maximum.
    
    \item $\det(H) > 0$ then saddle.

    \item $\det(H) = 0$ then degenerate.
\end{enumerate}

\begin{example}
    Find and classify the stationary points of $f(x, y) = -\frac{x ^ 2}{2}(y - 1) + \frac{y ^ 2}{2} + y$.

    \begin{solution}
        For a stationary point
        \begin{align*}
            0 &= f_x = -x(y - 1) \\
            0 &= f_y = -\frac{x ^ 2}{2} + y + 1
        \end{align*}

        The first implies $x = 0$ or $y = 1$,
        then the second would imply $y = -1$.

        If $y = 1$ then the first would imply $x = \pm 2$.

        Hence three stationary points,
        $(0, -1), (2, 1), (-2, 1)$.

        Hessian matrix
        \[
        H = \begin{pmatrix}
            f_{xx} & f_{xy} \\
            f_{yx} & f_{yy}
        \end{pmatrix}
        = \begin{pmatrix}
            -y + 1 & -x \\
            -x & 1
        \end{pmatrix}
        \]
        \[
        \det(H) = 1 - y - x ^ 2.
        \]
        At $(0, -1)$ $\det(H) = 2 > 0$ $f_{xx} = 2 > 0$ so $(0, -1)$ is a minimum.

        At $(\pm 2, 1)$ $\det(H) = -4 < 0$ so it is a saddle point.
    \end{solution}
\end{example}

\textbf{Method 2}

This method relies on diagonalising $H = \begin{pmatrix}
    f_{xx} & f_{xy} \\
    f_{yx} & f_{yy}
\end{pmatrix}$,
$Q = (x, y)\begin{pmatrix}
    f_{xx} & f_{xy} \\
    f_{yx} \\ f_{yy}
\end{pmatrix}\begin{pmatrix}
    x \\ y
\end{pmatrix} = \mbf{x} ^ TH\mbf{x} = \mbf{x} \cdot H\mbf{x}$.

\begin{example}
    $Q(x, y) = 2x ^ 2 + 2xy + y ^ 2 = (x, y)\begin{pmatrix}
        2 & 1 \\ 1 & 1
    \end{pmatrix}\begin{pmatrix}
        x \\ y
    \end{pmatrix}$

    Looking at the contour plot.
    The "natural" axes for ellipse are the axis of symmetry $\nabla Q$ parallel to $\mbf{x}$ so $\nabla Q = 2\lambda\mbf{x}$,
    $Q = f_{xx}x ^ 2 + 2f_{xy}xy + f_{yy}y ^ 2$.
    \[
    \nabla Q = \begin{pmatrix}
        \pd[Q]{x} \\
        \pd[Q]{y}
    \end{pmatrix} = \begin{pmatrix}
        2f_{xx}x + 2f_{xy}y \\
        2f_{xx}x + 2f_{yy}y
    \end{pmatrix} = 2\begin{pmatrix}
        f_{xx} & f_{xy} \\
        f_{yx} & f_{yy}
    \end{pmatrix}\begin{pmatrix}
        x \\ y
    \end{pmatrix} = 2H\mbf{x}
    \]
    along the axes of symmetry $wH\mbf{x} = 2\lambda\mbf{x} \implies H\mbf{x} = \lambda\mbf{x}$.
\end{example}

If $\mbf{x}$ lies along axis of symmetry of ellipse,
eigenvector of $H$.
Two axes of symmetry if and only if two eigenvectors,
axes of symmetry are orthogonal,
$H\mbf{e}_1 = \lambda_1\mbf{e}_1$
$H\mbf{e}_2 = \lambda_2\mbf{e}_2$.
\begin{proof}
    Consider
    \begin{align*}
        \mbf{e}_1(H\mbf{e}_2) &= \mbf{e}_1 ^ TH\mbf{e}_2 \\
        &= (\mbf{e}_1 ^ TH\mbf{e}_2) ^ T \\
        &= \mbf{e}_2 ^ TH ^ T\mbf{e}_1
    \end{align*}
    $H$ is symmetric since $H_{ij} = \frac{\partial ^ 2f}{\partial x_i\partial x_j} = \frac{\partial ^ 2f}{\partial x_j\partial x_i} = H_{ji}$,
    therefore $\mbf{e}_1(H\mbf{e}_2) = \mbf{e}_2 ^ TH\mbf{e}_1 = \mbf{e}_2(H\mbf{e}_1)$,
    therefore
    \[
    \mbf{e}_1 \cdot H\mbf{e}_2 = \mbf{e}_1 \cdot \lambda_2\mbf{e}_2 = \mbf{e}_2 \cdot H\mbf{e}_1 = \mbf{e}_2 \cdot \lambda_1\mbf{e}_1 \implies (\lambda_2 - \lambda_1)\mbf{e}_1 \cdot \mbf{e}_2 = 0.
    \]
    $\lambda_2 \neq \lambda_1 \implies \mbf{e}_1 \cdot \mbf{e}_2 = 0$,
    $\mbf{e}_1, \mbf{e}_2$ are orthogonal.
    If $\lambda_1 = \lambda_2$,
    ellipse becomes a circle all directions are axes of symmetry,
    all vectors are eigenvectors of $H$ can pick $\mbf{e}_1, \mbf{e}_2$ to be orthogonal.
\end{proof}

Lets work in a basis of eigenvectors $\mbf{e}_1, \mbf{e}_2$,
$\mbf{e}_1 \cdot \mbf{e}_2 = 0$.
Choose $\mbf{e}_1 \cdot \mbf{e}_1 = \mbf{e}_2 \cdot \mbf{e}_2 = 1$.

\[
Q = \mbf{x} \cdot H\mbf{x}
\]
\[
\mbf{x} = x_1\mbf{e}_1 + x_2\mbf{e}_2.
\]
\begin{align*}
    Q &= (x_1\mbf{e}_1, x_2\mbf{e}_2) \cdot H(x\mbf{e}_1 + x_2\mbf{e}_2) \\
    &= (x_2\mbf{e}_1 + x_2\mbf{e}_2) \cdot (x_1H\mbf{e}_1 + x_2H\mbf{e}_2) \\
    &= (x_1\mbf{e}_1 + x_2\mbf{e}_2) \cdot (x_1\lambda_1\mbf{e}_1 + x_2\lambda_2\mbf{e}_2) \\
    &= \lambda_1x_1 ^ 2 + \lambda_2x_2 ^ 2 \\
    &= \begin{pmatrix}
        x_1 & x_2
    \end{pmatrix}\begin{pmatrix}
        \lambda_1 & 0 \\
        0 & \lambda_2
    \end{pmatrix}\begin{pmatrix}
        x_1 \\ x_2
    \end{pmatrix}.
\end{align*}

$Q > 0$ for all $(x_1, x_2) \neq (0, 0)$ if $\lambda_1, \lambda_2 > 0$ is a minimum.

$Q < 0$ for all $(x_1, x_2) \neq (0, 0)$ if $\lambda_1, \lambda_2 < 0$ is a maximum.

If $\lambda_1, \lambda_2 \neq 0$ and different signs then it is a saddle point.

For functions of $n$ variables $f(x_1, \dotsc, x_n)$ we will find
(when we work in an basis of eigenvectors)
$\mbf{x} = x_1\mbf{e}_1 + x_2\mbf{e}_2 + \dotsi + x_n\mbf{e}_n$,
$\mbf{e}_1, \dotsc, \mbf{e}_n$ are eigenvectors of $H$,
$H\mbf{e}_j = \lambda_j\mbf{e}_j$
\[
Q = \lambda_1x_1 ^ 2 + \lambda_2x_2 ^ 2 + \dotsi + \lambda_nx_n ^ 2.
\]
If we move away from the stationary point in the direction of the $\mbf{e}_j$ eigenvector $\mbf{x} = (0, 0, 0, \dotsc, \underset{\text{$j$th entry}}{\delta x_j}, 0, 0)$.
\[
Q = \lambda_j(\delta x_j) ^ 2.
\]

\textbf{Rules for Method $2$}

Suppose $f(x_1, \dotsc, x_n)$ has a stationary point at $\mbf{x} = \mbf{a}$.
Evaluate the Hessian $H$ at $\mbf{x} = \mbf{a}$ and find its $n$ eigenvalues $\lambda_1, \dotsc, \lambda_n$
(guaranteed real by symmetry of $H$).

\begin{enumerate}[label = (\arabic*)]
    \item If $\lambda_j > 0$ for all $j$ $1 \leq j \leq n$ $Q > 0$ for all $\mbf{x} \neq \mbf{0}$ is minimum.
    
    \item If $\lambda_j < 0$ for all $j$ $1 \leq j \leq n$ $Q < 0$ for all $\mbf{x} \neq \mbf{0}$ is maximum.

    \item If $\lambda_j \neq 0$ for all $j$ and have different signs is a saddle point.

    \item If $\lambda_j = 0$ for some $j$,
    $\det(H) = \prod_{i = 1}^{n}\lambda_i = 0$ is degenerate.
\end{enumerate}

\begin{example}
    $f(x, y, z) = xy + xz + yz$.
    Find and classify the stationary points.

    \begin{solution}
        \begin{align*}
            0 = f_x &= y + z \\
            0 = f_y &= x + z \\
            0 = f_z &= x + y
        \end{align*}
        this implies $(x, y, z) = (0, 0, 0)$ is the only solution.
        Hessian matrix
        \[
        H = \begin{pmatrix}
            f_{xx} & f_{xy} & f_{xz} \\
            f_{yx} & f_{yy} & f_{yz} \\
            f_{zx} & f_{zy} & f_{zz}
        \end{pmatrix}
        =
        \begin{pmatrix}
            0 & 1 & 1 \\
            1 & 0 & 1 \\
            1 & 1 & 0
        \end{pmatrix}.
        \]
        Eigenvalues given by
        \[
        \det(H - \lambda I) = \begin{vmatrix}
            -\lambda & 1 & 1 \\
            1 & -\lambda & 1 \\
            1 & 1 & -\lambda
        \end{vmatrix} = -\lambda ^ 3 + 3\lambda + 2 \implies \det{H} = -(\lambda - 2)(\lambda + 1) ^ 2
        \]
        therefore $\lambda = -1, -1, 2$.
        $\lambda$'s not zero and different signs implies saddle point.
    \end{solution}
\end{example}

\textbf{Method $3$}

Sylvester's Criterion.
Suppose $H$ is a symmetric $n \times n$ matrix.
Define $H ^ {(i)}$ to be the $j \times j$ matrix restriction of $H$ in the top left hand corner.

Sylvester's Criterion states:
$H$ has positive eigenvalues $\lambda_j > 0$ if and only if $\det(H ^ {(i)}) > 0$ for all $1 \leq i \leq n$.

Therefore $H$ will correspond to a minimum.

If $H$ has all negative eigenvalues then $-H$ must have all positive eigenvalues and satisfy Sylvester's Criterion,
i.e. we must have $\det{H ^ {(j)}}(-1) ^ k > 0$.

For maximum need pattern of determinants with alternating signs starting with a minus.

If $H$ corresponds to a degenerate stationary point this implies $\lambda_j = 0$ for some $j$ which implies $\det(H) = 0$.

If the pattern is none of the above $H$ corresponds to a saddle point.

\begin{enumerate}[label = (\roman*)]
    \item Minimum $\det(H ^ {(i)}) > 0$.
    
    \item Maximum $\det(H ^ {(i)})(-1) ^ {(i)} > 0$.

    \item Degenerate $\det(H ^ {(n)}) = \det(H) = 0$.

    \item Saddle point is anything else.
\end{enumerate}

\begin{example}
    \[
    f(x, y, z) = -\frac{3}{2}x ^ 2 - \frac{3}{2}y ^ 2 - \frac{1}{2}z ^ 2 + xy + yz - xz.
    \]

    \begin{solution}
        Stationary point at $(0, 0, 0)$,
        $f_x = f_y = f_z = 0$,
        \[
        H = \begin{pmatrix}
            f_{xx} & f_{xy} & f_{xz} \\
            f_{yx} & f_{yy} & f_{yz} \\
            f_{zx} & f_{zy} & f_{zz}
        \end{pmatrix} = \begin{pmatrix}
            -3 & 1 & -1 \\
            1 & -3 & 1 \\
            -1 & 1 & -1
        \end{pmatrix}
        \]
        so
        \begin{align*}
            H ^ {(1)} &= -3 < 0 \\
            H ^ {(2)} &= \begin{vmatrix}
                -3 & 1 \\ 1 & -3
            \end{vmatrix} = (-3) ^ 2 - 1 = 8 > 0 \\
            H ^ {(3)} &= \det{H} \\
            &= \begin{vmatrix}
                -3 & 1 & - 1 \\
                1 & -3 & 1 \\
                -1 & 1 & -1
            \end{vmatrix} \\
            &= -3\begin{vmatrix}
                -3 & 1 \\ 1 & -1
            \end{vmatrix} - \begin{vmatrix}
                1 & 1 \\ -1 & -1
            \end{vmatrix} - \begin{vmatrix}
                1 & -3 \\ -1 & 1
            \end{vmatrix} \\
            &= -3 \cdot 2 - 0 + 2 \\
            &= -6 + 2 \\
            &= -4 \\
            &< 0.
        \end{align*}
        We have a $(-+-)$ pattern meaning that the stationary point is a maximum.
    \end{solution}
\end{example}

\subsection{Constrained Optimisation - method of Lagrange multipliers}

\begin{example}
    What is the maximum area of a rectangle,
    given the constraint the rectangle has perimeter $P = 4$?

    $A = f(x, y) = xy$
    constraint
    \[
    P - 4 = 0 = g(x, y) = 2x + 2y - 4
    \]
    could solve for constraint
    \[
    2y = 4 - 2x \implies y = 2 - x
    \]
    \[
    f(x, y) = xy = x(2 - x) = 2x - x ^ 2
    \]
    \[
    0 = \frac{d}{dx}(2x - x ^ 2) \implies 2 - 2x = 0 \implies x = 1 \implies y = 2 - x = 1.
    \]
\end{example}

Lagrange's method allows solutions of $n$ variables without first solving the constraint.

\textbf{Lagrange multipliers}

Find stationary points of $f(x, y)$ subject to constraint $g(x, y) = 0$

At max/min of $f(x, y)$ along $g = 0$
$\nabla f \cdot \hat{\mbf{n}} = 0$.
The path is a level set of $g(x, y)$,
$g(x, y) = 0 \implies \nabla g \cdot \mbf{n} = 0$,
$\nabla g$ is normal to level set $g = 0$ which implies $\nabla f, \nabla g$ are
(anti)
parallel,
i.e. $\nabla f = \lambda\nabla g$,
$\lambda$ is called the Lagrange multiplier.

If we consider the function $\phi = f - \lambda g$ then the constrained maximum of $f$,
$\nabla\phi = \nabla(f - \lambda g) = \nabla f - \lambda\nabla g = 0$.
$\phi$ has an unconstrained maximum.

\begin{example}[Rectangle problem]
    Area $f(x, y) = xy$ constraint $P - 4 = g(x, y) = 2x + 2y - 4 = 0$
    \[
    \phi = f - \lambda g = xy - \lambda(2x + 2y - 4)
    \]
    \begin{align*}
        0 = \phi_x &= y - 2\lambda \\
        0 = \phi_y &= x - 2\lambda \\
        0 = g &= 2x + 2y - 4
    \end{align*}
    \[
    2\lambda = x = y
    \]
    \[
    2x + 2y = 4x = 4
    \]
    \[
    x = y = 1.
    \]
\end{example}

Generalise to find max/min of $f(x_1, \dotsc, x_n)$ subject to $m < n$ constraints $g_j(x_1, \dotsc, x_n) = 0$ $1 \leq j \leq m$.

At max/min of $f(x_1, \dotsc, x_n)$ on $n-m$ dimensional constraint surface.
Split $\R ^ n = \underbrace{V_{\text{tangent}}}_{\dim = n - m} \oplus \underbrace{V_{\text{normal}}}_{m \text{ dimensional}}$.
$\nabla g_i$
$1 \leq i \leq m$
$\nabla g_i \in V_{\text{normal}}$
(because normal to level sets $g_j = 0$).
Assume $\nabla g_i$ is a basis for $V_{\text{normal}}$.

If $f$ max/min on constraint surface then $\nabla f \cdot \hat{\mbf{n}} = 0$
($\hat{\mbf{n}}$ tangent vector to constraint surface)
i.e. $\nabla f$ is orthogonal to $V_{\text{tangent}}$ therefore $\nabla f \in V_{\text{normal}}$.
Since $\nabla g_i$ are a basis for $V_{\text{normal}}$ and $\nabla f \in V_{\text{normal}} \implies \nabla f = \sum_{i = 1}^{m}\lambda_i\nabla g_i$.
Construct
\[
\phi = f - \lambda_1 g_1 - \lambda_2g_2 - \dotsc - \lambda_mg_m
\]
then $\nabla \phi = 0$.
Now have $n$ equations $\pd[\phi]{x_i} = 0$ $1 \leq i \leq n$.
$m$ constraint equations $g_j = 0$ $1 \leq j \leq m$.
$m + n$ variables and equations.











\end{document}