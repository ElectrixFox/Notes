\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\DeclareMathOperator{\Dom}{Dom}
\DeclareMathOperator{\Ran}{Ran}

\title{Calculus I}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\section{Functions}

\subsection{Functions, domain \& range}

\textbf{Gr10}

\begin{definition}
    A function $f : D \rightarrow C$ is a map ($f$) from a set $D$ called the domain ($\Dom f$) to a set $C$ called the codomain ($\mathrm{Codom}\ f$).
    
    For any $x \in D$, there is a unique $f(x) \in C$ called the image of $x$ under $f$.
\end{definition}

Sometimes  write $f : D \rightarrow C$ $x \mapsto f(x)$.

\begin{definition}
    The range of a function $f$ is the set of all images of $f$. This is a subset of the codomain.
    
    We can write $\Ran f = \{f(x)\,|\,x \in \Dom f\} \subseteq \mathrm{Codom}\ f$
\end{definition}

Real-valued functions\footnote{Always assume codomain is $\R$.} (i.e. the codomain is $\R$, or a subset) of a single real variable (i.e. the domain is $\R$, or a subset)

\begin{example}
    $f : \R \rightarrow \R$ given by $f(x) = x ^ 2\ \forall x \in \R$

    So $\Dom f = \R$ and $\mathrm{Codom}\ f = \R$ \& $\Ran f = [0, \infty)$.
\end{example}

If domain of $f$ isn't explicitly given, we take it to be the maximal subset of $\R$ for which the function is defined.

\subsection{The graph of a function}
\begin{definition}[Graph]
    The graph of $f$ is the set of all points $(x, y) \in \R ^ 2$ such that $x \in \Dom f$ $y = f(x)$.
\end{definition}
To sketch $f$, we often just restrict to an interval of the domain, $[a, b]$.

We often pick a region such that the graph shows significant features like turning points \& intercepts.

When necessary we use a closed shaded circle to indicate an included point, and an open unshaded circle to denote an excluded point.

\textbf{The vertical line test}

If any vertical line intersects the curve more than once then the curve is not the graph of a function.

\subsection{Even and odd functions}
\begin{definition}
    A function is even if $f(x) = f(-x)$ $\forall x \in \Dom f$
    
    A function is odd if $f(x) = -f(-x)$ $\forall x \in \Dom f$
\end{definition}
\begin{example}
    $f(x) = x ^ 2$ is even

    $f(x) = x ^ 3$ is odd
\end{example}

All functions can be written as the sum of an odd part and an even part.
\[
f(x) = f_{even}(x) + f_{odd}(x)
\]
where $f_{even}(x) = \frac{1}{2}\left(f(x) + f(-x)\right)$,
$f_{odd}(x) = \frac{1}{2}\left(f(x) - f(-x)\right)$

\begin{example}
    $f(x) = e ^ x$ with
    
    $f_{even}(x) = \frac{1}{2}(e ^ x + e ^ {-x}) =: \cosh(x)$
    
    $f_{odd}(x) = \frac{1}{2}(e ^ x - e ^ {-x}) =: \sinh(x)$
\end{example}

\begin{example}
    $f(x) = (\cos x + x)\sin x$ with

    $f_{even}(x) = x\sin x$
    
    $f_{odd}(x) = \cos x \sin x$
\end{example}

Product of two odd functions is an even function, product of an odd function and an even function is an odd function.

\subsection{Piecewise functions}
Some functions may be defined piecewise, i.e. with different expressions for different intervals in the domain.

\begin{example}
    \begin{align*}
        f(x) &= |x| \\
        &= \begin{cases}
            \phantom{-}x\quad \text{if} &x \geq 0 \\
            -x\quad \text{if} &x < 0
        \end{cases}
    \end{align*}
\end{example}

A step function is a piecewise function which is constant on each piece.
\begin{example}
    Heaviside step function
    \[
    H(x) = \begin{cases}
        0 & x < 0 \\
        1 & x > 0
    \end{cases}
    \]
    Here $\Dom H = \R \setminus \{0\}$ though it is sometimes useful to extend the domain to $\R$ by defining $H(0): (0, 1, \frac{1}{2})$
\end{example}

\subsection{Operations with functions}
Given two functions $f, g\  \&\  c \in \R$, we can define the following functions:
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Name & Definition & Domain \\
        \hline
        Sum $(f + g)$ & $(f + g)(x) = f(x) + g(x)$ & $\Dom (f + g) = \Dom f \cap \Dom g$ \\
        Difference $(f - g)$ & $(f - g)(x) = f(x) - g(x)$ & $\Dom (f - g) = \Dom f \cap \Dom g$ \\
        Product $(fg)$ & $(fg)(x) = f(x)g(x)$ & $\Dom (fg) = \Dom f \cap \Dom g$ \\
        Ratio $\left(\frac{f}{g}\right)$ & $\left(\frac{f}{g}\right)(x) = \frac{f(x)}{g(x)}$ & $\Dom \left(\frac{f}{g}\right) = (\Dom f \cap \Dom g) \setminus \{x\,|\,g(x) = 0\}$ \\
        Composition $(f\circ g)$ & $(f \circ g)(x) = f(g(x))$ & $\Dom (f \circ g) = \{x \in \Dom g\,|\, g(x) \in \Dom f\}$ \\
        Scalar Multiplication $(cf)$ & $(cf)(x) = cf(x)$ & $\Dom (cf) = \Dom f$ \\
        \hline
    \end{tabular}
\end{table}

Unlike multiplication, composing functions in opposite order changes the function (generally).

Using the above we can also form more general linear combinations, such as $(af + bg)(x)$ with $(af + bg)(x) = af(x) + bg(x)$ \& $\Dom (af + bg) = \Dom f \cap \Dom g$.

\subsection{Inverse Functions}
\begin{definition}[Surjective function]
    A function $f : D \rightarrow C$ is surjective if $\Ran f = \mathrm{Codom}\, f = C$, i.e. if $\forall\, y \in C\, \exists\, x \in D$ such that $f(x) = y$
\end{definition}
\begin{example}
    $f : \R \rightarrow \R$, $x \mapsto 2x + 1$ is surjective.

    $f : \R \rightarrow \R$, $x \mapsto x ^ 2$ is not surjective since for example $(-2)$ is not the image of any $x \in \R$ (in fact true $\forall\,u < 0,\, u \in \R$).
\end{example}

\begin{definition}[Injective function]
    $f : D \rightarrow C$ is injective if $\forall\, x_1, x_2 \in \R$ with $x_1 \neq x_2,\, f(x_1) \neq f(x_2)$.
\end{definition}

\begin{example}
    $f(x) = 2x + 1$ is injective since $f(x_1) = f(x_2)$ implies that $x_1 = x_2$.

    $f(x) = x ^ 2$ is not injective since $f(2) = f(-2)$ (in fact $f(x) = f(-x)$ but $x \neq -x\, \forall\, x\neq 0$).
\end{example}

\textbf{Horizontal line test}

If no horizontal line intersects the graph of $f$ more than once then $f$ is injective, otherwise it is not.

\begin{definition}[Bijective function]
    A function $f : D \rightarrow C$ is bijective if it is both injective and surjective.
\end{definition}

\begin{example}
    We have showed that $f(x) = 2x + 1$ is both injective and surjective, so it is bijective.
\end{example}

A bijective function $f : D \rightarrow C$ has a unique inverse $f ^ {-1}$ such 
\[
(f \circ f ^ {-1})(x) = x = (f ^ {-1} \circ f)(x).
\]
We can see that $\Ran f ^ {-1} = \Dom f$, $\Dom f ^ {-1} = \Ran f$. Equivalently, $y = f ^ {-1} (x) \iff f(y) = x$.

\begin{example}
    Since $f : \R \rightarrow \R$, $x \mapsto 2x + 1$ is bijective, we can find its inverse.

    Let $y = f^{-1}(x)$ then $f(y) = x = 2y + 1$ so $y = \frac{x - 1}{2}$.
\end{example}

\begin{example}
    $f : \R \rightarrow \R$, $x \mapsto x ^ 2$ is neither injective nor surjective, but if we reduce the domain to $[0, \infty)$ then it is injective with range $[0, \infty)$. So $g : [0, \infty) \rightarrow [0, \infty)$ $x \mapsto x ^ 2$ is bijective, and so we can find an inverse.

    Let $y = f^{-1}(x)$, then $f(y) = x = y ^ 2$ so $y = f^{-1}(x) = \sqrt{x}$.
\end{example}

\newpage

\section{Limits and continuity}

Rough idea: A function $f(x)$ has a limit $L$ at $x = a$ if $f(x)$ is close to $L$ whenever $x$ is close to $a$.

More precise idea: If we have an acceptable 'error' ($\epsilon,\ \epsilon > 0$) between $f$ and $L$, then I would need
\[
|f(x) - L| < \epsilon\quad\forall x \in \text{some interval around } a.
\]
If for any $\epsilon > 0$ can find some distance ($\delta,\ \delta > 0$) such that
\[
|f(x) - L| < \epsilon\quad\forall\quad 0 < |x - a| < \delta
\]
then we say that $f(x)$ has a limit $L$ as $x$ tends to $a$.

\begin{definition}
    $f(x)$ has a limit $L$ as $x$ tends to $a$ if
    
    $\forall \epsilon > 0,\,\exists \delta > 0$ such that
    \[
    |f(x) - L| < \epsilon
    \]
    $\forall x$ such that
    \[
    0 < |x - a| < \delta
    \]
\end{definition}
We write $\displaystyle\lim_{x \rightarrow a}f(x) = L$ or $f(x) \rightarrow L \text{ as } x \rightarrow a$.

If there is no such $L$ then the limit doesn't exist.

Note: we do not require $f(a) = L$ or for $f(a)$ to even exist, as we only consider the interval $0 < |x - a| < \delta$.

\subsection{Continuity}

\begin{definition}
    A function is continuous at the point $a$ if
    \begin{itemize}
        \item $f(a)$ exists,
        \item $\lim_{x \rightarrow a}f(x)$ exists,
        \item $\lim_{x \rightarrow a}f(x) = f(a)$.
    \end{itemize}
\end{definition}

\begin{definition}
    $f(x)$ is continuous on a subset $S$ of its domain if it is continuous at all points in $S$.
\end{definition}

\begin{definition}
    $f(x)$ is continuous if it is continuous at every point in its domain.
\end{definition}

\begin{example}
    Let $f(x) = \begin{cases}
        x ^ 2 &\text{if}\  -1\leq x < 0 \\
        1 &\text{if}\  x = 0 \\
        x ^ 2 &\text{if}\  0 < x \leq 1 
    \end{cases}$
    Then $f(x)$ is continuous on $[-1,\, 1] \setminus \{0\}$, but not continuous at $x = 0$ as $\displaystyle\lim_{x \rightarrow 0}f(x) = 0 \neq f(0)$.
\end{example}

\begin{example}
    Let $f(x) = \begin{cases}
        1 &\text{if}\  x \leq 0 \\
        x ^ 2 &\text{if}\  x > 0
    \end{cases}$
    is not continuous at $x = 0$ as $\displaystyle\lim_{x \rightarrow 0}f(x)$ does not exist.
\end{example}

\subsection{Classification of discontinuities}
\begin{definition}
    $f(x)$ has a right-sided limit $\displaystyle L ^ + = \lim_{x \rightarrow a+}f(x)$ as $x$ tends to $a$ from above (or the right) if
    $\forall\epsilon >0,\ \exists\delta > 0$ such that
    \[
    |f(x) - L^+| < \epsilon
    \]
    $\forall x$ such that $0 < x - a < \delta$.
\end{definition}

\begin{definition}
    $f(x)$ has a left-sided limit $\displaystyle L ^ - = \lim_{x \rightarrow a-}f(x)$ as $x$ tends to $a$ from below (or the left) if
    $\forall\epsilon >0,\ \exists\delta > 0$ such that
    \[
    |f(x) - L^-| < \epsilon
    \]
    $\forall x$ such that $0 < a - x < \delta$.
\end{definition}

$\displaystyle L = \lim_{x \rightarrow a}f(x)$ exists if and only if $L ^ +,\ L ^ -$ both exist and $L ^ + = L ^ -$. Then $L = L ^ + = L ^ -$.

Using the definitions of right and left sided limits, there are three types of discontinuity.
\begin{enumerate}[label = \roman*)]
    \item Removable discontinuity. $L$ exists but $f(a) \neq L$, we can always 'remove' the discontinuity to make a continuous function
    \[
    g(x) = \begin{cases}
        f(x) &\text{if}\ x \neq a \\
        L &\text{if}\ x = a
    \end{cases}
    \]

    \begin{example}
        \[
        f(x) = \begin{cases}
            x ^ 2 &\text{if}\ x \neq 0 \\
            1 &\text{if}\ x = 0
        \end{cases}
        \]
        has a removable discontinuity at $x = 0$ removing this gives the continuous function $g(x) = x ^ 2$
    \end{example}
    
    \item Jump discontinuity. $L ^ +$ and $L ^ -$ both exist, but $L ^ + \neq L ^ -$.
    \begin{example}
        \[
        f(x) = \begin{cases}
            1 &\text{if}\ x \leq 0 \\
            x ^ 2 &\text{if}\ x > 0
        \end{cases}
        \]
        Here $L ^ + = 0,\ L ^ - = 1$, sp $L ^ + \neq L ^ -$ and no limit exists
    \end{example}

    \item Infinite (Essential) discontinuity. At least one of $L ^ +$ and $L ^ -$ does not exist.
    \begin{example}
        $f(x) = \frac{1}{x}$ has an infinite discontinuity at $x = 0$. Here neither $L ^ +$ nor $L ^ -$ exists.
    \end{example}
    \begin{example}
        $f(x) = \sin\left(\frac{1}{x}\right)$ has an infinite discontinuity at $x = 0$. Neither $L ^ +$ nor $L ^ -$ exists.
    \end{example}
\end{enumerate}

\subsection{Facts about limits and continuity}
Facts about limits
\begin{itemize}
    \item The limit is unique.
    \item If $f(x) = g(x)$ (except possibly at $x = a$) in some open interval containing $a$, then
    \[
    \lim_{x \rightarrow a}f(x) = \lim_{x \rightarrow a}g(x).
    \]
    \item If $f(x) \geq K$ on either some interval $(a, b)$ or $(c, a)$ and if $\lim_{x \rightarrow a}f(x) = L$ then $L \geq K$ (Similarly replacing $\geq$ with $\leq$).
    \item Calculus of Limits Theorem (COLT)
    
    If $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M$, then
    \begin{enumerate}[label = \roman*)]
        \item \[
        \lim_{x \rightarrow a}(f(x) + g(x)) = L + M
        \]
        \item \[
        \lim_{x \rightarrow a}(f(x) \cdot g(x)) = L \cdot M
        \]
        \item If $M \neq 0$ then \[
            \lim_{x \rightarrow a}\left(\frac{f(x)}{g(x)}\right) = \frac{L}{M}.
        \]
    \end{enumerate}
\end{itemize}

The following are facts about continuity
\begin{itemize}
    \item If $f$ and $g$ are continuous, then so are $(f + g),\ (fg), \left(\frac{f}{g}\right)$ and $|f|$.
    \item All polynomial, rational, trigonometric and hyperbolic functions are continuous.
    \item If $\lim_{x \rightarrow a}g(x) = L$ and $f(x)$ is continuous at $x = L$, then
    \[
    \lim_{x \rightarrow a}(f \circ g)(x) = f(L).
    \]
\end{itemize}

\begin{example}
    The following are continuous
    \[
    f(x) = 2x ^ 3 + x + 7,\quad g(x) = \frac{3x}{x - 1},\quad h(x) = \left|\frac{1 + x ^ 2}{\sin x}\right|
    \]
\end{example}

\begin{example}
    \[
    \lim_{x \rightarrow 0}\,\frac{1}{x}\ \text{does not exist.}
    \]
\end{example}

\begin{example}
    \begin{align*}
    \lim_{x \rightarrow \frac{\pi}{2}}\,x ^ 2 \sin x &=\footnotemark \left(\lim_{x \rightarrow \frac{\pi}{2}} x^ 2 \right)\left(\lim_{x \rightarrow \frac{\pi}{2}} \sin x\right) \\
    &= \left(\frac{\pi}{2}\right) ^ 2 \cdot 1 = \frac{\pi ^ 2}{4}.
    \end{align*}
    \footnotetext{COLT}
\end{example}

\begin{example}
    \begin{align*}
    \lim_{x \rightarrow 3}\,\frac{2x ^ 2 - 18}{x - 3} &= \lim_{x \rightarrow 3}\, \frac{2(x + 3)(x - 3)}{x - 3} \\
    &= \lim_{x \rightarrow 3}\,2(x + 3)\quad\text{Since the value at $x = 3$ is irrelevant for the limit.} \\
    &= 2(6) = 12.
    \end{align*}
\end{example}

\subsection{The pinching (squeezing) theorem}
\begin{theorem}[Squeezing theorem]\label{calc_thm_squeze}
    If $g(x) \leq f(x) \leq h(x)$ for all $x \neq a$ in some open interval containing $a$, and $\displaystyle \lim_{x \rightarrow a}g(x) = \lim_{x \rightarrow a} h(x) = L$ then
    \[
    \lim_{x \rightarrow a}f(x) = L.
    \]
\end{theorem}

\begin{example}
    Calculate
    \[
    \lim_{x \rightarrow 0} x ^ 2 \sin \left(\frac{1}{x}\right).
    \]
    As $-1 \leq \sin \left(\frac{1}{x}\right) \leq 1$ then $-x ^ 2 \leq x ^ 2 \sin \left(\frac{1}{x}\right) \leq x ^ 2$.

    We have $\lim_{x \rightarrow 0}(-x ^ 2) = 0$, $\lim_{x \rightarrow 0}(x ^ 2) = 0$ so by \autoref{calc_thm_squeze}
    \[
    \lim_{x \rightarrow 0} x ^ 2 \sin \left(\frac{1}{x}\right) = 0.
    \]
\end{example}

\subsection{Two trigonometric limits}
Two important limits are the following
\[
\lim_{x \rightarrow 0}\frac{\sin x}{x} = 1
\]
\[
\lim_{x \rightarrow 0}\frac{1 - \cos x}{x} = 0
\]

We can prove the first limit using \autoref{calc_thm_squeze}.

\textbf{Gr11}

Let $T_1$ be the area of the triangle OAB, $T_2$ be the area of the triangle OAC, $S$ be the area of sector OAB. Clearly $T_1 < S < T_2$. The length of BD is $\sin x$, the length of AC is $\tan x$
\[
T_1 = \frac{1}{2}\sin x\quad T_2 = \frac{1}{2}\tan x\quad S = \left(\frac{x}{2\pi}\right) \cdot \pi = \frac{x}{2}.
\]
Therefore $\sin x < x < \tan x$. Now for $x \in \left(0, \frac{\pi}{2}\right)$
\[
1 = \frac{\sin x}{\sin x} < \frac{x}{\sin x} < \frac{\tan x}{\sin x} = \frac{1}{\cos x}
\]
and since all these functions are even, the same holds on $\left(-\frac{\pi}{2}, 0\right)$.

We also have
\[
\lim_{x \rightarrow 0} 1 = 1\quad\lim_{x \rightarrow 0}\frac{1}{\cos x} = 1,
\]
so by \autoref{calc_thm_squeze}
\[
\lim_{x \rightarrow 0}\frac{x}{\sin x} = 1\text{ and therefore } \lim_{x \rightarrow 0}\frac{\sin x}{x} = 1
\]

We can use this to prove the second limit
\begin{align*}
\lim_{x \rightarrow 0}\frac{1 - \cos x}{x} &= \lim_{x \rightarrow 0}\frac{(1 - \cos x)(1 + \cos x)}{x(1 + \cos x)} \\ 
&= \lim_{x \rightarrow 0}\frac{\sin ^ 2 x}{x(1 + \cos x)} \\
&= \lim_{x \rightarrow 0}\left(\frac{\sin x}{x}\right)\left(\frac{\sin x}{1 + \cos x}\right) \\
&= 1 \cdot 0 = 0.
\end{align*}

\begin{example}
    \[
    \lim_{x \rightarrow 0}\frac{\tan x}{x}
    \]

    \[
    \lim_{x \rightarrow 0}\frac{\tan x}{x} = \lim_{x \rightarrow 0} \left(\frac{\sin x}{x}\right) \left(\frac{1}{\cos x}\right) = 1 \cdot 1 = 1.
    \]
\end{example}

\subsection{Limits as $x \rightarrow \infty$}

A function has a limit $L$ as $x \rightarrow \infty$ if $f(x)$ can be kept arbitrarily close to $L$ as long as $x$ is sufficiently large.

\begin{definition}
    $f(x)$ has a limit $L$ as $x \rightarrow \infty$ if $\forall \epsilon > 0,\, \exists S > 0$ such that $\forall x > S$,
    \[
    |f(x) - L| < \epsilon.
    \]
\end{definition}

\begin{example}
    \[
    \lim_{x \rightarrow \infty}\frac{1}{x} = 0
    \]
    as $\frac{1}{x}$ can be made as close to zero as wanted by taking $x$ sufficiently large.
\end{example}

An easy way to calculate limits as $x \rightarrow \infty$ it to make the substitution $x = \frac{1}{u}$. Then 
\[
\lim_{x \rightarrow \infty} f(x) = \lim_{x \rightarrow 0^+}f\left(\frac{1}{u}\right).
\]
\begin{example}
    \[
    \lim_{x \rightarrow \infty}\frac{1}{x} = \lim_{u \rightarrow 0^+}u = 0 
    \]
\end{example}

\begin{example}
    \begin{align*}
    \lim_{x \rightarrow \infty}\frac{x\cos \left(\frac{1}{x}\right) + 2}{x} &= \lim_{u \rightarrow 0^+}(\cos u + 2u) \\
    &= 1 + 0 = 1.
    \end{align*}
\end{example}

\begin{example}
    \begin{align*}
    \lim_{x \rightarrow \infty}\frac{2x + 3}{x + 5} &= \lim_{x \rightarrow \infty}\frac{2 + \frac{3}{x}}{1 + \frac{5}{x}} \\
    &= \lim_{u \rightarrow 0^+}\frac{2 + 3u}{1 + 5u} \\
    &= 2
    \end{align*}
\end{example}

Note the graph of $f(x)$ has a horizontal asymptote to the right (or left) at $y = 1$ if
\[
\lim_{x \rightarrow \infty}f(x) = L\quad\text{(or} \lim_{x \rightarrow -\infty}f(x) = L\text{ respectively})
\]


\subsection{The Intermediate Value Theorem}
If $f(x)$ is continuous on $[a, b]$, and it is any number between $f(a)$ and $f(b)$ then $\exists c \in (a, b)$ such that $f(c) = u$.

\begin{example}
    $f(x) = \sin x$ is continuous on $\left[0, \frac{\pi}{2}\right]$ and
    \[
    f(0) = 0 < \frac{1}{2} < 1 = f\left(\frac{\pi}{2}\right) 
    \]
    so by the intermediate value theorem there exists at least one $x \in \left(0, \frac{\pi}{2}\right)$ such that $\sin x = \frac{1}{2}$.
\end{example}

Note: we do require continuity for the intermediate value theorem to apply
\begin{example}
    Let \[
    \mathrm{sgn}(x) = \begin{cases}
        1 & x > 0 \\    
        0 & x = 0 \\ 
        -1 & x < 0    
    \end{cases}
    \]
    then $f(x) = \frac{\mathrm{sgn}(x)}{1 + x ^ 2}$ has $f(-1) = -\frac{1}{2}$ and $f(1) = \frac{1}{2}$ but $\nexists x \in (-1, 1)$ such that $f(x) = \frac{1}{5}$.

    The intermediate value theorem does not apply, as $f$ has a jump discontinuity at $x = 0$
    $\displaystyle L ^ - = \lim_{x \rightarrow 0^-}f(x) = -1\ L ^ + = \lim_{x \rightarrow 0^+}f(x) = 1$, so $L^+ \neq L^-$
\end{example}

One important application of the intermediate value theorem is finding the zeros (roots) of a function. If $f$ is continuous on $[a, b]$ and $f(a) < 0 < f(b)$ or $f(a) > 0 > f(b)$, then by the intermediate value theorem, there is at least one root such that $f(x) = 0$ between $a$ and $b$.

\begin{example}
    $f(x) = x ^ 2 - 2$ is continuous on $[1, 2]$ with $f(1) = -1 < 0$ and $f(2) = 2 > 0$, so there is at least one root of $x ^ 2 - 2 = 0$ between $1$ and $2$ (given by $x = \sqrt{2}$).
\end{example}

Iterating this leads to the bisection method, which can be used to find roots to arbitrary accuracy.

\newpage

\section{Differentiation}

\subsection{Derivative as a limit}
Geometrically, the derivative $f'(a)$ (or $\frac{df}{dx}(a)$ or $\frac{df}{dx}\left|_{x = a}\right)$)
of a function $f(x)$ at $x = a$ is equal to the slope of the (unique, non-vertical) tangent to the graph of $f$ at $a$ (if it exists).

This is naturally defined as the limit of the slope of a line segment joining two nearby points on the graph of $f$.
\[
f'(a) = \lim_{h \rightarrow 0}\frac{f(a + h) - f(a)}{h}
\]
if the limit exists.

If the limit exists we say $f$ is differentiable at $x = a$.

\begin{example}
    With $f(x) = x ^ 2$ calculate $f'(a)$ using the limit definition.
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        \begin{align*}
        f'(a) &= \lim_{h \rightarrow 0}\frac{f(a + h) - f(a)}{h} \\
        &= \lim_{h \rightarrow 0}\frac{(a + h) ^ 2 - a ^ 2}{h} \\
        &= \lim_{h \rightarrow 0}\frac{2ah + h ^ 2}{h} \\
        &= \lim_{h \rightarrow 0}(2a + h) \\
        &= 2a.
        \end{align*}
    \end{proof}
\end{example}
If $f'(a)$ exists $\forall a \in \Dom f$, we say that $f$ is differentiable and the function $f'(x)$ is called the derivative.

\begin{example}
    Calculate the derivative of $f(x) = \sin(x)$
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        \begin{align*}
            f'(x) &= \lim_{h \rightarrow 0}\frac{f(x + h) - f(x)}{h} \\
            &= \lim_{h \rightarrow 0}\frac{\sin(x + h) - \sin(x)}{h} \\
            &= \lim_{h \rightarrow 0}\frac{\sin(x)\cos(h) + \cos(x)\sin(h) - \sin(x)}{h} \\
            &= \lim_{h \rightarrow 0}\left(\sin(x)\frac{\cos(h) - 1)}{h} + \cos(x)\frac{\sin(h)}{h}\right) \\
            &= \cos(x)
        \end{align*}
    \end{proof}
\end{example}

\textbf{Notes on differentiability}

\begin{itemize}
    \item The equation of the tangent to $f$ at a point $(a, f(a))$ is given in terms of the derivative as $y = f(a) + f'(a)(x - a)$.
    \item A necessary condition for $f'(a)$ to exist is for $f$ to be continuous at $a$, but this is not a sufficient condition.
    \item There are two ways a function can fail to be differentiable at a point where it is continuous:
        \begin{itemize}
            \item The tangent line is vertical at the point e.g. $f(x) = x ^ {\frac{1}{3}}$ which is continuous but not differentiable at $x = 0$.
            \item There is no tangent line at the point e.g. $f(x) = |x|$ is continuous everywhere but not differentiable at $x = 0$.
        \end{itemize}
\end{itemize}

\subsection{The Leibniz and chain rules}
Smooth functions are functions which are infinitely differentiable.

If $f(x)$ is differentiable, then $f'(x)$ may also be differentiable we say $f$ is twice differentiable. We write $(f'(x))'$ as $f''(x)$, the $2 ^ {\text{nd}}$ derivative. More generally $f^{(n)}(x)$ is the $n^{\text{th}}$ derivative.

Other common notation includes $\frac{d^nf}{dx^n}$ or $Df,\,D^nf$.

If we have a function with time as the variable e.g. $r(t)$ (position), then we usually write $r'(t) = \dot{r}$ (velocity) and $\ddot{r}(t)$ (acceleration).

\textbf{Leibniz rule}: If $f(x)$ and $g(x)$ are both differentiable $n$ times, then so is $(fg)(x)$. We have
\[
D(fg) = (Df)g + f(Dg).
\]
\begin{align*}
    D^2(fg) &= (D^2f)g + (Df)(Dg) + (Df)(Dg) + fD ^ 2g \\
    &= (D^2f)g + 2(Df)(Dg) + fD^2g.
\end{align*}

General Leibniz rule
\[
D^n(fg) = \sum_{k = 0}^{n}\binom{n}{k}(D ^ kf)(D ^ {n - k}g)
\]

\begin{example}
    Calculate $D ^ 3(x ^ 2 \sin x)$.

    We know $D ^ 3(fg) = (D ^ 3 f)g + 3(D ^ 2f)g + 3(Df)(D ^ 2 g) + f D ^ 3g$
    
    so with
    \[
    f(x) = x ^ 2,\,Df = 2x,\,D^2f = 2,\,D^3f = 0
    \]
    \[
    g(x) = \sin x,\,Dg = \cos x,\,D^2g = -\sin x,\,D ^ 3g = -\cos x
    \]
    So
    \begin{align*}
        D ^ 3(x ^ 2 \sin x) &= 3(2)\cos x + 3(2x)(-\sin x) + x ^ 2(-\cos x) \\
        &= (6 - x ^ 2)\cos x - 6x\sin x.
    \end{align*}
\end{example}

For differentiating the composition of functions $(f \circ g)(x) = f(g(x))$
\[
(f \circ g)'(x) = f'(g(x))g'(x)
\]
or $\frac{d}{dx}f(g(x)) = \frac{df}{dg}\frac{dg}{dx}$

\begin{example}
    Calculate $\frac{d}{dx}\left((x ^ 2 + 3x) ^ 4\right)$.

    Let $f(x) = x ^ 4,\,g(x) = x ^ 2 + 3x$ then $\left((x ^ 2 + 3x) ^ 4\right) = f(g(x))$. Then $f'(x) = 4x ^ 3,\quad g'(x) = 2x + 3$
    \[
    \frac{d}{dx}\left((x ^ 2 + 3x) ^ 4\right) = 4(x ^ 2 + 3x) ^ 3 \cdot (2x + 3)
    \]
\end{example}

\subsection{L' H\^opital's rule}
Recall, if $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M \neq 0$, then
\[
\lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \frac{L}{M}.
\]
If $L \neq 0$ and $M = 0$ then $\lim_{x \rightarrow a}\frac{f(x)}{g(x)}$ does not exist.

If $L = M = 0$ then we say the limit is of in-determinant form. This means the limit might or might not have a well defined value, but we cannot tell simply by knowing that $L$ and $M$ are both $0$.

Limits of the following form are all in-determinant
\begin{align*}
    \frac{0}{0} & \frac{\infty}{\infty} & 0 \cdot \infty & \infty - \infty \\
    0 ^ 0 & 1 ^ \infty & \infty^0
\end{align*}

\textbf{L' H\^opital's rule} applies to the $1^{\text{st}}$ (and $2^{\text{nd}}$) case.

If $f(x)$ and $g(x)$ are differentiable on
\[
I = (a - h,\,a) \cup (a,\,a + h)\quad\text{for some }h > 0
\]
with $\lim_{x \rightarrow a}f(x) = \lim_{x \rightarrow a}g(x) = 0$ and if $\lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}$ exists and $g'(x) \neq 0\ \forall x \in I$, then
\[
\lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}.
\]
\begin{proof}
    For the case when $f'(a)$ and $g'(a)$ both exist and continuous and $g'(a) \neq 0$.

    \begin{align*}
        \lim_{x \rightarrow a}\frac{f(x)}{g(x)} &= \lim_{x \rightarrow a}\frac{f(x) - f(a)}{g(x) - g(a)}\footnotemark \\
        &= \lim_{x \rightarrow a}\frac{\frac{f(x) - f(a)}{x - a}}{\frac{g(x) - g(a)}{x - a}}\footnotemark \\
        &= \frac{\lim_{x \rightarrow a}\frac{f(x) - f(a)}{x - a}}{\lim_{x \rightarrow a}\frac{g(x) - g(a)}{x - a}} \\
        &= \frac{f'(a)}{g'(a)} \\
        &= \lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}.
    \end{align*}
        \footnotetext{Adding $0$ by subtracting $f(a)$ as it approaches $0$.}
        \footnotetext{Dividing by $1$ as $x - a \rightarrow 0 \text{ as } x \rightarrow a$.}
\end{proof}

\begin{example}
    Calculate
    \[
    \lim_{x \rightarrow 0}\frac{\sin x}{x}.
    \]
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        This satisfies all the needed properties,
        with $f(x) = \sin x$ and $g(x) = x$, $f'(x) = \cos x$, $g'(x) = 1$
        and so by L'H\^opital's rule
        $\lim_{x \rightarrow 0}\frac{\sin x}{x} = \lim_{x \rightarrow 0}\frac{\cos x}{1} = 1$.
    \end{proof}
\end{example}

If applying L'H\^opital's rule leaves another indetermininat form,
then we can apply L'H\^opital's rule again, and so on.

\begin{example}
    \begin{align*}
    \lim_{x \rightarrow 0}\frac{2\sin x - \sin (2x)}{x - \sin x} &= \lim_{x \rightarrow 0}\frac{2\cos x - 2\cos (2x)}{1 - \cos x} \\
    &= \lim_{x \rightarrow 0}\frac{-2\sin x + 4\sin(2x)}{\sin x} \\
    &= \lim_{x \rightarrow 0}\frac{-2\cos x + 8\cos (2x)}{\cos x} \\
    &= \frac{-2 + 8}{1} = 6.
    \end{align*}
\end{example}

\subsection{Boundedness and Monotonicity}
Let $f(x)$ be a function defined on some interval $I$.
\begin{definition}
    $f(x)$ is bounded (from) above in $I$ by an upper bound $k_1$ if $f(x) \leq k_1\ \forall x \in I$.
\end{definition}

$f(x)$ need not take the value $k_1$ in $I$.
If $\exists x_1 \in I$ such that $f(x_1) = k_1$ then
we say the upper bound $k_1$ is attained,
and $k_1$ is the global (in some interval) maximum value of $f(x)$ in $I$.

Similarly
\begin{definition}
    If $f(x)$ is bounded (from) below in $I$ by a lower bound $k_2$ if $f(x) \geq k_2,\ \forall x \in I$.
\end{definition}

If $\exists x_2 \in I$ such that $f(x_2) = k_2$,
then the lower bound is attained,
and $k_2$ is the global (in some interval) minimum.

\begin{definition}
    $f(x)$ is bounded in $I$ if it is bounded both above and below.
    That is if $\exists k$ such that
    \[
    |f(x)| \leq k\quad\forall x \in I
    \]
\end{definition}

As for other properties if we don't specify an interval,
we take it to be the domain.

\begin{example}
    $\cos x$ is bounded (in $\R$) as $|\cos x| \leq 1\quad\forall x \in \R$.
    Both the upper and lower bounds ($1$, $-1$ respectively) are attained (e.g. $0$ and $\pi$),
    and so these are the global maximum and minimum values.
\end{example}

\begin{example}
    \[
    f(x) = \frac{\mathrm{sgn}(x)}{1 + x ^ 2}\quad\text{for } x \in [-1,\,1].
    \]
    $f(x)$ is bounded on $[-1,\, 1]$,
    as $|f(x)| \leq 1\ \forall x \in [-1,\,1]$,
    but neither bound is attained (as $f(0) = 0$) and $f(x)$ has no global maximum or minimum on $[-1,\,1]$.
\end{example}

\begin{example}
    $\tan x$ is bounded from below on $\left[0,\,\frac{\pi}{2}\right)$ but not above.
\end{example}

\begin{theorem}[Extreme Value Theorem]\label{calc_thm_extvalthm}
    If $f(x)$ is continuous on a closed interval $[a,\,b]$,
    then it is bounded on that interval,
    and has upper and lower bounds which are attained,
    i.e. $\exists x_1, x_2 \in [a, b]$ such that $f(x_2) \leq f(x) \leq f(x_1)\ \forall x \in [a,\,b]$.
\end{theorem}

\begin{definition}
    $f(x)$ is monotonic increasing in $[a,\,b]$ if
    $f(x_1) \leq f(x_2)\ \forall x_1, x_2$ such that
    $a \leq x_1 < x_2 \leq b$.
\end{definition}

\begin{definition}
    $f(x)$ is strictly monotonic increasing in $[a,\,b]$
    if $f(x_1) < f(x_2)\ \forall x_1, x_2$ such that
    $a \leq x_1 < x_2 \leq b$.
\end{definition}

Similarly for decreasing.

\begin{example}
    $\mathrm{sgn}(x)$ is monotonic increasing in $[-1,\,1]$
    but not strictly monotonic increasing.
\end{example}

\begin{example}
    $x ^ 2$ is strictly monotonic increasing on $[0,\,b]$ for any $b > 0$.
\end{example}

\subsection{Critical points}

\begin{definition}
    We say that $f(x)$ has a local maximum (or local minimum) at $x = a$ if $\exists h > 0$ such that $f(a) \geq f(x)$ (or $f(a) \leq f(x)$ respectively) $\forall x \in (a - h, a + h)$.
\end{definition}

A local extremum is either a local maximum (max) or a local minimum (min).

If $f(x)$ has a local maximum (or minimum) at $x = a$ and is differentiable at $x = a$,
then $f'(a) = 0$.
\begin{proof}
    Since $f(x)$ is differentiable at $x = a$
    \[
    \lim_{x \rightarrow a ^ {-}}\frac{f(x) - f(a)}{x - a} = 
    \lim_{x \rightarrow a ^ {+}}\frac{f(x) - f(a)}{x - a} =
    f'(a).
    \]
    If $f(x)$ has a local maximum at $x = a$,
    then
    \[
    \frac{f(x) - f(a)}{x - a} \leq 0\quad\text{for}\quad x \in (a, a + h)
    \]
    so $\displaystyle \lim_{x \rightarrow a^{+}}\frac{f(x) - f(a)}{x - a} = f'(a) \leq 0$
    and
    \[
    \frac{f(x) - f(a)}{x - a} \geq 0\quad\text{for}\quad x \in (a - h, a)
    \]
    so $\displaystyle \lim_{x \rightarrow a^{-}}\frac{f(x) - f(a)}{x - a} = f'(a) \geq 0$.
    Since $0 \leq f'(a) \leq 0$,
    $f'(a) = 0$.
\end{proof}

\begin{definition}
    $f(x)$ has a stationary point at $x = a$ if it is differentiable at $x = a$ and $f'(a) = 0$.
\end{definition}

\begin{definition}
    An interior point $x = a$ of the domain of $f(x)$ is a critical point if either $f'(a) = 0$ or $f'(a)$ doesn't exist.
\end{definition}

\begin{example}
    $f(x) = x ^ 3$ has $f'(0) = 0$ so $x = 0$ is a stationary point (and critical point\footnote{As all stationary points are critical.}) but not a local extremum, as $f(x) > f(0)\quad\forall x \in (0, h)$ but $f(x) < f(0)\quad \forall x \in (-h, 0)$.
\end{example}

\begin{definition}
    If $f(x)$ is twice differentiable in an open interval around $x = a$ with $f''(a) = 0$ and if $f''(x)$ changes sign at $x = a$,
    then $x = a$ is a point of inflection.
\end{definition}

\begin{example}
    If $f(x) = x ^ 3,\ f''(x) = 6x$, so $f''(0) = 0$.
    Since $f''(x) < 0$ for $x < 0$ and $f''(x) > 0$ for $x > 0$,
    then $x = 0$ is a point of inflection.
\end{example}

\textbf{The first derivative test}

Suppose $f(x)$ is continuous at a critical point $x = a$.
\begin{enumerate}[label = \roman*)]
    \item If $\exists h > 0$ such that $f'(x) < 0\ \forall x \in (a - h, a)$ and $f'(x) > 0\ \forall x \in (a, a + h)$,
    then $x = a$ is a local minimum.
    \item If $\exists h > 0$ such that $f'(x) > 0\ \forall x \in (a - h, a)$ and $f'(x) < 0\ \forall x \in (a, a + h)$,
    then $x = a$ is a local maximum.
    \item If $\exists h > 0$ such that $f'(x)$ has constant sign $\forall x \neq a$ in $(a - h, a + h)$,
    then $x = a$ is not a local extremum.
\end{enumerate}

\begin{example}
    $f(x) = |x|$ is a continuous function,
    but $f'(x) \neq 0$ so there doesn't exist any stationary point,
    $x = 0$ is a critical point
    (as the $1^{\text{st}}$ derivative doesn't exist here)
    $f'(x) < 0\ \forall x \in (-1, 0)$ and $f'(x) > 0 \ \forall x \in (0, 1)$
    so $x = 0$ is a local minimum.
\end{example}

\begin{example}
    $f(x) = x ^ 4 - 2x ^ 3$ has $f'(x) = 4x ^ 3 - 6x ^ 2 = 2x ^ 2(2x - 3)$
    we have critical points (all stationary) at $x = 0, \frac{3}{2}$.
    Since $f'(x) < 0$ for $x < \frac{3}{2}$ near $\frac{3}{2}$ and
    $f'(x) > 0\ \forall x > \frac{3}{2}$,
    $x = \frac{3}{2}$ is a local minimum.
    Since $f'(x) < 0$ in a small interval around $x = 0$,
    $x = 0$ is not a local extremum.
\end{example}


\textbf{Second derivative test}

Suppose $f(x)$ is twice differentiable at $x = a$,
with $f'(a) = 0$.
\begin{enumerate}[label = \roman*)]
    \item If $f''(a) > 0$ then $x = a$ is a local minimum.
    \item If $f''(a) < 0$ then $x = a$ is a local maximum.
\end{enumerate}
If $f''(x) = 0$ the $2^{\text{nd}}$ derivative test doesn't tell us anything.

\begin{example}
    $f(x) = 2x ^ 3 - 9x ^ 2 + 12x$ has $f'(x) = 6x ^ 2 - 18x + 12 = 6(x ^ 2 - 3x + 2) = 6(x - 1)(x - 2)$.
    So $x = 1$ and $x = 2$ are stationary.
    $f''(x) = 12x - 18 = 6(2x - 3)$
    so $f''(1) < 0$ and $x = 1$ is a local maximum.
    $f''(2) > 0$ and $x = 2$ is a local minimum.
\end{example}

\textbf{Endpoints}
\begin{definition}
    If $c \in \Dom f$ and if $\exists h > 0$ such that $f(x)$ is defined on $[c, c + h)$ but not $(c - h, c]$ (or vice-versa)
    then $x = c$ is an endpoint of $f(x)$.
\end{definition}

\begin{example}
    If $\Dom f = [a, b]$ then $x = a$ and $x = b$ are endpoints.
\end{example}

\begin{definition}
    If $x = c$ is an endpoint of $f(x)$,
    then $f(x)$ has an endpoint maximum (or minimum) at $x = c$
    if $f(x) \leq f(c)$ (or $f(x) \geq f(c)$ respectively)
    for points in the domain close to $c$.
\end{definition}

Note: If $f(x)$ is continuous and differentiable close to an endpoint,
then the sign of the derivative can determine the nature of the endpoint.

If $f(x)$ is continuous on $[a, b]$,
then all global extrema in this interval are attained either at critical points or endpoints.

\begin{example}
    Find the global extrema of $f(x) = x ^ 2 - 2|x| + 2$ for $x \in \left[-\frac{1}{2}, 2\right]$.
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        First, note
        \[
        f(x) = \begin{cases}
            x ^ 2 - 2x + 2 & x \in [0, 2] \\
            x ^ 2 + 2x + 2 & x \in \left[-\frac{1}{2}, 0\right]
        \end{cases}
        \]
        On $(0, 2)$ $f'(x) = 2x - 2$,
        so critical point at $x = 1$, $f(1) = 1$.

        On $\left(-\frac{1}{2}, 0\right)$ $f'(x) = 2x + 2$ so no critical point in this interval.
        \[
        \lim_{x \rightarrow 0^{+}}f'(x) = -2 \neq 2 = \lim_{x \rightarrow 0^{-}}f'(x)
        \]
        so $f'(x)$ does not exist at $x = 0$.
        So $x = 0$ is a critical point with $f(0) = 2$.
        The endpoint values are $f\left(-\frac{1}{2}\right) = \frac{5}{4}$ and $f(2) = 2$.
        So the global minimum is $1$ (at $x = 1$) and the global maximum is $2$ (at $x = 0$ and $x = 2$).
    \end{proof}
\end{example}

\subsection{Rolle's theorem}

\begin{theorem}[Rolle's theorem]\label{calc_thm_rolle}
    If $f(x)$ is continuous on $[a, b]$, differentiable on $(a, b)$ and $f(a) = f(b)$, then $\exists c \in (a, b)$ such that $f'(c) = 0$.
    \begin{proof}
        By \autoref{calc_thm_extvalthm} $\exists x_1, x_2 \in [a, b]$ such that
        \[
        f(x_1) \leq f(x) \leq f(x_2)\ \forall x \in [a, b]
        \]
        If $x_2 \in (a, b)$ then $x_2$ is a local maximum and so $f'(x_2) = 0$.

        If $x_1 \in (a, b)$ then $x_1$ is a local minimum and so $f'(x_1) = 0$.

        Otherwise, $x_1$ and $x_2$ are endpoints $a, b$.
        But since $f(a) = f(b),\quad f(x_1) = f(x_2) = f(a)$.
        So $f(a) \leq f(x) \leq f(a)\ \forall x \in [a, b]$.
        So $f(x)$ is constant on $[a, b]$ and $f'(x) = 0\ \forall x \in (a, b)$.
    \end{proof}
\end{theorem}

\begin{corollary}\label{calc_corl_rolle_1}
    If $f(x)$ is differentiable on an open interval $I$,
    then each pair of zeros of $f$ is separated by at least one zero of $f'(x)$.
    \begin{proof}
        This follows from \autoref{calc_thm_rolle} with $a$ and $b$ zeros of $f(x)$ such that $f(a) = f(b) = 0$.
    \end{proof}
\end{corollary}
We can use this to find a bound on the number of distinct zeros of a function
\begin{example}
    Show $f(x) = \frac{1}{7}x ^ 7 - \frac{1}{5}x ^ 6 + x ^ 3 - 3x ^ 2 - x$ has no more than $3$ distinct real roots.
    \begin{proof}
        We prove this by contradiction.
        Assume there are at least $4$ real roots of $f(x)$.
        Then by \autoref{calc_corl_rolle_1} there are at least $4$ distinct real roots of $f'(x)$.
        \[
        f'(x) = x ^ 6 - \frac{6}{5}x ^ 5 + 3x ^ 2 - 6x - 1.
        \]
        Then applying \autoref{calc_corl_rolle_1} to $f'(x)$,
        there are at least $2$ distinct real roots of $f''(x)$.
        \begin{align*}
            f''(x) &= 6x ^ 5 - 6x ^ 4 + 6x - 6 \\
            &= 6(x ^ 5 - x ^ 4 + x - 1) \\
            &= 6(x - 1)(x ^ 4 + 1).
        \end{align*}
        But $x ^ 4 \geq 0$, so $x ^ 4 \neq -1$ in $\R$.
        So $f''(x)$ has only $1$ real root.
    \end{proof}
\end{example}

\subsection{The mean value theorem}

\begin{theorem}[The mean value theorem]\label{calc_thm_meanval}
    If $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$,
    then $\exists c \in (a, b)$ such that
    \[
    f'(c) = \frac{f(b) - f(a)}{b - a}.
    \]
    \begin{proof}
        Let $g(x) = (b - a)(f(x) - f(a)) - (x - a)(f(b) - f(a))$.
        Then $g(a) = 0 = g(b)$ and $g(x)$ satisfies the requirements of \autoref{calc_thm_rolle}.
        So $\exists c \in (a, b)$ such that $g'(c) = 0$.
        Bur $g'(x) = (b - a)f'(x) - (f(b) - f(a))$ so $g'(c) = (b - a)f'(c) - (f(b) - f(a)) = 0$
        implies $f'(c) = \frac{f(b) - f(a)}{b - a}$.
    \end{proof}
\end{theorem}

We can use \autoref{calc_thm_meanval} to prove results relating monotonicity to the sign of the derivative.
    
\begin{example}
    Suppose $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$ with $f'(x) \geq 0\ \forall x \in (a, b)$, then $f(x)$ is monotonic increasing.
    \begin{proof}
        If $a \leq x_1 < x_2 \leq b$,
        then by \autoref{calc_thm_meanval},
        $\exists c \in (x_1, x_2)$ such that $f'(c) = \frac{f(_x) - f(x_1)}{x_2 - x_1}$.
        Since $f'(x) \geq 0\ \forall x \in (x_1, x_2)$,
        then we must have $f(x_2) \geq f(x_1)$, i.e. $f(x)$ is monotonic increasing.
    \end{proof}
\end{example}

We can show similar results for monotonic decreasing,
and strictly increasing/decreasing.

\subsection{The inverse function rule}
The inverse function rule: if $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$ with $f'(x) > 0\ \forall  x \in (a, b)$,
then its inverse exists $g(y)$
(such that $g(f(x)) = x$)
is differentiable $\forall y$ such that
\[
f(a) < y < f(b)
\]
with
\[
g'(y) = \frac{1}{f'(g(y))}.
\]
Similarly if $f'(x) < 0$ then $g(y)$ is differentiable then true $\forall y$ such that
\[
f(b) < y < f(a).
\]
Assuming $g(y)$ is differentiable, this follows from the chain rule:
\[
\frac{d}{dx}g(f(x)) = g'(f(x))f'(x) = 1
\]
and so with $y = f(x)\quad g'(y) = \frac{1}{f'(g(y))}$.

\begin{example}
    $f(x) = \sin x$ has $f'(x) = \cos x$ so $f'(x) > 0\ \forall x \in \left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$.
    The inverse $g(y) = \sin ^ {-1}(y)$ is therefore differentiable $(-1, 1)$ with
    \begin{align*}
        g'(y) &= \frac{1}{f'(g(y))} \\
        &= \frac{1}{\cos(g(y))} \\
        &= \frac{1}{\sqrt{1 - \sin ^ 2 (g(y))}}
    \end{align*}
    But $\sin(g(y)) = f(g(y)) = y$ so
    $g'(y) = \frac{1}{\sqrt{1 - y ^ 2}}$
    So $\frac{d}{dx}\sin ^ {-1}(x) = \frac{1}{\sqrt{1 - x ^ 2}}$.
\end{example}

\subsection{Partial derivatives}
We can also consider functions of more than one variables
where $f(x, y)$ can be thought of as the height of a function over the $xy$-plane.
We can now think of the rate of change of $f(x, y)$ as we vary $x$ or $y$,
keeping the other fixed.

\begin{definition}
    The partial derivative of $f(x, y)$ with respect to $x$ is the following
    \[
    \frac{\partial f}{\partial x}(x, y) = \lim_{h \rightarrow 0}\frac{f(x + h, y) - f(x, y)}{h}
    \]
    assuming this limit exists.

    Similarly
    \[
    \frac{\partial f}{\partial y}(x, y) = \lim_{h \rightarrow 0}\frac{f(x, y + h) - f(x, y)}{h}
    \]
    is the partial derivative of $f$ with respect to $y$.
\end{definition}

In practice we can often simply differentiate with respect to one variable while treating the other as constant.

\begin{example}
    Let $f(x, y) = x ^ 2 y ^ 3 - \sin(x)\cos(y) - y$.
    Then
    \[
    \frac{\partial f}{\partial x} = 2xy ^ 3 - \cos(x)\cos(y).
    \]
    \[
    \frac{\partial f}{\partial y} = 3x ^ 2y ^ 2 + \sin(x)\sin(y) - 1.
    \]
\end{example}

We can also use the notation $f_x = \frac{\partial f}{\partial x}$ and $f_y = \frac{\partial f}{\partial y}$.

We can take partial derivatives of partial derivatives,
to get second partial derivatives, such as
\[
\frac{\partial ^ 2 f}{\partial x ^ 2} = \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial x}\right),
\quad
\frac{\partial ^ 2 f}{\partial y ^ 2} = \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial y}\right),
\quad
\frac{\partial ^ 2 f}{\partial x \partial y} = \frac{\partial}{\partial x}\left(\frac{\partial f}{\partial y}\right),
\quad
\frac{\partial ^ 2 f}{\partial y \partial x} = \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right).
\]
Or $f_{xx} = \frac{\partial ^ 2 f}{\partial x ^ 2}$ etc.
$f_{xy}$ is used as $(f_x)_y) = \frac{\partial}{\partial y}\left(\frac{\partial f}{\partial x}\right) = \frac{\partial ^ 2 f}{\partial y \partial x}$.

Some people use this to mean $\frac{\partial ^ 2 f}{\partial x \partial y}$.

\begin{example}
    Let $f(x, y) = x ^ 2 y ^ 3 - \sin(x)\cos(y) - y$.
    We have 
    \[
    \frac{\partial ^ 2 f}{\partial x ^ 2} = 2y ^ 3 + \sin(x)\cos(y).
    \]
    \[
    \frac{\partial ^ 2 f}{\partial y ^ 2} = 6x ^ 2 y + \sin(x)\cos(y).
    \]
    \[
    \frac{\partial ^ 2 f}{\partial x \partial y} = 6xy ^ 2 + \cos(x)\sin(y).
    \]
    \[
    \frac{\partial ^ 2 f}{\partial y \partial x} = 6xy ^ 2 + \cos(x)\sin(x).
    \]
\end{example}

Note: we have $\frac{\partial ^ 2 f}{\partial x \partial y} = \frac{\partial ^ 2 f}{\partial y \partial x}$.
This is true in general if $f$ and all first and second order partial derivatives are continuous (Clairaut's theorem).

\newpage

\section{Integration}

\subsection{Indefinite and Definite integrals}
\begin{definition}
    A function $F(x)$ is called an indefinite integral (or antiderivative) of a function $f(x)$ in the interval $(a, b)$ if $F(x)$ is differentiable
    $F'(x) = f(x) \forall x \in (a, b)$.
    We write
    \[
    F(x) = \int f(x)\,dx
    \]
\end{definition}

Note
\begin{itemize}
    \item If $F(x)$ is an indefinite integral,
    then so is $F(x) + c$,
    for any constant $c$,
    $\frac{d}{dx}(F(x) + c) = \frac{d}{dx}(F(x))$.
    It is important to include this arbitrary constant when writing indefinite integrals.
    \item If $F_1(x)$ and $F_2(x)$ are both indefinite integrals of $f(x)$,
    then $F_1(x) - F_2(x) = c$.
    \begin{proof}
        $\frac{d}{dx}(F_1(x) - F_2(x)) = f(x) - f(x) = 0$.
        Therefore $F_1(x) - F_2(x)$ is constant.
    \end{proof}
\end{itemize}

\begin{example}\phantom{}
    \begin{itemize}
        \item $\int\cos(x)\,dx = \sin(x) + c$ in $\R$.
        \item $\int\frac{1}{x ^ 2} = -\frac{1}{x} + c$ in $\R \setminus\{0\}$.
        \item $\int\mathrm{sgn}(x)\,dx = |x| + c$ in $\R \setminus \{0\}$.
    \end{itemize}
\end{example}

\begin{definition}
    We say $f(x)$ is integrable in $(a, b)$ if it has an indefinite integral on $(a, b)$ which is continuous on $[a, b]$.
\end{definition}

\begin{example}\phantom{}
    \begin{itemize}
        \item $\cos(x)$ is integrable in any finite interval $(a, b)$ as $\sin(x)$ is continuous on $\R$.
        \item $\frac{1}{x ^ 2}$ is not integrable on $(0, 1)$ because the indefinite integral $-\frac{1}{x}$ is not continuous at $x = 0$.
        \item $\mathrm{sgn}(x)$ is integrable on $(0, 1)$ because its indefinite integral $|x|$ is continuous on $[0, 1]$.
    \end{itemize}
\end{example}

Definite integrals

A definite integral of a function $f(x)$ gives the (signed) area under the graph of $f(x)$.

We define this as the limit of a Riemann sum,
by splitting the area into rectangles.
There are many ways we could do this.
If $f(x)$ is continuous and the rectangles has small width,
we expect this to be a good approximation and the error to be reduced to zero by taking the limit of the width of the rectangles to zero.

We say this limit exists if it is independent of how we construct the rectangles.

\begin{definition}
    A subdivision of $[a, b]$ is a partition of the interval into a finite number of sub-intervals (i.e. breaking the interval up)
    \[
    [x_0, x_1], [x_1, x_2], \dotsc, [x_{n - 1}, x_n]
    \]
    with $a = x_0 < x_1 < x_2 < \dotsi < x_n - b$
\end{definition}
The norm of the subdivision $|S|$ is the maximum of the subinterval lengths.

The numbers $z_1, z_2, \dotsc z_n$ for a set of sample points from $S$ if $z_j \in [x_{j - 1}, x_j]$
for $j = 1, \dotsc, n$.
\begin{definition}
    Let $f(x)$ be a function defined on $[a, b]$.
    The Riemann sum $\mathcal{R}$ (which really depends on a subdivision and set of sample points,
    $\mathcal{R}(S, \{z_i\}_{i = 1}^{n})$ is
    \[
    \mathcal{R} = \sum_{j = 1}^{n}(x_j - x_{j - 1})f(z_j).
    \]
\end{definition}

The definite integral is then given in terms of a Riemann sum by taking a limit as $|S|$ goes to zero.
We say this limit exists if it is independent of subdivision and set of sample points.
\begin{definition}
    Let $f(x)$ be defined $\forall x \in [a, b]$.
    The definite integral of $f(x)$ from $a$ to $b$ is then
    \[
    \int_{a}^{b}f(x)\,dx = \lim_{|S| \rightarrow 0}\mathcal{R}.
    \]
\end{definition}
It can be shown that this limit exists if $f(x)$ is continuous on the interval $[a, b]$.

The definite integral satisfies the following properties.
Let $f(x)$ and $g(x)$ be integrable in $(a, b)$.
\begin{enumerate}[label = \roman*)]
    \item Linearity. If $\lambda, \mu \in \R$,
    then $(\lambda f + \mu g)(x)$ is integrable in $(a, b)$, with 
    \[
    \int_a^b(\lambda f + \mu g)(x)\,dx = \lambda \int_a^bf(x)\,dx + \mu\int_a^bg(x)\,dx.
    \]
    \item If $c \in [a, b]$, then
    \[
    \int_a^bf(x)\,dx = \int_a^cf(x)\,dx + \int_c^bf(x)\,dx.
    \]
    \item If $f(x) \geq g(x)\ \forall x \in (a, b)$,
    then
    \[
    \int_a^bf(x)\,dx \geq \int_a^bg(x)\,dx.
    \]
    \item If $m \geq f(x) \leq M\ \forall x \in [a, b]$ then,
    \[
    m(b - a) \leq \int_a^bf(x)\,dx \leq M(b - a).
    \]
\end{enumerate}

\subsection{The fundamental theorem of calculus}
The following theorem connects definite and indefinite integrals.
\begin{theorem}[The fundamental theorem of calculus (FTOC)]
    If $f(x)$ is continuous on $[a, b]$,
    then the function
    \[
    F(x) = \int_a^xf(t)\,dt
    \]
    defined for $x \in [a, b]$ it is continuous on $[a, b]$,
    differentiable on $(a, b)$ and is an indefinite integral of $f(x)$ on $(a, b)$,
    i.e. $F'(x) = \frac{d}{dx}\int_a^xf(t)\,dt = f(x)\ \forall x \in (a, b)$.

    Furthermore if $\Tilde{F}(x)$ is any indefinite integral of $f(x)$ on $(a, b)$,
    then
    \[
    \int_a^bf(t)\,dt = \Tilde{F}(b) - \Tilde{F}(a) = \left[\Tilde{F}(x)\right]^b_a
    \]
    \begin{proof}[Proof Sketch]\renewcommand{\qedsymbol}{$\triangle$}
        For $a \leq x < x + h < b$,
        we have
        \begin{align*}
            F(x + h) - F(x) &= \int_a^{x + h}f(t)\,dt - \int_a^xf(t)\,dt \\
            &= \int_x^af(t)\,dt + \int_a^{x + h}f(t)\,dt \\
            &= \int_x^{x + h}f(t)\,dt&\text{using property (ii)}.
        \end{align*}
        Let $m(h)$ and $M(h)$ denote the minimum and maximum values of $f(x)$ on $[x, x + h]$.
        Then by (iv)
        \[
        h \cdot m(h) \leq \int_x^{x + h}f(t)\,dt \leq h \cdot M(h)
        \]
        and so $m(h) \leq \frac{F(x + h) - F(x)}{h} \leq M(h)$.
        Since $f(x)$ is continuous on $[x, x + h]$ we have $\displaystyle\lim_{h \rightarrow 0 ^ +}m(h) = f(x) = \lim_{h \rightarrow 0 ^ +}M(h)$
        and so
        \[
        \lim_{h \rightarrow 0 ^ +}\frac{F(x + h) - F(x)}{h} = f(x)
        \]
        by \autoref{calc_thm_squeze}.

        We can give a similar argument for $\lim_{h \rightarrow 0 ^ -}$,
        and putting these together
        \[
        \lim_{h \rightarrow 0}\frac{F(x + h) - F(x)}{h} = f(x).
        \]
        Showing $F(x)$ is an indefinite integral of $f(x)$.

        For the final part,
        if $\Tilde{F}(x)$ is an indefinite integral of $f(x)$ in $(a, b)$,
        then $\Tilde{F}(x) = F(x) + c$ for some $c \in \R$,
        and so
        \begin{align*}
        \left[\Tilde{F}(x)\right]_a^b &= \Tilde{F}(b) - \Tilde{F}(a) = (F(b) + c) - (F(a) + c) \\
        &= F(b) - F(a) \\
        &= \int_a^bf(b)\,dt - \int_a^af(t)\,dt \\
        &= \int_a^bf(b)\,dt.
        \end{align*}
    \end{proof}
\end{theorem}

The fundamental theorem of calculus also gives a simple rule for differentiating a definite integral with respect to its limits.
\begin{example}
    \[
    \frac{d}{dx}\int_0^x\frac{1}{1 + \sin ^ 2 (t)}\,dt = \frac{1}{1 + \sin ^ 2 (x)}.
    \]
\end{example}
This can be combined with the chain rule for more complicated expressions.
\begin{example}
    \[
    \frac{d}{dx}\int_0^{x ^ 2}\frac{1}{1 + e ^ t}\,dt
    \]
    let $u = x ^ 2$
    \begin{align*}
    \frac{d}{dx}\int_0^u\frac{1}{1 + e ^ t}\,dt &= \left(\frac{d}{du}\int_0^u\frac{1}{1 + e ^ t}\,dt\right)\left(\frac{du}{dx}\right) \\
    &= \frac{1}{1 + e ^ u} \cdot 2x \\
    &= \frac{2x}{1 + e ^ {x ^ 2}}
    \end{align*}
\end{example}

\subsection{Limits with logarithms powers and exponentials}
\begin{lemma}\label{calc_lem_lemma1}
    $\forall x \geq 0\quad e ^ x \geq 1 + x$
    \begin{proof}
        Consider $f(x) = e ^ x - (1 + x)$.
        We have $f(0) = 0$ and $f'(x) = e ^ x - 1 \geq 0\ \forall x \geq 0$,
        so $f(x)$ is monotonic increasing on $[0, \infty]$.
        So $f(x) \geq 0$ on $[0, \infty)$.
    \end{proof}
\end{lemma}
\begin{lemma}\label{calc_lem_lemma2}
    $\forall x \geq 0$ and positive integer $n$
    \[
    e ^ x \geq \sum_{j = 0}^{n}\frac{x ^ j}{j!}.
    \]
    \begin{proof}
        Note that when $n = 1$,
        this is \autoref{calc_lem_lemma1}.
        Now let $f_n(x) = e ^ x - \sum_{j = 0}^n\frac{x ^ j}{j!} = e ^ x - \left(1 + x + \frac{x ^ 2}{2} + \dotsc + \frac{x ^ n}{n!}\right)$.
        Note $f_n(0) = 0\ \forall n \in \N$
        \[
        f_n'(x) = e ^ x - \left(1 + x + \frac{x ^ 2}{2} + \dotsc + \frac{x ^ {n - 1}}{(n - 1)!}\right) = e ^ x - \sum_{j = 0}^{n = 1}\frac{x ^ j}{j!} = f_{n - 1}(x).
        \]
        So since $f_1(x) \geq 0$ by \autoref{calc_lem_lemma1},
        $f_2'(x) = f_{n - 1}(x) \geq 0$,
        so
        $f_3(x) \geq 0\ \forall x \geq 0$.
        Then $f_3'(x) \geq 0$ and so $f_3(x) \geq 0$ and so on (induction).
    \end{proof}
\end{lemma}

Result 1: Powers beat logs

For any $a > 0$, $\displaystyle\lim_{x \rightarrow \infty}\frac{\log x}{x ^ a} = 0$
\footnote{$\log x$ generally means $\ln x$.}
\begin{proof}
    Let $x = e ^ y$ then $\displaystyle\lim_{x \rightarrow \infty}\frac{\log(x)}{x ^ a} = \lim_{y \rightarrow \infty}\frac{y}{e ^ {ay}}$
    For $y > 0$,
    then using \autoref{calc_lem_lemma2} with $n = 2$
    \[
    0 \leq \frac{y}{e ^ {ay}} \leq \frac{y}{1 + ay + \frac{(ay) ^ 2}{2}} \leq \frac{y}{\frac{1}{2}a ^ 2 y ^ 2} = \frac{2}{a ^ 2 y}
    \]
    since $\lim_{y \rightarrow \infty}\frac{2}{a ^ 2y} = 0$ then by \autoref{calc_thm_squeze}
    \[
    \lim_{y \rightarrow \infty}\frac{y}{e ^ {ay}} = 0 = \lim_{x \rightarrow \infty}\frac{\log(x)}{x ^ a}.
    \]
\end{proof}

Result 2: Exponentials beat powers

For any $a > 0$
\[
\lim_{x \rightarrow \infty}\frac{x ^ a}{e ^ x} = 0
\]
\begin{proof}
    Let $n$ be the smallest integer such that $n > a$.
    By \autoref{calc_lem_lemma2} (for $x > 0$)
    \[
    0 \leq \frac{x ^ a}{e ^ x} \leq \frac{x ^ a}{1 + x + \frac{x ^ 2}{2} + \dotsc + \frac{x ^ n}{n!}} = \frac{x ^ {a - n}}{x ^ {-n} + x ^ {1 - n} + \dotsc + \frac{1}{n!}}
    \]
    As $a - n < 0$,
    $\displaystyle\lim_{x \rightarrow \infty}\frac{x ^ {a - n}}{x ^ {-n} + \dotsc + \frac{1}{n!}} = 0$
    and so by \autoref{calc_thm_squeze}
    \[
    \lim_{x \rightarrow \infty}\frac{x ^ a}{e ^ x} = 0.
    \]
\end{proof}

Result 3: Exponential as a limit

For any $a \in \R$ $\displaystyle\lim_{x \rightarrow \infty}\left(1 + \frac{a}{x}\right) ^ x = e ^ a$
\begin{proof}
    Let $f(x) = \log(x)$ then $f'(x) = \frac{1}{x}$ and
    \[
    1 = f'(1) = \lim_{h \rightarrow 0}\frac{\log(1 + h) - \log(1)}{h} = \lim_{h \rightarrow 0}\frac{\log(1 + h)}{h}.
    \]
    Now letting $h = \frac{a}{x}$
    \[
    1 = \lim_{x \rightarrow \infty}\frac{\log\left(1 + \frac{a}{x}\right)}{\frac{a}{x}}.
    \]
    So
    \[
    a = \lim_{x \rightarrow \infty}x\log\left(1 + \frac{a}{x}\right) = \lim_{x \rightarrow \infty}\left(1 + \frac{a}{x}\right) ^ x.
    \]
    Now exponentiating
    \[
    e ^ a = e ^ {\lim_{x \rightarrow \infty}\log\left(1 + \frac{a}{x}\right) ^ x} = \lim_{x \rightarrow \infty}\left(1 + \frac{a}{x}\right) ^ x
    \]
    using that $e ^ x$ is continuous.
\end{proof}

\subsection{Integration using a recurrence relation}
One common way we find integrals related by recurrence relations is when integrating by parts.
\[
(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)\text{ by Leibniz rule}.
\]
So integrating over $[a, b]$ and using the fundamental theorem of calculus
\[
\left[f(x)g(x)\right]_a^b = \int_a^bf'(x)g(x)\,dx + \int_a^bf(x)g'(x)\,dx,
\]
which is usually written as
\[
\int_a^bf'(x)g(x)\,dx = \left[f(x)g(x)\right]_a^b - \int_a^bf(x)g'(x)\,dx.
\]
\begin{example}
    Calculate
    \[
    \int_0^1x ^ 3 e ^ x\,dx
    \]
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        Let $I_n = \int_0^1x ^ n e ^ x\,dx$ for all integers $n \geq 0$ and consider
        \begin{align*}
        I_{n + 1} &= \int_0^1 x ^ {n + 1}e ^ x\,dx \\
        &= \left[x ^ {n + 1} e ^ x\right]_0^1 - \int_0^1(n + 1)x ^ n e ^ x\,dx &\text{Integrating by parts ($f'(x) = e ^ x,\ g(x) = x ^ {n + 1}$)} \\
        &= e - (n + 1)I_n.
        \end{align*}
        Since we can easily calculate $I_0 = \int_0^1e ^ x\,dx = \left[e ^ x\right]_0^1 = (e - 1)$.
        The recurrence relation $I_{n + 1} = e - (n + 1)I_n$ can be used to calculate $I_n$ for any $n > 0$.
        \begin{align*}
        I_1 &= e - I_0 = e - (e - 1) = 1 \\
        I_2 &= e - 2I_1 = e - 2 \\
        I_3 &= e - 3I_2 = e - 3(e - 2) = 6 - 2e
        \end{align*}
        
    \end{proof}
\end{example}

Note that one can also obtain recurrence relations without integrating by parts (e.g. using trig identities).
\begin{example}
    Calculate
    \[
    \int\tan ^ 4 x\,dx.
    \]
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        \[
        I_n = \int\tan ^ n x\,dx\text{ for } n \geq 0\, (n \neq 1)
        \]
        we want to compute $I_4$.
        \begin{align*}
            I_n &= \int\tan ^ {n - 2}x(\sec ^ 2 x - 1)\,dx \\
            &= \int\tan ^ {n - 2} x\sec ^ 2 x\,dx - \int\tan ^ {n - 2}\,dx
        \end{align*}
        Then letting $u = \tan x\quad du = \sec ^ 2 x\,dx$
        \begin{align*}
            I_n &= \int u ^ {n - 2}\,du - I_{n - 2} \\
            &= \frac{u ^ {n - 1}}{n - 1} - I_{n - 2} \\
            &= \frac{\tan ^ {n - 1} x}{n - 1} - I_{n - 2}
        \end{align*}
        gives a recurrence relation.
        We have $I_0 = \int 1\,dx = x + c$,
        so $I_2 = \tan x - (x + c)$, $I_4 = \frac{1}{3}\tan ^ 3 x - \tan x + x + a$.
    \end{proof}
\end{example}

\subsection{Definite integrals using odd and even functions}
If $f_{\text{odd}}(x)$ is an integrable odd function on $[-a, a]$ then
\[
\int_{-a}^af_{\text{odd}}(x)\,dx = 0.
\]
\begin{proof}
    \begin{align*}
        \int_{-a}^af_{\text{odd}}(x)\,dx &= \int_{-a}^0f_{\text{odd}}(x)\,dx + \int_0^af_{\text{odd}}(x)\,dx \\
        &= -\int_a^0f_{\text{odd}}(-x)\,dx + \int_0^af_{\text{odd}}(x)\,dx \\
        &= \int_0^af_{\text{odd}}(x)\,dx + \int_0^af_{\text{odd}}(x)\,dx \\
        &= \int_0^a(f_{\text{odd}}(-x) + f_{\text{odd}}(x))\,dx \\
        &= 0
    \end{align*}
\end{proof}
Similarly if $f_{\text{even}}(x)$ is an integrable even function on $[-a, a]$ then
\[
\int_{-a}^af_{\text{even}}(x)\,dx = 2\int_0^af_{\text{even}}(x)\,dx.
\]
Recall that any function can be decomposed into the sum of an odd and even function,
$f(x) = f_{\text{even}}(x) + f_{\text{odd}}(x)$ then
\[
\int_{-a}^{a}f(x)\,dx = 2\int_0^af_{\text{even}}(x)\,dx
\]
\begin{example}
    Calculate
    \[
    \int_{-1}^{1}\frac{e ^ x x ^ 4}{\cosh x}
    \]
    with
    \[
    f(x) = \frac{e ^ x x ^ 4}{\cosh x}\,dx,
    \]
    recalling $\cosh(x) = \frac{e ^ x + e ^ {-x}}{2}$
    \begin{align*}
        f_{\text{even}}(x) &= \frac{1}{2}(f(x) + f(-x)) \\
        &= \frac{1}{2}\left(\frac{e ^ x x ^ 4}{\cosh(x)} + \frac{e ^ {-x} x ^ 4}{\cosh(x)}\right) \\
        &= \frac{x ^ 4}{\cosh(x)}\left(\frac{e ^ x + e ^ {-x}}{2}\right) \\
        &= x ^ 4.
    \end{align*}
    So $\int_{-1}^{1}f(x)\,dx = 2\int_0^1x ^ 4\,dx = \frac{2}{5}\left[x ^ 5\right]_0^1 = \frac{2}{5}$.
\end{example}

\begin{example}
    Calculate
    \[
    \int_{-\frac{\pi}{4}}^{\frac{\pi}{4}}\frac{(1 + 2x) ^ 3\cos(x)}{1 +  12x ^ 2}\,dx
    \]
    we have
    \begin{align*}
        f_{\text{even}}(x) &= \frac{1}{2}\left(f(x) + f(-x)\right) \\
        &= \frac{\cos(x)}{2(1 + 12x ^ 2)}((1 + 2x) ^ 3 + (1 - 2x) ^ 3) \\
        &= \frac{\cos(x)}{2(1 + 12x ^ 2)}(2 + 24x ^ 2) \\
        &= \cos(x)
    \end{align*}
    so $\displaystyle\int_{-\frac{\pi}{4}}^{\frac{\pi}{4}}f(x)\,dx = 2\int_0^{\frac{\pi}{4}}\cos(x)\,dx = 2\left[\sin(x)\right]_0^{\frac{\pi}{4}} = \frac{2}{\sqrt{2}} = \sqrt{2}$.
\end{example}

\newpage

\section{Double integrals}

\subsection{Rectangular regions}
Given a function $f(x, y)$ and a region $D$ in the $xy$-plane the double integral $\iint\limits_Df(x, y)\,dxdy$ is the signed volume of the region between the surface $Z = f(x, y)$ and the region $D$ in the $z = 0$ plane.

We define the double (definite) integral like the single definite integral,
as the limit of a Riemann sum.

For now,
let $D$ be rectangular,
$D = [a_0, a_1] \times [b_0, b_1]$
and construct a subdivision $S$ by specifying the points of a $2$D lattice,
with
\[
a_0 = x_0 < x_1 < x_2 < \dotsc < x_n = a_1\qquad b_0 = y_0 < y_1 < \dotsc < y_m = b_1.
\]
The edge lengths of these rectangles in the $xy$-plane are $dx_i = x_i - x_{i - 1},\ i = 1, \dotsc, n$ and $dy_j = y_j - y_{j - 1},\ j = 1, \dotsc, m$.

The norm $|S|$ is the maximum of the $dx_i$ and $dy_j$ we can take sample points $(p_i, q_j) \in [x_{i - 1}, x_i] \times [y_{j - 1}, y_j]$.

The area of this rectangle is $dx_idy_j$ and we can associate to each rectangle a cuboid of height $f(p_i, q_j)$.
The volume of all the cuboids is the Riemann sum
\[
R = \sum_{i = 1}^n\sum_{j = 1}^mf(p_i, q_j)dx_idy_j
\]
and the double integral is defined as the limit
\[
\iint\limits_D f(x, y)\,dxdy = \lim_{|S| \rightarrow 0}R
\]
Note that in the special case $f(x, y)$ is constant,
$f(x, y) = c$ then
\[
\iint\limits_Df(x, y)\,dxdy = C \cdot \mathrm{Area}(D)
\]

For a rectangular region $D = [a_0, a_1] \times [b_0, b_1]$,
a practical way to compute the double integral is as an iterated integral
(Fubinís theorem, valid if $f$ is continuous on $D$),
\[
\iint\limits_{D}f(x, y)\,dxdy = \int_{a_0}^{a_1}\left(\int_{b_0}^{b_1}f(x, y)\,dy\right)\,dx = \int_{b_0}^{b_1}\left(\int_{a_0}^{a_1}f(x, y)\,dx\right)\,dy.
\]
\begin{example}
    Let $D = [-2, 1] \times [0, 1]$.
    Calculate
    \[
    \iint\limits_D(x ^ 2 + y ^ 2)\,dxdy.
    \]
    \begin{align*}
        \iint\limits_D(x ^ 2 + y ^ 2)\,dxdy &= \int_{-2}^{1}\left(\int_0^1(x ^ 2 + y ^ 2)\,dy\right)\,dx \\
        &= \int_{-2}^{1}\left[yx ^ 2 + \frac{1}{3}y ^ 3\right]^1_0\,dx \\
        &= \int_{-2}^{1}\left(x ^ 2 + \frac{1}{3}\right)\,dx \\
        &= \left[\frac{1}{3}x ^ 3 + \frac{1}{3}x\right]_{-2}^1 \\
        &= \left(\frac{1}{3} + \frac{1}{3}\right) - \left(-\frac{8}{3} - \frac{2}{3}\right) \\
        &= 4.
    \end{align*}
\end{example}

\subsection{Beyond Rectangular Regions}
\begin{definition}
    A region $D$ is $y$-simple if every line parallel to the $y$-axis which intersects $D$ does so in a single line segment or point.
\end{definition}
A line must be able to be drawn such that a vertical line can go through it and not go back through it again.
The lower boundary and upper boundaries are given by the functions $y = \phi_1(x)$ and $y = \phi_2(x)$ respectively.








\end{document}