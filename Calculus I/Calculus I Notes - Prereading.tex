\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\DeclareMathOperator{\Dom}{Dom}
\DeclareMathOperator{\Ran}{Ran}

\newcommand{\limas}[3][n]{#2 \rightarrow #3 \text{ as } #1 \rightarrow \infty}

\title{Calculus I \\
    \large Prereading}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\section{Functions}

A function $f$ is a correspondence between two sets $D$ (called the domain) and $C$ (called the codomain), that assigns to each element of $D$ one and only one element of $C$. The notation to indicate the domain and codomain is $f : D \mapsto C$.

For $x \in D$ we write $f(x)$ to denote the assigned element in $C$, this value of $f$ at $x$, or the image of $x$ under $f$, where $x$ is called the argument of the function.

The set of all images is called the range of $f$ and our notation for the domain and range is
\[
\Dom f\qquad\text{and}\qquad\Ran f = \{f(x): x \in \Dom f\}.
\]

If the domain of a function $f$ is not explicitly given, then it is taken to be the maximal set of real numbers $x$ for which $f(x)$ is a real number.

E.g. $f(x) = \sqrt{2x + 4}$. Here $\Dom f = [-2, \infty)$ and $\Ran f = [0, \infty)$.

The element $x$ in the domain is called the independent variable and the element $y$ in the range is called the dependent variable.

The graph of a function $f$ is the set of all points $(x, y)$ in the $xy$-plane with $x \in \Dom f$ and $y = f(x)$, ie.
\[
\mathrm{graph}\, f = \{(x, y) : x \in \Dom f \text{ and } y = f(x)\}.
\]

The vertical line test

If any vertical line intersects the curve more than once then the curve is not the graph of a function, otherwise it is. \\


\textbf{Even and odd functions}

A function $f$ is even if $f(x) = f(-x)\,\forall\pm x\in\Dom f$.

A function $f$ is odd if $f(x) = -f(-x)\,\forall\pm x\in\Dom f$.

\textbf{Piecewise functions}

Some functions are defined piecewise, different expressions are given for different intervals in the domain.

A \textbf{step function} is a piecewise function which is constant on each piece. An example is the Heaviside step function $H(x)$ defined by
\[
H(x) = \begin{cases}
    0\quad\text{if } x < 0 \\
    1\quad\text{if } x > 0
\end{cases}
\]
Note that with this definition $\Dom H = \R \setminus \{0\}$. It is sometimes convenient to extend the domain to $\R$ by defining the value of $H(0)$.

\subsection{Operations with functions}
Given two functions $f$ and $g$ we can define the following:

\begin{itemize}
    \item the sum is $(f + g)(x) = f(x) + g(x)$, with domain $\Dom f \cap \Dom g$.
    \item the difference is $(f - g)(x) = f(x) - g(x)$, with domain $\Dom f \cap \Dom g$.
    \item the product is $(fg)(x) = f(x)g(x)$, with domain $\Dom f \cap \Dom g$.
    \item the ratio is $\left(\frac{f}{g}\right)(x) = \frac{f(x)}{g(x)}$, with domain $(\Dom f \cap \Dom g) \setminus \{x: g(x) = 0\}$.
    \item the composition is $(f \circ g)(x) = f(g(x))$, with domain $\{x \in \Dom g : g(x) \in \Dom f\}$.
\end{itemize}

\subsection{Inverse functions}
\begin{definition}
    A function $f: D \mapsto C$ is surjective (or onto) if $\Ran f = C$,
    \[
    \text{if}\  \forall y \in C\, \exists x \in D \text{ s.t. } f(x) = y.
    \]
\end{definition}

\begin{definition}
    A function $f: D \mapsto C$ is injective (or one-to-one) if $\forall x_1, x_2 \in D$ with $x_1 \neq x_2$ then $f(x_1) \neq f(x_2)$.
\end{definition}

\textbf{The horizontal line test}

If no horizontal line intersects the graph of $f$ more than once then $f$ is injective, otherwise it is not.

\begin{definition}
    A function $f : D \mapsto C$ is bijective if it is both surjective and injective.
\end{definition}

\subsection{Theorem of inverse functions}
A bijective function $f$ admits a unique inverse, denoted $f ^ {-1}$, such that
\[
(f ^ {-1} \circ f)(x) = x = (f \circ f ^ {-1})(x).
\]
It is clear from the definition that $\Dom f^{-1} = \Ran f$ and $\Ran f^{-1} = \Dom f$.

As the inverse undoes the effect of the function an equivalent definition is
\[
f(x) = y\quad\text{iff}\quad f^{-1}(y) = x.
\]

\newpage

\section{Limits and continuity}

\subsection{Definition of a limit and continuity}
A function $f(x)$ has a limit $L$ at $x = a$ if $f(x)$ is close to $L$ whenever $x$ is close to $a$.

\begin{definition}
    $f(x)$ has a limit $L$ as $x$ tends to $a$ if
    \[
    \forall\,\epsilon > 0\ \exists\,\delta > 0 \text{ s.t. } |f(x) - L| < \epsilon \text{ when } 0 < |x - a| < \delta.
    \]    
\end{definition}

We write
\[
\lim_{x \rightarrow a}f(x) = L\text{ or equivalently } f(x)\rightarrow L \text{ as } x \rightarrow a.
\]

If there is no such $L$ we can say that no limit exists.

\begin{definition}
    A function $f(x)$ is continuous at the point $x = a$ if the following three properties all hold:
    \begin{itemize}
        \item $f(a)$ exists.
        \item $\lim_{x \rightarrow a}$ exists.
        \item $\lim_{x \rightarrow a}f(x)$ is equal to $f(a)$.
    \end{itemize}
\end{definition}

\begin{definition}
    A function $f(x)$ is continuous on a subset $S$ of its domain if it is continuous at every point in $S$.
\end{definition}

\begin{definition}
    A function $f(x)$ is continuous if it is continuous at every point in its domain.
\end{definition}

\subsection{Facts about limits and continuity}
Here are some simple facts about limits (that follow from the definition)

\begin{itemize}
    \item The limit is unique
    \item If $f(x) = g(x)$ (except possibly at $x = a$) on some open interval containing $a$ then $\lim_{x \rightarrow a}f(x) = \lim_{x \rightarrow a}g(x)$.
    \item If $f(x) \geq K$ on either an interval $(a,\,b)$ or on an interval $(c,\,a)$ and if $\lim_{x \rightarrow a}f(x) = L$ then $L \geq K$.

    \item If $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M$ then\footnote{The following results are sometimes (only at Durham) known as the Calculus Of Limits Theorem (COLT).}
    \begin{enumerate}[label = (\roman*)]
        \item $\lim_{x \rightarrow a}(f(x) + g(x)) = L + M$
        \item $\lim_{x \rightarrow a}(f(x)g(x)) = LM$
        \item if $M \neq 0$ then $\lim_{x \rightarrow a}\left(\frac{f(x)}{g(x)}\right) = \frac{L}{M}$
    \end{enumerate}
\end{itemize}

Here are some simple facts about continuous functions that follow from the definition and the above results.

\begin{itemize}
    \item If $f(x)$ and $g(x)$ are continuous functions then so are $f(x) + g(x)$, $f(x)g(x)$ and $\frac{f(x)}{g(x)}$.
    \item All polynomial, rational, trigonometric and hyperbolic functions are continuous.
    \item If $f(x)$ is continuous then so is $|f(x)|$.
    \item If $\lim_{x \rightarrow a}g(x) = L$ exists and $f(x)$ is continuous at $x = L$ then $\lim_{x \rightarrow a}(f \circ g)(x) = f(L)$.
\end{itemize}

\subsection{The pinching theorem}
The pinching (squeezing) theorem: If $g(x) \leq f(x) \leq h(x)$ for all $x \neq a$ in some open interval containing $a$ and $\lim_{x \rightarrow a}g(x) = \lim_{x \rightarrow a}h(x) = L$ then $\lim_{x \rightarrow a}f(x) = L$.

\subsection{Two trigonometric limits}
Two important trigonometric limits that may be assumed to be true and used are:
\[
\lim_{x \rightarrow 0}\frac{\sin x}{x} = 1\qquad\text{and}\qquad\lim_{x \rightarrow 0}\frac{1 - \cos x}{x} = 0.
\]

\begin{example}
    \[
    \lim_{x \rightarrow 0}\frac{\sin x}{x} = 1.
    \]
    \begin{proof}
        To prove this we will use the pinching theorem, in order to use this we will need to prove the following inequalities $\sin x < x < \tan x$ for $0 < x < \frac{\pi}{2}$.

        \textit{Insert Proof}

        Next consider upper and lower bounding functions for $\frac{x}{\sin x}$ where $x \in (0, \frac{\pi}{2})$. Using the above we have
        \[
        1 =  \frac{\sin x}{\sin x} < \frac{x}{\sin x} < \frac{\tan x}{\sin x} = \frac{1}{\cos x}.
        \]
        As all combinations of functions in this inequality produce even functions then the result extends to $x \in (-\frac{\pi}{2}, 0) \cup (0, \frac{\pi}{2})$. The limits of the bounding functions are $\lim_{x \rightarrow 0}1 = 1$ and $\lim_{x \rightarrow 0}\left(\frac{1}{\cos x}\right) = 1$ hence by the pinching theorem $\lim_{x \rightarrow 0}\left(\frac{x}{\sin x}\right) = 1$ and therefore $\lim_{x \rightarrow 0}\left(\frac{\sin x}{x}\right) = 1$
    \end{proof}
\end{example}

\begin{example}
    \[
    \lim_{x \rightarrow 0}\frac{1 - \cos x}{x} = 0.
    \]
    \begin{proof}
        \[
        \lim_{x \rightarrow 0}\frac{1 - \cos x}{x} = \lim_{x \rightarrow 0}\frac{(1 - \cos x)(1 + \cos x)}{x(1 + \cos x)} = \lim_{x \rightarrow 0}\frac{\sin ^ 2 x}{x(1 + \cos x)} = \lim_{x \rightarrow 0}\left(\frac{\sin x}{x}\right) ^ 2 \frac{x}{(1 + \cos x)} = 0.
        \]
    \end{proof}
\end{example}

\subsection{Classification of discontinuities}
If a function $f(x)$ is not continuous at a point $x = a$ then we say it has a discontinuity at $x = a$.

\begin{definition}
    $f(x)$ has a right-sided limit $L^+ = \lim_{x \rightarrow a+}f(x)$ as $x$ tends to $a$ from above if, $\forall\epsilon > 0\, \exists\delta > 0$ such that
    \[
    |f(x) - L^+| < \epsilon\quad\text{when}\quad 0 < x - a < \delta.
    \]
\end{definition}

\begin{definition}
    $f(x)$ has a left-sided limit $L^- = \lim_{x \rightarrow a^-}f(x)$ as $x$ tends to $a$ from below if $\forall\epsilon > 0\, \exists\delta > 0$ such that
    \[
    |f(x) - L^-| < \epsilon\quad\text{when}\quad 0 < a - x < \delta.
    \]
\end{definition}

From these we can see that $L = \lim_{x \rightarrow a}$ exists if and only if $L^+$ and $L^-$ both exist and are equal. In which case $L = L^+ = L^-$.

There are $3$ types of discontinuity:
\begin{enumerate}[label = (\roman*)]
    \item Removable discontinuity

    In this case $L$ exists but $f(a) \neq L$.

    The discontinuity can be removed to make the continuous function
    \[
    g(x) = \begin{cases}
        f(x) &\text{if } x \neq a \\
        L &\text{if } x = a
    \end{cases}
    \]

    \item Jump discontinuity

    In this case both $L^+$ and $L^-$ exist but $L^+ \neq L^-$.

    \item Infinite discontinuity

    In this case at least one of $L^+$ or $L^-$ does not exist.
\end{enumerate}

\subsection{Limits as $x \rightarrow \infty$}

\begin{definition}
    $f(x)$ has a limit $L$ as $x$ tends to $\infty$ if
    $\forall \epsilon > 0\ \exists S > 0$ such that
    \[
    |f(x) - L| < \epsilon\quad\text{when } x > S.
    \]
\end{definition}
We write
\[
\lim_{x \rightarrow \infty}f(x) = L\quad\text{or equivalently}\quad\limas[x]{f(x)}{L}.
\]
If there is no such $L$ then we say that no limit exists.

\begin{definition}
    $f(x)$ has a limit $L$ as $x$ tends to $-\infty$ if, $\forall \epsilon > 0\ \exists S < 0$ such that
    \[
    |f(x) - L| < \epsilon\quad\text{when}\quad x < S.
    \]
\end{definition}

\subsection{The intermediate value theorem}
The intermediate value theorem states that if $f(x)$ is continuous on $[a, b]$ and $u$ is any number between $f(a)$ and $f(b)$ then $\exists c \in (a, b)$ such that $f(c) = u$.

\section{Differentiation}

\subsection{Derivative as a limit}
The derivative $f'(a)$ of a function $f(x)$ at $x = a$ is geometrically equal to the slope of the tangent to the graph of $f(x)$ at $x = a$.

A secant is a line that intersects a curve at two points.

The tangent is obtained in the limit as $h \rightarrow 0$ and the derivative at $a$ is the slope of the secant in this limit i.e.
\[
f'(a) = \lim_{h \rightarrow 0}\frac{f(a + h) - f(a)}{h}
\]
providing this limit exists.

If this limit exists then we say that $f(x)$ is differentiable at $x = a$.

If $f'(a)$ exists for all $a$ in $\Dom f$ we say that $f(x)$ is differentiable and then $f'(a)$ defines a function $f'(x)$ called the derivative.

We can use the fact that the derivative is equal to the slope of the tangent of the graph to determine a Cartesian equation for the tangent by calculating the derivative. Explicitly, the tangent line at the point $(a, f(a))$ is given by $y = f(a) + f'(a)(x - a)$.

A necessary condition for a function to be differentiable at a point is that it is continuous at that point.

Geometrically, there are two ways a function can fail to be differentiable at a point where it is continuous: the tangent line is vertical at that point, there is no tangent line at that point.

\subsection{The Leibniz and chain rules}
If $f(x)$ is differentiable then its derivative $f'(x)$ may also be differentiable, we denote its derivative by $f''(x)$ (the second derivative of $f(x)$). In general the $n^{\text{th}}$ derivative is $f^{(n)}(x)$ (alternately $f^{(n)}(x) = \frac{d^n f}{dx ^ n}$ or $f^{(n)}(x) = D ^ n f(x)$).

If the independent variable represents time, say $f(t)$, then $f'(t) = \dot{f}$ and $f''(t) = \ddot{f}$.

The product rule for differentiation is just the first case of the more general Leibniz rule:
If $f(x)$ and $g(x)$ are both differentiable $n$ times then so is the product $f(x)g(x)$ with
\[
D ^ n (fg) = \sum_{k = 0}^{n}\binom{n}{k}(D ^ k f)(D ^ {n - k} g).
\]

\begin{example}
    Use the Leibniz rule to calculate $D ^ 3 (x ^ 2 \sin x)$.

    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        \[
        D ^ 3 (fg) = f(D ^ 3 g) + 3(Df)(D ^ 2 g) + 3(D ^ 2 f)(D g) + (D ^ 3 f)g.
        \]
        $f = x ^ 2,\quad Df = 2x\quad D ^ 2f = 2\quad D ^ 3 f = 0$.
        
        $g = \sin x,\quad Dg = \cos x\quad D ^ 2g = -\sin x\quad D ^ 3 g = -\cos x$.

        \begin{align*}
            D ^ 3(x ^ 2 \sin x) &= -x ^ 2 \cos x + 3(2x)(-\sin x) + 3(2)(\cos x) + 0(\sin x) \\
            &= -x ^ 2 \cos x - 6x\sin x + 6\cos x \\
            &= (6 - x ^ 2)\cos x - 6x\sin x.
        \end{align*}
    \end{proof}
\end{example}

To differentiate composite functions $(f \circ g)(x) = f(g(x))$ we use the following theorem
\begin{theorem}[Chain rule theorem]
    If $g(x)$ is differentiable at $x$ and $f(x)$ is differentiable at $g(x)$ then the composition $(f \circ g)(x)$ is differentiable at $x$ with
    \[
    (f \circ g)'(x) = f'(g(x))g'(x).
    \]
\end{theorem}
We can use Leibniz notation to rewrite the above formula
\[
\frac{d}{dx}f(g(x)) = \frac{df}{dg}\frac{dg}{dx}.
\]

\subsection{L'H\^opital's rule}
Recall that if $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M \neq 0$ then $\lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \frac{L}{M}$. This is not applicable to the situation where $L = M = 0$, which is called an in-determinant form.

\textbf{L'H\^opital's rule}

Let $f(x)$ and $g(x)$ be differentiable on $I = (a - h, a) \cup (a, a + h)$ for some $h > 0$, with $\lim_{x \rightarrow a}f(x) = \lim_{x \rightarrow a}g(x) = 0$.

If $\lim_{x \rightarrow a}\frac{f'(x)}{g(x)}$ exists and $g'(x) \neq 0\, \forall x \in I$ then
\[
\lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}.
\]
\begin{proof}
    We shall only consider the proof for the slightly easier situation in which $f(x)$ and $g(x)$ are both differentiable at $x = a$ and $g'(a) \neq 0$. In this case
    \[
    \lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a}\frac{f(x) - f(a)}{g(x) - g(a)} = \lim_{x \rightarrow a}\frac{\frac{f(x) - f(a)}{x - a}}{\frac{g(x) - g(a)}{x - a}} = \frac{\lim_{x \rightarrow a}\frac{f(x) - f(a)}{x - a}}{\lim_{x \rightarrow a}\frac{g(x) - g(a)}{x - a}} = \frac{f'(a)}{g'(a)} = \lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}.
    \]
\end{proof}


\subsection{Boundedness and monotonicity}
The following definitions apply to a function $f(x)$ defined in some interval $I$.

\begin{definition}
    If there exists a constant $k_1$ such that $f(x) \leq k_1\ \forall x \in I$ we say that $f(x)$ is bounded above in $I$ and we call $k_1$ an upper bound of $f(x)$ in $I$.

    If there exists a point $x_1$ in $I$ such that $f(x_1) = k_1$ we say that the upper bound of $k_1$ is attained and we call $k_1$ the global maximum value of $f(x)$ in $I$.
\end{definition}

\begin{definition}
    If there exists a constant $k_2$ such that $f(x) \geq k_2\ \forall x \in I$ we say that $f(x)$ is bounded below in $I$ and we call $k_2$ a lower bound of $f(x)$ in $I$.

    If there exists a point $x_2$ in $I$ such that $f(x_2) = k_2$ we say that the lower bound of $k_2$ is attained and we call $k_2$ the global minimum value of $f(x)$ in $I$.
\end{definition}

\begin{definition}
    $f(x)$ is bounded in $I$ if it is both bounded above and bounded below in $I$, i.e. if there exists a constant $k$ such that $|f(x)| \leq k\ \forall x \in I$.\footnote{If no interval $I$ is specified then it is taken to be the domain of the function.}
\end{definition}

A condition that guarantees the existence of both a global maximum and minimum value (called extreme values) is provided by the following.

The extreme value theorem states that if $f$ is a continuous function on a closed interval $[a,\, b]$ then it is bounded on that interval and has upper and lower bounds that are attained i.e. there exists points $x_1$ and $x_2$ in $[a,\,b]$ such that $f(x_2) \leq f(x) \leq f(x_1)\ \forall x \in [a,\,b]$.

\begin{definition}
    $f(x)$ is monotonic increasing in $[a,\,b]$ if $f(x_1) \leq f(x_2)$ for all pairs $x_1, x_2$ with $a \leq x_1 < x_2 \leq b$.
\end{definition}

\begin{definition}
    $f(x)$ is strictly monotonic increasing in $[a,\,b]$ if $f(x_1) < f(x_2)$ for all pairs $x_1, x_2$ with $a \leq x_1 < x_2 \leq b$.
\end{definition}

\subsection{Critical points}
\begin{definition}
    We say that $f(x)$ has a local maximum at $x = a$ if $\exists\, h > 0$ such that $f(a) \geq f(x)\, \forall x \in (a - h, a + h)$.
\end{definition}

\begin{remark}
    If $f(x)$ has a local maximum at $x = a$ and is differentiable at this point then $f'(a) = 0$.

    \begin{proof}
        As $f(x)$ is differentiable at $a$ then
        \[
        \lim_{x \rightarrow a^{-}}\frac{f(x) - f(a)}{x - a} = \lim_{x \rightarrow a^{+}}\frac{f(x) - f(a)}{x - a} = f'(a).
        \]
        $f(x)$ has a local maximum at $a$ implies
        \[
        \frac{f(x) - f(a)}{x - a} \geq 0 \text{ for } x \in (a - h, a)
        \]
        \[
        \text{hence } f'(a) = \lim_{x \rightarrow a^{-}}\frac{f(x) - f(a)}{x - a} \geq 0.
        \]
        Similarly, $f(x)$ has a local maximum at $a$ also implies
        \[
        \frac{f(x) - f(a)}{x - a} \leq 0 \text{ for } x \in (a, a + h)
        \]
        \[
        \text{hence } f'(a) = \lim_{x \rightarrow a^{+}}\frac{f(x) - f(a)}{x - a} \geq 0.
        \]
        Putting these two together gives $f'(a) \leq 0 \leq f'(a)$ and thus $f'(a) = 0$.
    \end{proof}
\end{remark}

\begin{definition}
    We say that $f(x)$ has a local minimum at $x = a$ if $\exists h > 0$ such that $f(x) \geq f(a)\ \forall x \in (a - h, a + h)$.
\end{definition}

\begin{remark}
    If $f(x)$ has a local minimum at $x = a$ and is differentiable at this point then $f'(a) = 0$.
\end{remark}

\begin{definition}
    $f(x)$ has a stationary point at $x = a$ if it is differentiable at $x =a$ with $f'(a) = 0$.
\end{definition}

\begin{definition}
    An interior point $x = a$ of the domain of $f(x)$ is called a critical point if either $f'(a) = 0$ or $f'(a)$ does not exist.
\end{definition}

Every local maximum and local minimum of a differentiable function is a stationary point but there may be other stationary points too.

\begin{definition}
    If $f(x)$ is twice differentiable in an open interval around $x = a$ with $f''(a) = 0$ and if $f''(x)$ changes sign at $x = a$ then we say that $x = a$ is a point of inflection.
\end{definition}

\textbf{The first derivative test}

Suppose $f(x)$ is continuous at a critical point $x = a$.

\begin{enumerate}[label = (\roman*)]
    \item If $\exists h > 0$ such that $f'(x) < 0\ \forall x \in (a - h, a)$ and $f'(x) > 0\, \forall x \in (a, a + h)$ then $x = a$ is a local minimum.
    \item If $\exists h > 0$ such that $f'(x) > 0\ \forall x \in (a - h, a)$ and $f'(x) < 0\, \forall x \in (a, a + h)$ then $x = a$ is a local maximum.
    \item If $\exists h > 0$ such that $f'(x)$ has a constant sign $\forall x \neq a$ in $(a - h, a + h)$ then $x = a$ is a not a local extreme value (minimum/maximum).
\end{enumerate}

\textbf{The second derivative test}

Suppose $f(x)$ is twice differentiable at $x = a$ with $f'(a) = 0$.
\begin{enumerate}[label = (\roman*)]
    \item If $f''(a) > 0$ then $x = a$ is a local minimum.
    \item If $f''(a) < 0$ then $x = a$ is a local maximum.
\end{enumerate}

If a function is defined on an interval then extreme values can occur at the endpoints of the interval. Here an endpoint $x = c$ is a point at which the function is defined but where the function is undefined either to the left or right of this point. Example: if the domain of a function is $[a, b]$ then $a$ and $b$ are both endpoints.

\begin{definition}
    If $c$ is an endpoint of $f(x)$ then $f$ has an endpoint maximum at $x = c$ if $f(x) \leq f(c)$ for $x$ sufficiently close to $c$.
\end{definition}

The obvious similar definition applies for an endpoint minimum.

If a function is differentiable sufficiently close to an endpoint then examining the sign of the derivative can determine if there is an endpoint maximum or minimum.

If $f(x)$ is continuous on an interval $[a, b]$ then the global extreme values in the interval are attained at either critical points or end points, can be determined by examining all the possibilities.

\subsection{Rolle's theorem}
Rolle's theorem states that if $f$ is differentiable on the open interval $(a, b)$ and continuous on the closed interval $[a, b]$, with $f(a) = f(b)$, then there is at least one $c \in (a, b)$ for which $f'(c) = 0$.

\begin{proof}
    By the extreme value theorem there exists $x_1$ and $x_2$ in $[a, b]$ such that
    \[
    f(x_1) \leq f(x) \leq f(x_2)\ \forall x \in [a, b].
    \]
    If $x_1 \in (a, b)$ then $x_1$ is a local minimum and $f'(x_1) = 0$ so we are done.
    If $x_2 \in (a, b)$ then $x_2$ is a local maximum and $f'(x_2) = 0$ so we are done.

    The only case that is left is if both $x_1$ and $x_2$ are end points, $a, b$.
    Since $f(a) = f(b)$ then in this case $f(x_1) = f(x_2) = f(a)$ so the above bound becomes $f(a) \leq f(x) \leq f(a)\, \forall x \in [a, b]$.
    Thus $f(x) = f(a)$ is constant in the interval and $f'(x) = 0\, \forall x \in [a, b]$.
\end{proof}

A corollary of Rolle's theorem is that if $f(x)$ is differentiable at every point of an open interval $I$ then each pair of zeros of the function in $I$ is separated by at least one zero of $f'(x)$.
This is simply the special case of Rolle's theorem with $a$ and $b$ taken to be the zeros of $f(x)$ so that $f(a) = f(b) = 0$.

The above result can be used to provide a limit on the number of distinct real zeros of a function.

\begin{example}
    Show that $f(x) = \frac{1}{7}x ^ 7 - \frac{1}{5}x ^ 6 + x ^ 3 - 3x ^ 2 -x$ has no more than $3$ distinct real roots.

    Suppose the result is false and there are (at least) $4$ distinct real roots of $f(x)$.
    Then by Rolle's theorem there are (at least) $3$ distinct real roots of $f'(x)$.
    Then continually applying this we get that there are (at least) $2$ distinct real roots of $f''(x)$.
    However, $f''(x) = 6x ^ 5 - 6x ^ 4 + 6x - 6 = 6(x - 1)(x ^ 4 + 1)$ has only one real root (at $x = 1$).
    This contradiction proves the required result.
\end{example}

\subsection{The mean value theorem}
The mean value theorem states that if $f$ is differentiable on the open interval $(a, b)$ and continuous on the closed interval $[a, b]$,
then there is at least one $c \in (a, b)$ for which
\[
f'(c) = \frac{f(b) - f(a)}{b - a}.
\]
\begin{proof}
    Define the function
    \[
    g(x) = (b - a)(f(x) - f(a)) - (x - a)(f(b) - f(a)).
    \]
    Then $g(a) = g(b) = 0$ and as $g(x)$ satisfies the requirements of Rolle's theorem then $\exists c \in (a, b)$ for which $g'(c) = 0$.
    As $g'(x) = (b - a)f'(x) - (f(b) - f(a))$ then setting $x = c$ yields the required result $f'(c) = \frac{f(b) - f(a)}{b - a}$.
\end{proof}

Note that Rolle's theorem is just a special case of the mean value theorem with $f(a) = f(b)$.

Suppose $f(x)$ is continuous in $[a, b]$ and differentiable in $(a, b)$ with $f'(x) \geq 0$ throughout this interval.
Then $f(x)$ is monotonic increasing in $(a, b)$.
\begin{proof}
    If $a \leq x_1 < x_2 \leq b$ then by the mean value theorem $\exists c \in (a, b)$ such that $f'(c) = \frac{f(x_2) - f(x_1)}{x_2 - x_1}$.
    However, since $f'(x) \geq 0$ in $(a, b)$ then $f'(c) \geq 0$ which implies that $f(x_2) \geq f(x_1)$ i.e. $f$ is monotonic increasing.
\end{proof}

\subsection{The inverse function rule}
The inverse function rule states that if $f(x)$ is continuous in $[a, b]$ and differentiable in $(a, b)$ with $f'(x) > 0$ throughout this interval then its inverse function $g(y)$ (recall $g(f(x)) = x$) is differentiable for all $f(a) < y < f(b)$ with $g'(y) = \frac{1}{f'(g(y))}$.

\begin{example}
    $f(x) = \sin x$ has $f'(x) = \cos x > 0$ for $x \in (-\pi / 2, \pi / 2)$.
    Its inverse function $g(y) = \sin^{-1}y$ is therefore differentiable in $(-1, 1)$ with
    \[
    g'(y) = \frac{1}{f'(g(y))} = \frac{1}{\cos g(y)} = \frac{1}{\sqrt{1 - \sin ^ 2 g(y)}}
    \]
    but $y = f(g(y)) = \sin (g(y))$ hence $g'(y) = \frac{1}{\sqrt{1 - y ^ 2}}$.
    Thus we have shown that
    \[
    \frac{d}{dx}\sin^{-1}x = \frac{1}{\sqrt{1 - x ^ 2}}.
    \]
\end{example}

\subsection{Partial derivatives}

For differentiating functions of two variables we will need to use a partial derivative.
The partial derivative of $f$ with respect to $x$ is written as $\frac{\delta f}{\delta x}$ and is obtained by differentiating $f$ with respect to $x$,
keeping $y$ fixed, i.e. $y$ may be treated as a constant in performing the partial differentiation with respect to $x$.
\[
\text{E.g.}\quad f(x, y) = x ^ 2 y ^ 3 - \sin x \cos y + y \quad\text{has}\quad\frac{\delta f}{\delta y} = 2xy ^ 3 - \cos x \cos y.
\]
Similarly, the partial derivative of $f$ with respect to $y$ is written as $\frac{\delta f}{\delta y}$,
obtained by differentiating $f$ with respect to $y$, keeping $x$ fixed.

Partial derivatives are defined in terms of the following limits (we assume exist)
\[
\frac{\delta f}{\delta x}(x, y) = \lim_{h \rightarrow 0}\frac{f(x + h, y) -f(x, y)}{h},
\quad
\frac{\delta f}{\delta y}(x, y) = \lim_{h \rightarrow 0}\frac{f(x, y + h) -f(x, y)}{h}
\]

An alternative notation for partial derivatives is $f_x$ for $\frac{\delta f}{\delta x}$.

By applying partial differentiation to these partial derivatives we may obtain the second order partial derivatives.

\[
\frac{\delta ^ 2 f}{\delta x ^ 2} = \frac{\delta}{\delta x}\left(\frac{\delta f}{\delta x}\right),
\quad
\frac{\delta ^ 2 f}{\delta y ^ 2} = \frac{\delta}{\delta y}\left(\frac{\delta f}{\delta y}\right),
\quad
\frac{\delta ^ 2 f}{\delta y \delta x} = \frac{\delta}{\delta y}\left(\frac{\delta f}{\delta x}\right),
\quad
\frac{\delta ^ 2 f}{\delta x \delta y} = \frac{\delta}{\delta x}\left(\frac{\delta f}{\delta y}\right).
\]
It can be shown that
\[
\frac{\delta ^ 2 f}{\delta x \delta y} = \frac{\delta ^ 2 f}{\delta y \delta x}
\]
is true in general, if $f$ and all first and second order partial derivatives are continuous.

An alternative notation for partial derivatives is
\[
\frac{\delta ^ 2 f}{\delta x ^ 2} = f_{xx}.
\]

\newpage

\section{Integration}

\subsection{Indefinite and definite integrals}
\begin{definition}
    A function $F(x)$ is called an indefinite integral or antiderivative of a function $f(x)$ in the interval $(a, b)$ if $F(x)$ is differentiable with

    $F'(x) = f(x)$ throughout $(a, b)$. We then write $F(x) = \int f(x)\,dx$.
\end{definition}

Note: If $F(x)$ is an indefinite integral of $f(x)$ in $(a, b)$ then so is $F(x) + c$ for any constant $c$.

Note: If $F_1(x)$ and $F_2(x)$ are both indefinite integrals of $f(x)$ in $(a, b)$ then $F_1(x) - F_2(x) = c$ for constant $c$.

\begin{definition}
    We say that $f(x)$ is integrable in $(a, b)$ if it has an indefinite integral $F(x)$ in $(a, b)$ that is continuous in $[a, b]$.
\end{definition}

\begin{definition}
    A subdivision $S$ of $[a, b]$ is a partition into a finite number of subintervals
    \[
    [a, x_1], [x_1, x_2], \dotsc, [x_{n - 1}, b]
    \]
    where $a = x_0 < x_1 < x_2 < \dotsc < x_n = b$.
\end{definition}
The norm $|S|$ of the subdivision is the maximum of the subinterval lengths $|a - x_1|, |x_1 - x_2|, \dotsc, |x_{n - 1} - b|$.
The numbers $z_1, z_2, \dotsc, z_n$ form a set of sample points from $S$ if $z_j \in [x_{j - 1}, x_j]$ for $j = 1,\dotsc, n$.

\begin{definition}
    Suppose that $f(x)$ is a function defined for $x \in [a, b]$.
    The Riemann sum is
    \[
    \mathcal{R} = \sum_{j = 1}^{n}(x_j - x_{j - 1})f(z_j).
    \]
\end{definition}
This leads to the definition of the definite integral as
\[
\int_{a}^{b}f(x)\,dx = \lim_{|S| \rightarrow 0}\mathcal{R}
\]
and it can be shown that this limit exists if $f(x)$ is continuous in $[a, b]$.

The following results can be proved.
In the following let $f(x)$ and $g(x)$ both be integrable in $(a, b)$.
\begin{enumerate}[label = (\roman*)]
    \item Linearity.
    If $\lambda, \mu$ are any constants then $\lambda f(x) + \mu g(x)$ is integrable in $(a, b)$ with
    \[
    \int_{a}^{b}\left(\lambda f(x) + \mu g(x)\right)\,dx = \lambda\int_{a}^{b}f(x)\,dx + \mu\int_{a}^{b}g(x)\,dx.
    \]
    \item If $c \in [a, b]$ then $\int_{a}^{b}f(x)\,dx = \int_{a}^{c}f(x)\,dx + \int_{c}^{b}f(x)\,dx$.
    \item If $f(x) \geq g(x)\,\forall x \in (a, b)$ then $\int_{a}^{b}f(x)\,dx \geq \int_{a}^{b}g(x)\,dx$.
    \item If $m \leq f(x) \leq M\,\forall x \in [a, b]$ then
    \[
    m(b - a) \leq \int_{a}^{b}f(x)\,dx \leq M(b - a).
    \]
\end{enumerate}

\subsection{The fundamental theorem of calculus}
The fundamental theorem of calculus

If $f(x)$ is continuous on $[a, b]$ then the function
\[
F(x) = \int_{a}^{x}f(t)\,dt
\]
defined for $x \in [a, b]$ is continuous on $[a, b]$ and differentiable on $(a, b)$ and is an indefinite integral of $f(x)$ on $(a, b)$ i.e.
\[
F'(x) = \frac{d}{dx}\int_{a}^{x}f(t)\,dt = f(x)
\]
throughout $(a, b)$.
Furthermore if $\Tilde{F}(x)$ is any indefinite integral of $f(x)$ on $[a, b]$ then
\[
\int_{a}^{b}f(t)\,dt = \Tilde{F}(b) - \Tilde{F}(a) = [\Tilde{F}(x)]^{b}_{a}.
\]

We will now look at the important points of the proof.

For $a \leq x < x + h < b$ we have that
\[
F(x + h) - F(x) = \int_{a}^{x + h}f(t)\,dt - \int_{a}^{x}f(t)\,dt = \int_{x}^{x + h}f(t)\,dt
\]
where we have used property (ii).

Let $m(h)$ and $M(h)$ denote the minimum and maximum values of $f(x)$ on the interval $[x, x + h]$.
Then by property (iv) we have that
\[
m(h)h \leq \int_{x}^{x + h}f(t)\,dt \leq M(h)h.
\]
Thus
\[
m(h) \leq \frac{F(x + h) - F(x)}{h} \leq M(h).
\]
Since $f(x)$ is continuous on $[x, x + h]$ we have that $\lim_{h \rightarrow 0^+}m(h) = \lim_{h \rightarrow 0^+}M(h) = f(x)$ and so by the pinching theorem
\[
\lim_{h \rightarrow 0^+}\frac{F(x + h) - F(x)}{h} = f(x).
\]
A similar argument applies to the limit from below and together they give
\[
F'(x) = \lim_{h \rightarrow 0}\frac{F(x + h) - F(x)}{h} = f(x)
\]
which proves that $F(x)$ is an indefinite integral of $f(x)$.
The proof of the first part of the theorem is completed by showing that $F(x)$ is continuous from the right at $x = a$ and from the left at $x = b$.
Both these follow by a simple consideration of the limit of the relevant quotient.
Finally, to prove the last part of the theorem the key observation is that any indefinite integral $\Tilde{F}(x)$ is related to $F(x)$ by the addition of a constant.

\subsection{Limits with logarithms, powers and exponentials}
There are some important results concerning limits as $x \rightarrow \infty$ for the logarithm and exponential functions.

\begin{lemma}
    $\forall\, x \geq 0,\quad e ^ x \geq 1 + x$
    \begin{proof}
        Consider $f(x) = e ^ x - (1 + x)$ then $f(0) = 0$ and $f'(x) = e ^ x - 1 \geq 0$.
        Hence $f(x)$ is monotonic increasing in $[0, \infty)$ so $f(x) \geq 0$ for all $x \geq 0$.
    \end{proof}
\end{lemma}

\begin{lemma}
    $\forall\, x \geq 0$,
    and for any positive integer $n$,
    $e ^ x \geq \sum_{j = 0}^{n}\frac{x ^ j}{j!}$.
    \begin{proof}
    The case $n = 1$ corresponds to the previous lemma.
    The proof for general $n$ is similar to the proof of the previous lemma.
    
    \Large\textbf{Prove!}.
    \end{proof}
\end{lemma}







\end{document}