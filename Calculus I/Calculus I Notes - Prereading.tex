\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\DeclareMathOperator{\Dom}{Dom}
\DeclareMathOperator{\Ran}{Ran}

\newcommand{\limas}[3][n]{#2 \rightarrow #3 \text{ as } #1 \rightarrow \infty}

\title{Calculus I \\
    \large Prereading}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\section{Functions}

A function $f$ is a correspondence between two sets $D$ (called the domain) and $C$ (called the codomain), that assigns to each element of $D$ one and only one element of $C$. The notation to indicate the domain and codomain is $f : D \mapsto C$.

For $x \in D$ we write $f(x)$ to denote the assigned element in $C$, this value of $f$ at $x$, or the image of $x$ under $f$, where $x$ is called the argument of the function.

The set of all images is called the range of $f$ and our notation for the domain and range is
\[
\Dom f\qquad\text{and}\qquad\Ran f = \{f(x): x \in \Dom f\}.
\]

If the domain of a function $f$ is not explicitly given, then it is taken to be the maximal set of real numbers $x$ for which $f(x)$ is a real number.

E.g. $f(x) = \sqrt{2x + 4}$. Here $\Dom f = [-2, \infty)$ and $\Ran f = [0, \infty)$.

The element $x$ in the domain is called the independent variable and the element $y$ in the range is called the dependent variable.

The graph of a function $f$ is the set of all points $(x, y)$ in the $xy$-plane with $x \in \Dom f$ and $y = f(x)$, ie.
\[
\mathrm{graph}\, f = \{(x, y) : x \in \Dom f \text{ and } y = f(x)\}.
\]

The vertical line test

If any vertical line intersects the curve more than once then the curve is not the graph of a function, otherwise it is. \\


\textbf{Even and odd functions}

A function $f$ is even if $f(x) = f(-x)\,\forall\pm x\in\Dom f$.

A function $f$ is odd if $f(x) = -f(-x)\,\forall\pm x\in\Dom f$.

\textbf{Piecewise functions}

Some functions are defined piecewise, different expressions are given for different intervals in the domain.

A \textbf{step function} is a piecewise function which is constant on each piece. An example is the Heaviside step function $H(x)$ defined by
\[
H(x) = \begin{cases}
    0\quad\text{if } x < 0 \\
    1\quad\text{if } x > 0
\end{cases}
\]
Note that with this definition $\Dom H = \R \setminus \{0\}$. It is sometimes convenient to extend the domain to $\R$ by defining the value of $H(0)$.

\subsection{Operations with functions}
Given two functions $f$ and $g$ we can define the following:

\begin{itemize}
    \item the sum is $(f + g)(x) = f(x) + g(x)$, with domain $\Dom f \cap \Dom g$.
    \item the difference is $(f - g)(x) = f(x) - g(x)$, with domain $\Dom f \cap \Dom g$.
    \item the product is $(fg)(x) = f(x)g(x)$, with domain $\Dom f \cap \Dom g$.
    \item the ratio is $\left(\frac{f}{g}\right)(x) = \frac{f(x)}{g(x)}$, with domain $(\Dom f \cap \Dom g) \setminus \{x: g(x) = 0\}$.
    \item the composition is $(f \circ g)(x) = f(g(x))$, with domain $\{x \in \Dom g : g(x) \in \Dom f\}$.
\end{itemize}

\subsection{Inverse functions}
\begin{definition}
    A function $f: D \mapsto C$ is surjective (or onto) if $\Ran f = C$,
    \[
    \text{if}\  \forall y \in C\, \exists x \in D \text{ s.t. } f(x) = y.
    \]
\end{definition}

\begin{definition}
    A function $f: D \mapsto C$ is injective (or one-to-one) if $\forall x_1, x_2 \in D$ with $x_1 \neq x_2$ then $f(x_1) \neq f(x_2)$.
\end{definition}

\textbf{The horizontal line test}

If no horizontal line intersects the graph of $f$ more than once then $f$ is injective, otherwise it is not.

\begin{definition}
    A function $f : D \mapsto C$ is bijective if it is both surjective and injective.
\end{definition}

\subsection{Theorem of inverse functions}
A bijective function $f$ admits a unique inverse, denoted $f ^ {-1}$, such that
\[
(f ^ {-1} \circ f)(x) = x = (f \circ f ^ {-1})(x).
\]
It is clear from the definition that $\Dom f^{-1} = \Ran f$ and $\Ran f^{-1} = \Dom f$.

As the inverse undoes the effect of the function an equivalent definition is
\[
f(x) = y\quad\text{iff}\quad f^{-1}(y) = x.
\]

\newpage

\section{Limits and continuity}

\subsection{Definition of a limit and continuity}
A function $f(x)$ has a limit $L$ at $x = a$ if $f(x)$ is close to $L$ whenever $x$ is close to $a$.

\begin{definition}
    $f(x)$ has a limit $L$ as $x$ tends to $a$ if
    \[
    \forall\,\epsilon > 0\ \exists\,\delta > 0 \text{ s.t. } |f(x) - L| < \epsilon \text{ when } 0 < |x - a| < \delta.
    \]    
\end{definition}

We write
\[
\lim_{x \rightarrow a}f(x) = L\text{ or equivalently } f(x)\rightarrow L \text{ as } x \rightarrow a.
\]

If there is no such $L$ we can say that no limit exists.

\begin{definition}
    A function $f(x)$ is continuous at the point $x = a$ if the following three properties all hold:
    \begin{itemize}
        \item $f(a)$ exists.
        \item $\lim_{x \rightarrow a}$ exists.
        \item $\lim_{x \rightarrow a}f(x)$ is equal to $f(a)$.
    \end{itemize}
\end{definition}

\begin{definition}
    A function $f(x)$ is continuous on a subset $S$ of its domain if it is continuous at every point in $S$.
\end{definition}

\begin{definition}
    A function $f(x)$ is continuous if it is continuous at every point in its domain.
\end{definition}

\subsection{Facts about limits and continuity}
Here are some simple facts about limits (that follow from the definition)

\begin{itemize}
    \item The limit is unique
    \item If $f(x) = g(x)$ (except possibly at $x = a$) on some open interval containing $a$ then $\lim_{x \rightarrow a}f(x) = \lim_{x \rightarrow a}g(x)$.
    \item If $f(x) \geq K$ on either an interval $(a,\,b)$ or on an interval $(c,\,a)$ and if $\lim_{x \rightarrow a}f(x) = L$ then $L \geq K$.

    \item If $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M$ then\footnote{The following results are sometimes (only at Durham) known as the Calculus Of Limits Theorem (COLT).}
    \begin{enumerate}[label = (\roman*)]
        \item $\lim_{x \rightarrow a}(f(x) + g(x)) = L + M$
        \item $\lim_{x \rightarrow a}(f(x)g(x)) = LM$
        \item if $M \neq 0$ then $\lim_{x \rightarrow a}\left(\frac{f(x)}{g(x)}\right) = \frac{L}{M}$
    \end{enumerate}
\end{itemize}

Here are some simple facts about continuous functions that follow from the definition and the above results.

\begin{itemize}
    \item If $f(x)$ and $g(x)$ are continuous functions then so are $f(x) + g(x)$, $f(x)g(x)$ and $\frac{f(x)}{g(x)}$.
    \item All polynomial, rational, trigonometric and hyperbolic functions are continuous.
    \item If $f(x)$ is continuous then so is $|f(x)|$.
    \item If $\lim_{x \rightarrow a}g(x) = L$ exists and $f(x)$ is continuous at $x = L$ then $\lim_{x \rightarrow a}(f \circ g)(x) = f(L)$.
\end{itemize}

\subsection{The pinching theorem}
The pinching (squeezing) theorem: If $g(x) \leq f(x) \leq h(x)$ for all $x \neq a$ in some open interval containing $a$ and $\lim_{x \rightarrow a}g(x) = \lim_{x \rightarrow a}h(x) = L$ then $\lim_{x \rightarrow a}f(x) = L$.

\subsection{Two trigonometric limits}
Two important trigonometric limits that may be assumed to be true and used are:
\[
\lim_{x \rightarrow 0}\frac{\sin x}{x} = 1\qquad\text{and}\qquad\lim_{x \rightarrow 0}\frac{1 - \cos x}{x} = 0.
\]

\begin{example}
    \[
    \lim_{x \rightarrow 0}\frac{\sin x}{x} = 1.
    \]
    \begin{proof}
        To prove this we will use the pinching theorem, in order to use this we will need to prove the following inequalities $\sin x < x < \tan x$ for $0 < x < \frac{\pi}{2}$.

        \textit{Insert Proof}

        Next consider upper and lower bounding functions for $\frac{x}{\sin x}$ where $x \in (0, \frac{\pi}{2})$. Using the above we have
        \[
        1 =  \frac{\sin x}{\sin x} < \frac{x}{\sin x} < \frac{\tan x}{\sin x} = \frac{1}{\cos x}.
        \]
        As all combinations of functions in this inequality produce even functions then the result extends to $x \in (-\frac{\pi}{2}, 0) \cup (0, \frac{\pi}{2})$. The limits of the bounding functions are $\lim_{x \rightarrow 0}1 = 1$ and $\lim_{x \rightarrow 0}\left(\frac{1}{\cos x}\right) = 1$ hence by the pinching theorem $\lim_{x \rightarrow 0}\left(\frac{x}{\sin x}\right) = 1$ and therefore $\lim_{x \rightarrow 0}\left(\frac{\sin x}{x}\right) = 1$
    \end{proof}
\end{example}

\begin{example}
    \[
    \lim_{x \rightarrow 0}\frac{1 - \cos x}{x} = 0.
    \]
    \begin{proof}
        \[
        \lim_{x \rightarrow 0}\frac{1 - \cos x}{x} = \lim_{x \rightarrow 0}\frac{(1 - \cos x)(1 + \cos x)}{x(1 + \cos x)} = \lim_{x \rightarrow 0}\frac{\sin ^ 2 x}{x(1 + \cos x)} = \lim_{x \rightarrow 0}\left(\frac{\sin x}{x}\right) ^ 2 \frac{x}{(1 + \cos x)} = 0.
        \]
    \end{proof}
\end{example}

\subsection{Classification of discontinuities}
If a function $f(x)$ is not continuous at a point $x = a$ then we say it has a discontinuity at $x = a$.

\begin{definition}
    $f(x)$ has a right-sided limit $L^+ = \lim_{x \rightarrow a+}f(x)$ as $x$ tends to $a$ from above if, $\forall\epsilon > 0\, \exists\delta > 0$ such that
    \[
    |f(x) - L^+| < \epsilon\quad\text{when}\quad 0 < x - a < \delta.
    \]
\end{definition}

\begin{definition}
    $f(x)$ has a left-sided limit $L^- = \lim_{x \rightarrow a^-}f(x)$ as $x$ tends to $a$ from below if $\forall\epsilon > 0\, \exists\delta > 0$ such that
    \[
    |f(x) - L^-| < \epsilon\quad\text{when}\quad 0 < a - x < \delta.
    \]
\end{definition}

From these we can see that $L = \lim_{x \rightarrow a}$ exists if and only if $L^+$ and $L^-$ both exist and are equal. In which case $L = L^+ = L^-$.

There are $3$ types of discontinuity:
\begin{enumerate}[label = (\roman*)]
    \item Removable discontinuity

    In this case $L$ exists but $f(a) \neq L$.

    The discontinuity can be removed to make the continuous function
    \[
    g(x) = \begin{cases}
        f(x) &\text{if } x \neq a \\
        L &\text{if } x = a
    \end{cases}
    \]

    \item Jump discontinuity

    In this case both $L^+$ and $L^-$ exist but $L^+ \neq L^-$.

    \item Infinite discontinuity

    In this case at least one of $L^+$ or $L^-$ does not exist.
\end{enumerate}

\subsection{Limits as $x \rightarrow \infty$}

\begin{definition}
    $f(x)$ has a limit $L$ as $x$ tends to $\infty$ if
    $\forall \epsilon > 0\ \exists S > 0$ such that
    \[
    |f(x) - L| < \epsilon\quad\text{when } x > S.
    \]
\end{definition}
We write
\[
\lim_{x \rightarrow \infty}f(x) = L\quad\text{or equivalently}\quad\limas[x]{f(x)}{L}.
\]
If there is no such $L$ then we say that no limit exists.

\begin{definition}
    $f(x)$ has a limit $L$ as $x$ tends to $-\infty$ if, $\forall \epsilon > 0\ \exists S < 0$ such that
    \[
    |f(x) - L| < \epsilon\quad\text{when}\quad x < S.
    \]
\end{definition}

\subsection{The intermediate value theorem}
The intermediate value theorem states that if $f(x)$ is continuous on $[a, b]$ and $u$ is any number between $f(a)$ and $f(b)$ then $\exists c \in (a, b)$ such that $f(c) = u$.

\section{Differentiation}

\subsection{Derivative as a limit}
The derivative $f'(a)$ of a function $f(x)$ at $x = a$ is geometrically equal to the slope of the tangent to the graph of $f(x)$ at $x = a$.

A secant is a line that intersects a curve at two points.

The tangent is obtained in the limit as $h \rightarrow 0$ and the derivative at $a$ is the slope of the secant in this limit i.e.
\[
f'(a) = \lim_{h \rightarrow 0}\frac{f(a + h) - f(a)}{h}
\]
providing this limit exists.

If this limit exists then we say that $f(x)$ is differentiable at $x = a$.

If $f'(a)$ exists for all $a$ in $\Dom f$ we say that $f(x)$ is differentiable and then $f'(a)$ defines a function $f'(x)$ called the derivative.

We can use the fact that the derivative is equal to the slope of the tangent of the graph to determine a Cartesian equation for the tangent by calculating the derivative. Explicitly, the tangent line at the point $(a, f(a))$ is given by $y = f(a) + f'(a)(x - a)$.

A necessary condition for a function to be differentiable at a point is that it is continuous at that point.

Geometrically, there are two ways a function can fail to be differentiable at a point where it is continuous: the tangent line is vertical at that point, there is no tangent line at that point.

\subsection{The Leibniz and chain rules}
If $f(x)$ is differentiable then its derivative $f'(x)$ may also be differentiable, we denote its derivative by $f''(x)$ (the second derivative of $f(x)$). In general the $n^{\text{th}}$ derivative is $f^{(n)}(x)$ (alternately $f^{(n)}(x) = \frac{d^n f}{dx ^ n}$ or $f^{(n)}(x) = D ^ n f(x)$).

If the independent variable represents time, say $f(t)$, then $f'(t) = \dot{f}$ and $f''(t) = \ddot{f}$.

The product rule for differentiation is just the first case of the more general Leibniz rule:
If $f(x)$ and $g(x)$ are both differentiable $n$ times then so is the product $f(x)g(x)$ with
\[
D ^ n (fg) = \sum_{k = 0}^{n}\binom{n}{k}(D ^ k f)(D ^ {n - k} g).
\]

\begin{example}
    Use the Leibniz rule to calculate $D ^ 3 (x ^ 2 \sin x)$.

    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        \[
        D ^ 3 (fg) = f(D ^ 3 g) + 3(Df)(D ^ 2 g) + 3(D ^ 2 f)(D g) + (D ^ 3 f)g.
        \]
        $f = x ^ 2,\quad Df = 2x\quad D ^ 2f = 2\quad D ^ 3 f = 0$.
        
        $g = \sin x,\quad Dg = \cos x\quad D ^ 2g = -\sin x\quad D ^ 3 g = -\cos x$.

        \begin{align*}
            D ^ 3(x ^ 2 \sin x) &= -x ^ 2 \cos x + 3(2x)(-\sin x) + 3(2)(\cos x) + 0(\sin x) \\
            &= -x ^ 2 \cos x - 6x\sin x + 6\cos x \\
            &= (6 - x ^ 2)\cos x - 6x\sin x.
        \end{align*}
    \end{proof}
\end{example}

To differentiate composite functions $(f \circ g)(x) = f(g(x))$ we use the following theorem
\begin{theorem}[Chain rule theorem]
    If $g(x)$ is differentiable at $x$ and $f(x)$ is differentiable at $g(x)$ then the composition $(f \circ g)(x)$ is differentiable at $x$ with
    \[
    (f \circ g)'(x) = f'(g(x))g'(x).
    \]
\end{theorem}
We can use Leibniz notation to rewrite the above formula
\[
\frac{d}{dx}f(g(x)) = \frac{df}{dg}\frac{dg}{dx}.
\]

\subsection{L'H\^opital's rule}
Recall that if $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M \neq 0$ then $\lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \frac{L}{M}$. This is not applicable to the situation where $L = M = 0$, which is called an in-determinant form.

\textbf{L'H\^opital's rule}

Let $f(x)$ and $g(x)$ be differentiable on $I = (a - h, a) \cup (a, a + h)$ for some $h > 0$, with $\lim_{x \rightarrow a}f(x) = \lim_{x \rightarrow a}g(x) = 0$.

If $\lim_{x \rightarrow a}\frac{f'(x)}{g(x)}$ exists and $g'(x) \neq 0\, \forall x \in I$ then
\[
\lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}.
\]
\begin{proof}
    We shall only consider the proof for the slightly easier situation in which $f(x)$ and $g(x)$ are both differentiable at $x = a$ and $g'(a) \neq 0$. In this case
    \[
    \lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a}\frac{f(x) - f(a)}{g(x) - g(a)} = \lim_{x \rightarrow a}\frac{\frac{f(x) - f(a)}{x - a}}{\frac{g(x) - g(a)}{x - a}} = \frac{\lim_{x \rightarrow a}\frac{f(x) - f(a)}{x - a}}{\lim_{x \rightarrow a}\frac{g(x) - g(a)}{x - a}} = \frac{f'(a)}{g'(a)} = \lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}.
    \]
\end{proof}


\subsection{Boundedness and monotonicity}
The following definitions apply to a function $f(x)$ defined in some interval $I$.

\begin{definition}
    If there exists a constant $k_1$ such that $f(x) \leq k_1\ \forall x \in I$ we say that $f(x)$ is bounded above in $I$ and we call $k_1$ an upper bound of $f(x)$ in $I$.

    If there exists a point $x_1$ in $I$ such that $f(x_1) = k_1$ we say that the upper bound of $k_1$ is attained and we call $k_1$ the global maximum value of $f(x)$ in $I$.
\end{definition}

\begin{definition}
    If there exists a constant $k_2$ such that $f(x) \geq k_2\ \forall x \in I$ we say that $f(x)$ is bounded below in $I$ and we call $k_2$ a lower bound of $f(x)$ in $I$.

    If there exists a point $x_2$ in $I$ such that $f(x_2) = k_2$ we say that the lower bound of $k_2$ is attained and we call $k_2$ the global minimum value of $f(x)$ in $I$.
\end{definition}

\begin{definition}
    $f(x)$ is bounded in $I$ if it is both bounded above and bounded below in $I$, i.e. if there exists a constant $k$ such that $|f(x)| \leq k\ \forall x \in I$.\footnote{If no interval $I$ is specified then it is taken to be the domain of the function.}
\end{definition}

A condition that guarantees the existence of both a global maximum and minimum value (called extreme values) is provided by the following.

The extreme value theorem states that if $f$ is a continuous function on a closed interval $[a,\, b]$ then it is bounded on that interval and has upper and lower bounds that are attained i.e. there exists points $x_1$ and $x_2$ in $[a,\,b]$ such that $f(x_2) \leq f(x) \leq f(x_1)\ \forall x \in [a,\,b]$.

\begin{definition}
    $f(x)$ is monotonic increasing in $[a,\,b]$ if $f(x_1) \leq f(x_2)$ for all pairs $x_1, x_2$ with $a \leq x_1 < x_2 \leq b$.
\end{definition}

\begin{definition}
    $f(x)$ is strictly monotonic increasing in $[a,\,b]$ if $f(x_1) < f(x_2)$ for all pairs $x_1, x_2$ with $a \leq x_1 < x_2 \leq b$.
\end{definition}

\end{document}