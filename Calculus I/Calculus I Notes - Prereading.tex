\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\DeclareMathOperator{\Dom}{Dom}
\DeclareMathOperator{\Ran}{Ran}

\newcommand{\limas}[3][n]{#2 \rightarrow #3 \text{ as } #1 \rightarrow \infty}

\title{Calculus I \\
    \large Prereading}
\author{Luke Phillips}
\date{October 2024}

\begin{document}

\maketitle

\newpage

\section{Functions}

A function $f$ is a correspondence between two sets $D$ (called the domain) and $C$ (called the codomain), that assigns to each element of $D$ one and only one element of $C$. The notation to indicate the domain and codomain is $f : D \mapsto C$.

For $x \in D$ we write $f(x)$ to denote the assigned element in $C$, this value of $f$ at $x$, or the image of $x$ under $f$, where $x$ is called the argument of the function.

The set of all images is called the range of $f$ and our notation for the domain and range is
\[
\Dom f\qquad\text{and}\qquad\Ran f = \{f(x): x \in \Dom f\}.
\]

If the domain of a function $f$ is not explicitly given, then it is taken to be the maximal set of real numbers $x$ for which $f(x)$ is a real number.

E.g. $f(x) = \sqrt{2x + 4}$. Here $\Dom f = [-2, \infty)$ and $\Ran f = [0, \infty)$.

The element $x$ in the domain is called the independent variable and the element $y$ in the range is called the dependent variable.

The graph of a function $f$ is the set of all points $(x, y)$ in the $xy$-plane with $x \in \Dom f$ and $y = f(x)$, ie.
\[
\mathrm{graph}\, f = \{(x, y) : x \in \Dom f \text{ and } y = f(x)\}.
\]

The vertical line test

If any vertical line intersects the curve more than once then the curve is not the graph of a function, otherwise it is.


\textbf{Even and odd functions}

A function $f$ is even if $f(x) = f(-x)\,\forall\pm x\in\Dom f$.

A function $f$ is odd if $f(x) = -f(-x)\,\forall\pm x\in\Dom f$.

\textbf{Piecewise functions}

Some functions are defined piecewise, different expressions are given for different intervals in the domain.

A \textbf{step function} is a piecewise function which is constant on each piece. An example is the Heaviside step function $H(x)$ defined by
\[
H(x) = \begin{cases}
    0\quad\text{if } x < 0 \\
    1\quad\text{if } x > 0
\end{cases}
\]
Note that with this definition $\Dom H = \R \setminus \{0\}$. It is sometimes convenient to extend the domain to $\R$ by defining the value of $H(0)$.

\subsection{Operations with functions}
Given two functions $f$ and $g$ we can define the following:

\begin{itemize}
    \item the sum is $(f + g)(x) = f(x) + g(x)$, with domain $\Dom f \cap \Dom g$.
    \item the difference is $(f - g)(x) = f(x) - g(x)$, with domain $\Dom f \cap \Dom g$.
    \item the product is $(fg)(x) = f(x)g(x)$, with domain $\Dom f \cap \Dom g$.
    \item the ratio is $\left(\frac{f}{g}\right)(x) = \frac{f(x)}{g(x)}$, with domain $(\Dom f \cap \Dom g) \setminus \{x: g(x) = 0\}$.
    \item the composition is $(f \circ g)(x) = f(g(x))$, with domain $\{x \in \Dom g : g(x) \in \Dom f\}$.
\end{itemize}

\subsection{Inverse functions}
\begin{definition}
    A function $f: D \mapsto C$ is surjective (or onto) if $\Ran f = C$,
    \[
    \text{if}\  \forall y \in C\, \exists x \in D \text{ s.t. } f(x) = y.
    \]
\end{definition}

\begin{definition}
    A function $f: D \mapsto C$ is injective (or one-to-one) if $\forall x_1, x_2 \in D$ with $x_1 \neq x_2$ then $f(x_1) \neq f(x_2)$.
\end{definition}

\textbf{The horizontal line test}

If no horizontal line intersects the graph of $f$ more than once then $f$ is injective, otherwise it is not.

\begin{definition}
    A function $f : D \mapsto C$ is bijective if it is both surjective and injective.
\end{definition}

\subsection{Theorem of inverse functions}
A bijective function $f$ admits a unique inverse, denoted $f ^ {-1}$, such that
\[
(f ^ {-1} \circ f)(x) = x = (f \circ f ^ {-1})(x).
\]
It is clear from the definition that $\Dom f^{-1} = \Ran f$ and $\Ran f^{-1} = \Dom f$.

As the inverse undoes the effect of the function an equivalent definition is
\[
f(x) = y\quad\text{iff}\quad f^{-1}(y) = x.
\]

\newpage

\section{Limits and continuity}

\subsection{Definition of a limit and continuity}
A function $f(x)$ has a limit $L$ at $x = a$ if $f(x)$ is close to $L$ whenever $x$ is close to $a$.

\begin{definition}
    $f(x)$ has a limit $L$ as $x$ tends to $a$ if
    \[
    \forall\,\epsilon > 0\ \exists\,\delta > 0 \text{ s.t. } |f(x) - L| < \epsilon \text{ when } 0 < |x - a| < \delta.
    \]    
\end{definition}

We write
\[
\lim_{x \rightarrow a}f(x) = L\text{ or equivalently } f(x)\rightarrow L \text{ as } x \rightarrow a.
\]

If there is no such $L$ we can say that no limit exists.

\begin{definition}
    A function $f(x)$ is continuous at the point $x = a$ if the following three properties all hold:
    \begin{itemize}
        \item $f(a)$ exists.
        \item $\lim_{x \rightarrow a}$ exists.
        \item $\lim_{x \rightarrow a}f(x)$ is equal to $f(a)$.
    \end{itemize}
\end{definition}

\begin{definition}
    A function $f(x)$ is continuous on a subset $S$ of its domain if it is continuous at every point in $S$.
\end{definition}

\begin{definition}
    A function $f(x)$ is continuous if it is continuous at every point in its domain.
\end{definition}

\subsection{Facts about limits and continuity}
Here are some simple facts about limits (that follow from the definition)

\begin{itemize}
    \item The limit is unique
    \item If $f(x) = g(x)$ (except possibly at $x = a$) on some open interval containing $a$ then $\lim_{x \rightarrow a}f(x) = \lim_{x \rightarrow a}g(x)$.
    \item If $f(x) \geq K$ on either an interval $(a,\,b)$ or on an interval $(c,\,a)$ and if $\lim_{x \rightarrow a}f(x) = L$ then $L \geq K$.

    \item If $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M$ then\footnote{The following results are sometimes (only at Durham) known as the Calculus Of Limits Theorem (COLT).}
    \begin{enumerate}[label = (\roman*)]
        \item $\lim_{x \rightarrow a}(f(x) + g(x)) = L + M$
        \item $\lim_{x \rightarrow a}(f(x)g(x)) = LM$
        \item if $M \neq 0$ then $\lim_{x \rightarrow a}\left(\frac{f(x)}{g(x)}\right) = \frac{L}{M}$
    \end{enumerate}
\end{itemize}

Here are some simple facts about continuous functions that follow from the definition and the above results.

\begin{itemize}
    \item If $f(x)$ and $g(x)$ are continuous functions then so are $f(x) + g(x)$, $f(x)g(x)$ and $\frac{f(x)}{g(x)}$.
    \item All polynomial, rational, trigonometric and hyperbolic functions are continuous.
    \item If $f(x)$ is continuous then so is $|f(x)|$.
    \item If $\lim_{x \rightarrow a}g(x) = L$ exists and $f(x)$ is continuous at $x = L$ then $\lim_{x \rightarrow a}(f \circ g)(x) = f(L)$.
\end{itemize}

\subsection{The pinching theorem}
The pinching (squeezing) theorem: If $g(x) \leq f(x) \leq h(x)$ for all $x \neq a$ in some open interval containing $a$ and $\lim_{x \rightarrow a}g(x) = \lim_{x \rightarrow a}h(x) = L$ then $\lim_{x \rightarrow a}f(x) = L$.

\subsection{Two trigonometric limits}
Two important trigonometric limits that may be assumed to be true and used are:
\[
\lim_{x \rightarrow 0}\frac{\sin x}{x} = 1\qquad\text{and}\qquad\lim_{x \rightarrow 0}\frac{1 - \cos x}{x} = 0.
\]

\begin{example}
    \[
    \lim_{x \rightarrow 0}\frac{\sin x}{x} = 1.
    \]
    \begin{proof}
        To prove this we will use the pinching theorem, in order to use this we will need to prove the following inequalities $\sin x < x < \tan x$ for $0 < x < \frac{\pi}{2}$.

        \textit{Insert Proof}

        Next consider upper and lower bounding functions for $\frac{x}{\sin x}$ where $x \in (0, \frac{\pi}{2})$. Using the above we have
        \[
        1 =  \frac{\sin x}{\sin x} < \frac{x}{\sin x} < \frac{\tan x}{\sin x} = \frac{1}{\cos x}.
        \]
        As all combinations of functions in this inequality produce even functions then the result extends to $x \in (-\frac{\pi}{2}, 0) \cup (0, \frac{\pi}{2})$. The limits of the bounding functions are $\lim_{x \rightarrow 0}1 = 1$ and $\lim_{x \rightarrow 0}\left(\frac{1}{\cos x}\right) = 1$ hence by the pinching theorem $\lim_{x \rightarrow 0}\left(\frac{x}{\sin x}\right) = 1$ and therefore $\lim_{x \rightarrow 0}\left(\frac{\sin x}{x}\right) = 1$
    \end{proof}
\end{example}

\begin{example}
    \[
    \lim_{x \rightarrow 0}\frac{1 - \cos x}{x} = 0.
    \]
    \begin{proof}
        \[
        \lim_{x \rightarrow 0}\frac{1 - \cos x}{x} = \lim_{x \rightarrow 0}\frac{(1 - \cos x)(1 + \cos x)}{x(1 + \cos x)} = \lim_{x \rightarrow 0}\frac{\sin ^ 2 x}{x(1 + \cos x)} = \lim_{x \rightarrow 0}\left(\frac{\sin x}{x}\right) ^ 2 \frac{x}{(1 + \cos x)} = 0.
        \]
    \end{proof}
\end{example}

\subsection{Classification of discontinuities}
If a function $f(x)$ is not continuous at a point $x = a$ then we say it has a discontinuity at $x = a$.

\begin{definition}
    $f(x)$ has a right-sided limit $L^+ = \lim_{x \rightarrow a+}f(x)$ as $x$ tends to $a$ from above if, $\forall\epsilon > 0\, \exists\delta > 0$ such that
    \[
    |f(x) - L^+| < \epsilon\quad\text{when}\quad 0 < x - a < \delta.
    \]
\end{definition}

\begin{definition}
    $f(x)$ has a left-sided limit $L^- = \lim_{x \rightarrow a^-}f(x)$ as $x$ tends to $a$ from below if $\forall\epsilon > 0\, \exists\delta > 0$ such that
    \[
    |f(x) - L^-| < \epsilon\quad\text{when}\quad 0 < a - x < \delta.
    \]
\end{definition}

From these we can see that $L = \lim_{x \rightarrow a}$ exists if and only if $L^+$ and $L^-$ both exist and are equal. In which case $L = L^+ = L^-$.

There are $3$ types of discontinuity:
\begin{enumerate}[label = (\roman*)]
    \item Removable discontinuity

    In this case $L$ exists but $f(a) \neq L$.

    The discontinuity can be removed to make the continuous function
    \[
    g(x) = \begin{cases}
        f(x) &\text{if } x \neq a \\
        L &\text{if } x = a
    \end{cases}
    \]

    \item Jump discontinuity

    In this case both $L^+$ and $L^-$ exist but $L^+ \neq L^-$.

    \item Infinite discontinuity

    In this case at least one of $L^+$ or $L^-$ does not exist.
\end{enumerate}

\subsection[Limits as x rightarrow infty]{Limits as $x \rightarrow \infty$}

\begin{definition}
    $f(x)$ has a limit $L$ as $x$ tends to $\infty$ if
    $\forall \epsilon > 0\ \exists S > 0$ such that
    \[
    |f(x) - L| < \epsilon\quad\text{when } x > S.
    \]
\end{definition}
We write
\[
\lim_{x \rightarrow \infty}f(x) = L\quad\text{or equivalently}\quad\limas[x]{f(x)}{L}.
\]
If there is no such $L$ then we say that no limit exists.

\begin{definition}
    $f(x)$ has a limit $L$ as $x$ tends to $-\infty$ if, $\forall \epsilon > 0\ \exists S < 0$ such that
    \[
    |f(x) - L| < \epsilon\quad\text{when}\quad x < S.
    \]
\end{definition}

\subsection{The intermediate value theorem}
The intermediate value theorem states that if $f(x)$ is continuous on $[a, b]$ and $u$ is any number between $f(a)$ and $f(b)$ then $\exists c \in (a, b)$ such that $f(c) = u$.

\section{Differentiation}

\subsection{Derivative as a limit}
The derivative $f'(a)$ of a function $f(x)$ at $x = a$ is geometrically equal to the slope of the tangent to the graph of $f(x)$ at $x = a$.

A secant is a line that intersects a curve at two points.

The tangent is obtained in the limit as $h \rightarrow 0$ and the derivative at $a$ is the slope of the secant in this limit i.e.
\[
f'(a) = \lim_{h \rightarrow 0}\frac{f(a + h) - f(a)}{h}
\]
providing this limit exists.

If this limit exists then we say that $f(x)$ is differentiable at $x = a$.

If $f'(a)$ exists for all $a$ in $\Dom f$ we say that $f(x)$ is differentiable and then $f'(a)$ defines a function $f'(x)$ called the derivative.

We can use the fact that the derivative is equal to the slope of the tangent of the graph to determine a Cartesian equation for the tangent by calculating the derivative. Explicitly, the tangent line at the point $(a, f(a))$ is given by $y = f(a) + f'(a)(x - a)$.

A necessary condition for a function to be differentiable at a point is that it is continuous at that point.

Geometrically, there are two ways a function can fail to be differentiable at a point where it is continuous: the tangent line is vertical at that point, there is no tangent line at that point.

\subsection{The Leibniz and chain rules}
If $f(x)$ is differentiable then its derivative $f'(x)$ may also be differentiable, we denote its derivative by $f''(x)$ (the second derivative of $f(x)$). In general the $n^{\text{th}}$ derivative is $f^{(n)}(x)$ (alternately $f^{(n)}(x) = \frac{d^n f}{dx ^ n}$ or $f^{(n)}(x) = D ^ n f(x)$).

If the independent variable represents time, say $f(t)$, then $f'(t) = \dot{f}$ and $f''(t) = \ddot{f}$.

The product rule for differentiation is just the first case of the more general Leibniz rule:
If $f(x)$ and $g(x)$ are both differentiable $n$ times then so is the product $f(x)g(x)$ with
\[
D ^ n (fg) = \sum_{k = 0}^{n}\binom{n}{k}(D ^ k f)(D ^ {n - k} g).
\]

\begin{example}
    Use the Leibniz rule to calculate $D ^ 3 (x ^ 2 \sin x)$.

    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        \[
        D ^ 3 (fg) = f(D ^ 3 g) + 3(Df)(D ^ 2 g) + 3(D ^ 2 f)(D g) + (D ^ 3 f)g.
        \]
        $f = x ^ 2,\quad Df = 2x\quad D ^ 2f = 2\quad D ^ 3 f = 0$.
        
        $g = \sin x,\quad Dg = \cos x\quad D ^ 2g = -\sin x\quad D ^ 3 g = -\cos x$.

        \begin{align*}
            D ^ 3(x ^ 2 \sin x) &= -x ^ 2 \cos x + 3(2x)(-\sin x) + 3(2)(\cos x) + 0(\sin x) \\
            &= -x ^ 2 \cos x - 6x\sin x + 6\cos x \\
            &= (6 - x ^ 2)\cos x - 6x\sin x.
        \end{align*}
    \end{proof}
\end{example}

To differentiate composite functions $(f \circ g)(x) = f(g(x))$ we use the following theorem
\begin{theorem}[Chain rule theorem]
    If $g(x)$ is differentiable at $x$ and $f(x)$ is differentiable at $g(x)$ then the composition $(f \circ g)(x)$ is differentiable at $x$ with
    \[
    (f \circ g)'(x) = f'(g(x))g'(x).
    \]
\end{theorem}
We can use Leibniz notation to rewrite the above formula
\[
\frac{d}{dx}f(g(x)) = \frac{df}{dg}\frac{dg}{dx}.
\]

\subsection{L'H\^opital's rule}
Recall that if $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M \neq 0$ then $\lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \frac{L}{M}$. This is not applicable to the situation where $L = M = 0$, which is called an in-determinant form.

\textbf{L'H\^opital's rule}

Let $f(x)$ and $g(x)$ be differentiable on $I = (a - h, a) \cup (a, a + h)$ for some $h > 0$, with $\lim_{x \rightarrow a}f(x) = \lim_{x \rightarrow a}g(x) = 0$.

If $\lim_{x \rightarrow a}\frac{f'(x)}{g(x)}$ exists and $g'(x) \neq 0\, \forall x \in I$ then
\[
\lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}.
\]
\begin{proof}
    We shall only consider the proof for the slightly easier situation in which $f(x)$ and $g(x)$ are both differentiable at $x = a$ and $g'(a) \neq 0$. In this case
    \[
    \lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a}\frac{f(x) - f(a)}{g(x) - g(a)} = \lim_{x \rightarrow a}\frac{\frac{f(x) - f(a)}{x - a}}{\frac{g(x) - g(a)}{x - a}} = \frac{\lim_{x \rightarrow a}\frac{f(x) - f(a)}{x - a}}{\lim_{x \rightarrow a}\frac{g(x) - g(a)}{x - a}} = \frac{f'(a)}{g'(a)} = \lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}.
    \]
\end{proof}


\subsection{Boundedness and monotonicity}
The following definitions apply to a function $f(x)$ defined in some interval $I$.

\begin{definition}
    If there exists a constant $k_1$ such that $f(x) \leq k_1\ \forall x \in I$ we say that $f(x)$ is bounded above in $I$ and we call $k_1$ an upper bound of $f(x)$ in $I$.

    If there exists a point $x_1$ in $I$ such that $f(x_1) = k_1$ we say that the upper bound of $k_1$ is attained and we call $k_1$ the global maximum value of $f(x)$ in $I$.
\end{definition}

\begin{definition}
    If there exists a constant $k_2$ such that $f(x) \geq k_2\ \forall x \in I$ we say that $f(x)$ is bounded below in $I$ and we call $k_2$ a lower bound of $f(x)$ in $I$.

    If there exists a point $x_2$ in $I$ such that $f(x_2) = k_2$ we say that the lower bound of $k_2$ is attained and we call $k_2$ the global minimum value of $f(x)$ in $I$.
\end{definition}

\begin{definition}
    $f(x)$ is bounded in $I$ if it is both bounded above and bounded below in $I$, i.e. if there exists a constant $k$ such that $|f(x)| \leq k\ \forall x \in I$.\footnote{If no interval $I$ is specified then it is taken to be the domain of the function.}
\end{definition}

A condition that guarantees the existence of both a global maximum and minimum value (called extreme values) is provided by the following.

The extreme value theorem states that if $f$ is a continuous function on a closed interval $[a,\, b]$ then it is bounded on that interval and has upper and lower bounds that are attained i.e. there exists points $x_1$ and $x_2$ in $[a,\,b]$ such that $f(x_2) \leq f(x) \leq f(x_1)\ \forall x \in [a,\,b]$.

\begin{definition}
    $f(x)$ is monotonic increasing in $[a,\,b]$ if $f(x_1) \leq f(x_2)$ for all pairs $x_1, x_2$ with $a \leq x_1 < x_2 \leq b$.
\end{definition}

\begin{definition}
    $f(x)$ is strictly monotonic increasing in $[a,\,b]$ if $f(x_1) < f(x_2)$ for all pairs $x_1, x_2$ with $a \leq x_1 < x_2 \leq b$.
\end{definition}

\subsection{Critical points}
\begin{definition}
    We say that $f(x)$ has a local maximum at $x = a$ if $\exists\, h > 0$ such that $f(a) \geq f(x)\, \forall x \in (a - h, a + h)$.
\end{definition}

\begin{remark}
    If $f(x)$ has a local maximum at $x = a$ and is differentiable at this point then $f'(a) = 0$.

    \begin{proof}
        As $f(x)$ is differentiable at $a$ then
        \[
        \lim_{x \rightarrow a^{-}}\frac{f(x) - f(a)}{x - a} = \lim_{x \rightarrow a^{+}}\frac{f(x) - f(a)}{x - a} = f'(a).
        \]
        $f(x)$ has a local maximum at $a$ implies
        \[
        \frac{f(x) - f(a)}{x - a} \geq 0 \text{ for } x \in (a - h, a)
        \]
        \[
        \text{hence } f'(a) = \lim_{x \rightarrow a^{-}}\frac{f(x) - f(a)}{x - a} \geq 0.
        \]
        Similarly, $f(x)$ has a local maximum at $a$ also implies
        \[
        \frac{f(x) - f(a)}{x - a} \leq 0 \text{ for } x \in (a, a + h)
        \]
        \[
        \text{hence } f'(a) = \lim_{x \rightarrow a^{+}}\frac{f(x) - f(a)}{x - a} \geq 0.
        \]
        Putting these two together gives $f'(a) \leq 0 \leq f'(a)$ and thus $f'(a) = 0$.
    \end{proof}
\end{remark}

\begin{definition}
    We say that $f(x)$ has a local minimum at $x = a$ if $\exists h > 0$ such that $f(x) \geq f(a)\ \forall x \in (a - h, a + h)$.
\end{definition}

\begin{remark}
    If $f(x)$ has a local minimum at $x = a$ and is differentiable at this point then $f'(a) = 0$.
\end{remark}

\begin{definition}
    $f(x)$ has a stationary point at $x = a$ if it is differentiable at $x =a$ with $f'(a) = 0$.
\end{definition}

\begin{definition}[Critical point]
    An interior point $x = a$ of the domain of $f(x)$ is called a critical point if either $f'(a) = 0$ or $f'(a)$ does not exist.
\end{definition}

Every local maximum and local minimum of a differentiable function is a stationary point but there may be other stationary points too.

\begin{definition}
    If $f(x)$ is twice differentiable in an open interval around $x = a$ with $f''(a) = 0$ and if $f''(x)$ changes sign at $x = a$ then we say that $x = a$ is a point of inflection.
\end{definition}

\textbf{The first derivative test}

Suppose $f(x)$ is continuous at a critical point $x = a$.

\begin{enumerate}[label = (\roman*)]
    \item If $\exists h > 0$ such that $f'(x) < 0\ \forall x \in (a - h, a)$ and $f'(x) > 0\, \forall x \in (a, a + h)$ then $x = a$ is a local minimum.
    \item If $\exists h > 0$ such that $f'(x) > 0\ \forall x \in (a - h, a)$ and $f'(x) < 0\, \forall x \in (a, a + h)$ then $x = a$ is a local maximum.
    \item If $\exists h > 0$ such that $f'(x)$ has a constant sign $\forall x \neq a$ in $(a - h, a + h)$ then $x = a$ is a not a local extreme value (minimum/maximum).
\end{enumerate}

\textbf{The second derivative test}

Suppose $f(x)$ is twice differentiable at $x = a$ with $f'(a) = 0$.
\begin{enumerate}[label = (\roman*)]
    \item If $f''(a) > 0$ then $x = a$ is a local minimum.
    \item If $f''(a) < 0$ then $x = a$ is a local maximum.
\end{enumerate}

If a function is defined on an interval then extreme values can occur at the endpoints of the interval. Here an endpoint $x = c$ is a point at which the function is defined but where the function is undefined either to the left or right of this point. Example: if the domain of a function is $[a, b]$ then $a$ and $b$ are both endpoints.

\begin{definition}
    If $c$ is an endpoint of $f(x)$ then $f$ has an endpoint maximum at $x = c$ if $f(x) \leq f(c)$ for $x$ sufficiently close to $c$.
\end{definition}

The obvious similar definition applies for an endpoint minimum.

If a function is differentiable sufficiently close to an endpoint then examining the sign of the derivative can determine if there is an endpoint maximum or minimum.

If $f(x)$ is continuous on an interval $[a, b]$ then the global extreme values in the interval are attained at either critical points or end points, can be determined by examining all the possibilities.

\subsection{Rolle's theorem}
\begin{theorem}[Rolle's theorem]
    If $f$ is differentiable on the open interval $(a, b)$ and continuous on the closed interval $[a, b]$, with $f(a) = f(b)$,
    then there is at least one $c \in (a, b)$ for which $f'(c) = 0$.
    \begin{proof}
        By the extreme value theorem there exists $x_1$ and $x_2$ in $[a, b]$ such that
        \[
        f(x_1) \leq f(x) \leq f(x_2)\ \forall x \in [a, b].
        \]
        If $x_1 \in (a, b)$ then $x_1$ is a local minimum and $f'(x_1) = 0$ so we are done.
        If $x_2 \in (a, b)$ then $x_2$ is a local maximum and $f'(x_2) = 0$ so we are done.
    
        The only case that is left is if both $x_1$ and $x_2$ are end points, $a, b$.
        Since $f(a) = f(b)$ then in this case $f(x_1) = f(x_2) = f(a)$ so the above bound becomes $f(a) \leq f(x) \leq f(a)\, \forall x \in [a, b]$.
        Thus $f(x) = f(a)$ is constant in the interval and $f'(x) = 0\, \forall x \in [a, b]$.
    \end{proof}
\end{theorem}

\begin{corollary}
    If $f(x)$ is differentiable at every point of an open interval $I$ then each pair of zeros of the function in $I$ is separated by at least one zero of $f'(x)$.
    \begin{proof}
        This is simply the special case of Rolle's theorem with $a$ and $b$ taken to be the zeros of $f(x)$ so that $f(a) = f(b) = 0$.
    \end{proof}
\end{corollary}
The above result can be used to provide a limit on the number of distinct real zeros of a function.

\begin{example}
    Show that $f(x) = \frac{1}{7}x ^ 7 - \frac{1}{5}x ^ 6 + x ^ 3 - 3x ^ 2 -x$ has no more than $3$ distinct real roots.

    Suppose the result is false and there are (at least) $4$ distinct real roots of $f(x)$.
    Then by Rolle's theorem there are (at least) $3$ distinct real roots of $f'(x)$.
    Then continually applying this we get that there are (at least) $2$ distinct real roots of $f''(x)$.
    However, $f''(x) = 6x ^ 5 - 6x ^ 4 + 6x - 6 = 6(x - 1)(x ^ 4 + 1)$ has only one real root (at $x = 1$).
    This contradiction proves the required result.
\end{example}

\subsection{The mean value theorem}
\begin{theorem}[Mean value theorem]
    If $f$ is differentiable on the open interval $(a, b)$ and continuous on the closed interval $[a, b]$,
    then there is at least one $c \in (a, b)$ for which
    \[
    f'(c) = \frac{f(b) - f(a)}{b - a}.
    \]
    \begin{proof}
        Define the function
        \[
        g(x) = (b - a)(f(x) - f(a)) - (x - a)(f(b) - f(a)).
        \]
        Then $g(a) = g(b) = 0$ and as $g(x)$ satisfies the requirements of Rolle's theorem then $\exists c \in (a, b)$ for which $g'(c) = 0$.
        As $g'(x) = (b - a)f'(x) - (f(b) - f(a))$ then setting $x = c$ yields the required result $f'(c) = \frac{f(b) - f(a)}{b - a}$.
    \end{proof}
\end{theorem}
\textit{Note that Rolle's theorem is just a special case of the mean value theorem with $f(a) = f(b)$.}

\begin{proposition}
    Suppose $f(x)$ is continuous in $[a, b]$ and differentiable in $(a, b)$ with $f'(x) \geq 0$ throughout this interval.
    Then $f(x)$ is monotonic increasing in $(a, b)$.
    \begin{proof}
        If $a \leq x_1 < x_2 \leq b$ then by the mean value theorem $\exists c \in (a, b)$ such that $f'(c) = \frac{f(x_2) - f(x_1)}{x_2 - x_1}$.
        However, since $f'(x) \geq 0$ in $(a, b)$ then $f'(c) \geq 0$ which implies that $f(x_2) \geq f(x_1)$ i.e. $f$ is monotonic increasing.
    \end{proof}
\end{proposition}

\subsection{The inverse function rule}
The inverse function rule states that if $f(x)$ is continuous in $[a, b]$ and differentiable in $(a, b)$ with $f'(x) > 0$ throughout this interval then its inverse function $g(y)$ (recall $g(f(x)) = x$) is differentiable for all $f(a) < y < f(b)$ with $g'(y) = \frac{1}{f'(g(y))}$.

\begin{example}
    $f(x) = \sin x$ has $f'(x) = \cos x > 0$ for $x \in (-\pi / 2, \pi / 2)$.
    Its inverse function $g(y) = \sin^{-1}y$ is therefore differentiable in $(-1, 1)$ with
    \[
    g'(y) = \frac{1}{f'(g(y))} = \frac{1}{\cos g(y)} = \frac{1}{\sqrt{1 - \sin ^ 2 g(y)}}
    \]
    but $y = f(g(y)) = \sin (g(y))$ hence $g'(y) = \frac{1}{\sqrt{1 - y ^ 2}}$.
    Thus we have shown that
    \[
    \frac{d}{dx}\sin^{-1}x = \frac{1}{\sqrt{1 - x ^ 2}}.
    \]
\end{example}

\subsection{Partial derivatives}

For differentiating functions of two variables we will need to use a partial derivative.
The partial derivative of $f$ with respect to $x$ is written as $\frac{\delta f}{\delta x}$ and is obtained by differentiating $f$ with respect to $x$,
keeping $y$ fixed, i.e. $y$ may be treated as a constant in performing the partial differentiation with respect to $x$.
\[
\text{E.g.}\quad f(x, y) = x ^ 2 y ^ 3 - \sin x \cos y + y \quad\text{has}\quad\frac{\delta f}{\delta y} = 2xy ^ 3 - \cos x \cos y.
\]
Similarly, the partial derivative of $f$ with respect to $y$ is written as $\frac{\delta f}{\delta y}$,
obtained by differentiating $f$ with respect to $y$, keeping $x$ fixed.

Partial derivatives are defined in terms of the following limits (we assume exist)
\[
\frac{\delta f}{\delta x}(x, y) = \lim_{h \rightarrow 0}\frac{f(x + h, y) -f(x, y)}{h},
\quad
\frac{\delta f}{\delta y}(x, y) = \lim_{h \rightarrow 0}\frac{f(x, y + h) -f(x, y)}{h}
\]

An alternative notation for partial derivatives is $f_x$ for $\frac{\delta f}{\delta x}$.

By applying partial differentiation to these partial derivatives we may obtain the second order partial derivatives.

\begin{align*}
    &\frac{\delta ^ 2 f}{\delta x ^ 2} = \frac{\delta}{\delta x}\left(\frac{\delta f}{\delta x}\right),
    &\frac{\delta ^ 2 f}{\delta y ^ 2} = \frac{\delta}{\delta y}\left(\frac{\delta f}{\delta y}\right),
    &\frac{\delta ^ 2 f}{\delta y \delta x} = \frac{\delta}{\delta y}\left(\frac{\delta f}{\delta x}\right),
    &\frac{\delta ^ 2 f}{\delta x \delta y} = \frac{\delta}{\delta x}\left(\frac{\delta f}{\delta y}\right).
\end{align*}
It can be shown that
\[
\frac{\delta ^ 2 f}{\delta x \delta y} = \frac{\delta ^ 2 f}{\delta y \delta x}
\]
is true in general, if $f$ and all first and second order partial derivatives are continuous.

An alternative notation for partial derivatives is
\[
\frac{\delta ^ 2 f}{\delta x ^ 2} = f_{xx}.
\]

\newpage

\section{Integration}

\subsection{Indefinite and definite integrals}
\begin{definition}
    A function $F(x)$ is called an indefinite integral or antiderivative of a function $f(x)$ in the interval $(a, b)$ if $F(x)$ is differentiable with

    $F'(x) = f(x)$ throughout $(a, b)$. We then write $F(x) = \int f(x)\,dx$.
\end{definition}

Note: If $F(x)$ is an indefinite integral of $f(x)$ in $(a, b)$ then so is $F(x) + c$ for any constant $c$.

Note: If $F_1(x)$ and $F_2(x)$ are both indefinite integrals of $f(x)$ in $(a, b)$ then $F_1(x) - F_2(x) = c$ for constant $c$.

\begin{definition}
    We say that $f(x)$ is integrable in $(a, b)$ if it has an indefinite integral $F(x)$ in $(a, b)$ that is continuous in $[a, b]$.
\end{definition}

\begin{definition}
    A subdivision $S$ of $[a, b]$ is a partition into a finite number of subintervals
    \[
    [a, x_1], [x_1, x_2], \dotsc, [x_{n - 1}, b]
    \]
    where $a = x_0 < x_1 < x_2 < \dotsc < x_n = b$.
\end{definition}
The norm $|S|$ of the subdivision is the maximum of the subinterval lengths $|a - x_1|, |x_1 - x_2|, \dotsc, |x_{n - 1} - b|$.
The numbers $z_1, z_2, \dotsc, z_n$ form a set of sample points from $S$ if $z_j \in [x_{j - 1}, x_j]$ for $j = 1,\dotsc, n$.

\begin{definition}
    Suppose that $f(x)$ is a function defined for $x \in [a, b]$.
    The Riemann sum is
    \[
    \mathcal{R} = \sum_{j = 1}^{n}(x_j - x_{j - 1})f(z_j).
    \]
\end{definition}
This leads to the definition of the definite integral as
\[
\int_{a}^{b}f(x)\,dx = \lim_{|S| \rightarrow 0}\mathcal{R}
\]
and it can be shown that this limit exists if $f(x)$ is continuous in $[a, b]$.

The following results can be proved.
\begin{proposition}
    In the following let $f(x)$ and $g(x)$ both be integrable in $(a, b)$.
    \begin{enumerate}[label = (\roman*)]
        \item Linearity.
        If $\lambda, \mu$ are any constants then $\lambda f(x) + \mu g(x)$ is integrable in $(a, b)$ with
        \[
        \int_{a}^{b}\left(\lambda f(x) + \mu g(x)\right)\,dx = \lambda\int_{a}^{b}f(x)\,dx + \mu\int_{a}^{b}g(x)\,dx.
        \]
        \item If $c \in [a, b]$ then $\int_{a}^{b}f(x)\,dx = \int_{a}^{c}f(x)\,dx + \int_{c}^{b}f(x)\,dx$.
        \item If $f(x) \geq g(x)\,\forall x \in (a, b)$ then $\int_{a}^{b}f(x)\,dx \geq \int_{a}^{b}g(x)\,dx$.
        \item If $m \leq f(x) \leq M\,\forall x \in [a, b]$ then
        \[
        m(b - a) \leq \int_{a}^{b}f(x)\,dx \leq M(b - a).
        \]
    \end{enumerate}
    \begin{enumerate}[label = (\roman*)]
        \item
        \begin{proof}
        \end{proof}
        \item
        \begin{proof}
        \end{proof}
        \item
        \begin{proof}
        \end{proof}
        \item
        \begin{proof}
        \end{proof}
    \end{enumerate}
\end{proposition}

\subsection{The fundamental theorem of calculus}
\begin{theorem}[The fundamental theorem of calculus]
    If $f(x)$ is continuous on $[a, b]$ then the function
    \[
    F(x) = \int_{a}^{x}f(t)\,dt
    \]
    defined for $x \in [a, b]$ is continuous on $[a, b]$ and differentiable on $(a, b)$ and is an indefinite integral of $f(x)$ on $(a, b)$ i.e.
    \[
    F'(x) = \frac{d}{dx}\int_{a}^{x}f(t)\,dt = f(x)
    \]
    throughout $(a, b)$.
    Furthermore if $\Tilde{F}(x)$ is any indefinite integral of $f(x)$ on $[a, b]$ then
    \[
    \int_{a}^{b}f(t)\,dt = \Tilde{F}(b) - \Tilde{F}(a) = [\Tilde{F}(x)]^{b}_{a}.
    \]
    \begin{proof}[Proof sketch]\renewcommand{\qedsymbol}{$\triangle$}
    For $a \leq x < x + h < b$ we have that
    \[
    F(x + h) - F(x) = \int_{a}^{x + h}f(t)\,dt - \int_{a}^{x}f(t)\,dt = \int_{x}^{x + h}f(t)\,dt
    \]
    where we have used property (ii).
    
    Let $m(h)$ and $M(h)$ denote the minimum and maximum values of $f(x)$ on the interval $[x, x + h]$.
    Then by property (iv) we have that
    \[
    m(h)h \leq \int_{x}^{x + h}f(t)\,dt \leq M(h)h.
    \]
    Thus
    \[
    m(h) \leq \frac{F(x + h) - F(x)}{h} \leq M(h).
    \]
    Since $f(x)$ is continuous on $[x, x + h]$ we have that
    \[
    \lim_{h \rightarrow 0^+}m(h) = \lim_{h \rightarrow 0^+}M(h) = f(x)
    \]
    and so by the pinching theorem
    \[
    \lim_{h \rightarrow 0^+}\frac{F(x + h) - F(x)}{h} = f(x).
    \]
    A similar argument applies to the limit from below and together they give
    \[
    F'(x) = \lim_{h \rightarrow 0}\frac{F(x + h) - F(x)}{h} = f(x)
    \]
    which proves that $F(x)$ is an indefinite integral of $f(x)$.
    The proof of the first part of the theorem is completed by showing that $F(x)$ is continuous from the right at $x = a$ and from the left at $x = b$.
    Both these follow by a simple consideration of the limit of the relevant quotient.
    Finally, to prove the last part of the theorem the key observation is that any indefinite integral $\Tilde{F}(x)$ is related to $F(x)$ by the addition of a constant.
    \end{proof}
\end{theorem}

\subsection{Limits with logarithms, powers and exponentials}
There are some important results concerning limits as $x \rightarrow \infty$ for the logarithm and exponential functions.

\begin{lemma}\label{pre_calc_lem_lemma1}
    $\forall\, x \geq 0,\quad e ^ x \geq 1 + x$
    \begin{proof}
        Consider $f(x) = e ^ x - (1 + x)$ then $f(0) = 0$ and $f'(x) = e ^ x - 1 \geq 0$.
        Hence $f(x)$ is monotonic increasing in $[0, \infty)$ so $f(x) \geq 0$ for all $x \geq 0$.
    \end{proof}
\end{lemma}

\begin{lemma}\label{pre_calc_lem_lemma2}
    $\forall\, x \geq 0$,
    and for any positive integer $n$,
    $e ^ x \geq \sum_{j = 0}^{n}\frac{x ^ j}{j!}$.
    \begin{proof}
    The case $n = 1$ corresponds to the previous lemma.
    The proof for general $n$ is similar to the proof of the previous lemma.
    
    \Large\textbf{Prove!}
    \end{proof}
\end{lemma}

Result $1$: powers beat logs

For any constant $a > 0$
\[
\lim_{x \rightarrow \infty}\frac{\log x}{x ^ a} = 0.
\]
\begin{proof}
    Put $x = e ^ y$ then
    \[
    \lim_{x \rightarrow \infty}\frac{\log x}{x ^ a} = \lim_{y \rightarrow \infty}\frac{y}{e ^ {ay}}.
    \]
    For $y > 0$ the by \autoref{pre_calc_lem_lemma2} with $n = 2$
    \[
    0 \leq \frac{y}{e ^ {ay}} \leq \frac{y}{1 + ay + \frac{1}{2}a ^ 2 y ^ 2} \leq \frac{y}{\frac{1}{2}a ^ 2 y ^ 2} = \frac{2}{a ^ 2 y}.
    \]
    As $\lim_{y \rightarrow \infty}\frac{2}{a ^ 2 y} = 0$ then by the pinching theorem
    \[
    \lim_{y \rightarrow \infty}\frac{y}{e ^ {ay}} = 0 = \lim_{x \rightarrow \infty}\frac{\log x}{x ^ a}.
    \]
\end{proof}

Result $2$: exponentials beat powers

For any constant $a > 0$
\[
\lim_{x \rightarrow \infty}\frac{x ^ a}{e ^ x} = 0.
\]
\begin{proof}
    Let $n$ be the smallest integer such that $n > a$.
    By \autoref{pre_calc_lem_lemma2},
    for $x > 0$ we have that
    \[
    0 \leq \frac{x ^ a}{e ^ x} \leq \frac{x ^ a}{1 + x + \dotsc + \frac{x ^ n}{n!}} = \frac{x ^ {a - n}}{x ^ {-n} + x ^ {1 - n} + \dotsc + \frac{1}{n!}}
    \]
    As $a - n < 0$ then $\lim_{x \rightarrow \infty}\frac{x ^ {a - n}}{x ^ {-n} + x ^ {1 - n} + \dotsc + \frac{1}{n!}} = 0$,
    hence by the pinching theorem $\lim_{x \rightarrow \infty}\frac{x ^ a}{e ^ x} = 0$.
\end{proof}

Result $3$: the exponential as a limit

For any constant $a$
\[
\lim_{x \rightarrow \infty}\left(1 + \frac{a}{x}\right) ^ x = e ^ a.
\]
\begin{proof}[Proof (for the case $a > 0$)]
    Recall the definition of the derivative of a function $f(x)$ at $x = b$.
    \[
    f'(b) = \lim_{h \rightarrow 0}\frac{f(b + h) - f(b)}{h}.
    \]
    Apply this to $f(x) = \log x$ so that $f'(x) = \frac{1}{x}$ and take $b = 1$.
    \[
    1 = \lim_{h \rightarrow 0}\frac{\log(1 + h) - \log(1)}{h} = \lim_{h \rightarrow 0}\frac{\log(1 + h)}{h}.
    \]
    With the change of variable $h = \frac{a}{x}$ this becomes
    \[
    1 = \lim_{x \rightarrow \infty}\frac{\log\left(1 + \frac{a}{x}\right)}{\frac{a}{x}}\text{ i.e. } a = \lim_{x \rightarrow \infty}\left(x\log\left(1 + \frac{a}{x}\right)\right).
    \]
    Since $e ^ x$ is continuous at $a$ we have that
    \[
    e ^ a = \lim_{x \rightarrow \infty}e ^ {x\log\left(1 + \frac{a}{x}\right)} = \lim_{x \rightarrow \infty}\left(1 + \frac{a}{x}\right) ^ x.
    \]
\end{proof}

\subsection{Integration using a recurrence relation}
\begin{example}
    Calculate $\int_0^1 x ^ 3 e ^ x\,dx$.
    \begin{proof}[Solution]\renewcommand{\qedsymbol}{}
        Define $I_n = \int_0^1x ^ n e ^ x\,dx$ for all integers $n \geq 0$.
        \[
        I_{n + 1} = \int_0^1 x ^ {n + 1}e ^ x\,dx = \left[x ^ {n + 1}e ^ x\right]_0^1 - \int_0^1(n + 1)x ^ n e ^ x\,dx = e - (n + 1)I_n.
        \]
        The recurrence relation $I_{n + 1} = e - (n + 1)I_n$ can be used to calculate $I_n$ for any positive integer $n$ from the starting value $I_0 = \int_0^1e ^ x\,dx = e - 1$.
    \end{proof}
\end{example}

\subsection{Definite integrals using even and odd function}
The definite integral of an odd function over a symmetric interval is zero.

If $f_{\text{odd}}(x)$ is an integrable odd function on $[-a, a]$ then
\[
\int_{-a}^{a}f_{\text{odd}}(x)\,dx = 0.
\]
\begin{proof}
    \begin{align*}
        \int_{-a}^{a}f_{\text{odd}}(x)\,dx &= \int_{-a}^{0}f_{\text{odd}}(u)\,du + \int_0^af_{\text{odd}}(x)\,dx \\
        &= -\int_{a}^{0}f_{\text{odd}}(-x)\,dx + \int_0^af_{\text{odd}}(x)\,dx \\
        &= \int_0^a(f_{\text{odd}}(-x) + f_{\text{odd}}(x))\,dx = 0.
    \end{align*}
\end{proof}

A similar argument shows that if $f_{\text{even}}(x)$ is an integrable even function on $[-a, a]$ then
\[
\int_{-a}^{a}f_{\text{even}}(x)\,dx = 2\int_0^af_{\text{even}}(x)\,dx.
\]
\begin{proof}
    \begin{align*}
        \int_{-a}^{a}f_{\text{even}}(x)\,dx &= \int_{-a}^{0}f_{\text{even}}(x)\,dx + \int_{0}^{a}f_{\text{even}}(x)\,dx \\
        &= \int_{0}^{a}f_{\text{even}}(-x)\,dx + \int_{0}^{a}f_{\text{even}}(x)\,dx \\
        &= \int_{0}^{a}f_{\text{even}}(x)\,dx + \int_{0}^{a}f_{\text{even}}(x)\,dx \\
        &= 2\int_{0}^{a}f_{\text{even}}(x)\,dx.
    \end{align*}
\end{proof}

If this limit switching seems confusing then consider the following proposition
\begin{proposition}
    Let $f(x)$ be an integrable function continuous on $[a, b]$.
    Then
    \[
    \int_{a}^{b}f(x)\,dx = \int_{-b}^{-a}f(-x)\,dx
    \]
    \begin{proof}
        \large \textbf{Please prove!}
    \end{proof}
\end{proposition}

Recall that any function $f$ can be decomposed into a sum of odd and even functions.
We can use this to calculate definite integrals over symmetric intervals as
\[
\int_{-a}^{a}f(x)\,dx = \int_{-a}^{a}f_{\text{even}}(x) + f_{\text{odd}}(x)\,dx = 2\int_0^af_{\text{even}}(x)\,dx.
\]

\newpage

\section{Double integrals}

\subsection{Rectangular regions}
For a function $f(x)$ the definite integral $\int_a^bf(x)\,dx$ is the signed area under the curve $y = f(x)$ between $x = a$ and $x = b$.
This interpretation of the definite integral can be generalised for a function of two variables to a volume under a surface.
Given a function $f(x, y)$ and a region $D$ in the $(x, y)$ plane,
the double integral
\[
\iint\limits_Df(x, y)\,dxdy
\]
is the signed volume of the region between the surface $z = f(x, y)$ and the region $D$ in the $z = 0$ plane.

We can define the double integral as the limit of a Riemann sum of volumes of cuboids.

For simplicity,
consider the case of a rectangular region $D = [a_0, a_1] \times [b_0, b_1]$.
We construct a subdivision $S$ of $D$ by defining the points of a two-dimensional lattice as $a_0 = x_0 < x_1 < \dotsi < x_n = a_1$ and $b_0 = y_0 < y_1 < \dotsi < y_m = b_1$.
The edge lengths of the lattice are equal to $dx_i = x_i - x_{i - 1}$ for $i = 1, \dotsc, n$ and $dy_j = y_j - y_{j - 1}$ for $j = 1, \dotsc, m$.
The norm of the subdivision $|S|$ is given by the maximum of all the $dx_i$ and $dy_j$.

We introduce sample points $(p_i, q_j) \in [x_{i - 1}, x_i] \times [y_{j - 1}, y_j]$ inside each rectangle of the lattice.
The area of the rectangle is $dx_idy_j$ and we associate to this rectangle a cuboid of height $f(p_i, q_j)$.

The volume of all the cuboids is the Riemann sum
\[
\mathcal{R} = \sum_{i = 1}^{n}\sum_{j = 1}^{m}f(p_i, q_j)dx_idy_j
\]
and the double integral is the limit
\[
\iint\limits_Df(x, y)\,dxdy = \lim_{|S| \rightarrow 0}\mathcal{R}.
\]
In the special case that $f(x, y)$ is constant,
say $f(x, y) = c$,
then
\[
\iint\limits_Df(x, y)\,dxdy = c \times \mathrm{area}(D).
\]

For a rectangular region $D = [a_0, a_1] \times [b_0, b_1]$ a practical way to compute the double integral is to use the following iterated integral result
(valid providing $f$ is continuous on $D$)
\[
\iint\limits_Df(x, y)\,dxdy = \int_{a_0}^{a_1}\left(\int_{b_0}^{b_1}f(x, y)\,dy\right)\,dx = \int_{b_0}^{b_1}\left(\int_{a_0}^{a_1}f(x, y)\,dx\right)\,dy.
\]

\subsection{Beyond rectangular regions}
Computing double integrals beyond rectangular regions requires us to consider the following definition.
\begin{definition}
    A region $D$ of the plane is called $y$-simple if every line that is parallel to the $y$-axis and intersects $D$,
    does so in a single line segment
    (or a single point if this is on the boundary of $D$).
\end{definition}

Given a $y$-simple region with boundaries given by the curves $y = \phi_1(x)$ and $y = \phi_2(x)$ for $a_0 \leq x \leq a_1$.

We can calculate the double integral as an iterated integral by integrating over $y$ first
\begin{gather*}
    \iint\limits_{D} f(x, y)\,dxdy = \int_{a_0}^{a_1}\left(\int_{\phi_1(x)}^{\phi_2(x)}f(x, y)\,dy\right)\,dx.
\end{gather*}
We can apply a similar definition for an $x$-simple region.
\begin{definition}
    A region $D$ of the plane is called $x$-simple if every line that is parallel to the $x$-axis and intersects $D$,
    does so in a single line segment
    (or a single point if this is on the boundary of $D$).
\end{definition}
Given an $x$-simple region with boundaries given by the curves $x = \psi_1(y)$ and $x = \psi_2(y)$ for $b_0 \leq y \leq b_1$.

As we did before,
the double integral can be calculated as an iterated integral,
integrating over $x$ first
\[
\iint\limits_Df(x, y)\,dxdy = \int_{b_0}^{b_1}\left(\int_{\psi_1(y)}^{\psi_2(y)}f(x, y)\,dx\right)\,dy.
\]
I would like to note,
that it is probably quite important that these expressions for calculating expressions for $x$ and $y$ simple regions are kept in mind.

However,
when a region is both $x$-simple and $y$-simple we can calculate the double integral as an iterated integral by integrating over $x$ or $y$ first.

From my current understanding a region is both $x$-simple and $y$-simple if,
the vertical line for $y$-simple can be drawn from one end of the region to another,
and the horizontal line for an $x$-simple region can be drawn from one end to the other.
These are both such that the line does not leave the bounds of the region,
in other words, they "can't leave through holes in the region".

\subsection{Integration using polar coordinates}
Sometimes it is useful to describe the region of integration $D$ in terms of polar coordinates $r$ and $\theta$,
where $x = r\cos\theta$ and $y = r\sin\theta$.
To convert a double integral into polar coordinates we need to know what to do with the area element $dA = dxdy$.
Thinking back to what this area element actually is,
we can see that it is just a division of $D$ into small rectangles.
The area which is calculated from a small rectangle with side lengths $dx$ and $dy$.
Hence,
we need to find the area $dA$ of a small region by taking the point with polar coordinates $(r, \theta)$
and extending $r$ by $dr$ and $\theta$ by $d\theta$.
We see that the area is approximately a rectangle with area $dA \approx rd\theta dr$
providing us with the final result $dA = dxdy = rd\theta dr$.

Now to convert the double integral into polar coordinates,
we would replace $dxdy$ with $rd\theta dr$.
We can then calculate the double integral as an iterated integral if the region $D$ is $\theta$-simple or $r$-simple
(with the obvious definitions of these terms).

\subsection{Change of variables and the Jacobian}
\begin{definition}[The Jacobian]
    The Jacobian of the transformation from the variables $x, y$ to $u, v$ is
    \[
    J = \frac{\partial(x, y)}{\partial(u, v)} = \left|\begin{array}{cc}
        \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
        \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} 
    \end{array}\right|
    \]
\end{definition}
i.e. it is the determinant of the $2 \times 2$ matrix of partial derivatives.

To obtain the area element $dxdy$ in terms of the new variables we first compute the Jacobian $J$ and then use the result that
\[
dxdy = |J|dudv
\]
where $|J|$ is the absolute value of the Jacobian.

\subsection{The Gaussian integral}
An important integral that appears in a wide range of contexts is the Gaussian integral
\[
\int_{-\infty}^{\infty}e ^ {-ax ^ 2}\,dx = \frac{\pi}{a}
\]
where $a$ is a positive constant.

We can derive this result using a trick that involves calculating a double integral,
as follows.
We define $I$ to e a required integral
\[
I = \int_{-\infty}^{\infty}e ^ {-ax ^ 2}\,dx
\]
and observe that
\[
I ^ 2 = \left(\int_{-\infty}^{\infty}e ^ {-ax ^ 2}\,dx\right)\left(\int_{-\infty}^{\infty}e ^ {-ay ^ 2}\,dy\right) = \iint\limits_{\R ^ 2}e ^ {-a(x ^ 2 + y ^ 2)}\,dxdy.
\]
We now use polar coordinates to evaluate this double integral
\begin{align*}
    I ^ 2 = \iint\limits_{\R}e ^ {-ar ^ 2}r\,drd\theta &= \int_0^{\infty}\left(\int_0^{2\pi}re ^ {-ar ^ 2}\,d\theta\right)\,dr = 2\pi\int_0^{\infty}re ^ {-ar ^ 2}\,dr \\
    &= -2\pi\left[\frac{e ^ {-ar ^ 2}}{2a}\right]_0^{\infty} = \frac{\pi}{a}.
\end{align*}
Hence $I = \sqrt{\frac{\pi}{a}}$,
which is the required result.

\newpage

\section{First order differential equations}

\subsection{First order separable ODEs}

A differential equation is an equation that relates an unknown function
(the dependent variable)
to one or more of its derivatives.
The order of the differential equation is the order of the highest derivative in the equation.
If a differential equation only depends on a single unknown variable
(the independent variable)
then the differential equation is called an ordinary differential equation
(ODE).

Differential equations for functions with several independent variables and involving partial derivatives are called partial differential equations
(PDEs).

We will usually discuss ODEs using $y$ to denote the dependent variable and $x$ for the independent variable,
so solving the ODE involves finding $y(x)$.

The generic first order ODE may be written in the form
\begin{equation}
    \frac{dy}{dx} = f(x, y)
\end{equation}
Generically,
this will have a one-parameter family of solutions i.e. the general solution will contain an arbitrary constant.
Assigning a particular value to this constant gives a particular solution.
A particular solution will be singled out by the assignment of an initial value i.e. requiring $y(x_0) = y_0$,
for given $x_0$ and $y_0$.
In this case the ODE is called an initial value problem (IVP).
To solve an IVP,
first solve the ODE to find the general solution and then determine the value of the constant so that the IVP is also satisfied.

Depending on the form of the function $f(x, y)$ there are various methods for solving certain kinds of ODEs.

The ODE (1) is separable if $f(x, y) = X(x)Y(y)$.
It can be solved by direct integration as
\[
\int\frac{1}{Y(y)}\,dy = \int X(x)\,dx.
\]

\subsection{First order homogeneous ODEs}
The ODE (1) is homogeneous if $f(tx, ty) = f(x, y)\,\forall t \in \R$.

In this case the substitution $y = xv$ produces a separable ODE for $v(x)$.

\subsection{First order linear ODEs}
The ODE (1) is linear if $f(x, y) = -p(x)y + q(x)$,
in which case we can write the standard form of a linear first order ODE as
\[
y' + py = q
\]
where $p$ and $Q$ can be any functions of $x$.
In this case the ODE can be solved in terms of the integrating factor
\[
I(x) e ^ {\int p(x)\,dx}.
\]
The solution is given by
\[
y = \frac{1}{I(x)}\int I(x)q(x)\,dx.
\]
\begin{proof}
    We need to show that $y$ satisfies the ODE $y' + py = q$.
    The first step is to observe that $I' = e ^ {\int p\,dx}p = Ip$.
    With $y = \frac{1}{I}\int Iq\,dx$ we have
    \[
    y' = -\frac{I'}{I ^ 2}\int Iq\,dx + \frac{1}{I}Iq = -\frac{I'}{I}y + q = -\frac{Ip}{I}y + q = -py + q,
    \]
    as required.
\end{proof}

\subsection{First order exact ODEs}
An alternative form in which to write a first order ODE is
\begin{equation}
    M(x, y)\,dx + N(x, y)\,dy = 0.
\end{equation}
Rearranging gives
\[
\frac{dy}{dx} = -\frac{M(x, y)}{N(x, y)}
\]

For any function $g(x, y)$ the total differential $dg$ is defined to be
\[
dg = \frac{\partial g}{\partial x}\,dx + \frac{\partial g}{\partial y}\,dy.
\]

The ODE ($2$) is called exact if there exists a function $g(x, y)$ such that the left side of ($2$) is equal to the total derivative $dg$ i.e.
\[
M = \frac{\partial g}{\partial x}\text{ and } N = \frac{\partial g}{\partial y}.
\]
In this case the ODE says that $dg = 0$ hence $g$ is constant and this yields the solution of the ODE.

The equality of the mixed partial derivatives $\frac{\partial ^ 2 g}{\partial y \partial x} = \frac{\partial ^ 2 g}{\partial x \partial y}$ requires that an exact equation satisfies $\frac{\partial M}{\partial y} = \frac{\partial N}{\partial x}$.

This results in the following test for exactness
\[
\text{The ODE } M(x, y)\,dx + N(x, y)\,dy = 0\text{ is exact if and only if } \frac{\partial M}{\partial y} = \frac{\partial N}{\partial x}.
\]
\begin{example}
    Show that $(3e ^ {3x}y + e ^ x)\,dx + (e ^ {3x} + 1)\,dy = 0$ is an exact equation and hence solve it.
    In this example $M = 3e ^ {3x}y + e ^ x$ and $N = e ^ {3x} + 1$
    \[
    \text{From } M = \frac{\partial g}{\partial x}\text{ we have } \frac{\partial g}{\partial x} = 3e ^ {3x}y + e ^ x\text{ giving } g = e ^ {3x}y + e ^ x + \phi(y)
    \]
    where the usual constant of integration is replaced by an arbitrary function of integration $\phi(y)$.
    To determine this function we use
    \[
    \frac{\partial g}{\partial y} = N,\ \frac{\partial g}{\partial y} = e ^ {3x} + 1 = e ^ {3x} + \phi',\text{ so } \phi' = 1,\text{ giving }\phi = y.
    \]
    /finally we have that $g = e ^ {3x}y + e ^ x + y = c$.
\end{example}

Consider an ODE $M\,dx + N\,dy = 0$ that is not exact.
By multiplying the ODE by a function $I(x, y)$ we can get an equivalent representation of the ODE as $m\,dx + n\,dy = 0$ where $m = MI$ and $n = NI$.
If this equation is exact then $I$ is called an integrating factor for the original ODE.

\begin{example}
    Show that $x$ is an integrating factor for $(3xy - y ^ 2)\,dx + x(x - y)\,dy = 0$.
    $M = 3xy - y ^ 2$ so $\frac{\partial M}{\partial y} = 3x - 2y$,
    but $N = x ^ 2 - xy$ so $\frac{\partial N}{\partial x} = 2x - y$.
    Therefore $\frac{\partial M}{\partial y} \neq \frac{\partial N}{\partial x}$ and the ODE is not exact.
    However,
    with the integrating factor $I = x$ we get $m = MI = 3x ^ 2y - y ^ 2x$ and $n = NI = x ^ 3 - x ^ 2 y$ so now $\frac{\partial m}{\partial y} = 3x ^ 2 - 2yx = \frac{\partial n}{\partial x}$ which is exact.
    So we can now solve the exact equation using the above method.
    $\frac{\partial g}{\partial x} = m = 3x ^ 2y - y ^ 2x$ giving $g = x ^ 3y - \frac{1}{2}y ^ 2x ^ 2 + \phi(y)$.
    $\frac{\partial g}{\partial y} = n = x ^ 3 - x ^ 2y = x ^ 3 - x ^ 2 y + \phi'$ so $\phi' = 0$ and we can take $\phi = 0$.
    This gives the final solution $g = c$ i.e. $x ^ 3 y - \frac{1}{2}y ^ 2 x ^ 2 = c$.
\end{example}

\subsection{Bernoulli equations}
A Bernoulli equation is a non-linear ODE of the form
\[
y' + p(x)y = q(x)y ^ n
\]
where $n \neq 0, 1$,
otherwise the equation is simply linear.

These ODEs can be solved by first performing the substitution $v = y ^ {1 - n}$,
which converts the equation to a linear ODE for $v(x)$.
\begin{example}
    Solve
    \[
    y' - \frac{2y}{x} = -x ^ 2y ^ 2.
    \]
    This is a Bernoulli equation with $n = 2$ hence put $v = \frac{1}{y}$,
    which gives $v' = -\frac{y'}{y ^ 2}$.
    Dividing the ODE by $y ^ 2$ yields
    \[
    \frac{y'}{y ^ 2} - \frac{2}{xy} = -x ^ 2
    \]
    which in terms of $v$ is
    \[
    -v' - \frac{2v}{x} = -x ^ 2
    \]
    which is the linear equation
    \[
    v' + \frac{2}{x}v = x ^ 2.
    \]
    Solving using this using the integrating factor $I = \exp\left(\int\frac{2}{x}\,dx\right) = \exp(2\log x) = x ^ 2$ as
    \[
    v = \frac{1}{x ^ 2}\int x ^ 2 x ^ 2\,dx = \frac{1}{x ^ 2}\left(\frac{1}{5}x ^ 5 + \frac{c}{5}\right).
    \]
    So finally
    \[
    y = \frac{1}{v} = \frac{5x ^ 2}{x ^ 5 + c}.
    \]
\end{example}

\newpage

\section{Second order differential equations}

\subsection{Linear constant coefficient homogeneous ODEs}
\begin{equation}\tag{$\dagger$}
    \alpha_2y'' + \alpha_1y' + \alpha_0y = \phi(x)
\end{equation}
where $\alpha_2 \neq 0, \alpha_1, \alpha_0$ are constants and $\phi(x)$ is an arbitrary function of $x$.

If $\phi(x) = 0$,
\begin{equation}\tag{$\dagger\dagger$}
    \alpha_2y'' + \alpha_1y' + \alpha_0y = 0
\end{equation}
is called homogeneous.

If $y_1$ and $y_2$ are any two solutions of the homogeneous ODE $(\dagger\dagger)$ then so is any arbitrary linear combination $y = Ay_1 + By_2$,
where $A$ and $B$ are constants.
\begin{proof}
    $y = Ay_1 + By_2$ so $y' = Ay_1' + By_2'$ and $y'' = Ay_1'' + By_2''$.
    Hence $\alpha_2y'' + \alpha_1y' + \alpha_0y = \alpha_2(Ay_1'' + By_2'') + \alpha_1(Ay_1' + By_2') + \alpha_0(Ay_1 + By_2) = A(\alpha_2y_1'' + \alpha_1y_1' + \alpha_0y_1) + B(\alpha_2y_2'' + \alpha_1y_2' + \alpha_0y_2) = 0 + 0 = 0$.
\end{proof}

The general solution of the ODE ($\dagger\dagger$) contains two arbitrary constants
(because the ODE is second order).
If $y_1$ and $y_2$ are any two independent particular solutions then the general solution is given by $y = Ay_1 + By_2$,
with $A$ and $B$ the two arbitrary constants.

To find a particular solution,
look for one in the form $y = e ^ {\lambda x}$.

Putting this into ($\dagger\dagger$) yields the characteristic (or auxiliary) equation
\[
\alpha_2\lambda ^ 2 + \alpha_1\lambda + \alpha_0 = 0.
\]
There are three cases to consider,
these depend upon the type of roots.

\begin{enumerate}[label = (\roman*)]
    \item Distinct real roots.
    If the auxiliary equation has distinct real roots $\lambda_1, \lambda_2$ then $y_1 = e ^ {\lambda_1x}$ and $y_2 = e ^ {\lambda_2x}$ are the particular solutions,
    the general solution is
    \[
    y = Ae ^ {\lambda_1x} + Be ^ {\lambda_2x}.
    \]
    \item Repeated real root.
    If the auxiliary equation has a repeated real root then there is only one particular solution leading us to the general solution being of the form
    \[
    y = Ae ^ {\lambda_1x} + Bxe ^ {\lambda_1x}.
    \]
    Alternatively,
    \[
    y = (A + Bx)e ^ {\lambda_1x}.
    \]
    \item Complex roots.
    If the auxiliary equation has complex roots then they're are in a complex conjugate pair,
    $\lambda_1 = \alpha + i\beta$ and $\lambda_2 = \alpha - i\beta$.
    These give us the general solution
    \[
    y = e ^ {\alpha x}(A\cos(\beta x) + B\sin(\beta x))
    \]
    with $A$ and $B$ real.
\end{enumerate}

\subsection{The method of undetermined coefficients}
\begin{equation}\tag{$\dagger$}
    \alpha_2y'' + \alpha_1y' + \alpha_0y = \phi(x)
\end{equation}

The general solution to ($\dagger$) is obtained as a sum of two parts
\[
y = y_{CF} + y_{PI}
\]
where $y_{CF}$ is called the complementary function and is the general solution to the homogeneous version of the ODE.
$y_{PI}$ is the particular integral and is any particular solution of ($\dagger$).

The combination $y = y_{CF} + y_{PI}$ is indeed a solution to ($\dagger$).
We have that $y' = y'_{CF} + y'_{PI}$ and $y'' = y''_{CF} + y''_{PI}$ so putting these into the left hand side of ($\dagger$) to get
\begin{gather*}
    \alpha_2y'' + \alpha_1y' + \alpha_0y = \alpha_2(y''_{CF} + y''_{PI}) + \alpha_1(y'_{CF} +y'_{PI}) + \alpha_0(y_{CF} + y_{PI}) \\
    = \underbrace{\alpha_2y''_{CF} + \alpha_1y'_{CF} + \alpha_0y_{CF}}_{= 0 \text{ by ($\dagger\dagger$)}} + \underbrace{\alpha_2y''_{PI} + \alpha_1 y'_{PI} + \alpha_0y_{PI}}_{=\phi(x) \text{ by ($\dagger$)}} = \phi(x).
\end{gather*}
To find a particular integral we shall use the method of undetermined coefficients,
which can be applied if $\phi(x)$ takes certain forms.

The idea is to try an appropriate form of the solution that contains some unknown constant coefficients which are then determined by explicit calculation.
Here are some of the particular solutions
\begin{table}[H]
    \begin{tabular}{|c|c|}
        \hline
         Term in $\phi(x)$ & Form to try for $y_{PI}$ \\
         \hline
         $e ^ {\gamma x}$ & $a_1e ^ {\gamma x}$ \\
         $x ^ n$ & $a_0 + a_1x + \dotsi + a_nx ^ n$ \\
         $\cos(\gamma x)$ & $a_1\cos(\gamma x) + a_2\sin(\gamma x)$ \\
         $\sin(\gamma x)$ & $a_1\cos(\gamma x) + a_2\sin(\gamma x)$ \\
         \hline
    \end{tabular}
\end{table}

The motivation is that the form of the try when differentiated zero,
once or twice,
should have a similar functional form to $\phi(x)$,
there is then a chance that the left hand side of ($\dagger$) could equal the right hand side.

\textbf{Special case rule}:
If the suggest form of the try for $y_{PI}$ is contained in $y_{CF}$ then we know that this form wont work as the left hand side of ($\dagger$) will then be identically zero and so can't equal $\phi(x)$.

In this case first multiply the suggested form of the try by $x$ to get the correct try.
In some special cases this rule may need to be applied twice as $x$ times the suggested try may also be contained in $y_{CF}$.

\subsection{Initial and boundary value problems}
An initial value problem (IVP) is when we require $y(x_0) = y_0$ and $y'(x_0) = \delta$ for given constants $x_0, y_0, \delta$.
In this case the two constants are given at the same value of the independent variable.

A boundary value problem (BVP) is when we require $y(x_0) = y_0$ and $y(x_1) = y_1$ for given constants $x_0 \neq x_1, y_0, y_1$.
In this case the two constraints are given at two different values of the independent variable.

The way to solve an IVP or BVP is to first find the general solution of the ODE and then determine the values of the two constants in this solution so that the extra conditions are satisfied.

\subsection{The method of variation of parameters}
Here we will look at a more general method to find a particular integral,
known as the method of variation of parameters.

\begin{definition}[Wronskian]
    Given two differentiable functions, $y_1(x), y_2(x)$,
    we define the Wronskian to be
    \[
    W(y_1, y_2) = \begin{vmatrix}
        y_1 & y_2 \\ y'_1 & y'_2
    \end{vmatrix}
    \]
\end{definition}

It is obvious that if $y_1$ and $y_2$ are linearly dependent then $W(y_1, y_2)$ is identically zero,
that is,
zero for all $x$.
Therefore,
if $W(y_1, y_2)$ is not identically zero then this implies that $y_1$ and $y_2$ are linearly independent.

We are looking to find a particular integral of the inhomogeneous ODE ($\dagger$),
\[
\alpha_2y'' + \alpha_1y' + \alpha_0y = \phi.
\]
The starting point is to consider the two linearly independent solutions $y_1, y_2$ of the homogeneous version of the ODE ($\dagger\dagger$).
$y_{CF} = Ay_1 + By_2$.
The method of variation of parameters is to look for a particular integral by replacing the constants in $y_{CF}$ by functions.
Namely,
a solution to ($\dagger$) is sought in the form
\[
y = u_1y_1 + u_2y_2
\]
for function $u_1(x), u_2(x)$.
Given this form then
\[
y' = u'_1y_1 + u'_2y_2 + u_1y'_1 + u_2u'_2.
\]
The form we have chosen has two arbitrary functions and the ODE will only give one relation,
hence we need to impose a second condition.
This condition is chosen to be
\[
u'_1y_1 + u'_2y_2 = 0
\]
so that the derivative now simplifies to
\[
y' = u_1y'_1 + u_2y'_2.
\]
Differentiating once more gives
\[
y'' = u'_1y'_1 + u'_2y'_2 + u_1y''_1 + u_2y''_2.
\]
Putting these expressions into ($\dagger$) yields
\[
\alpha_2(u'_1y'_1 + u'_2y'_2 + u_1y''_1 + u_2y''_2) + \alpha_1(u_1y'_1 + u_2y'_2) + \alpha_0(u_1y_1 + u_2y_2) = \phi.
\]
Using the fact that both $y_1$ and $y_2$ solve ($\dagger\dagger$) simplifies the above equation to
\[
\alpha_2(u'_1y'_1 + u'_2y'_2) = \phi.
\]
We now have two equations for the two unknown functions $u'_1, u'_2$ and this gives
\[
u'_1 = -\frac{y_2\frac{\phi}{\alpha_2}}{W(y_1, y_2)}\qquad u'_2 = \frac{y_1\frac{\phi}{\alpha_2}}{W(y_1, y_2)}
\]
which are solved by direct integration
\[
u'_1 = -\int\frac{y_2\frac{\phi}{\alpha_2}}{W(y_1, y_2)}\,dx,\qquad u_2 = \int\frac{y_1\frac{\phi}{\alpha_2}}{W(y_1, y_2)}\,dx.
\]
As a simple first example,
we will apply the method of variation of parameters to find a particular integral that could also be found using the method of undetermined coefficients.
\begin{example}
    Solve $y'' - y = e ^ {2x}$.

    We have that $y_{CF} = Ae ^ x + Be ^ {-x}$ hence $y_1 = e ^ x$ and $y_2 = e ^ {-x}$.
    \[
    W(y_1, y_2) = \begin{vmatrix}
        e ^ x & e ^ {-x} \\
        e ^ x & -e ^ {-x}
    \end{vmatrix}
    = -2.
    \]
    Using the above formulae
    \begin{gather*}
        u_1 = -\int\frac{e ^ {-x}e ^ {2x}}{-2} = \int\frac{1}{2}e ^ x\,dx = \frac{1}{2}e ^ x. \\
        u_2 = \int\frac{e ^ x e ^ {2x}}{-2}\,dx = \int\frac{-1}{2}e ^ {3x}\,dx = -\frac{1}{6}e ^ {3x}. \\
        y_{PI} = u_1y_1 + u_2y_2 = \frac{1}{2}e ^ {x}e ^ {x} - \frac{1}{6}e ^ {3x}e ^ {-x} = \frac{1}{3}e ^ {2x}. \\
        y = y_{CF} + y_{PI} = Ae ^ x + Be ^ {-x} + \frac{1}{3}e ^ {2x}.
    \end{gather*}
\end{example}
Note that if you don't want to remember the formulae you can simply remember that the key step in the method is to remove the $u'_1, u'_2$ terms from $y'$.

\subsection{Systems of first order linear ODEs}
A system of $n$ coupled first order linear ODEs for $n$ dependent variables can be written as a single $n$th order linear ODE for a dependent variable by eliminating the other dependent variables.
An associated IVP is obtained if all the values of the dependent variables are specified at a single value of the independent variable.
\begin{example}
    Find the solution $y(x), z(x)$ of the pair of first order linear ODEs
    \[
    y' = -y + z + x ^ 2,\quad z' = -8y + 5z + 8x ^ 2 - 7x + 1,
    \]
    satisfying the initial condition $y(0) = 2, z(0) = 0$.

    We solve by first finding a second order ODE for $y(x)$.

    From the first equation $z = y' + y - x ^ 2$ hence $z' = y'' + y' - 2x$.
    Substituting these expressions into the second equation gives
    \[
    y'' + y' - 2x = -8y + 5(y' + y - x ^ 2) + 8x ^ 2 - 7x + 1
    \]
    i.e.
    the second order ODE $y'' - 4y' + 3y = 3x ^ 2 - 5x + 1$.
    We can now solve this ODE using the earlier methods.
    The characteristic equation is
    \[
    \lambda ^ 2 - 4\lambda + 3 = 0 = (\lambda - 3)(\lambda - 1),
    \]
    with roots $\lambda = 1, 3$.
    Hence $y_{CF} = Ae ^ x + Be ^ {3x}$.
    Trying $y = a_0 + a_1x + a_2x ^ 2$ for $y_{PI}$ gives
    \[
    y'' - 4y' + 3y = 2a_2 - 4(a_1 + 2a_2x) + 3(a_0 + a_1x + a_2x ^ 2) = 3a_2x ^ 2 + (3a_1 - 8a_2)x + 2a_2 - 4a_1 + 3a_0 = 3x ^ 2 - 5x + 1.
    \]
    Comparing coefficients gives the solution $a_2 = 1, a_1 = 1, a_0 = 1$.
    Giving us the general solution for $y$
    \[
    y = Ae ^ x + Be ^ {3x} + x ^ 2 + x + 1.
    \]
    Obtaining $z$ is done through using the earlier relation $z = y' + y - x ^ 2 = 2Ae ^ x + 4Be ^ {3x} + 3x + 2$.
    
    Now we have a general solution for both $y$ and $z$.

    We can now solve the IVPs
    \[
    y(0) = A + B + 1 = 2
    \]
    \[
    z(0) = 2A + 4B + 2 = 0
    \]
    which have solutions $A = 3, B = -2$.

    Hence the solution of the IVP is
    \[
    y = 3e ^ x - 2e ^ {3x} + x ^ 2 + x + 1,
    \]
    \[
    z = 6e ^ x - 8e ^ {3x} + 3x + 2.
    \]
\end{example}






















\end{document}