\documentclass[10pt, a4paper]{article}
\usepackage{preamble}

\newcommand{\limas}[3][n]{#2 \rightarrow #3 \text{ as } #1 \rightarrow \infty}

\title{Calculus I \\
    \large Important Results}
\author{Luke Phillips}
\date{January 2025}

\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage

\section{Functions}

\begin{definition}[Even function]
    A function $f$ is even if $\forall \pm x \in \Dom{f}$
    \[
    f(x) = f(-x).
    \]
\end{definition}

\begin{definition}[Odd function]
    A function $f$ is odd if $\forall \pm x \in \Dom{f}$
    \[
    f(x) = -f(-x).
    \]
\end{definition}

\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
    Remember a function can be broken into its odd and even parts
    \[
    f(x) = \underbrace{\frac{1}{2}(f(x) - f(-x))}_{f_\text{odd}(x)} + \underbrace{\frac{1}{2}(f(x) + f(-x))}_{f_\text{even}(x)}
    \]
\end{minipage}
}
\end{center}
\hfill

\textbf{Horizontal line test}

If no horizontal line intersects the graph of $f$ more than once then $f$ is injective,
otherwise it is not.

\newpage

\section{Limits and continuity}

\begin{definition}[Continuity at a point]
    A function $f(x)$ is continuous at the point $x = a$ if the following properties all hold:
    \begin{enumerate}[label = (\roman*)]
        \item $f(a)$ exists.
        
        \item $\lim_{x \rightarrow a}f(x)$ exists.
        
        \item $\lim_{x \rightarrow a}f(x) = f(a)$.
    \end{enumerate}
\end{definition}

\begin{definition}[Continuity]
    A function $f(x)$ is continuous if it is continuous at every point in its domain.
\end{definition}

\begin{proposition}[Two trigonometric limits]
    \[
    \lim_{x \rightarrow 0}\frac{\sin{x}}{x} = 1\qquad\text{and}\qquad\lim_{x \rightarrow 0}\frac{1 - \cos{x}}{x} = 0.
    \]
\end{proposition}

For a function $f(x)$ with $L = \lim_{x \rightarrow a}f(x)$,
we have the following discontinuities:

\begin{definition}[Removable discontinuity]
    $L$ exists but $f(a) \neq L$.
    The discontinuity can be removed to make the continuous function
    \[
    g(x) = \begin{cases}
        f(x) & \text{if } x \neq a \\
        L & \text{if } x = a.
    \end{cases}
    \]
\end{definition}

\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
    I.e. if $f(x)$ is defined continuous on $\R \setminus\{a\}$ but $L$ exists and we want $L = f(a)$ we would say
    \[
    g(x) = \begin{cases}
        f(x) & \text{if } x \neq a \\
        L & \text{if } x = a.
    \end{cases}
    \]
\end{minipage}
}
\end{center}
\hfill

\begin{definition}[Jump discontinuity]
    Both $L ^ {+}$ and $L ^ {-}$ exist but $L ^ {+} \neq L ^ {-}$.
\end{definition}

\begin{definition}[Infinite discontinuity]
    In this case at least one of $L ^ {+}$ or $L ^ {-}$ does not exist.
\end{definition}

\begin{theorem}[Intermediate Value Theorem]
    If $f(x)$ is continuous on $[a, b]$ and $u$ is any number between $f(a)$ and $f(b)$ then $\exists c \in (a, b)$ such that $f(c) = u$.
\end{theorem}

\newpage

\section{Differentiation}

\begin{definition}[The derivative]
    Given a function $f(x)$.
    $f(x)$ is differentiable at $x = a$ if
    \[
    f'(a) = \lim_{h \rightarrow 0}\frac{f(a + h) - f(a)}{h}
    \]
    exists.
\end{definition}

\begin{proposition}[The Leibniz rule]
    \[
    D ^ n(fg) = \sum_{k = 0}^{n}\binom{n}{k}(D ^ kf)(D ^ {n - k}g).
    \]
\end{proposition}

\begin{theorem}[Chain rule theorem]
    If $g(x)$ is differentiable at $x$ and $f(x)$ is differentiable at $g(x)$ then the composition $(f \circ g)(x)$ is differentiable at $x$ with
    \[
    (f \circ g)'(x) = f'(g(x))g'(x).
    \]
    Using the Leibniz notation we have
    \[
    \frac{d}{dx}f(g(x)) = \frac{df}{dg}\frac{dg}{dx}.
    \]
\end{theorem}

\begin{theorem}[L'H\^opital's rule]
    Let $f(x)$ and $g(x)$ be differentiable on $I = (a - h, a) \cup (a, a + h)$ for some $h > 0$,
    with $\lim_{x \rightarrow a}f(x) = \lim_{x \rightarrow a}g(x) = 0$.

    If $\lim_{x \rightarrow a}\frac{f'(x)}{g(x)}$ exists and $g'(x) \neq 0\ \forall x \in I$ then
    \[
    \lim_{x \rightarrow a}\frac{f(x)}{g(x)} = \lim_{x \rightarrow a}\frac{f'(x)}{g'(x)}.
    \]
\end{theorem}

For a function $f(x)$ defined in some interval $I$.
\begin{definition}[Global maximum]
    If there exists a constant $k_1$such that $f(x) \leq k_1\ \forall x \in I$ we say that $f(x)$ is bounded above in $I$ and we call $k_1$ an upper bound of $f(x)$ in $I$.

    If there exists a point $x_1$ in $I$ such that $f(x_1) = k_1$ we say that the upper bound of $k_1$ is attained and we call $k_1$ the global maximum value of $f(x)$ in $I$.
\end{definition}

\begin{theorem}[Rolle's theorem]
    If $f$ is differentiable on the open interval $(a, b)$ and continuous on the closed interval $[a, b]$,
    with $f(a) = f(b)$,
    then there is at least one $c \in (a, b)$ for which $f'(c) = 0$.
\end{theorem}
If $f$ is continuous on $[a, b]$ and differentiable on $(a, b)$ so $f(a) = f(b)$ then $f'(c) = 0$ for some $c \in (a, b)$.

\begin{theorem}[Mean value theorem]
    If $f$ differentiable on $(a, b)$ continuous on $[a, b]$,
    there is at least one $c \in (a, b)$ such that
    \[
    f'(c) = \frac{f(b) - f(a)}{b - a}.
    \]
\end{theorem}

\textbf{Inverse Function Rule}

If $f(x)$ continuous on $[a, b]$ differentiable on $(a, b)$ with $f'(x) > 0$ for $x \in (a, b)$ then its inverse function $g(y)$ is differentiable for all $f(a) < y < f(b)$ with
\[
g'(y) = \frac{1}{f'(g(y))}.
\]

\newpage

\section{Integration}

\begin{theorem}[Fundamental theorem of calculus]
    If $f(x)$ continuous on $[a, b]$
    \[
    F(x) = \int_{a}^{x}f(t)\,dt
    \]
    defined for $x \in [a, b]$ is continuous on $[a, b]$ differentiable on $(a, b)$ with
    \[
    F'(x) = \frac{d}{dx}\int_{a}^{x}f(t)\,dt = f(x).
    \]
\end{theorem}
\textit{Note that the chain rule can be used with this for when $x = f(x)$ in the bounds.}


\textbf{Odd integration}
\[
\int_{-a}^{a}f_{\text{odd}}(x)\,dx = 0.
\]

\textbf{Even integration}
\[
\int_{-a}^{a}f_{\text{even}}(x)\,dx = 2\int_{0}^{a}f_{\text{even}}(x).
\]

\newpage

\section{Double integrals}

\textit{Note that a region is $x$-simple if a horizontal line can be drawn such that it intersects the surface but \textbf{does not} leave the surface and intersect it again at a later point.}

\textit{This is the same for a $y$-simple region but with a vertical line.}

\begin{definition}[The Jacobian]
    The Jacobian of the transformation from the variables $x, y$ to $u, v$ is
    \[
    J = \frac{\partial(x, y)}{\partial(u, v)} = \begin{vmatrix}
        \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
        \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
    \end{vmatrix}.
    \]
\end{definition}

To obtain the area element $dxdy$ in terms of the new variables we first compute the Jacobian $J$ and then use the result that
\[
dxdy = |J|dudv.
\]

The Jacobian does this:
\[
\iint\limits_{D}\dotsc\,dxdy \to \iint\limits_{D}\dotsc\cdot|J|\,dudv.
\]

\begin{definition}[Gaussian integral]
    \[
    \int_{-\infty}^{\infty}e ^ {-ax ^ 2}\,dx = \sqrt{\frac{\pi}{a}}
    \]
    where $a$ is a positive constant.
\end{definition}

\textbf{DO NOT FORGET!!!}
\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
    \[
    \int_{-\infty}^{\infty}e ^ {-ax ^ 2}\,dx = \sqrt{\frac{\pi}{a}}
    \]
    where $a$ is a positive constant.
\end{minipage}
}
\end{center}
\hfill

\newpage

\section{First order differential equations}
\textbf{The integrating factor}
\[
y' + p(x)y = q(x) \implies y = \frac{1}{I(x)}\int I(x)q(x)\,dx
\]
with $I(x) = e ^ {\int p(x)\,dx}$.

\[
M(x, y)\,dx + N(x, y)\,dy = 0.
\]
Rearranging gives
\[
\frac{dy}{dx} = -\frac{M(x, y)}{N(x, y)}.
\]
For any function $g(x, y)$ the total differential $dg$ is defined to be
\[
dg = \frac{\partial g}{\partial x}\,dx + \frac{\partial g}{\partial y}\,dy.
\]
i.e.
\[
M = \frac{\partial g}{\partial x}\quad\text{and}\quad N = \frac{\partial g}{\partial y}.
\]

\textbf{Test for exactness}

The ODE $M(x, y)\,dx + N(x, y)\,dy = 0$ is exact if and only if
\[
\frac{\partial M}{\partial y} = \frac{\partial N}{\partial x}.
\]

A \textbf{Bernoulli equation} is a non-linear ODE of the form
\[
y' + p(x)y = q(x)y ^ n.
\]
To solve a Bernoulli equation use the substitution $v = y ^ {1 - n}$,
this converts the equation to a linear ODE for $v(x)$.

\newpage

\section{Second order differential equations}
\textit{Remember that if you cannot find a $y_{PI}$ from $= h(x)$,
you can then try just start multiplying by $x$ until a nice one pops out.}

\begin{definition}[The Wronskian]
    Given two differentiable functions,
    $y_1(x), y_2(x)$,
    we define the Wronskian to be
    \[
    W(y_1, y_2) = \begin{vmatrix}
        y_1 & y_2 \\ y'_1 & y'_2
    \end{vmatrix}
    = y_1y'_2 - y_2y'_1.
    \]
\end{definition}

Solving the ODE
\[
\alpha_2y'' + \alpha_1y' + \alpha_0y = \phi
\]
\[
y = y_{CF} + y_{PI}
\]
with $y_{CF} = Ay_1 + By_2$.

Finding the particular solution found using the following
\[
y_{PI} = u_1y_1 + u_2y_2
\]
where
\[
u_1 = -\int\frac{y_2 \phi / \alpha_2}{W(y_1, y_2)}\,dx\quad\text{and}\quad
u_2 = \int\frac{y_1 \phi / \alpha_2}{W(y_1, y_2)}\,dx.
\]

\textbf{VERY IMPORTANT EQUATIONS}
\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
    For $\alpha_2y'' + \alpha_1y' + \alpha_0y = \phi$
    \[
    W(y_1, y_2) = \begin{vmatrix}
        y_1 & y_2 \\ y'_1 & y'_2
    \end{vmatrix}
    = y_1y'_2 - y_2y'_1
    \]
    we get
    \[
    y_{PI} = u_1y_1 + u_2y_2
    \]
    where \textbf{THESE ARE CRUCIAL}
    \[
    u_1 = -\int\frac{y_2 \phi / \alpha_2}{W(y_1, y_2)}\,dx\quad\text{and}\quad
    u_2 = \int\frac{y_1 \phi / \alpha_2}{W(y_1, y_2)}\,dx.
    \]
\end{minipage}
}
\end{center}
\hfill

\newpage

\section{Taylor series}

\begin{definition}[Taylor series about $a$]
    \[
    f(x) = \infsumo\frac{f ^ {(k)}(a)}{k!}(x - a) ^ k = f(a) + f'(a)(x - a) + \frac{f''(a)}{2!}(x - a) ^ 2 + \dotsc + \underbrace{\frac{f ^ {(n)}(a)}{n!}(x - a) ^ n}_{= R_n(x)}
    \]
\end{definition}

\begin{definition}[Lagrange form for the remainder]
    \[
    R_n(x) = \frac{f ^ {(n + 1)}(c)}{(n + 1)!}(x - a) ^ {n + 1},\quad\text{for some $c \in (a, x)$}.
    \]
\end{definition}

\begin{definition}
    Let $n$ be a positive integer.
    We say that $f(x) = o(x ^ n)$ (as $x \rightarrow 0$)
    if
    \[
    \lim_{x \rightarrow 0}\frac{f(x)}{x ^ n} = 0.
    \]
\end{definition}

\newpage

\section{Fourier series}

\begin{definition}[Fourier definition of a function]
    \[
    f(x) = \frac{a_0}{2} + \infsum\left(a_n\cos\frac{n\pi x}{L} + b_n\sin\frac{n\pi x}{L}\right).
    \]
    with
    \[
    a_0 = \frac{1}{L}\int_{-L}^{L}f(x)\,dx.
    \]
    \[
    a_n = \frac{1}{L}\int_{-L}^{L}f(x)\cos\frac{n\pi x}{L}\,dx.
    \]
    \[
    b_n = \frac{1}{L}\int_{-L}^{L}f(x)\sin\frac{n\pi x}{L}\,dx.
    \]
\end{definition}

\begin{theorem}[Dirichlet's theorem]
    Let $f(x)$ be a periodic function,
    with period $2L$,
    such that on the interval $(-L, L)$ it has a finite number of extreme values,
    a finite number of jump discontinuities and $|f(x)|$ is integrable on $(-L, L)$.
    Then its Fourier series converges for all values $x$.
    Furthermore,
    it converges to $f(x)$ at all points where $f(x)$ is continuous and if $x = a$ is a jump discontinuity then it converges to $\frac{1}{2}\lim_{x \rightarrow a ^ {-}}f(x) + \frac{1}{2}\lim_{x \rightarrow a ^ {+}}f(x)$.
\end{theorem}

\hfill

\begin{theorem}[Parsevalâ€™s theorem]
    If $f(x)$ is a function of period $2L$ with Fourier coefficients $a_n, b_n$ then
    \[
    \frac{1}{2L}\int_{-L}^{L}((f(x))) ^ 2\,dx = \frac{1}{4}a_0 ^ 2 + \frac{1}{2}\infsum[n = 1](a_n ^ 2 + b_n ^ 2).
    \]
\end{theorem}

\begin{proposition}[Complex extension of Fourier series]
    A Fourier series can be written in complex form with
    \[
    c_n = \frac{1}{2L}\int_{-L}^{L}f(x)e ^ {\frac{-in\pi x}{L}}\,dx,
    \]
    with the Fourier series
    \[
    f(x) = \infsum[n = -\infty]c_ne ^ {\frac{in\pi x}{L}}.
    \]
\end{proposition}

\newpage

\section{Calculus of functions of two and more variables}

\textbf{Contour plots}

Multi-variable functions can be shown through contour plots,
get your multivariate function $f(x, y)$ then set it equal to a constant $c$,
i.e. $f(x, y) = c$.
Plot the function for various values of $c$.

\begin{definition}[Continuity for multiple variables]
    Let $f(x_1, \dotsc, x_n)$ be a function of $n$ variables $f(x_1, \dotsc, x_n) = f(\mbf{x})$,
    where $f : D \to \R$ and $D \subseteq \R ^ n$,
    $f(x_1, \dotsc, x_n)$ is continuous at the point $\mbf{x}_0$ if
    \[
    \forall \varepsilon > 0, \exists \delta > 0\text{ such that if } \mbf{x} \in D\text{ and } |\mbf{x} - \mbf{x}_0| < \delta
    \]
    this implies that $|\delta f| = |f(\mbf{x}) - f(\mbf{x}_0)| < \varepsilon$.
    Defining
    \[
    |\mbf{x} - \mbf{x}_0| = \sqrt{(\mbf{x} - \mbf{x}_0, \mbf{x} - \mbf{x}_0)}
    \]
    with the standard inner product.
\end{definition}

\subsection{Chain rule and Taylor's theorem}

\textbf{Chain rule for functions of two variables}
Given a function $f(x, y)$ of two variables we have the total derivative of this function
\[
\frac{df}{dx} = \pd[f]{x} + \frac{dy}{dx}\pd[f]{y}.
\]

For parametrically defined functions
\[
\frac{df}{dt} = \frac{dx}{dt}\pd[f]{x} + \frac{dy}{dx}\pd[f]{y}.
\]

The gradient of $f$ is the vector $\nabla f = \left(\pd[f]{x}, \pd[f]{y}\right)$.

The directional derivative of $f$ in the direction of the unit vector $\hat{\mbf{n}}$ is $\hat{\mbf{n}}\cdot\nabla f$.

An $n$ dimensional gradient of $f(x_1, x_2, \dotsc, x_n)$ to be
\[
\nabla f = \left(\pd[f]{x_1}, \pd[f]{x_2}, \dotsc, \pd[f]{x_n}\right).
\]

The \textbf{direction} of $\hat{\mbf{n}}$ is equal to
\[
\hat{\mbf{n}}\cdot \nabla f = |\nabla f|\cos{\theta}.
\]

\textbf{Changing variables from $(x, y)$ to $(u, v)$.}

Suppose there is some bijection such that we can write $u = u(x, y)$ and $v = v(x, y)$ and $x = x(u, v)$ and $y = y(u, v)$ suppose all these functions are differentiable.
Then we can get:
\begin{align*}
    \pd[f]{x} &= \pd[u]{x}\pd[f]{u} + \pd[v]{x}\pd[f]{v} \\
    \pd[f]{y} &= \pd[u]{y}\pd[f]{u} + \pd[v]{y}\pd[f]{v}.
\end{align*}

\textbf{Taylor's theorem for functions of multiple variables}
\begin{align*}
    f(x, y) &= f(x_0, y_0) + (x - x_0)\pd[f]{x}(x_0, y_0) + (y - y_0)\pd[f]{y}(x_0, y_0) \\
    &+ \frac{1}{2!}((x - x_0) ^ 2f_{xx}(x_0, y_0) + 2(x - x_0)(y - y_0)f_{xy}(x_0, y_0) + (y - y_0) ^ 2f_{yy}(x_0, y_0)) + \dotsc
\end{align*}

\subsection{Stationary points}

A multivariable function $f(\mbf{x}) = f(x_1, \dotsc, x_n)$ has a stationary point at $\mbf{x} = \mbf{a}$ if the function is differentiable at $\mbf{x} = \mbf{a}$ and
\[
\nabla f = \left(\pd[f]{x_1}, \pd[f]{x_2}, \dotsc, \pd[f]{x_n}\right) = 0.
\]
I.e. all the partial derivatives are $0$.

\textbf{Determining the type of stationary point}

Have the Hessian matrix:
\[
H_{ij} = \frac{\partial ^ 2f}{\partial x_i \partial x_j}
\]
for example,
for two variables
\[
H = \begin{pmatrix}
    f_{xx} & f_{xy} \\
    f_{yx} & f_{yy}
\end{pmatrix}.
\]

We classify the points in the following way:
\begin{enumerate}[label = (\arabic*)]
    \item $\det(H ^ {(m)}) > 0$ for $m = 1, \dotsc, n$,
    the stationary point is a \textbf{minimum}.

    \item If $\det(H ^ {(m)})(-1) ^ m > 0$ for $m = 1, \dotsc, n$ the stationary point is a \textbf{maximum}.

    \item If $\det(H ^ {(n)}) = \det(H) = 0$ the stationary point is \textbf{degenerate}.

    \item If none of the above conditions hold,
    the stationary point is a saddle point.
\end{enumerate}

\textbf{Lagrange multipliers}

To find the maximum/minimum of a function $f(x, y)$ of two variables subject to a constraint $g(x, y) = 0$.

\[
\nabla f = \lambda\nabla g
\]
for some constant $\lambda$,
where we have the Lagrange multiplier.

Consider the function $\phi = f - \lambda g$ we see that
\[
\nabla \phi = \nabla(f - \lambda g) = \nabla f - \lambda\nabla g = 0
\]
then we use
\begin{align*}
    \phi_x &= 0 \\
    \phi_y &= 0 \\
    g(x, y) &= 0
\end{align*}
we can use this to determine three unknowns $x, y$ and $\lambda$.

To find the maximum/minimum of a function $f(x_1, \dotsc, x_n)$ of $n$ variables subject to $m$ constraints $g_i(x_1, \dotsc, x_n) = 0$ for $i = 1, \dotsc, m$.

We can define the function
\[
\phi = f - \lambda_1g_1 - \lambda_2g_2 - \dotsc - \lambda_mg_m,
\]
then at a constrained maximum/minimum of $f$
\begin{align*}
    \nabla \phi &= \left(\pd[\phi]{x_1}, \pd[\phi]{x_2}, \dotsc, \pd[\phi]{x_n}\right) = 0 \\
    g_i(x_1, \dotsc, x_n) &= 0\text{ for }i = 1, \dotsc, m.
\end{align*}

Then we can get $n$ equations from $\nabla \phi = 0$ together with the $m$ constraint equations $g_i = 0$ giving us $n + m$ equations.

\newpage

\section{Linear ordinary differential equations}

A point $x_0$ is \textbf{regular} if the functions $p(x)$ and $q(x)$ themselves admit a Taylor series representation at $x = x_0$.


A point $x_0$ which is not a regular point of the differential equation is referred to as a \textbf{singular} point.


$x_0$ is a \textbf{regular singular} point of the differential equation if $(x - x_0)p(x)$,
$(x - x_0) ^ 2q(x)$ can be represented by a Taylor expansion about $x = x_0$.


$x_0$ is an \textbf{irregular singular} point of the differential equation if it is neither regular or a regular singular point.

\subsection{Frobenius' Method}
\[
y(x) = \infsumo a_n(x - x_0) ^ {n + r}
\]
where $r$ is a constant.

\begin{example}
    Find a solution to
    \[
    2x(x + 1)y'' + (x + 1)y' - 3y = 0
    \]
    expanded around $x = 0$.

    \begin{solution}
        This can be written as follows
        \[
        y'' + \frac{1}{2x}y' - \frac{3}{2x(x + 1)}y = 0,
        \]
        so that $p(x) = \frac{1}{2x}$ and $q(x) = -\frac{3}{2x(x + 1)}$.

        $x = 0$ is not a regular point.

        \[
        xp(x) = \frac{1}{2},\quad x ^ 2q(x) = -\frac{3x}{2(x + 1)}
        \]
        both have Taylor expansions about $x = 0$,
        so the point $x = 0$ is a regular single point.
        \[
        \infsum[n = 0][(n + r)(2n + 2r - 1) - 3]a_nx ^ {n + r} + \infsum[n = 0](n + r)(2n + 2r - 1)a_nx ^ {n + r - 1} = 0
        \]
        to have the same power we get
        \[
        \infsum[n = 0][(n + r)(2n + 2r - 1) - 3]a_nx ^ {n + r} + \infsum[n = -1](n + r)(2n + 2r - 1)a_{n + 1}x ^ {n + r} = 0.
        \]
        Since the solution is true for all $x$ this can then be solved as usual.
    \end{solution}
\end{example}

\newpage

\section{Differential operators}

\subsection{Legendre expansions}
We can find the Legendre expansion of an equation using the orthogonality of the differential operators.
Given the two orthogonal eigenfunctions $P_i(x), P_j(x)$ we have
\[
(P_i(x), P_j(x)) = \int_{-1}^{1}P_i(x)P_j(x)\,dx = 0\quad\text{for $i \neq j$}.
\]
Using $f(x) = \infsum[i = 0]b_iP_i(x)$ we can find the coefficients of the Legendre expansion.
Taking the inner product we get
\[
(P_j(x), f(x)) = \left(P_j(x), \infsum[i = 0]b_iP_i(x)\right) = \infsum[i = 0]b_j(P_j(x), P_i(x)) = b_j(P_j(x), P_j(x))
\]
where $(P_i(x), P_j(x)) = 0$ for $i \neq j$.
We find
\[
b_j = \frac{(P_j(x), f(x))}{(P_j(x), P_j(x))}.
\]
Giving us
\[
b_j = \frac{2j + 1}{2}\int_{-1}^{1}f(x)P_j(x)\,dx.
\]
\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
    This gives the following procedure:
    \begin{enumerate}[label = (\roman*)]
        \item Show that the eigenfunctions are mutually orthogonal.

        \item Set the function $f(x) = \infsum[i = 0]b_iP_i(x)$,
        then take the inner product $(P_j(x), f(x)) = b_j(P_j(x), P_j(x))$.

        \item Then rearrange to find $b_j$.

        \item Get a simpler expression for $b_j$.

        \item The final formula gives us the coefficients for the expansion.
    \end{enumerate}
\end{minipage}
}
\end{center}
\hfill

\newpage

\section{Solving PDEs}

To separate variables we let $f(x, y, \dotsc, t) = \sum_na_nX_n(x)Y_n(y)\dotsc T_n(t)$.

\begin{example}
    A string lies along the $x$-axis between $x = 0$ $x = \pi$.
    Its displacement from the $x$-axis,
    $u(x, t)$ obeys the wave equation $u_{tt} = c ^ 2u_{xx}$.
    Boundary conditions $u(0, t) = u(\pi, t) = 0$.
    Initially released from rest so that $u_t(x, 0) = 0$ and position
    \[
    u(x, 0) = R(x) = \begin{cases}
        x & 0 \leq x \leq \pi / 2 \\
        \pi - x & \pi / 2 \leq x \leq \pi.
    \end{cases}
    \]

    \begin{solution}
        Using $u(x, t) = X(x)T(t)$ and substituting we find:
        \[
        u_{tt} = X(x)\ddot{T}(t) = c ^ 2u_{xx} = c ^ 2X''(x)T(t).
        \]
        This is equivalent to
        \[
        \frac{X''(x)}{X(x)} = \frac{\ddot{T}(t)}{c ^ 2T(t)}
        \]
        so they both must be a constant $\lambda$.
        Giving us
        \[
        X''(x) = \lambda X(x),\quad \ddot{T}(t) = \lambda c ^ 2T(t).
        \]

        Now we need to find the form of $X(x)$ which match the boundary conditions.

        $\lambda = n ^ 2 > 0$ we get
        \[
        X'' = n ^ 2X \implies X = Ae ^ {nx} + Be ^ {-nx}
        \]
        which gives $A = B = 0$,
        hence no non-zero solutions.

        $\lambda = 0$ we get
        \[
        X'' = 0 \implies X = Ax + B \implies A = B = 0
        \]
        no non-zero solutions.

        $\lambda = -n ^ 2 < 0$ getting us
        \[
        X'' = -n ^ 2X \implies X = A\sin(nx) + B\cos(nx) \implies B = 0\text{ and } X(\pi) = A\sin(n\pi) = 0
        \]
        which has non-zero solutions.

        Given this we can now solve $T(t)$,
        \[
        \ddot{T}(t) = -n ^ 2c ^ 2T(t)
        \]
        (\textbf{since we found that $\lambda = -n ^ 2$ for $X(x)$ to have non-zero solutions})
        this gives general solution
        \[
        T(t) = A_n\cos(nct) + B_n\sin(nct).
        \]
        \[
        u_n(x, t) = \sin(nx)(A_n\cos(nct) + B_n\sin(nct)).
        \]
        \[
        u(x, t) = \infsum[n = 1]\sin(nx)(A_n\cos(nct) + B_n\sin(nct)).
        \]
        \[
        0 = u_t(x, 0) \implies B_n = 0.
        \]
        Finding the final boundary condition:
        \[
        u(x, 0) = R(x),
        \]
        so we have
        \[
        u(x, 0) = R(x) = \infsum[n = 1]A_n\sin(nx).
        \]
        We can use the Fourier series to see that
        \[
        R(x) = \infsum[n = 1]A_n\sin(nx)
        \]
        then
        \[
        \infsum[n = 1]A_n\sin(nx) = R(x) = \frac{a_0}{2} + \infsum[n = 1]a_n\cos(n\pi x / L) + b_n\sin(n\pi x / L) \implies L = \pi, a_n = 0, b_n = A_n
        \]
        so we use the formula for $b_n$ which is
        \begin{align*}
            \frac{1}{\pi}\int_{-L}^{L}R(x)\sin(n\pi x / L)\,dx &= \frac{1}{\pi}\int_{-\pi}^{\pi}R(x)\sin(nx)\,dx \\
            &= \frac{2}{\pi}\int_{0}^{\pi}R(x)\sin(nx)\,dx\qquad(\text{since $\text{odd} \cdot \text{odd} = \text{even}$}) \\
            &= \frac{2}{\pi}\int_{0}^{\pi / 2}R(x)\sin(nx)\,dx + \frac{2}{\pi}\int_{\pi / 2}^{\pi}R(x)\sin(nx)\,dx \\
            &= \frac{2}{\pi}\int_{0}^{\pi / 2}x\sin(nx)\,dx + \frac{2}{\pi}\int_{\pi / 2}^{\pi}(\pi - x)\sin(nx)\,dx \\
            &= \frac{4}{n ^ 2\pi}\sin(n\pi / 2).
        \end{align*}
        Giving us the final answer
        \[
        u(x, t) = \infsum[n  1]\frac{4}{n ^ 2\pi}\sin(n\pi / 2)\sin(nx)\cos(nct).
        \]
    \end{solution}
    
\end{example}

\newpage

\section{Fourier transforms}
\begin{align*}
    f(x) &= \frac{1}{2\pi}\int_{-\infty}^{\infty}\tilde{f}(p)e ^ {ipx}\,dp \\
    \tilde{f}(p) &= \int_{-\infty}^{\infty}f(x)e ^ {-ipx}\,dx.
\end{align*}

\textbf{Important results}

Let $f, g$ be two functions with Fourier transforms $\tilde{f}(p), \tilde{g}(p)$
\begin{enumerate}[label = (\roman*)]
    \item
    If $u(x) = \alpha f(x) + \beta g(x)$
    \[
    \tilde{u}(p) = \alpha\tilde{f}(p) + \beta\tilde{g}(p)
    \]
    for all $\alpha, \beta \in \R$.

    \item
    If $g(x) = f(x + a)$ then
    \[
    \tilde{g}(p) = e ^ {ipa}\tilde{f}(p).
    \]

    \item
    If $F(x) = f(bx)$ then
    \[
    \tilde{F}(p) = \frac{1}{|b|}\tilde{f}\left(\frac{p}{b}\right).
    \]

    \item
    If $f$ is odd
    (even)
    then $\tilde{f}$ will be even
    (odd).

    \item
    If $F(x) = f'(x)$ then
    \[
    \tilde{F}(p) = ip\tilde{f}(p).
    \]

    \item
    Let $f \star g$ be the convolution of two functions
    \[
    f \star g(x) = \int_{-\infty}^{\infty}f(t)g(x - t)\,dt.
    \]
    If $F(x) = f \star g(x)$ then
    \[
    \tilde{F}(p) = \tilde{f}(p)\tilde{g}(p).
    \]
\end{enumerate}

\newpage

\section{Methods}

\subsection{Solving ODEs using the Bernoulli equation}
Given a non-linear ODE of the form
\[
y' + p(x)y = q(x)y ^ n
\]
then we can solve the ODE using a substitution $v = y ^ {1 - n}$.

It is best to see this method through an example
\begin{example}
    \[
    y' - \frac{2y}{x} = -x ^ 2y ^ 2
    \]
    then we use the substitution $v = \frac{1}{y}$.
    Once we have the substitution we then use the chain rule/product rule
    \[
    v' = -\frac{y'}{y ^ 2}
    \]
    which eventually leads to
    \[
    v' + \frac{2}{x}v = x ^ 2
    \]
    then we can use an integrating factor to get the final result
    \[
    y = \frac{1}{v} = \frac{5x ^ 2}{x ^ 5 + c}.
    \]
\end{example}


\subsection{Solving ODEs with Fourier transforms}
Using the following
\[
F(x) = f'(x) \implies \tilde{F}(p) = ip\tilde{f}(p)
\]
we can simplify ODEs.

\begin{example}
    \[
    -y'' + y = f(x).
    \]

    We can then get
    \[
    -y'' + y = f(x) \implies -(ip) ^ 2\tilde{y}(p) + \tilde{y}(p) = \tilde{f}(p)
    \]
    solving we get
    \[
    \tilde{y}(p) = \frac{\tilde{f}(p)}{p ^ 2 + 1}
    \]
    then we can use the convolution theorem to see if we let $\tilde{g}(p) = \frac{1}{p ^ 2 + 1}$ then
    \[
    \tilde{y}(p) = \tilde{f}(p)\tilde{g}(p)
    \]
    which means
    \[
    y(x) = f(x) \star g(x)
    \]
    then using some other facts
    (from lecture notes)
    we get $g(x) = e ^ {-|x|} / 2$ giving us
    \[
    y(x) = \frac{1}{2}\int_{-\infty}^{\infty}f(t)e ^ {-|x - t|}\,dt.
    \]
\end{example}

\subsection{Solving PDEs with Fourier transforms}
For functions $u(x, t)$ we can take Fourier transforms of one of the variables to get
\[
\tilde{u}(p, t) = \int_{-\infty}^{\infty}u(x, t)e ^ {-ipx}\,dx.
\]
Then we get $u_{x}(x, t) \implies (ip)\tilde{u}(p, t)$ giving us an ODE in $t$.

\begin{example}
    \[
    u_t - 3u_x = -5u
    \]
    initially $u(x, 0) = R(x)$.

    We can take Fourier transforms of the equation
    \[
    \tilde{u}_t - 3(ip)\tilde{u} = -5\tilde{u} \implies \pd[\tilde{u}]{t} = (3ip - t)\tilde{u}.
    \]
    Which is a separable ODE
    \[
    \tilde{u} = A(p)e ^ {(3ip - 5)t}
    \]
    applying the initial conditions to find $A(p)$ we get
    \[
    A(p) = \tilde{u}(p, 0) = \int_{-\infty}^{\infty}u(x, 0)e ^ {-ipx}\,dx = \int_{-\infty}^{\infty}R(x)e ^ {-ipx}\,dx = \tilde{R}(p)
    \]
    hence
    \[
    \tilde{u}(p, t) = \tilde{R}(p)e ^ {(3ip - 5)t} = e ^ {-5t}(\tilde{R}(p)e ^ {3ipt}).
    \]
    So we can apply the shift theorem to get
    \[
    u(x, t) = e ^ {-5t}R(x + 3t).
    \]
\end{example}




\end{document}